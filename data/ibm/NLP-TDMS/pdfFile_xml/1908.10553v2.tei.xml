<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Wang</forename><surname>Bian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">TuSimple</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">TuSimple</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Nankai University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work has shown that CNN-based depth and ego-motion estimators can be learned using unlabelled monocular videos. However, the performance is limited by unidentified moving objects that violate the underlying static scene assumption in geometric image reconstruction. More significantly, due to lack of proper constraints, networks output scale-inconsistent results over different samples, i.e., the ego-motion network cannot provide full camera trajectories over a long video sequence because of the per-frame scale ambiguity. This paper tackles these challenges by proposing a geometry consistency loss for scale-consistent predictions and an induced self-discovered mask for handling moving objects and occlusions. Since we do not leverage multi-task learning like recent works, our framework is much simpler and more efficient. Comprehensive evaluation results demonstrate that our depth estimator achieves the state-of-the-art performance on the KITTI dataset. Moreover, we show that our ego-motion network is able to predict a globally scale-consistent camera trajectory for long video sequences, and the resulting visual odometry accuracy is competitive with the recent model that is trained using stereo videos. To the best of our knowledge, this is the first work to show that deep networks trained using unlabelled monocular videos can predict globally scale-consistent camera trajectories over a long video sequence.</p><p>inconsistent results over different samples, i.e., the ego-motion network cannot provide a full camera trajectory over a long video sequence because of the per-frame scale ambiguity 1 .</p><p>To the best of our knowledge, no previous work (unsupervised learning from monocular videos) addresses the scale-inconsistency issue mentioned above. To this end, we propose a geometry consistency loss for tackling the challenge. Specifically, for any two consecutive frames sampled from a video, we convert the predicted depth map in one frame to 3D space, then project it to the other frame using the estimated ego-motion, and finally minimize the inconsistency of the projected and the estimated depth maps. This explicitly enforces the depth network to predict geometryconsistent (of course scale-consistent) results over consecutive frames. With iterative sampling and training from videos, depth predictions on each consecutive image pair would be scale-consistent, and the frame-to-frame consistency can eventually propagate to the entire video sequence. As the scale of ego-motions is tightly linked to the scale of depths, the proposed ego-motion network can predict scale-consistent relative camera poses over consecutive snippets. We show that just simply accumulating pose predictions can result in globally scale-consistent camera trajectories over a long video sequence <ref type="figure">(Fig. 3)</ref>.</p><p>Regarding the challenge of moving objects, recent work addresses it by introducing an additional optical flow <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b12">13]</ref> or semantic segmentation network <ref type="bibr" target="#b13">[14]</ref>. Although this improves performance significantly, it also brings about huge computational cost during training. Here we show that we could automatically discover a mask from the proposed geometry consistency term for solving the problem without introducing new networks. Specifically, we can easily locate pixels that belong to dynamic objects/occluded regions or difficult regions (e.g., textureless regions) using the proposed term. By assigning lower weights to those pixels, we can avoid their impact to the fragile image reconstruction loss (see <ref type="figure">Fig. 2</ref> for mask visualization). Compared with these recent approaches [9-11] that leverage multi-task learning, the proposed method is much simpler and more efficient.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Depth and ego-motion estimation is crucial for various applications in robotics and computer vision. Traditional methods are usually hand-crafted stage-wise systems, which rely on correspondence search <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and multi-view geometry <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> for estimation. Recently, deep learning based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> show that the depth can be inferred from a single image by using Convolutional Neural Network (CNN). Especially, unsupervised methods <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> show that CNN-based depth and ego-motion networks can be solely trained on monocular video sequences without using ground-truth depth or stereo image pairs (pose supervision). The principle is that one can warp the image in one frame to another frame using the predicted depth and ego-motion, and then employ the image reconstruction loss as the supervision signal <ref type="bibr" target="#b6">[7]</ref> to train the network. However, the performance limitation arises due to the moving objects that violate the underlying static scene assumption in geometric image reconstruction. More significantly, due to lack of proper constraints the network predicts scale-33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</p><p>We conduct detailed ablation studies that clearly demonstrate the efficacy of the proposed approach. Furthermore, comprehensive evaluation results on the KITTI <ref type="bibr" target="#b14">[15]</ref> dataset show that our depth network outperforms state-of-the-art models that are trained in more complicated multi-task learning frameworks <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b15">16]</ref>. Meanwhile, our ego-motion network is able to predict scale-consistent camera trajectories over long video sequences, and the accuracy of trajectory is competitive with the state-of-the-art model that is trained using stereo videos <ref type="bibr" target="#b16">[17]</ref>.</p><p>To summarize, our main contributions are three-fold:</p><p>• We propose a geometry consistency constraint to enforce the scale-consistency of depth and ego-motion networks, leading to a globally scale-consistent ego-motion estimator. • We propose a self-discovered mask for dynamic scenes and occlusions by the aforementioned geometry consistency constraint. Compared with other approaches, our proposed approach does not require additional optical flow or semantic segmentation networks, which makes the learning framework simpler and more efficient. • The proposed depth estimator achieves state-of-the-art performance on the KITTI dataset, and the proposed ego-motion predictor shows competitive visual odometry results compared with the state-of-the-art model that is trained using stereo videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Traditional methods rely on the disparity between multiple views of a scene to recover the 3D scene geometry, where at least two images are required <ref type="bibr" target="#b2">[3]</ref>. With the rapid development of deep learning, Eigen et al. <ref type="bibr" target="#b4">[5]</ref> show that the depth can be predicted from a single image using Convolution Neural Network (CNN). Specifically, they design a coarse-to-fine network to predict the single-view depth and use the ground truth depths acquired by range sensors as the supervision signal to train the network. However, although these supervised methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> show high-quality flow and depth estimation results, it is expensive to acquire ground truth in real-world scenes.</p><p>Without requiring the ground truth depth, Garg et al. <ref type="bibr" target="#b21">[22]</ref> show that a single-view depth network can be trained using stereo image pairs. Instead of using depth supervision, they leverage the established epipolar geometry <ref type="bibr" target="#b2">[3]</ref>. The color inconsistency between a left image and a synthesized left image warped from the right image is used as the supervision signal. Following this idea, Godard et al. <ref type="bibr" target="#b22">[23]</ref> propose to constrain the left-right consistency for regularization, and Zhan et al. <ref type="bibr" target="#b16">[17]</ref> extend the method to stereo videos. However, though stereo pairs based methods do not require the ground truth depth, accurately rectifying stereo cameras is also non-trivial in real-world scenarios.</p><p>To that end, Zhou et al. <ref type="bibr" target="#b6">[7]</ref> propose a fully unsupervised framework, in which the depth network can be learned solely from monocular videos. The principle is that they introduce an additional ego-motion network to predict the relative camera pose between consecutive frames. With the estimated depth and relative pose, image reconstruction as in <ref type="bibr" target="#b21">[22]</ref> is applied and the photometric loss is used as the supervision signal. However, the performance is limited due to dynamic objects that violate the underlying static scene assumption in geometric image reconstruction. More importantly, Zhou et al. <ref type="bibr" target="#b6">[7]</ref>'s method suffers from the per-frame scale ambiguity, in that a single and consistent scaling of the camera translations is missing and only direction is known. As a result, the ego-motion network cannot predict a full camera trajectory over a long video sequence.</p><p>For handling moving objects, recent work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> proposes to introduce an additional optical flow network. Even more recently <ref type="bibr" target="#b10">[11]</ref> introduces an extra motion segmentation network. Although they show significant performance improvement, there is a huge additional computational cost added into the basic framework, yet they still suffer from the scale-inconsistency issue. Besides, Liu et al. <ref type="bibr" target="#b23">[24]</ref> use depth projection loss for supervision density, similar to the proposed consistency loss, but their method relies on the pre-computed 3D reconstruction for supervision.</p><p>To the best of our knowledge, this paper is the first one to show that the ego-motion network trained in monocular videos can predict a globally scale-consistent camera trajectory over a long video sequence. This shows significant potentials to leverage deep learning methods in Visual SLAM <ref type="bibr" target="#b11">[12]</ref> for robotics and autonomous driving.</p><p>3 Unsupervised Learning of Scale-consistent Depth and Ego-motion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Method Overview</head><p>Our goal is to train depth and ego-motion networks using monocular videos, and constrain them to predict scale-consistent results. Given two consecutive frames (I a , I b ) sampled from an unlabeled video, we first estimate their depth maps (D a , D b ) using the depth network, and then predict the relative 6D camera pose P ab between them using the pose network.</p><p>With the predicted depth and relative camera pose, we can synthesize the reference image I a by interpolating the source image I b <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b6">7]</ref>. Then, the network can be supervised by the photometric loss between the real image I a and the synthesized one I a . However, due to dynamic scenes that violate the geometric assumption in image reconstruction, the performance of this basic framework is limited. To this end, we propose a geometry consistency loss L GC for scale-consistency and a self-discovered mask M for handling the moving objects and occlusions. <ref type="figure">Fig. 1</ref> shows an illustration of the proposed loss and mask.</p><p>Our overall objective function can be formulated as follows:</p><formula xml:id="formula_0">L = αL M p + βL s + γL GC ,<label>(1)</label></formula><p>where L M p stands for the weighted photometric loss (L p ) by the proposed mask M , and L s stands for the smoothness loss. We train the network in both forward and backward directions to maximize the data usage, and for simplicity we only derive the loss for the forward direction.</p><p>In the following sections, we first introduce the widely used photometric loss and smoothness loss in Sec. 3.2, and then describe the proposed geometric consistency loss in Sec. 3.3 and the self-discovered mask in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Photometric loss and smoothness loss</head><p>Photometric loss. Leveraging the brightness constancy and spatial smoothness priors used in classical dense correspondence algorithms <ref type="bibr" target="#b25">[26]</ref>, previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> have used the photometric  <ref type="figure">Figure 1</ref>: Illustration of the proposed geometry consistency loss and self-discover mask. Given two consecutive frames (I a , I b ), we first estimate their depth maps (D a , D b ) and relative pose (P ab ) using the network, then we get the warped (D a b ) by converting D a to 3D space and projecting to the image plane of I b using P ab , and finally we use the inconsistency between D a b and the D b interpolated from D b as the geometric consistency loss L GC (Eqn. 6) to supervise the network training. Here, we interpolate D b because the projection flow does not lie on the pixel grid of I b . Besides, we discover a mask M (Eqn. 7) from the inconsistency map for handling dynamic scenes and ill-estimated regions ( <ref type="figure" target="#fig_1">Fig. 2</ref>). For clarity, the photometric loss and smoothness loss are not shown in this figure.</p><p>error between the warped frame and the reference frame as an unsupervised loss function for training the network.</p><p>With the predicted depth map D a and the relative camera pose P ab , we synthesize I a by warping I b , where differentiable bilinear interpolation <ref type="bibr" target="#b24">[25]</ref> is used as in <ref type="bibr" target="#b6">[7]</ref>. With the synthesized I a and the reference image I a , we formulate the objective function as</p><formula xml:id="formula_1">L p = 1 |V | p∈V I a (p) − I a (p) 1 ,<label>(2)</label></formula><p>where V stands for valid points that are successfully projected from I a to the image plane of I b , and |V | defines the number of points in V . We choose L 1 loss due to its robustness to outliers. However, it is still not invariant to illumination changes in real-world scenarios. Here we add an additional image dissimilarity loss SSIM <ref type="bibr" target="#b26">[27]</ref> for better handling complex illumination changes, since it normalizes the pixel illumination. We modify the photometric loss term Eqn. 2 as:</p><formula xml:id="formula_2">L p = 1 |V | p∈V (λ i I a (p) − I a (p) 1 + λ s 1 − SSIM aa (p) 2 ),<label>(3)</label></formula><p>where SSIM aa stands for the element-wise similarity between I a and I a by the SSIM function <ref type="bibr" target="#b26">[27]</ref>. Following <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>, we use λ i = 0.15 and λ s = 0.85 in our framework.</p><p>Smoothness loss. As the photometric loss is not informative in low-texture nor homogeneous region of the scene, existing work incorporates a smoothness prior to regularize the estimated depth map. We adopt the edge-aware smoothness loss used in <ref type="bibr" target="#b10">[11]</ref>, which is formulated as:</p><formula xml:id="formula_3">L s = p (e −∇Ia(p) · ∇D a (p)) 2 ,<label>(4)</label></formula><p>where ∇ is the first derivative along spatial directions. It ensures that smoothness is guided by the edge of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Geometry consistency loss</head><p>As mentioned before, we enforce the geometry consistency on the predicted results. Specifically, we require that D a and D b (related by P ab ) conform the same 3D scene structure, and minimize their differences. The optimization not only encourages the geometry consistency between samples in a batch but also transfers the consistency to the entire sequence. e.g., depths of I 1 agree with depths of I 2 in a batch; depths of I 2 agree with depths of I 3 in another training batch. Eventually, depths With this constraint, we compute the depth inconsistency map D diff . For each p ∈ V , it is defined as:</p><formula xml:id="formula_4">D diff (p) = |D a b (p) − D b (p)| D a b (p) + D b (p)<label>(5)</label></formula><p>where D a b is the computed depth map of I b by warping D a using P ab , and D b is the interpolated depth map from the estimated depth map D b (Note that we cannot directly use D b because the warping flow does not lie on the pixel grid ). Here we normalize their difference by their sum. This is more intuitive than the absolute distance as it treats points at different absolute depths equally in optimization. Besides, the function is symmetric and the outputs are naturally ranging from 0 to 1, which contributes to numerical stability in training.</p><p>With the inconsistency map, we simply define the proposed geometry consistency loss as:</p><formula xml:id="formula_5">L GC = 1 |V | p∈V D diff (p),<label>(6)</label></formula><p>which minimizes the geometric distance of predicted depths between each consecutive pair and enforces their scale-consistency. With training, the consistency can propagate to the entire video sequence. Due to the tight link between ego-motion and depth predictions, the ego-motion network can eventually predict globally scale-consistent trajectories <ref type="figure" target="#fig_2">(Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Self-discovered mask</head><p>To handle moving objects and occlusions that may impair the network training, recent work propose to introduce an additional optical flow <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> or semantic segmentation network <ref type="bibr" target="#b13">[14]</ref>. This is effective, however it also introduces extra computational cost and training burden. Here, we show that these regions can be effectively located by the proposed inconsistency map D diff in Eqn. 5.</p><p>There are several scenarios that result in inconsistent scene structure observed from different views, including (1) dynamic objects, (2) occlusions, and (3) inaccurate predictions for difficult regions. Without separating them explicitly, we observe each of these will result in D diff increasing from its ideal value of zero.</p><p>Based on this simple observation, we propose a weight mask M as D diff is in [0, 1]:</p><formula xml:id="formula_6">M = 1 − D diff ,<label>(7)</label></formula><p>which assigns low/high weights for inconsistent/consistent pixels. It can be used to re-weight the photometric loss. Specifically, we modify the photometric loss in Eqn. 3 as</p><formula xml:id="formula_7">L M p = 1 |V | p∈V (M (p) · L p (p)).<label>(8)</label></formula><p>By using the mask, we mitigate the adverse impact from moving objects and occlusions. Further, the gradients computed on inaccurately predicted regions carry less weight during back-propagation. <ref type="figure" target="#fig_1">Fig. 2</ref> shows visual results for the proposed mas, which coincides with our anticipation stated above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>Network architecture. For the depth network, we experiment with DispNet <ref type="bibr" target="#b6">[7]</ref> and DispRes-Net <ref type="bibr" target="#b10">[11]</ref>, which takes a single RGB image as input and outputs a depth map. For the ego-motion network, PoseNet without the mask prediction branch <ref type="bibr" target="#b6">[7]</ref> is used. The network estimates a 6D relative camera pose from a concatenated RGB image pair. Instead of computing the loss on multiple-scale outputs of the depth network (4 scales in <ref type="bibr" target="#b6">[7]</ref> or 6 scales in <ref type="bibr" target="#b10">[11]</ref>), we empirically find that using single-scale supervision (i.e., only compute the loss on the finest output) is better (Tab. 4). Our single-scale supervision not only improves the performance but also contributes a more concise training pipeline. We hypothesize the reason of this phenomenon is that the photometric loss is not accurate in low-resolution images, where the pixel color is over-smoothed.</p><p>Single-view depth estimation. The proposed learning framework is implemented using PyTorch Library <ref type="bibr" target="#b27">[28]</ref>. For depth network, we train and test models on KITTI raw dataset <ref type="bibr" target="#b14">[15]</ref> using Eigen <ref type="bibr" target="#b4">[5]</ref>'s split that is the same with related works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7]</ref>. Following <ref type="bibr" target="#b6">[7]</ref>, we use a snippet of three sequential video frames as a training sample, where we set the second image as reference frame to compute loss with other two images and then inverse their roles to compute loss again for maximizing the data usage. The data is also augmented with random scaling, cropping and horizontal flips during training, and we experiment with two input resolutions (416 × 128 and 832 × 256). We use ADAM <ref type="bibr" target="#b28">[29]</ref> optimizer, and set the batch size to 4 and the learning rate to 10 −4 . During training, we adopt α = 1.0, β = 0.1, and γ = 0.5 in Eqn. 1. We train the network in 200 epochs with 1000 randomly sampled batches in one epoch, and validate the model at per epoch. Also, we pre-train the network on CityScapes <ref type="bibr" target="#b29">[30]</ref> and finetune on KITTI <ref type="bibr" target="#b14">[15]</ref>, each for 200 epochs. Here we follow Eigen et al. <ref type="bibr" target="#b4">[5]</ref>'s evaluation metrics for depth evaluation.</p><p>Visual odometry prediction. For pose network, following Zhan et al. <ref type="bibr" target="#b16">[17]</ref>, we evaluate visual odometry results on KITTI odometry dataset <ref type="bibr" target="#b14">[15]</ref>, where sequence 00-08/09-10 are used for training/testing. We use the standard evaluation metrics by the dataset for trajectory evaluation rather than Zhou et al. <ref type="bibr" target="#b6">[7]</ref>'s 5-frame pose evaluation, since they are more widely used and more meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons with the state-of-the-art</head><p>Depth results on KITTI raw dataset. Tab. 1 shows the results on KITTI raw dataset <ref type="bibr" target="#b14">[15]</ref>, where our method achieves the state-of-the-art performance when compared with models trained on monocular video sequences. Note that recent work <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b30">31]</ref> all jointly learn multiple tasks, while our approach does not. This effectively reduces the training and inference overhead. Moreover, our method competes quite favorably with other methods using stronger supervision signals such as calibrated stereo image pairs (i.e., pose supervision) or even ground-truth depth annotation.</p><p>Visual odometry results on KITTI odometry dataset. We compare with SfMLearner <ref type="bibr" target="#b6">[7]</ref> and the methods trained with stereo videos <ref type="bibr" target="#b16">[17]</ref>. We also report the results of ORB-SLAM <ref type="bibr" target="#b11">[12]</ref> system (without loop closing) as a reference, though emphasize that this results in a comparison note between a simple frame-to-frame pose estimation framework with a Visual SLAM system, in which the latter has a strong back-end optimization system (i.e., bundle adjustment <ref type="bibr" target="#b31">[32]</ref>) for improving the performance. Here, we ignore the frames (First 9 and 30 respectively) from the sequences (09 and 10) for which ORB-SLAM <ref type="bibr" target="#b11">[12]</ref> fails to output camera poses because of unsuccessful initialization.   Tab. 2 shows the average translation and rotation errors for the testing sequence 09 and 10, and <ref type="figure" target="#fig_2">Fig. 3</ref> shows qualitative results. Note that the comparison is highly disadvantageous to the proposed method: i) we align per-frame scale to the ground truth scale for <ref type="bibr" target="#b6">[7]</ref> due to its scale-inconsistency, while we only align one global scale for our method; ii) <ref type="bibr" target="#b16">[17]</ref> requires stereo videos for training, while we only use monocular videos. Although it is unfair to the proposed method, the results show that our method achieves competitive results with <ref type="bibr" target="#b16">[17]</ref>. Even when compared with the ORB-SLAM <ref type="bibr" target="#b11">[12]</ref> system, our method shows a lower translational error and a better visual result on sequence 09. This is a remarkable progress that deep models trained on unlabelled monocular videos can predict a globally scale-consistent visual odometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head><p>In this section, we first validate the efficacy of the proposed geometry-consistency loss L GC and the self-discovered weight mask M . Then we experiment with different scale numbers, network architectures, and image resolutions.</p><p>Validating proposed L GC and M . We conduct ablation studies using DispNet <ref type="bibr" target="#b6">[7]</ref> and images of 416×128 resolution. Tab. 3 shows the depth results for both single-scale and multi-scale supervisions.</p><p>The results clearly demonstrate the contribution of our proposed terms to the overall performance.</p><p>Besides, <ref type="figure" target="#fig_3">Fig. 4</ref> shows the validation error during training, which indicates that the proposed L GC can effectively prevent the model from overfitting.  Proposed single-scale vs multi-scale supervisions. As mentioned in Sec. 4.1, we empirically find that using single-scale supervision leads to better performance than using the widely-used multi-scale solution. Tab. 4 shows the depth results. We hypothesis the reason is that the photometric loss is not accurate in low-resolution images, where the pixel color is over-smoothed. Besides, as the displacement between two consecutive views is small, the multi-scale solution is unnecessary.</p><p>Network architectures and image resolutions. Tab. 5 shows the results of different network architectures on different resolution images, where DispNet and DispResNet are both borrowed from CC <ref type="bibr" target="#b10">[11]</ref>, and DispNet is also used in SfMLearner <ref type="bibr" target="#b6">[7]</ref>. It shows that higher resolution images and deeper networks can results in better performance.  Inference time. We test models on a single RTX 2080 GPU. The batch size is 1, and the time is averaged over 100 iterations. Tab. 7 shows the results. The DispNet and DispResNet architectures are same with SfMLearner <ref type="bibr" target="#b6">[7]</ref> and CC <ref type="bibr" target="#b10">[11]</ref>, respectively, so their speeds are theoretically same. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presents an unsupervised learning framework for scale-consistent depth and ego-motion estimation. The core of the proposed approach is a geometry consistency loss for scale-consistency and a self-discovered mask for handling dynamic scenes. With the proposed learning framework, our depth model achieves the state-of-the-art performance on the KITTI <ref type="bibr" target="#b14">[15]</ref> dataset, and our ego-motion network can show competitive visual odometry results with the model that is trained using stereo videos. To the best of our knowledge, this is the first work to show that deep models training on unlabelled monocular videos can predict a globally scale-consistent camera trajectory over a long sequence. In future work, we will focus on improving the visual odometry accuracy by incorporating drift correcting solutions into the current framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Supplementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Pose estimation results on 5-frame snippets</head><p>Although the visual odometry results shown in the main paper is more important, we also evaluate pose estimation results using Zhou et al. <ref type="bibr" target="#b6">[7]</ref>'s evaluation metric on 5-frame snippets. Tab. 8 shows the results, where our method shows slightly lower performances with the state-of-the-art methods but the gap is small. 0.014 ± 0.008 0.012 ± 0.011 ORB-SLAM (short) 0.064 ± 0.141 0.064 ± 0.130</p><p>Mean Odometry 0.032 ± 0.026 0.028 ± 0.023 Zhou et al. <ref type="bibr" target="#b6">[7]</ref> 0.021 ± 0.017 0.020 ± 0.015 Mahjourian et al. <ref type="bibr" target="#b7">[8]</ref> 0.013 ± 0.010 0.012 ± 0.011</p><p>GeoNet <ref type="bibr" target="#b8">[9]</ref> 0.012 ± 0.007 0.012 ± 0.009 DF-Net <ref type="bibr" target="#b9">[10]</ref> 0.017 ± 0.007 0.015 ± 0.009 CC <ref type="bibr" target="#b10">[11]</ref> 0.012 ± 0.007 0.012 ± 0.009 Ours 0.016 ± 0.007 0.015 ± 0.015</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Depth estimation results on Make3D dataset.</head><p>To verify the generalization ability of the trained model, we also test it on Make3D dataset <ref type="bibr" target="#b32">[33]</ref>. Tab. 9 shows the relative depth error, where our model is trained on KITTI <ref type="bibr" target="#b14">[15]</ref> without fine-tuning on Make3D <ref type="bibr" target="#b5">[6]</ref>. The results demonstrate that our method performs slightly better than other state-ofthe-art methods.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Visual results. Top to bottom: sample image, estimated depth, self-discovered mask. The proposed mask can effectively identify occlusions and moving objects.of I i of a sequence should all agree with each other. As the pose network is naturally coupled with the depth network during training, our method yields scale-consistent predictions over the entire sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results on the testing sequences of KITTI odometry dataset<ref type="bibr" target="#b14">[15]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Validation error. Both Basic and Basic+SSIM overfit after about 50 epochs, while others do not due to proposed L GC . Besides, models with the single-scale supervision in training outperforms those with multi-scale (4) supervisions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 Figure 5 :</head><label>55</label><figDesc>illustrates visual results of depth estimation and occlusion detection by the proposed approach. Visual results. Top to bottom: sample image, estimated depth, self-discovered mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Single-view depth estimation results on test split of KITTI raw dataset<ref type="bibr" target="#b14">[15]</ref>. The methods trained on KITTI raw dataset<ref type="bibr" target="#b14">[15]</ref> are denoted by K. Models with pre-training on CityScapes<ref type="bibr" target="#b29">[30]</ref> are denoted by CS+K. (D) denotes depth supervision, (B) denotes binocular/stereo input pairs, (M) denotes monocular video clips. (J) denotes joint learning of multiple tasks. The best performance in each block is highlighted as bold.</figDesc><table><row><cell>Error ↓</cell><cell>Accuracy ↑</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Visual odometry results on KITTI odometry dataset<ref type="bibr" target="#b14">[15]</ref>. We report the performance of ORB-SLAM<ref type="bibr" target="#b11">[12]</ref> as a reference and compare with recent deep methods. K denotes the model trained on KITTI, and CS+K denotes the model with pre-training on Cityscapes<ref type="bibr" target="#b29">[30]</ref>.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>Seq. 09</cell><cell></cell><cell>Seq. 10</cell></row><row><cell></cell><cell cols="4">t err (%) r err ( • /100m) t err (%) r err ( • /100m)</cell></row><row><cell>ORB-SLAM [12]</cell><cell>15.30</cell><cell>0.26</cell><cell>3.68</cell><cell>0.48</cell></row><row><cell>Zhou et al. [7]</cell><cell>17.84</cell><cell>6.78</cell><cell>37.91</cell><cell>17.78</cell></row><row><cell>Zhan et al. [17]</cell><cell>11.93</cell><cell>3.91</cell><cell>12.45</cell><cell>3.46</cell></row><row><cell>Ours (K)</cell><cell>11.2</cell><cell>3.35</cell><cell>10.1</cell><cell>4.96</cell></row><row><cell>Ours (CS+K)</cell><cell>8.24</cell><cell>2.19</cell><cell>10.7</cell><cell>4.58</cell></row><row><cell>(a) sequence 09</cell><cell></cell><cell></cell><cell></cell><cell>(b) sequence 10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies on L GC and M . Brackets show results of multi-scale (4) supervisions.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Error ↓</cell><cell></cell><cell></cell><cell></cell><cell>Accuracy ↑</cell></row><row><cell></cell><cell></cell><cell>Methods</cell><cell></cell><cell>AbsRel</cell><cell cols="6">SqRel RMS RMSlog &lt; 1.25 &lt; 1.25 2 &lt; 1.25 3</cell></row><row><cell></cell><cell></cell><cell>Basic</cell><cell cols="4">0.161 (0.185) 1.225 5.765</cell><cell>0.237</cell><cell>0.780</cell><cell>0.927</cell><cell>0.972</cell></row><row><cell></cell><cell></cell><cell>Basic+SSIM</cell><cell cols="4">0.160 (0.163) 1.230 5.950</cell><cell>0.243</cell><cell>0.775</cell><cell>0.923</cell><cell>0.969</cell></row><row><cell></cell><cell></cell><cell>Basic+SSIM+GC</cell><cell cols="4">0.158 (0.161) 1.247 5.827</cell><cell>0.235</cell><cell>0.786</cell><cell>0.927</cell><cell>0.971</cell></row><row><cell></cell><cell></cell><cell cols="5">Basic+SSIM+GC+M 0.151 (0.158) 1.154 5.716</cell><cell>0.232</cell><cell>0.798</cell><cell>0.930</cell><cell>0.972</cell></row><row><cell></cell><cell>0.36</cell><cell cols="2">single-scale</cell><cell></cell><cell></cell><cell>0.36</cell><cell></cell><cell cols="2">multi-scale</cell></row><row><cell></cell><cell>0.34</cell><cell>Basic Basic+SSIM</cell><cell></cell><cell></cell><cell></cell><cell>0.34</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.32</cell><cell>Basic+SSIM+GC Basic+SSIM+GC+M</cell><cell></cell><cell></cell><cell></cell><cell>0.32</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.28</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AbsRel</cell><cell>0.24 0.26</cell><cell></cell><cell></cell><cell></cell><cell>AbsRel</cell><cell>0.24 0.26</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.22</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.22</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.18</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.16</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>0</cell><cell>50</cell><cell></cell><cell>100</cell><cell>150</cell><cell>200</cell></row><row><cell></cell><cell></cell><cell cols="2">Epochs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epochs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on scale numbers of supervision.Error ↓ Accuracy ↑ #Scales AbsRel SqRel RMS RMSlog &lt; 1.25 &lt; 1.25 2 &lt; 1.25<ref type="bibr" target="#b2">3</ref> </figDesc><table><row><cell>1</cell><cell>0.151</cell><cell>1.154 5.716</cell><cell>0.232</cell><cell>0.798</cell><cell>0.930</cell><cell>0.972</cell></row><row><cell>2</cell><cell>0.152</cell><cell>1.192 5.900</cell><cell>0.235</cell><cell>0.795</cell><cell>0.927</cell><cell>0.971</cell></row><row><cell>3</cell><cell>0.159</cell><cell>1.226 5.987</cell><cell>0.240</cell><cell>0.780</cell><cell>0.921</cell><cell>0.969</cell></row><row><cell>4</cell><cell>0.158</cell><cell>1.214 5.898</cell><cell>0.239</cell><cell>0.782</cell><cell>0.925</cell><cell>0.971</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies on different network architectures and image resolutions. Timing and memory analysisTraining time and parameter numbers. We compare with CC<ref type="bibr" target="#b10">[11]</ref>, and both methods are trained on a single 16GB Tesla V100 GPU. We measure the time taken for each training iteration consisting of forward and backward pass using a batch size of 4. The image resolution is 832 × 256. CC<ref type="bibr" target="#b10">[11]</ref> needs train 3 parts, including (Depth, Pose), Flow, and Mask. In contrast our method only trains (Depth, Pose). In total, CC takes about 7 days for training as reported by authors, while our method takes about 32 hours. Tab. 6 shows the per-iteration time and model parameters of each network.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Error ↓</cell><cell></cell><cell></cell><cell>Accuracy ↑</cell><cell></cell></row><row><cell>Methods</cell><cell cols="7">Resolutions AbsRel SqRel RMS RMSlog &lt; 1.25 &lt; 1.25 2 &lt; 1.25 3</cell></row><row><cell>DispNet DispResNet</cell><cell>416 × 128</cell><cell>0.151 0.149</cell><cell>1.154 5.716 1.137 5.771</cell><cell>0.232 0.230</cell><cell>0.798 0.799</cell><cell>0.930 0.932</cell><cell>0.972 0.973</cell></row><row><cell>DispNet DispResNet</cell><cell>832 × 256</cell><cell>0.146 0.137</cell><cell>1.197 5.578 1.089 5.439</cell><cell>0.223 0.217</cell><cell>0.814 0.830</cell><cell>0.940 0.942</cell><cell>0.975 0.975</cell></row><row><cell>4.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Training time per iteration and model parameters for each network.</figDesc><table><row><cell></cell><cell cols="2">CC [11]</cell><cell></cell><cell>Ours</cell></row><row><cell>Network</cell><cell>(Depth, Pose)</cell><cell>Flow</cell><cell>Mask</cell><cell>(Depth, Pose)</cell></row><row><cell>Time</cell><cell>0.96s</cell><cell>1.32s</cell><cell>0.48s</cell><cell>0.55s</cell></row><row><cell cols="5">Parameter Numbers (80.88M, 2.18M) 39.28M 5.22M (80.88M, 1.59M)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Inference time on per image or image pair.</figDesc><table><row><cell></cell><cell cols="3">DispNet DispResNet PoseNet</cell></row><row><cell>128 × 416</cell><cell>4.9 ms</cell><cell>9.6 ms</cell><cell>0.6 ms</cell></row><row><cell>256 × 832</cell><cell>9.2 ms</cell><cell>15.5 ms</cell><cell>1.0 ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Pose estimation results on KITTI odometry dataset.</figDesc><table><row><cell>Seq. 09</cell><cell>Seq. 10</cell></row><row><cell>ORB-SLAM (full)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Depth results (AbsRel) on Make3D<ref type="bibr" target="#b32">[33]</ref> test set without finetuning.</figDesc><table><row><cell cols="6">Methods Zhou et al. [7] Godard et al. [23] DF-Net et al. [10] CC [11] Ours</cell></row><row><cell>AbsRel</cell><cell>0.383</cell><cell>0.544</cell><cell>0.331</cell><cell>0.320</cell><cell>0.312</cell></row><row><cell cols="2">6.3 More qualitative results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Monocular systems such as ORB-SLAM<ref type="bibr" target="#b11">[12]</ref> suffer from the scale ambiguity issue, but their predictions are globally scale-consistent. However, recently learned models using monocular videos not only suffer from the scale ambiguity, but also predict scale-inconsistent results over different snippets.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work was supported by the Australian Centre for Robotic Vision. Jiawang would also like to thank TuSimple, where he started research in this field.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">GMS: Grid-based motion statistics for fast, ultra-robust feature correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Wang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan-Dat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An evaluation of feature matchers for fundamental matrix estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Wang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Huan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Recognition and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GeoNet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DF-Net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Competitive Collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ORB-SLAM: a versatile and accurate monocular slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose Maria Martinez</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of optical flow and depth by watching stereo videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Look deeper into depth: Monocular depth estimation with semantic booster and attention-driven loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vision meets Robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><forename type="middle">Miguel</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chamara</forename><surname>Saroj Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BA-Net: Dense bundle adjustment network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical discrete distribution decomposition for match density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-supervised learning for dense depth estimation in monocular endoscopy. In OR 2.0 Context-Aware Operating Theaters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingtong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayushi</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Unberath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Clinical Image-Based Procedures, and Skin Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="128" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lucas-kanade 20 years on: A unifying framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image Quality Assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">ADAM: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised learning of geometry with edge-aware depth-normal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bundle adjustment-a modern synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">I</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on vision algorithms</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
