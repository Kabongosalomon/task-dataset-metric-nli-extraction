<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Lab of Traffic Data Analysis and Mining</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044, {18120367, 18120345, 17112079</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boli</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Lab of Traffic Data Analysis and Mining</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044, {18120367, 18120345, 17112079</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Lab of Traffic Data Analysis and Mining</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044, {18120367, 18120345, 17112079</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liping</forename><surname>Jing</surname></persName>
							<email>lpjing@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Lab of Traffic Data Analysis and Mining</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044, {18120367, 18120345, 17112079</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Extreme Multi-label Text Classification · Deep Neural Net- work · Attention · Tail Label · Lable-aware Document Representation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Extreme multi-label text classification (XMTC) aims at tagging a document with most relevant labels from an extremely large-scale label set. It is a challenging problem especially for the tail labels because there are only few training documents to build classifier. This paper is motivated to better explore the semantic relationship between each document and extreme labels by taking advantage of both document content and label correlation. Our objective is to establish an explicit label-aware representation for each document with a hybrid attention deep neural network model(LAHA). LAHA consists of three parts. The first part adopts a multi-label self-attention mechanism to detect the contribution of each word to labels. The second part exploits the label structure and document content to determine the semantic connection between words and labels in a same latent space. An adaptive fusion strategy is designed in the third part to obtain the final label-aware document representation so that the essence of previous two parts can be sufficiently integrated. Extensive experiments have been conducted on five benchmark datasets by comparing with the state-of-the-art methods. The results show the superiority of our proposed LAHA method, especially on the tail labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extreme multi-label text classification(XMTC) aims at automatically tagging a document with most relevant labels from an extremely large label set. For instance, there are millions of categories on Wikipedia and one might wish to build a classifier that can annotate a given message with the subset of most relevant categories <ref type="bibr" target="#b7">[8]</ref>. XMTC has become increasingly important due to the boom of big data, while it becomes significantly challenging because it has to simultaneously handle massive documents, features and labels. Thus it is emergency to develop effective extreme multi-label classifer for various real-applications such as product categorization in e-commerce, news annotation and etc.</p><p>Multi-label text classification, unlike the traditional multi-class classification, allows for the co-existence of more than one labels for a single document. Meanwhile, there may be a large number of 'tail labels' with very few positive documents in XMTC tasks. To tackle the aforementioned issues, researchers pay much attention on two facets: 1) how to represent label so that the correlation among labels can be accurately mined, and 2) how to represent document so that the dependency among text can be sufficiently captured. Recently, state-ofthe-art extreme multi-label learning methods have been proposed in each facet. Among them, tree-based and embedding-based methods become popular to find the label correlation as they can obtain notable accuracy improvement by constructing a hierarchy structure <ref type="bibr" target="#b16">[17]</ref> or learning a low-dimensional latent space <ref type="bibr" target="#b7">[8]</ref>. Deep learning-based methods (e.g., convolutional neural network <ref type="bibr" target="#b4">[5]</ref>) have achieved great success to represent text data. These methods usually characterize one document with the same representation on all labels. In this case, the probability of document belonging to a class is determined by their overall matching score regardless of the label-aware semantic information. Recent works, AttentionXML <ref type="bibr" target="#b5">[6]</ref> and EXAM <ref type="bibr" target="#b6">[7]</ref>, turn attention to this issue with the aid of attentive neural network. However, they only focus on document or label content but ignoring the label structure among extreme labels which has been proved very important in extreme multi-label learning <ref type="bibr" target="#b7">[8]</ref>.</p><p>To solve the above-mentioned problems, we introduce a Label-Aware document representation model via a Hybrid Attention neural network (LAHA) by considering both document content and label structure. LAHA consists of three parts. The first part aims at detecting the importance of each word to all labels via a self-attention bidirectional LSTM neural network. The second part tries to explore the semantic connection between words and labels in a latent space. Here the word embedding is obtained by the bidirectional LSTM neural network. The label embedding is determined from the label co-exist graph so that the label structure can be sufficiently maintained in the same latent space with words'. Based on these two embeddings, we introduce an interaction-attention mechanism to explicitly compute the semantic relation between the words and labels. The last part is to represent each document along each label via an adaptive fusion strategy. The goal of fusion strategy is to adaptively extract proper information from the previous two parts so that the final document representation has discriminative ability to construct classifier. The proposed XMTC model LAHA has been evaluated on five benchmark datasets and get competitive results, we summarize the major contributions.</p><p>-LAHA is the first work to construct label-aware document representation by simultaneously considering document content and label structure.</p><p>-The hybrid attention mechanism is firstly designed to adaptively extract the semantic relation between each document and all labels for XMTC.</p><p>-The performance of LAHA was thoroughly investigated on widely-used benchmark datasets, indicating the advantage over the baselines.</p><p>-The code and hyper-parameter settings are released 1 to facilitate other researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Significant progress has been made for XMTC. They can be roughly categorized into two categories: embedding-based and tree-based methods. Recently, due to the powerful ability of representation, deep learning technology has been introduced to effectively represent document for XMTC tasks. Next, we will briefly review them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Embedding-based Methods</head><p>Embedding-based methods aim at reducing the huge label space to a low dimensional space while preserving the label correlation as much as possible, and then compressed label embedding are decompressed for prediction. Various approaches have been presented such as compressed sensing <ref type="bibr" target="#b17">[18]</ref>, output codes <ref type="bibr" target="#b18">[19]</ref>, Singular Value Decomposition <ref type="bibr" target="#b19">[20]</ref>, landmark labels <ref type="bibr" target="#b20">[21]</ref>, Bloom filters <ref type="bibr" target="#b21">[22]</ref>, etc.</p><p>To efficiently handle large-scale label set, these embedding-based methods usually assume that the label matrix is low-rank. However, such methods have been proved unable to deliver high prediction accuracies as the low rank assumption is violated in most real world applications <ref type="bibr" target="#b7">[8]</ref>. SLEEC <ref type="bibr" target="#b7">[8]</ref> can be taken as the most representative embedding-based method due to its significant accuracy and computationally efficiency. Its main idea is to learn a small ensemble of local distance preserving embeddings. Specifically, SLEEC divides the training data set into several clusters, and in each cluster it detects embedding vectors by capturing non-linear label correlation and preserving the pairwise distance between labels. The k-nearest neighbors search is used to do prediction only in the cluster into which the test document is fallen. Later, Zhang et al. <ref type="bibr" target="#b13">[14]</ref> adopted deep neural network for non-linear modeling the label embedding. Although these methods perform well, they play a heavy price in terms of prediction accuracy due to the loss of information during the compression and decompression phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tree-based Methods</head><p>Tree-based methods introduce a tree structure to divide the documents recursively at each non-leaf node, so that documents in each leaf node share similar label distribution. The most representative method FastXML <ref type="bibr" target="#b16">[17]</ref> implements this process by optimizing the normalized discounted cumulative gain (nDCG)based ranking loss function. Then, a base classifier is trained at each leaf node which only focuses on a few active labels. To enhance the robustness of predictions, an ensemble of multiple induced trees are learned. The main advantage of tree-based methods is that the prediction time complexity is typically sublinear in the training-set size and would be logarithmic if the induced tree is balanced.</p><p>A recent extension work of FastXML is PfastreXML <ref type="bibr" target="#b8">[9]</ref>, which adopted a propensity scored objective function instead of nDCG-based loss which is more friendly to tail labels. Parabel <ref type="bibr" target="#b9">[10]</ref> is another tree-based method, which constructs balanced trees partitioning labels rather than instances. These tree-based methods represent document via bag-of-words, where the words are treated as independent features, which will ignore the semantic dependency among words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Learning-based Methods</head><p>To capture semantic dependency among words, researchers adopted deep learning models in text classification task due to its strong ability of representation. The popular deep models include CNN <ref type="bibr" target="#b25">[26]</ref>, GRU <ref type="bibr" target="#b24">[25]</ref>, RNN <ref type="bibr" target="#b11">[12]</ref>, LSTM <ref type="bibr" target="#b12">[13]</ref>, Bi-LSTM <ref type="bibr" target="#b26">[27]</ref>, BERT <ref type="bibr" target="#b28">[29]</ref> and several combination networks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28]</ref>. Even though they have achieved great success in traditional NLP tasks, few work is designed for XMTC.</p><p>XML-CNN <ref type="bibr" target="#b4">[5]</ref> can be taken as the first and most representative work using deep learning model in XMTC. It takes advantage of CNN, dynamic maxpooling and bottle-neck layer to build the deep model. Due to the limited window size, XML-CNN can not capture the long-distance dependency among text. Later, GRU and Bi-LSTM language models are adopted in AttentionXML <ref type="bibr" target="#b5">[6]</ref> and EXAM <ref type="bibr" target="#b6">[7]</ref> to effectively represent document for XMTC. Meanwhile, these two methods consider the difference of one document represesntation along different labels. AttentionXML <ref type="bibr" target="#b5">[6]</ref> adopts self-attention mechanism <ref type="bibr" target="#b14">[15]</ref>, while EXAM <ref type="bibr" target="#b6">[7]</ref> exploits the label content information to calculate the relations between words and classes. Although AttentionXML obtains promising performance, it ignores the label structure which has been proved very important in embedding-based and tree-based multi-label learning methods. Therefore, in this paper, we propose a new XMTC deep model with hybrid attention to build label-aware document representation, which sufficiently exploits both document content and label structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LAHA model</head><p>In this section, we introduce the proposed deep model (LAHA) to handle XMTC tasks. The overall structure of LAHA is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>). Our goal is to build a multi-label learning model from the training documents with a large-size label set. Let D = {(x 1 , y 1 ), ..., (x N , y N )} be the given raw training document set containing total N documents and belonging to k labels. Each document has n tokens (or words) and each word is represented via a d-dimensional deep semantic dense vector acquired from word2vec technique, e t ∈ R d (t = 1, ..., n). y i ⊆ {0, 1} k is the corresponding label vector, and y ij = 1 iff the j-th label is turned on for the i-th document x i = (e 1 , ..., e n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Embedding</head><p>To build the proposed LAHA multi-label text classifier, the raw text data is preprocessed via word embedding technique so that each word is represented as a low-dimensional dense vector. The extreme labels are embedded into dense vectors from the label co-exist graph so that the label correlation and local structure can be sufficiently captured.</p><p>Word Embedding Once having the d-dimensional word vector e t ∈ R d for each word (t = 1, ..., n), the whole document can be taken as a sequence of words (e 1 , ..., e n ) as the input of LAHA. In order to capture the bi-directional contextual information, we adopt Bi-LSTM <ref type="bibr" target="#b26">[27]</ref> to learn the word embedding for each input document. So the whole output of Bi-LSTM can be obtained by</p><formula xml:id="formula_0">H = (H (f ) ; H (b) ) with H (f ) = ( − → h1, ..., − → hn) ∈ R r×n ; H (b) = ( ← − h1, ..., ← − hn) ∈ R r×n (1)</formula><p>where − → h t ∈ R r and ← − h t ∈ R r are the forward and backward word context representations respectively. The whole document is taken as a matrix H ∈ R 2r×n .</p><p>Label Embedding To better extract label correlation information, we firstly build a label co-exist graph from the training data where each labels are represented by nodes. There will be an edge connecting the i-th label and the j-th label if they share at least one document <ref type="bibr" target="#b13">[14]</ref>. Our goal is to represent the extreme labels in a low-dimensional latent space so that two nearby labels in the graph have similar representation, i.e., the local structure among labels are preserved as much as possible. Thus, the popular and powerful node2vec <ref type="bibr" target="#b15">[16]</ref> is adopted here because it has ability to explore the labels' diverse neighborhoods by a flexible biased random walk procedure in a breadth-first sampling as well as depth-first sampling fashion. Finally, each label will be represented by a rdimensional dense vector, i.e., l i ∈ R r for the i-th label (i = 1, ..., k) and the whole label set can be described by L = (l 1 , l 2 , ..., l k ) ∈ R r×k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hybrid Attention Mechanism</head><p>Hybrid attention mechanism aims at better representing each document by taking advantage of both document content and label structure. It is composed of self-attention mechanism on document content and interaction-attention mechanism to exploit document content and label structure.</p><p>Self-attention (SA) has been successful used in text mining tasks such as relation extraction <ref type="bibr" target="#b29">[30]</ref>. In multi-label data, since one document may be tagged by more than one labels, each document should have the most relative context to its corresponding labels. That is, the words in one document make different contributions to each label. To focus on different aspects of document, thus, we introduce self-attention mechanism (SA) <ref type="bibr" target="#b14">[15]</ref> on the output of Bi-LSTM (H). The attention score A (S) ∈ R n×k is calculated by</p><formula xml:id="formula_1">T = tanh(Ws1H); A (S) = sof tmax(Ws2T )<label>(2)</label></formula><p>where W s1 ∈ R da×2r and W s2 ∈ R k×da are parameters to be trained. A (S) j ∈ R n is the attention scores of words along the j-th label. To efficiently handle extreme multi-label data, we adopt negative sampling strategy <ref type="bibr" target="#b10">[11]</ref> to update W s2 and computer A (S) j , so that all positive labels and a random small subset of negative labels are considered. Then, we can obtain the linear combination of context words for each label through self-attention mechanism as C (S) j = HA (S) j , which can be taken as the representation of the input document along the j-th lable. The whole matrix C (S) ∈ R 2r×k is the label-aware document represenation under the self-attention mechanism.</p><p>Interaction-attention (IA) aims to determine the semantic connection between words and labels in a latent space. With the help of word embedding and label embedding technique, all words and labels are represented in the rdimensional latent space as H = (H (f ) ; H (b) ) and L respectively. To conveniently align the latent space of words and that of labels, a bridge mapping marix W q ∈ R r×r is trained via Q = W q L. Similar to SA, we can do negative sampling on L to produce L * ∈ R r×k * that is extracted from L according to sampled indices, and just use L * for the following computation.</p><p>Inspired by the interaction mechanism <ref type="bibr" target="#b6">[7]</ref>, we take Q ∈ R r×k as the attention querys for each label, and use H to construct the key-value pairs in terms of forward and backward information for each word. Then, the interactive matching score M (I) ∈ R n×k</p><formula xml:id="formula_2">M (I) = H (f ) T H (b) T Q Q<label>(3)</label></formula><p>To make sure the attention weight value fall into the range of [0, 1], we normalize M (I) to obtain the interaction-attention weight A (I) = A (I) tj t={1,...,n},j={1,...,k} as follows.</p><formula xml:id="formula_3">A (I) tj = e M (I) tj / n i=1 e M (I) ij<label>(4)</label></formula><p>Similar to self-attention mechanism, the label-aware document representation can be calculated by linear combining the label's context words as C (I) j = HA (I) j , which can be taken as the representation of the input document along the j-th lable. The whole matrix C (I) ∈ R 2r×k is the label-aware document represenation under the interaction-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention Fusion (FA)</head><p>The above C (S) and C (I) are label-aware document representation. The former focuses on document content, while the latter prefers to the label structure. In order to take advantage of these two parts, an attention fusion strategy is designed here to adaptively extract proper information from these two components and build accurate label-aware document representation. More specifically, a fully connected layer is used to transform the input (C (S) and C (I) ) to weights α ∈ R k×1 and β ∈ R k×1 via α = σ(F1(C (S) )); β = σ(F2(C (I) )) <ref type="bibr" target="#b4">(5)</ref> where σ is sigmoid function to ensure the weights falling into (0, 1). Among them, α j and β j indicates the importances of self-attention and interactionattetion to final representation along the j-th label respectively. Therefore, we normalize them as α j = α j /(α j + β j ) and β j = 1 − α j . With the aid of fusion weights, we can get the final label-aware representation of input document along the j-th label</p><formula xml:id="formula_4">Cj = αj × C (S) j + βj × C (I) j .<label>(6)</label></formula><p>The whole matrix C ∈ R 2r×k is the final label-aware document represenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prediction Layer</head><p>Once having C ∈ R 2r×k , we can build the classifier via a fully connected and output layer. The final predictions are obtained byŷ = σ(W o (f (W f C))) where W f ∈ R r×2r , W o ∈ R 1×r , f is the activation function ReLU, and σ is adopted to ensure that the output value can be taken as a probability. In this case, the binary cross-entropy loss can be used as loss function which has been proved suitable for XMTC tasks <ref type="bibr" target="#b4">[5]</ref> .</p><formula xml:id="formula_5">L loss = − 1 N N i=1 k j=1 [yijlog(ŷij) + (1 − yij)log(1 −ŷij)]<label>(7)</label></formula><p>where N is the number of training documents. The ground truth y ij = 1 if the i-th document belongs to the j-th class, otherwise y ij = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the proposed LAHA on five benchmark datasets by comparing with the state-of-the-art extreme multi-label learning methods in terms of widely used metrics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>A series of experiments were carried out on five multi-label datasets with label sizes from 54 to 29,947.The dataset statistics are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Methodology</head><p>Baseline Algorithms The proposed LAHA is a deep neural network model, thus the recent deep learning-based XMTC methods (XML-CNN <ref type="bibr" target="#b4">[5]</ref> and At-tentionXML <ref type="bibr" target="#b5">[6]</ref>) are selected as baselines. Meanwhile, the existing powerful SLEEC <ref type="bibr" target="#b7">[8]</ref> (an embedding-based method) and PfastreXML <ref type="bibr" target="#b8">[9]</ref> (a tree-based method) are used as baselines because they obtained the best performance in each type as shown in the Extreme Classification Repository 2 .</p><p>Parameter Settings For all the five datasets, we adopt Glove(300-dimension) <ref type="bibr" target="#b10">[11]</ref> as word embedding. The number of Bi-LSTM hidden units is set to r = 256. For the self-attention mechanism, d a = 256. In the prediction layer, ReLU is adopted as non-linear activation function. The whole deep model is trained using Adam with the initial learning rate (0.001) and the batch size (64).</p><p>Evaluation Metrics In XMTC tasks, rank-based evaluation metrics are popular used to evaluate model performance, including Precision at τ (P @τ ) and normalized Discounted Cumulative Gain at τ (nDCG@τ ). Both of them have been widely used in XMTC tasks. They are defined as</p><formula xml:id="formula_6">P @τ = 1 τ l∈rτ (ŷ) y l ; nDCG@τ = l∈rτ (ŷ) y l / log(l + 1) min(τ,||y||0) l=1 1/ log(l + 1)<label>(8)</label></formula><p>where y ∈ {0, 1} k is the ground truth label vector of a document and r τ (ŷ) is the label indexes of top τ highest scores of current prediction result. y 0 counts the number of relevant labels in the ground truth label vector y. Larger P @τ and nDCG@τ indicates better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Test of LAHA</head><p>In this section, we firstly demonstrate the effect of each component on LAHA.</p><p>To reach this goal, we do ablation test for self-attention mechanism (SA), interactionattention mechanism (IA) and attention fusion mechanism (FA) respectively with two datasets: one sparse dataset EUR-lex and one dense dataset AAPD.</p><p>(a) ablation test on EUR-Lex (b) ablation test on AAPD (a) EUR-Lex (b) AAPD <ref type="figure">Fig. 3</ref>. Weight distributions for two components on EUR-Lex and AAPD. x-axis indicates the range of weight from 0 to 1 with 0.1 gap. y-axis indicates the frequency that the specific range occurs in current label group. <ref type="figure" target="#fig_1">Fig.2</ref> lists the results on these two datasets in terms of P @τ (τ = {1, 3, 5}). It can be seen that SA performs well on dense dataset (AAPD). However, neither SA nor IA can obtain good result on sparse dataset (EUR-Lex ). Fortunately, combining SA and IA improves the prediction performance (SA+IA gets better performance than SA and IA). SA prefers to extract the useful content information when constructing the label-aware document representation, but SA ignores the label structure during the learning process. IA implements this by using the label embedding learnt from the label co-exist graph. However, in real application, such graph may contain noisy information (say in dense data). Therefore, coupling with both attention components does really helpful for final performance because they can benefit each other on different datasets.</p><p>To adaptively extract proper information to learn the final label-aware document representation, the attention fusion mechanism is introduced in LAHA. <ref type="figure">Fig.3</ref> lists the distribution of weights on SA and IA. It can be seen that for sparse data (EUR-Lex ), the interaction-attention plays much more important role than self-attention on learning process, vice verse for dense dataset (AAPD). This result further clarifies that IA mechanism can leverage the label structure to improve the prediction performance for sparse data. On the other hand, in AAPD, each label has sufficient documents, i.e., SA mechanism can sufficiently capture the label-aware document information and perform well. That is why larger weights are assigned to SA on dense data. Similar trend can be found on other datasets, which are omitted due to the page limitation. </p><formula xml:id="formula_7">(a) G1(F ≤ 5) (b) G2(5 &lt; F ≤ 50) (c) G3(50 &lt; F ≤ 764)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Deep Methods on Sparse Datasets</head><p>In order to explore the effect of LAHA on sparse datasets, we further divide labels into three groups according to their occurring frequencies. <ref type="figure" target="#fig_2">Fig.4</ref> shows the prediction performance obtained by three deep methods. Obviously, label prediction in G1 is much harder than in other two groups due to the lack of training documents. All methods become better from G1 to G3, which is reasonable since G3 contains more training documents than G1. LAHA has an overall improvement for all groups compared with two baselines. This result further demonstrates the superiority of the proposed hybrid attention mechanism on XMTC with large-scale tail labels. Similar phenomena can be found on other sparse datasets, which are omitted due to the page limitation.</p><p>To further investigate the attention-based methods, we visualize the attention weights on the original document using heat map, as shown in <ref type="figure" target="#fig_3">Fig.5</ref>. This example document belongs to 28 labels named as autism, children, childhood, disease, asperger, social norm, health, neurology, abnormal and etc. From the (a) The words with largest label-aware attention weights output by At-tentionXML (word→{labels}) are: autism→{autism}; cure others→{disease}; disorder→{abnormal}; social communication, approach others→{social norm}; children→{children, childhood}.</p><p>(b) The words with largest label-aware attention weights output by LAHA (word→{labels}) are: autism→{autism}; disorder→{abnormal}; child life, infancy childhood, children→{children, childhood}; diagnostic, genetic factor, lack intuition→{disease}; synapses connect organize→{neurology}; asperger syndrome→{asperger}; social communication→{social norm}; security→{disease, health}. attention weights, we can see that AttentionXML only captures few key words for few related labels. As expected, LAHA focuses on the related information as much as possible due to the capacity making full use of label structure and document content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison Results and Discussion</head><p>In this section, the proposed LAHA is evaluated on five benchmark datasets by comparing with four baselines in terms of P @τ and nDCG@τ (τ = {1, 3, 5}). <ref type="table" target="#tab_1">Table 2</ref> shows the averaged performance of all test documents. According to the formula (8), we know P @1 = nDCG@1, thus only nDCG@3 and nDCG@5 are listed. In each line, the best result is marked in bold, and the second best is underlined.</p><p>From <ref type="table" target="#tab_1">Table 2</ref>, we can make a number of observations about these results. Firstly, LAHA outperforms the traditional powerful embedding-based and treebased methods in most cases, while slightly underperforms the embedding-based method SLEEC on EUR-Lex and Wiki-30K. From <ref type="table" target="#tab_0">Table 1</ref>, we can see there are only 11,585 and 12,959 training documents in these two datasets, in this case, the deep model may be not sufficiently trained. Second, LAHA is consistently superior to the state-of-the-art deep XMTC methods. The main reason is that LAHA has ability to sufficiently determine the label-aware document representation while XML-CNN does not. Even though AttentionXML tries to find the relation between each pair of document and label, it only focuses on document content, which will degrade its performance on tail labels due to lack of information. Fortunately, LAHA addresses this issue by simultaneously considering label structure via a hybrid attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper, a new XMTC method, LAHA, is proposed. LAHA utilizes selfattention and interaction-attention to extract the semantic relation between words and labels, and an attenton fusion to construct the label-aware document representation. Extensive experiments on five benchmark datasets prove the superiority of LAHA by comparing with the state-of-the-art XMTC methods. In a nutshell, the novelty of LAHA lies in its providing a label-aware document representation that captures both document content and label structure, and has better discriminative ability than baselines. In real applications, more contents can be collected such as label content, which is proved to be helpful in XMTC <ref type="bibr" target="#b6">[7]</ref>. We therefore plan to extend the current model with such information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The architecture of LAHA. The solid green box indicates the self-attention process, the dashed red box represents interaction-attention process, and the dotted blue box indicates attention fusion to integrate self-attention and interaction-attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Ablation test on EUR-Lex and AAPD. SA=self-attention, IA=interactionattention, FA=attention fusion, LAHA=SA+IA+FA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Comparing XML-CNN, AttentionXML and LAHA on different label groups of sparse data(EUR-Lex ) in terms of P@τ (τ = {1, 3, 5}). F is frequency of label occurring in training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Heat map of label-aware attention weights obtained by (a) AttentionXML and (b) LAHA on an example document from Wiki30K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Summary of experimental datasets. N is the number of training documents, M is the number of testing documents, D is the number of features, L is the number of class labels,L is the average number of labels per document,N is the average number of documents per label.</figDesc><table><row><cell>datsets</cell><cell>N</cell><cell>M</cell><cell>D</cell><cell cols="3">LLN</cell></row><row><cell>AAPD [1]</cell><cell>54,840</cell><cell>1,000</cell><cell>69,399</cell><cell>54</cell><cell cols="2">2.41 2444.0</cell></row><row><cell cols="7">Kan-Shan Cup 2 2,799,967 200,000 411,721 1,999 2.3 3513.1</cell></row><row><cell>EUR-Lex [2]</cell><cell>11,585</cell><cell cols="4">3,865 171,120 3,956 5.3</cell><cell>15.6</cell></row><row><cell cols="7">Amazon-12K [4] 490,310 152,981 135,895 12,277 5.4 214.5</cell></row><row><cell>Wiki-30K [3]</cell><cell>12,959</cell><cell cols="4">5,992 100,819 29,947 18.7</cell><cell>8.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparing LAHA with four baselines in terms of various metrics on five benchmark datasets.</figDesc><table><row><cell>Datasets</cell><cell cols="6">Metric SLEEC PfastreXML XML-CNN AttentionXML LAHA</cell></row><row><cell></cell><cell>P @1</cell><cell>81.96%</cell><cell>82.35%</cell><cell>76.25%</cell><cell>83.02%</cell><cell>84.48%</cell></row><row><cell></cell><cell>P @3</cell><cell>57.48%</cell><cell>58.01%</cell><cell>54.34%</cell><cell>58.72%</cell><cell>60.72%</cell></row><row><cell>AAPD</cell><cell>P @5</cell><cell>38.99%</cell><cell>40.13%</cell><cell>37.84%</cell><cell>40.56%</cell><cell>41.19%</cell></row><row><cell></cell><cell cols="2">nDCG@3 77.65%</cell><cell>78.26%</cell><cell>72.01%</cell><cell>78.01%</cell><cell>80.11%</cell></row><row><cell></cell><cell cols="2">nDCG@5 81.59%</cell><cell>82.03%</cell><cell>76.40%</cell><cell>82.31%</cell><cell>83.70%</cell></row><row><cell></cell><cell>P @1</cell><cell>51.41%</cell><cell>52.29%</cell><cell>49.68%</cell><cell>53.69%</cell><cell>54.38%</cell></row><row><cell></cell><cell>P @3</cell><cell>32.81%</cell><cell>32.99%</cell><cell>32.27%</cell><cell>34.10%</cell><cell>34.60%</cell></row><row><cell>Kan-Shan Cup</cell><cell>P @5</cell><cell>24.29%</cell><cell>24.58%</cell><cell>24.17%</cell><cell>25.16%</cell><cell>25.88%</cell></row><row><cell></cell><cell cols="2">nDCG@3 49.32%</cell><cell>49.96%</cell><cell>46.65%</cell><cell>51.03%</cell><cell>51.70%</cell></row><row><cell></cell><cell cols="2">nDCG@5 49.74%</cell><cell>50.11%</cell><cell>49.60%</cell><cell>53.96%</cell><cell>54.65%</cell></row><row><cell></cell><cell cols="3">P @1 75.18% 73.03%</cell><cell>70.94%</cell><cell>71.89%</cell><cell>74.95%</cell></row><row><cell></cell><cell cols="3">P @3 61.67% 60.39%</cell><cell>56.02%</cell><cell>57.74%</cell><cell>61.48%</cell></row><row><cell>EUR-Lex</cell><cell>P @5</cell><cell>50.23%</cell><cell>49.69%</cell><cell>45.36%</cell><cell>47.35%</cell><cell>50.71%</cell></row><row><cell></cell><cell cols="2">nDCG@3 63.79%</cell><cell>62.51%</cell><cell>59.68%</cell><cell>61.29%</cell><cell>64.89%</cell></row><row><cell></cell><cell cols="2">nDCG@5 58.03%</cell><cell>57.72%</cell><cell>53.82%</cell><cell>56.71%</cell><cell>59.28%</cell></row><row><cell></cell><cell>P @1</cell><cell>93.49%</cell><cell>93.95%</cell><cell>93.15%</cell><cell>93.75%</cell><cell>94.87%</cell></row><row><cell></cell><cell>P @3</cell><cell>78.01%</cell><cell>78.33%</cell><cell>76.11%</cell><cell>78.36%</cell><cell>79.16%</cell></row><row><cell>Amazon-12K</cell><cell>P @5</cell><cell>62.09%</cell><cell>62.77%</cell><cell>60.51%</cell><cell>62.14%</cell><cell>63.16%</cell></row><row><cell></cell><cell cols="2">nDCG@3 86.89%</cell><cell>88.41%</cell><cell>86.75%</cell><cell>87.62%</cell><cell>89.13%</cell></row><row><cell></cell><cell cols="2">nDCG@5 84.53%</cell><cell>86.23%</cell><cell>84.01%</cell><cell>86.06%</cell><cell>87.57%</cell></row><row><cell></cell><cell cols="3">P @1 85.26% 82.81%</cell><cell>82.90%</cell><cell>81.98%</cell><cell>84.18%</cell></row><row><cell></cell><cell cols="3">P @3 73.91% 68.48%</cell><cell>67.46%</cell><cell>67.27%</cell><cell>73.14%</cell></row><row><cell>Wiki-30K</cell><cell>P @5</cell><cell>62.55%</cell><cell>59.93%</cell><cell>57.09%</cell><cell>56.43%</cell><cell>62.87%</cell></row><row><cell></cell><cell cols="3">nDCG@3 76.01% 72.15%</cell><cell>71.04%</cell><cell>70.77%</cell><cell>75.64%</cell></row><row><cell></cell><cell cols="3">nDCG@5 68.27% 63.83%</cell><cell>62.92%</cell><cell>62.35%</cell><cell>67.82%</cell></row><row><cell cols="2">Win times</cell><cell>6</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>19</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/HX-idiot/Hybrid Attention XML</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://manikvarma.org/downloads/XC/XMLRepository.html. 2 https://biendata.com/competition/zhihu/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SGM: sequence generation model for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3915" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient pairwise multilabel classification for large-scale problems in the legal domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E L</forename><surname>Mencia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frnkranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECML &amp; PAKDD</title>
		<meeting>of ECML &amp; PAKDD</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="50" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1202.5469</idno>
		<title level="m">Enhancing navigation on wikipedia with social tags</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: understanding rating dimensions with review text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM RecSyms</title>
		<meeting>of ACM RecSyms</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning for extreme multi-label text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 40th ACM SIGIR</title>
		<meeting>of the 40th ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">AttentionXML: extreme multilabel text classification with multi-label attention based recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01727</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Explicit Interaction Model towards Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09386</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sparse local embeddings for extreme multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="730" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extreme multi-label loss functions for recommendation, tagging, ranking &amp; other missing label applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGKDD</title>
		<meeting>of ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="935" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parabel: Partitioned label trees for extreme classification with application to dynamic search advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harsola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="993" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.08849</idno>
		<title level="m">Learning natural language inference with LSTM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep extreme multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM ICMR</title>
		<meeting>of ACM ICMR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="100" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A structured selfattentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Node2vec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGKDD</title>
		<meeting>of ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGKDD</title>
		<meeting>of ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-label prediction via compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="772" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-label output codes using canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AISTATS</title>
		<meeting>of AISTATS</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multilabel classification with principal label space transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2508" to="2542" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The landmark selection method for multiple output prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lebanon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6479</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust bloom filters for large multilabel classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Artieres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1851" to="1859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>A C-</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08630</idno>
		<title level="m">LSTM neural network for text classification</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural semantic encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">397</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrinboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Text classification improved by integrating bidirectional LSTM with two-dimensional max pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06639</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Combination of convolutional and recurrent neural network for sentiment analysis of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2428" to="2437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toutanova</forename><forename type="middle">K</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relation Classification via Multi-Level Attention CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
