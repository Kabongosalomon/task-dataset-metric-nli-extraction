<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BERT Based Multilingual Machine Comprehension in English and Hindi</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somil</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Khade</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BERT Based Multilingual Machine Comprehension in English and Hindi</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: • Computing methodologies → Information extraction; Additional Key Words and Phrases: Multilingual Question answering</term>
					<term>Answer Extraction</term>
					<term>Machine Compre- hension</term>
					<term>low-resourced languages</term>
					<term>BERT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multilingual Machine Comprehension (MMC) is a Question-Answering (QA) sub-task that involves quoting the answer for a question from a given snippet, where the question and the snippet can be in different languages. Recently released multilingual variant of BERT (m-BERT), pre-trained with 104 languages, has performed well in both zero-shot and fine-tuned settings for multilingual tasks; however, it has not been used for English-Hindi MMC yet. We, therefore, present in this article, our experiments with m-BERT for MMC in zero-shot, mono-lingual (e.g. Hindi Question-Hindi Snippet) and cross-lingual (e.g. English Question-Hindi Snippet) fine-tune setups. These model variants are evaluated on all possible multilingual settings and results are compared against the current state-of-the-art sequential QA system for these languages. Experiments show that m-BERT, with fine-tuning, improves performance on all evaluation settings across both the datasets used by the prior model, therefore establishing m-BERT based MMC as the new state-of-the-art for English and Hindi. We also publish our results on an extended version of the recently released XQuAD dataset, which we propose to use as the evaluation benchmark for future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The Task of Question Answering (QA) has been represented as an Information Retrieval (IR) task, in which questions asked in natural language by humans can be correctly answered automatically using structured databases, natural language documents or web pages. It is a lot different from a standard search engine approach as search engines output the relevant documents for a query while QA systems output the relevant information across those documents. Two major factors influence the quality of output of a QA system, first, the spectrum of information resource used to seek the result and, second, the understanding of that information to be able to output the requisite answer effectively. In the current age of information explosion, extracting exact information, even for a simple query, requires heavy resources for computation and evaluation. Therefore, most research in this domain focuses on the second factor. This factor of understanding the inherent contextual information is termed as Machine Reading Comprehension (MRC) in recent NLP literature.</p><p>The challenge of MRC is addressed by conversational AI systems in which QA agents need to provide concise, direct answers to user queries through conversation, extracted from a text given as context. This limits the focus on natural language understanding in a resource bounded manner and aids evaluation by restricting the scope of possible answers.</p><p>An important challenge faced by QA systems is the presence of knowledge base in different languages, thus restricting the source of monolingual systems. This can be efficiently handled with multilingual systems which can comprehend the syntactic and semantic structure of multiple languages simultaneously. These systems are known as Multilingual Question Answering (MQA) systems and are able to comprehend question in one language and answer using resources in another. MQA is a much needed requirement in IR, especially for non-Latin languages as most of the knowledge on the web is present is English, making search in foreign languages like Hindi, Japanese and Mandarin highly restricted. Multilingual Machine Comprehension(MMC) can then be called as a special case of MQA, where context is given as an excerpt from the available text and answer is required as a span of this text.</p><p>Various approaches have been employed to solve the MMC challenge across different language pairs, however, the recent multilingual variant of BERT <ref type="bibr" target="#b2">[Devlin et al. 2018</ref>] called m-BERT -a single, deep contextualized language model pre-trained from monolingual corpora in 104 languages -has given state-of-art-results across many language pairs like English-French and English-Japanese <ref type="bibr" target="#b11">[Siblini et al. 2019]</ref>. Surprisingly, m-BERT also performs well at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another <ref type="bibr" target="#b7">[Pires et al. 2019]</ref>. This encourages us to use m-BERT for English-Hindi MMC where it has not been tried for MQA setting yet. The latest state-of-the-art <ref type="bibr" target="#b3">[Gupta et al. 2019</ref>] uses a sequential model with joint training on English and Hindi to solve the problem.</p><p>One of the major drawbacks for research in English-Hindi MMC currently is the lack of a standard evaluation dataset. The prior state-of-the-art <ref type="bibr" target="#b3">[Gupta et al. 2019</ref>] used a translated subsection of the SQuAD <ref type="bibr" target="#b8">[Rajpurkar et al. 2018</ref>] dataset for evaluation, however, it is not publicly available and requires significant preprocessing as the instances are not in SQuAD-format and the translated answers have a lot of inconsistencies due to them being machine-translated. Though we managed to retrieve the dataset from the authors and preprocess it for comparison against the reported results, we do not recommend it for future research. DeepMind recently released XQuAD <ref type="bibr" target="#b0">[Artetxe et al. 2019</ref>] as a multilingual evaluation benchmark, which comprises a subsection of the SQuAD v1.1 <ref type="bibr" target="#b9">[Rajpurkar et al. 2016</ref>] development set translated into ten languages (including Hindi) by professional translators. We further extend this dataset by creating cross-lingual variants (e.g. English Question and Hindi Answer) and publish our results on this extended XQuAD dataset, which we propose as the new evaluation benchmark for future endeavours in this domain.</p><p>As part of our experiments, we use the preprocessed datasets of the previous model <ref type="bibr" target="#b3">[Gupta et al. 2019]</ref> as SQuAD <ref type="bibr" target="#b9">[Rajpurkar et al. 2016</ref>] style synthetic datasets, along with extended XQuAD dataset. We fine-tune m-BERT for MMC under zero-shot (no prior Hindi fine-tuning), Hindi monolingual and cross-lingual augmentation settings. Finally, all the model settings are evaluated for all possible multilingual use-cases (cross-and mono-) over the datasets. Although the zero-shot setting does not beat the benchmark, mono-and cross-lingual fine-tune settings show significant improvement over the baseline <ref type="bibr" target="#b3">[Gupta et al. 2019]</ref>. Major contributions of our work are therefore (i) establishing m-BERT based MMC as the new state-of-the-art for English and Hindi, and, (ii) standardizing evaluation in English-Hindi MMC by proposing our extended version of XQuAD as the new benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Although MQA has had its own track at the CLEF 1 since 2003 <ref type="bibr" target="#b6">[Magnini et al. 2006</ref>], much of the work in MQA has been centered around using Machine Translation to bring either of the two in the other language, followed by application of QA techniques <ref type="bibr" target="#b12">[Türe and Boschee 2016]</ref>. However, with the introduction of m-BERT 2 <ref type="bibr" target="#b2">[Devlin et al. 2018]</ref>, recent end-to-end neural QA systems have emerged <ref type="bibr" target="#b11">[Siblini et al. 2019]</ref> which have performed well even in zero-shot setting <ref type="bibr" target="#b7">[Pires et al. 2019]</ref>.</p><p>However, most of the research in the field of QA systems has happened in resource-rich languages such as English, which is difficult to port into other relatively low-resource languages. In the NLP literature on indic 3 languages, we see very few attempts in the direction of MQA. The first hindi monolingual QA system <ref type="bibr" target="#b10">[Sahu et al. 2012</ref>] was introduced in 2012, however, it was as recent as 2018 when the first MQA system for Hindi and English was released <ref type="bibr" target="#b4">[Gupta et al. 2018]</ref>. The system followed an IR-based approach of similarity computation and ranking for answering questions in either language, using comparable English and Hindi documents. The questions and documents formed their benchmark dataset called multi-domain multilingual QA (MMQA).</p><p>The current state-of-the-art English-Hindi MQA <ref type="bibr" target="#b3">[Gupta et al. 2019]</ref> by the same authors is perhaps the first MMC system in this domain. It applies a deep learning approach using Sequence models(GRU). Their approach relies on jointly training the model on mapped context-question pairs of both languages. First, it involves generation of a context snippet from given MMQA resources, using a graph-based language-independent algorithm which leverages the lexico-semantic similarity between the sentences. Then, it tries to learn the question representation by soft aligning words in both Hindi and English questions. Finally, the question-aware snippet representations are passed to an answer extraction layer, which extracts answer span from both the snippets simultaneously.</p><p>While both approaches are able to handle multilingual inputs without having to translate them into a single language, our approach differs significantly from <ref type="bibr" target="#b3">[Gupta et al. 2019]</ref>. Their approach relies on sequential modelling using bi-GRU <ref type="bibr" target="#b1">[Cho et al. 2014</ref>] which has become lack-lustre since BERT gained popularity as the state-of-the-art language model for MRC task. Our approach, on the other hand, leverages the multilingual contextual reasoning of m-BERT for MMC. None of the recent literature has used m-BERT to solve the MMC problem for Hindi-English pair (to the best of our knowledge). However, given the recent success of m-BERT in MQA in other languages <ref type="bibr" target="#b5">[Liu et al. 2019]</ref>, our aim seems like a good proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>Our approach is mainly built on top of m-BERT whose architecture is the same as that of monolingual BERT <ref type="bibr" target="#b2">[Devlin et al. 2018]</ref>, except for the corpus used in pre-training. Therefore, it is important to discuss MRC based fine-tune architecture of the original BERT and the reasons behind choosing m-BERT before discussing our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fine-tuning BERT for Machine Comprehension Task</head><p>In the MRC based fine-tuning task as shown in figure 1, input question and passage are tokenized and presented as a single packed sequence. Embedding E i and E ′ i stand for the question and answer tokens respectively, while T i and T ′ i denote their corresponding contextualized output embeddings respectively. Input embeddings incorporate the lexical embeddings, positional embeddings(to distinguish position) and segment embeddings (to differentiate question from context tokens). BERT performs contextual integration such that each output embedding is question-aware and [CLS] is a special symbol added in front of every input example whose corresponding output embedding represents contextualized embedding for the entire sequence, while [SEP] is a special separator token (e.g. separating questions/answers). Each output embedding represents the contextualized embedding of corresponding input token which is passage and question-aware. Source: <ref type="bibr" target="#b2">[Devlin et al. 2018]</ref> . context-aware. Additional component introduced during fine-tuning is the output classification layer characterized by the start vector S ∈ R H and an end vector E ∈ R H .</p><p>Answer span is decided by calculating the softmax on all the passage output embeddings to detect the start word index and end word index of the answer from the given paragraph. So, the probability of a word T ′ i to be the start index is P i = e S.T ′ i j e S.T ′ j . The similar formula is used to detect the most probable end of the answer span. The net score for every possible span from position i to position j is denoted as S.T ′ i + E.T ′ j . So, the final prediction for the answer is the span for which this combined score is maximum. For the training, summation of log-likelihood of the correct start and end positions is used as an objective function.</p><p>Although the above MRC approach provides contiguous span, it can be used for non-contiguous spans by finding the span that maximizes the F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Why m-BERT as a Base Model?</head><p>M-BERT is a multilingual variant of BERT, with exactly the same architecture and APIs. Both multilingual and monolingual language model variants are pretrained, in an unsupervised manner, using the same Masked Language Modelling(MLM) and Natural Language Inference(NLI) approaches outlined in <ref type="bibr" target="#b2">[Devlin et al. 2018</ref>]. However, while the original monolingual BERT was trained in English corpus of Wikipedia and BooksCorpus, m-BERT is trained on a concatenation of mono-lingual Wikipedia corpora from 104 languages, thus possessing multilingual text understanding.</p><p>It has been researched in <ref type="bibr" target="#b7">[Pires et al. 2019</ref>] that m-BERT enables a very straightforward approach to zero-shot cross-lingual model transfer. It is easier to fine-tune the model for a specific task in any language (known by the model) with the help of data from a readily available language, and evaluating that task in another desired, but less-resourced language. This helps to generate models which generalize information across languages.</p><p>There might be some apprehension with zero-shot cross lingual transfer between English and Hindi as both have different scripts (Latin and Devanagiri respectively). However, it has been empirically shown that m-BERT is also able to transfer between languages written in different scripts <ref type="bibr" target="#b11">[Siblini et al. 2019</ref>], even though they may have zero lexical overlap, thus indicating that m-BERT captures multilingual representations <ref type="bibr" target="#b13">[Wu and Dredze 2019]</ref>. Therefore, we have leveraged the fact that m-BERTâĂŹs multilingual representation is able to map learned structures onto new vocabularies and can work well in English-Hindi setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED APPROACH</head><p>We train m-BERT on MMC task for English and Hindi in both zero-shot and fine-tuned settings. These fine-tuned models are then evaluated in cross-lingual and mono-lingual fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Fine-tune Settings</head><p>These settings describe the model state before evaluation i.e. how the model was fine-tuned.</p><p>• Zero Shot baseline i.e. fine-tuning the model for MMC on a language different from the one in which it is evaluated. For this, the model is fine-tuned on the SQuAD dataset <ref type="bibr" target="#b9">[Rajpurkar et al. 2016</ref>] (in English) and then evaluated on instances with Hindi context(passage). This helps to test whether m-BERT performs well without any prior fine-tuning in Hindi. • Hindi mono-lingual QA augmentation: In this approach, we augment the zero-shot MMC model from the first approach, with the Hindi QA training set (mono-lingual QA with both question and answer in Hindi). Although this ensures the model is trained in Hindi reading comprehension, it does not explicitly establish any cross-lingual relationship between an English QA instance and its corresponding Hindi counterpart. • Hindi cross-lingual QA augmentation: This setting is a continuation to the second approach that involves further augmenting the fine-tuned model with a cross-lingual training set, for example, the set with questions in English and answers in Hindi. This may help achieve cross-lingual relationship between the languages. The cross-lingual variant to train is explored in the experiment section.</p><p>Aiming on different attributes of m-BERT in each fine-tune setting can also help identify which aspect is already handled by the m-BERT intrinsically, for instance, model may be capable of capturing cross-lingual ability between the two languages by itself, in which case, the second and the third settings would not have substantial difference in their cross-lingual evaluation.</p><p>As an additional approach, we had also thought of fine-tuning on joint input of English and Hindi QA similar to <ref type="bibr" target="#b3">[Gupta et al. 2019</ref>]. The approach was aimed at explicitly instilling translational ability in the model for both the languages, as both the text and its translation would be input simultaneously. However, as we would see in the results, our proposed training flow achieved state-of-the-art performance, indicating that m-BERT can handle multilingual QA without explicit training in translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Settings</head><p>In order to fully evaluate an MMC model, we need to evaluate the model on all possible multilingual settings (cross and mono) using English and Hindi. Following are the different evaluation settings. These settings are similar to the ones used in <ref type="bibr" target="#b3">[Gupta et al. 2019]</ref>, expect for the ones which require joint answering in both languages.</p><p>• Q E -P E : Both question and passage are in English. This setting tests English monolingual comprehension ability of the model. • Q H -P H : Both question and passage are in Hindi. This setting tests Hindi monolingual comprehension ability of the model.</p><p>• Q E -P H : The question is in English and the answer exists in the Hindi passage. This setting tests cross-lingual comprehension capability of the model. • Q H -P E : The question is in Hindi and the answer exists in the English snippet. This setting too tests the cross-lingual comprehension capability of the model. For metrics, we would rely on the Exact Match (EM) and F1 score. EM measures the percentage of predictions that match any one of the ground truth answers exactly. F1 score is a looser metric that measures the average overlap between the prediction and ground truth answer. <ref type="bibr" target="#b3">[Gupta et al. 2019]</ref> evaluates its model on a translated subsection of the SQuAD dataset and the MMQA dataset <ref type="bibr" target="#b4">[Gupta et al. 2018</ref>]. Since it is our baseline model, we use the same datasets for our evaluation as well. Besides, we also evaluate our model on the XQuAD dataset as benchmark for future research. SQuAD and its translated version are also used for fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DATA</head><p>M-BERT has predefined functionality for MRC on SQuAD dataset. To reuse it, we needed to transform the datasets to SQuAD format as outlined in <ref type="figure" target="#fig_2">Figure 2</ref>. Besides, our evaluation approach defined in section 4.2 requires us to create test variants for all different multilingual settings. Finetuning too needs Q H -P H and Q E -P H variants of the training set. Therefore, the datasets need preprocessing as well.  The authors of <ref type="bibr" target="#b3">[Gupta et al. 2019</ref>] created a translated SQuAD subset containing tuples of the format (question, passage, answer, start token index, end token index) for both English and Hindi. These tuples are generated using randomly chosen 18,454 SQuAD instances and their corresponding Hindi machine-translation (using Google translator). The dataset has a train-validation-test split of 10,454 to 2K to 6K respectively. Since the site 4 stated in their paper did not contain the dataset, we had to fetch it directly from the authors. 5.1.1 Preprocessing. : The translated SQuAD dataset had numerous inconsistencies which made creation of variants difficult, for example, the given answer text had extra or missing characters which prevented substring-match with the given context, answer start indexes did not correspond to exact location of answer text, and there were no question IDs or titles to tie to original text. To mitigate these issues, the text was sanitized using pattern match-and-replace style sanity checks (e.g. removing space before (,), adding dots in abbreviations, etc.), correct index of exact start index of the answer text was located using substring find operation, and questions in original SQuAD 2.0 dataset were looked up to locate question IDs and regroup disjoint questions using context passage as pivot. Training split helped create fine-tuning variants, while test split helped create evaluation variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SQuAD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MMQA</head><p>Multi-domain Multilingual QA (MMQA) dataset 5 was released by <ref type="bibr" target="#b4">[Gupta et al. 2018]</ref> as the first benchmark dataset for English-Hindi MQA. It consists of around 5,500 QA pairs formulated by human annotators over 500 articles curated from 6 different domains, with comparable English and Hindi document for each article. Questions may be factoid or short descriptive type. Answers can be abstractive (may not be exact quotations from the text, or contiguous spans of text), however, due to majority of questions being factoid, answers are generally contiguous spans from articles. <ref type="bibr" target="#b3">[Gupta et al. 2019</ref>] use a subset of about 3960 QA pairs from MMQA for evaluation and generate snippet for the pairs using their novel algorithm. We evaluate our model on this subset alongwith the generated snippets and compare our results against the ones reported in their paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Preprocessing:</head><p>The given set of QA pairs with their snippets additionally contains a field identifying the instance as English only, Hindi only or both. We use this field to bucket the instances into mono and cross lingual variants. While transforming the instances to SQuAD format, we only populate the answer text and not the start index, as the answers are not exact substrings. The SQuAD evaluation script does not use start index for calculating F1 and EM scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">XQuAD</head><p>XQuAD (Cross-lingual Question Answering Dataset) 6 is a benchmark dataset published by Deep-Mind for evaluating multilingual QA performance. Released as part of <ref type="bibr" target="#b0">[Artetxe et al. 2019]</ref>, the dataset consists of monolingual versions of a subset of 240 paragraphs and 1190 QA pairs from the development set of SQuAD v1.1 <ref type="bibr" target="#b9">[Rajpurkar et al. 2016</ref>] translated into ten languages including Hindi. It is the most appropriate benchmark evaluation set for English-Hindi MMC, because:</p><p>(1) The translations are accomplished by human translators, making the dataset more reliable for evaluation than the previous two datasets.</p><p>(2) Being a translated version of SQuAD, the dataset is compliant to MRC task API of BERT, without any additional preprocessing.</p><p>(3) Universal acceptance of SQuAD as default MRC benchmark implies that XQuAD can also be considered the de-facto benchmark dataset for MMC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Extension:</head><p>The dataset only contains monolingual variants of the SQuAD subset, however for complete evaluation of multilingual ability, it is important to test on cross-lingual variants as well, where the question and the context are in different languages. We create these cross-lingual variants using the monolingual variants of English and Hindi which are already present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENT</head><p>The experiment was conducted for all the approaches outlined in section 4 and SQuAD-style variants of the datasets were created by preprocessing as mentioned in section 5.</p><p>6.0.1 Training phase. For each training setup, 10 parallel runs of m-BERT fine-tuning were conducted for 30,000 steps with hyper-parameters as defined in table 1.</p><p>• Zero-shot:, BERT-Base, Multilingual Cased model 2 (104 languages, 12-layer, 768-hidden, 12heads, 110M parameters) was fine-tuned on the entire training set of original SQuAD 2.0 dataset, containing 150K QA pairs. No fine-tuning on Hindi was involved in this setup. • Hindi monolingual QA augmentation: Each zero-shot checkpoint was further fine-tuned on Q H -P H variant of the training split of translated SQuAD dataset, which consists of around 10.5K QA pairs. There was a one-to-one mapping between a zero-shot checkpoint and the new checkpoint, thus maintaining the same number of checkpoints for this setup. • Hindi cross-lingual QA augmentation: Each checkpoint after Hindi monolingual QA augmentation was further fine-tuned for this setup on Q E -P H variant of the training split of translated SQuAD dataset. Similar to the previous setup, one-to-one mapping was maintained between the checkpoints. It is important to note that the choice of cross-lingual variant for augmentation (Q E -P H or Q H -P E ) was completely arbitrary and any of them would have given better results as we see in the later section. For each training setup, all the checkpoints were evaluated on every multilingual evaluation variant of every dataset, thus creating a total of 10 checkpoints × 4 variants × 3 datasets = 120 prediction sets for each training setup. SQuAD's evaluation script 7 was used for generating F1 and EM scores on each such prediction set. Finally, averaged scores across all checkpoints for each dataset, train and evaluation setting are reported in the later section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS</head><p>As outlined in section 6.0.2, we compute F1 and EM scores for all pairs of fine-tune and evaluation settings on all datasets, averaged across checkpoints generated in parallel. <ref type="table" target="#tab_1">Table 2 provides the  evaluation results on translated SQuAD dataset while table 3</ref> provides results on the MMQA dataset. The baseline row in both tables provides the benchmark scores as reported in the paper <ref type="bibr" target="#b3">[Gupta et al. 2019]</ref>. We also publish our results on our proposed benchmark dataset, XQuAD, in table 4. Due to the unavailability of the source of the current state-of-the-art model, we could not compare our model with theirs on this dataset. In all the tables, with Q H -P H Aug refers to mono-lingual Hindi QA augmentation while with Q E -P H Aug refers to cross-lingual QA augmentation.   examples and also provide the answer reported by the baseline model in their paper <ref type="bibr" target="#b3">[Gupta et al. 2019]</ref>. The prediction of the proposed model comes from the best performing fine-tune setting on that dataset and evaluation variant. As an example, for SQuAD dataset and Q H -P H variant, we use a sample prediction from monolingual augmentation setting as it gives the best results. Besides, we have made an effort to provide sample predictions on all datasets (mentioned along with the question) and all multilingual variants (mono-and cross-lingual). Our examples also cover a diversity of question types-e.g. Q1, Q2 and Q4 are factoid (what and who types), while Q3, Q4 and Q6 are short-descriptive (why) type. Our examples also cover questions with exact match (Q1, Q2 and Q4), partial match (Q3) and mismatch (Q5, Q6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION</head><p>Following inferences can be drawn by analysing the quantitative and qualitative results published in section 7:</p><p>(1) Augmented m-BERT beats the baseline sequential model on all datasets (SQuAD and MMQA). Zero-shot at m-BERT alone can be insufficient for the task, as evident in SQuAD results on all Hindi based variants which are way below the baseline results on those evaluation settings.</p><p>(2) Explicitly fine-tuning m-BERT on a multilingual setting improves performance in that setting, but mildly degrades on the others. This is congruent with our motivation behind the fine-tune settings defined in section 4.1. Therefore, monolingual Hindi QA augmentation improves performance on Q H -P H while cross-lingual augmentation improves on cross-lingual evaluations on all datasets. Cross-lingual augmentation marginally degrades the mono-lingual results on both English and Hindi. (3) The size of training corpus used for fine-tuning impacts the final results. This is corroborated by the fact that the net improvement brought by mono-or cross-lingual augmentations over their previous state results are way less compared to the improvement brought by SQuAD fine-tuning in zero-shot for Q E -P E , since SQuAD with 100K instances has 10 times more instances than training set for augmentations (10.5K). This also implies that m-BERT has potential for better results on Hindi with a larger training set. (4) The choice of the cross-lingual variant to augment with does not impact our motive: Although we choose Q E -P H to fine-tune in cross-lingual augmentation setup, both the cross-lingual results (Q E -P H and Q H -P E ) are improved instead of reduction observed in other variants. <ref type="table">Table 5</ref>. Qualitative results of the best performing fine-tune setups with each example containing the question, snippet (with emphasized gold answer), and the predicted answers by both our proposed model and the baseline.</p><p>It does, however, impact the net improvement in results. Considering that our prime goal was to show the potential of m-BERT, either of the variants hold good. (5) M-BERT based MMC performs well in cases of anaphora and cataphora resolution. In Q2, the model is able to identify the antecedent "Pumban Island" over "It", while the baseline model gets stuck with the local reference, probably due to its sequential structure. In Q4, the model prefers to answer subsequent reference "Robert Lane and Benjamin Vail" over the phrase "two businessmen". (6) Qualitatively, performance on factoid questions is much better than that of descriptive/reasoning questions. Of the six questions, all factoid questions have exact match while only one descriptive question matches partially. This may be due to training set being skewed towards factoid type questions. An analysis of the translated SQuAD set reveals that descriptive questions constitute only 10% (1.3K of the 10.5K) of the entire dataset. SQuAD, its parent set, contains only 11.9% of such questions <ref type="bibr" target="#b9">[Rajpurkar et al. 2016</ref>]. (7) An error analysis of the descriptive questions shows that although the results are incorrect, the model is able to understand the context of the question and provides a phrase that paraphrases the question. While Q3 prediction is conceptually correct, "martyred" for "execution" in Q5 and literally restating the question in translation in Q6 show that model is able to develop a multilingual understanding but falls short in reasoning for the question. Since training set lacks such contextual reasoning questions, it would be a good direction to research whether m-BERT is capable of tackling such questions, given enough fine-tuning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Machine comprehension based fine-tuning procedure for BERT. Apart from output layers, the same architecture can be used by different downstream tasks, initializing all layers with the same pretrained model parameters. During fine-tuning, all parameters are fine-tuned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Standford</head><label></label><figDesc>Question Answering Dataset (SQuAD) is a monolingual, single-turn MRC dataset released by the Stanford NLP group consisting of questions posed by crowd workers on a set of Wikipedia articles. It is the first large-scale, high quality dataset released in MRC that greatly ushered research in Machine Comprehension. Version 1.0 [Rajpurkar et al. 2016] consists of 100K+ questions from a variety of answer types while version 2.0 [Rajpurkar et al. 2018] lays emphasis on unanswerable questions with over 50K additional questions. Figure 2 provides the format of SQuAD 2.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>An example of the SQuAD dataset. Each dataset contains an array of instances, each belonging to a Wikipedia article with title and paragraphs. Each paragraph JSON consists of an excerpt and a set of question answers pertaining to that context. Each question has a unique id, answer(text with start index). An unanswerable question is marked as impossible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Important hyperparameter values for m-BERT fine-tuning and evaluation.</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Lower the case for input</cell><cell>False</cell></row><row><cell cols="2">Max (sequence, query, answer) length 384, 64, 30</cell></row><row><cell>Train, predict batch sizes</cell><cell>12,8</cell></row><row><cell>Learning rate, warmup proportion</cell><cell>5e-5, 0.1</cell></row><row><cell>Number of epochs</cell><cell>3.0</cell></row><row><cell>6.0.2 Evaluation phase.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="5">. Results of evaluation on test set variants of translated SQuAD dataset (dataset 5.1) for each fine-tune</cell></row><row><cell cols="5">setup, compared against benchmark results [Gupta et al. 2019]. All values are averaged across results on 10</cell></row><row><cell cols="5">checkpoints. The highest score in each evaluation category is highlighted for ease of comparison.</cell></row><row><cell cols="4">(a) EM scores for all the experimental settings</cell><cell></cell></row><row><cell>Model settings</cell><cell cols="4">Q E -P E Q E -P H Q H -P E Q H -P H</cell></row><row><cell>Baseline</cell><cell>53.15</cell><cell>45.34</cell><cell>44.19</cell><cell>51.34</cell></row><row><cell>Zero Shot</cell><cell>88.76</cell><cell>30.08</cell><cell>28.11</cell><cell>31.23</cell></row><row><cell>with Q H -P H Aug. with Q E -P H Aug.</cell><cell>81.76 79.14</cell><cell>42.36 52.93</cell><cell>49.07 51.10</cell><cell>51.60 50.67</cell></row><row><cell cols="4">(b) F1 scores for all the experimental settings</cell><cell></cell></row><row><cell>Model settings</cell><cell cols="4">Q E -P E Q E -P H Q H -P E Q H -P H</cell></row><row><cell>Baseline</cell><cell>57.29</cell><cell>50.24</cell><cell>48.21</cell><cell>53.87</cell></row><row><cell>Zero Shot</cell><cell>94.56</cell><cell>38.23</cell><cell>37.15</cell><cell>41.08</cell></row><row><cell>with Q H -P H Aug. with Q E -P H Aug.</cell><cell>90.24 87.96</cell><cell>52.32 64.51</cell><cell>56.89 59.19</cell><cell>63.59 62.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results of evaluation on test set variants of MMQA dataset (dataset 5.2) for each fine-tune setup, compared against benchmark results<ref type="bibr" target="#b3">[Gupta et al. 2019]</ref>. All values are averaged across results on 10 checkpoints. The highest score in each evaluation category is highlighted for ease of comparison.</figDesc><table><row><cell cols="4">(a) EM scores for all the experimental settings</cell><cell></cell></row><row><cell>Model settings</cell><cell cols="4">Q E -P E Q E -P H Q H -P E Q H -P H</cell></row><row><cell>Benchmark</cell><cell>44.78</cell><cell>34.68</cell><cell>33.41</cell><cell>41.46</cell></row><row><cell>Zero Shot</cell><cell>53.04</cell><cell>37.23</cell><cell>34.69</cell><cell>34.11</cell></row><row><cell cols="2">with Q H -P H Aug. 55.64 with Q E -P H Aug. 54.32</cell><cell>44.37 45.12</cell><cell>47.66 47.71</cell><cell>46.41 45.71</cell></row><row><cell cols="4">(b) F1 scores for all the experimental settings</cell><cell></cell></row><row><cell>Model settings</cell><cell cols="4">Q E -P E Q E -P H Q H -P E Q H -P H</cell></row><row><cell>Benchmark</cell><cell>50.27</cell><cell>37.89</cell><cell>37.02</cell><cell>48.14</cell></row><row><cell>Zero Shot</cell><cell>71.95</cell><cell>51.51</cell><cell>52.33</cell><cell>50.97</cell></row><row><cell cols="2">with Q H -P H Aug. 74.43 with Q E -P H Aug. 73.26</cell><cell>61.39 63.75</cell><cell>65.06 64.37</cell><cell>64.34 63.81</cell></row><row><cell>7.1 Qualitative Result</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Table 5 provides example predictions containing the context snippet (with gold answer empha-</cell></row><row><cell cols="5">sized), question and proposed model's predicted answer. Wherever possible, we try to use common</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Results of evaluation on test set variants of XQuAD dataset (dataset 5.3) for each fine-tune setup. All values are averaged across results on 10 checkpoints. The highest score in each evaluation category is highlighted for ease of comparison.(a) EM scores for all the experimental settings.</figDesc><table><row><cell>Model settings</cell><cell cols="4">Q E -P E Q E -P H Q H -P E Q H -P H</cell></row><row><cell>Zero Shot</cell><cell>66.97</cell><cell>18.49</cell><cell>22.27</cell><cell>30.92</cell></row><row><cell cols="2">with Q H -P H Aug. 67.05 with Q E -P H Aug. 64.29</cell><cell>33.87 44.71</cell><cell>39.66 41.01</cell><cell>46.47 45.63</cell></row><row><cell cols="4">(b) F1 scores for all the experimental settings.</cell><cell></cell></row><row><cell>Model settings</cell><cell cols="4">Q E -P E Q E -P H Q H -P E Q H -P H</cell></row><row><cell>Zero Shot</cell><cell>76.61</cell><cell>24.05</cell><cell>31.06</cell><cell>40.85</cell></row><row><cell cols="2">with Q H -P H Aug. 78.83 with Q E -P H Aug. 76.51</cell><cell>45.79 57.31</cell><cell>49.42 51.04</cell><cell>61.08 59.80</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.clef-initiative.eu/home 2 https://github.com/google-research/bert/blob/master/multilingual.md 3 https://en.wikipedia.org/wiki/Indo-Aryan_languages</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://www.iitp.ac.in/~ai-nlp-ml/resources.html 5 https://github.com/deepaknlp/MMQA 6 https://github.com/deepmind/xquad</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://rajpurkar.github.io/SQuAD-explorer/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>Our experiments with m-BERT for Multilingual Machine Comprehension task showed that m-BERT, augmented with English and Hindi multilingual QA (fine-tuned in order), significantly outperforms the current state-of-the-art sequential model <ref type="bibr" target="#b3">[Gupta et al. 2019]</ref> for English and Hindi. We also adapted the recently released evaluation dataseet, XQuAD, for MQA by extending it with crosslingual English-Hindi variants. Being a widely accepted and human translated dataset, we propose this extended version of XQuAD as evaluation benchmark and publish our results on this dataset as baseline for future research in this domain. Our analysis of this BERT variant may be beneficial for study on cross-lingual information retrieval involving low-resource languages like Hindi.</p><p>Our research has two open-ended directions which can be pursued further. First, our analysis revealed that larger multilingual training set can improve results, thus indicating a scope for improvement in multilingual comprehension of m-BERT for Hindi QA. Since no such large scale multilingual dataset exists for English and Hindi, future efforts can be made to extend XQuAD for training as well, by incorporating the entire SQuAD corpus. SQuAD, however, is severely limited in its resources for descriptive (non-factoid) questions. Therefore, another important research direction can be to evaluate m-BERT on a dataset which has sufficient multilingual descriptive questions to fine-tune on.</p><p>We hope that our attempts towards setting a transfer learning based approach as the new baseline and standardization of evaluation will help surge this otherwise dormant domain of English-Hindi MMC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11856</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<ptr target="http://arxiv.org/abs/1406.1078" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Deep Neural Network Framework for English Hindi Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<idno type="DOI">10.1145/3359988</idno>
		<ptr target="https://doi.org/10.1145/3359988" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Asian Low-Resour. Lang. Inf. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MMQA: A multi-domain multi-lingual question-answering framework for English and Hindi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surabhi</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">XQA: A Cross-lingual Open-domain Question Answering Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1227</idno>
		<ptr target="https://doi.org/10.18653/v1/P19-1227" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2358" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Multilingual Question Answering Track at CLEF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Aunimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christelle</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petya</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Maarten De Rijke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutcliffe</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2006/pdf/816_pdf.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC&apos;06). European Language Resources Association (ELRA)</title>
		<meeting>the Fifth International Conference on Language Resources and Evaluation (LREC&apos;06). European Language Resources Association (ELRA)<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">How multilingual is Multilingual BERT? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01502</idno>
		<ptr target="http://arxiv.org/abs/1906.01502" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Know What You Don&apos;t Know: Unanswerable Questions for SQuAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<ptr target="http://arxiv.org/abs/1806.03822" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">SQuAD: 100, 000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<ptr target="http://arxiv.org/abs/1606.05250" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Prashnottar: A Hindi Question Answering System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shriya</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nandkishor</forename><surname>Vasnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devshri</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Science &amp; Information Technology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2012-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Axel Lavielle, and Cyril Cauchois</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wissam</forename><surname>Siblini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlotte</forename><surname>Pasqual</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04659</idno>
	</analytic>
	<monogr>
		<title level="m">Multilingual Question Answering from Formatted Text applied to Conversational Agents</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning to Translate for Multilingual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Türe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Boschee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08210</idno>
		<ptr target="http://arxiv.org/abs/1609.08210" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
