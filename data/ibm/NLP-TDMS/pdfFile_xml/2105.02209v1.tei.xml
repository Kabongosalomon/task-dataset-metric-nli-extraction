<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Physically Inspired Dense Fusion Networks for Relighting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirsaeed</forename><surname>Yazdani</surname></persName>
							<email>amiryazdani@psu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantong</forename><surname>Guo</surname></persName>
							<email>tiantong@ieee.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Monga</surname></persName>
							<email>vmonga@engr.psu.edu</email>
						</author>
						<title level="a" type="main">Physically Inspired Dense Fusion Networks for Relighting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image relighting has emerged as a problem of significant research interest inspired by augmented reality applications. Physics-based traditional methods, as well as black box deep learning models, have been developed. The existing deep networks have exploited training to achieve a new state of the art; however, they may perform poorly when training is limited or does not represent problem phenomenology, such as the addition or removal of dense shadows. We propose a model which enriches neural networks with physical insight. More precisely, our method generates the relighted image with new illumination settings via two different strategies and subsequently fuses them using a weight map (w). In the first strategy, our model predicts the material reflectance parameters (albedo) and illumination/geometry parameters of the scene (shading) for the relit image (we refer to this strategy as intrinsic image decomposition (IID)). The second strategy is solely based on the black box approach, where the model optimizes its weights based on the ground-truth images and the loss terms in the training stage and generates the relit output directly (we refer to this strategy as direct). While our proposed method applies to both one-to-one and any-to-any relighting problems, for each case we introduce problemspecific components that enrich the model performance: 1) For one-to-one relighting we incorporate normal vectors of the surfaces in the scene to adjust gloss and shadows accordingly in the image. 2) For any-to-any relighting, we propose an additional multiscale block to the architecture to enhance feature extraction. Experimental results on the VIDIT 2020 and the VIDIT 2021 dataset (used in the NTIRE 2021 relighting challenge) reveals that our proposal can outperform many state-of-the-art methods in terms of wellknown fidelity metrics and perceptual loss.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image enhancement problems have experienced significant recent research activity inspired by the proliferation of mobile devices and the availability of training data designed for particular enhancement goals. Image relighting, which is changing the illumination settings of an image, is one of these applications that has attracted significant at-tention. Another important reason for this growth is the development of augmented reality (AR), virtual reality (VR) based services such as online shopping, online teaching, and games, where the gloss and shadows of the scene should be adjusted based on the change in direction of light and its color temperature. On the other hand, controlling the light source in the level of the photography skills of an amateur user is not trivial, which in turn necessitates the development of techniques for relighting. From a physical viewpoint, the illumination of an image depends on many factors including the material reflectance property, geometry of the objects in the image, and the number of light sources. For a given light source (L ωi ) and with Lambertian reflectance assumption, the image formation follows the rendering rule <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref>:</p><formula xml:id="formula_0">L ωo = ωi∈Ωo f (ω i , ω o )L ωi &lt; n, ω i &gt; dω i<label>(1)</label></formula><p>Here ω i and ω o denote the input and output light direction relative to the surface normal n. L ωi and L ωo are the incident and reflected lights, and f (., .) is the bidirectional reflectance distribution function (BRDF) and &lt; n, ω i &gt; is the attenuation factor. This equation is usually simplified by assuming: A = f (ω i , ω o ) (constant) and S = ωi∈Ωo L ωi &lt; n, ω i &gt; dω i . Where A denotes albedo and preserves the reflectance properties of the objects and S denotes shading, which holds the illumination properties of the image. The simplified equation is often used as the rendering rule as the original equation is computationally complex. Deep learning methods have achieved state-of-the-art results for a vast variety of imaging inverse problems. High dynamic range (HDR) imaging algorithms <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b22">23]</ref> focus on increasing the local contrast of a low dynamic range image. Dehazing algorithms <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b47">48]</ref> seek for removing the haze artifacts caused by floating particles in the atmosphere. Shadow removal <ref type="bibr" target="#b25">[26]</ref> and light enhancement <ref type="bibr" target="#b14">[15]</ref> methods focus on enhancing the lighting and removing the artifacts in the image with the existing light source. While all the aforementioned methods deal with adjusting the parameters affected by the lighting of the image, they don't manipulate actual illumination parameters and can not deal with complexities of relighting. Therefore, we are still in the early stages of relighting research. Existing algorithms focus on particular objects such as portraits or faces <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b2">3]</ref>, hence lacking the versatility to generalize to other classes of objects (e.g buildings). Deep learning methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b8">9]</ref> for relighting are versatile; however, they show poor performance in extreme cases of shadow removal/addition as they often ignore the physics of the problem. Our central contribution is to generate relighted images with new illumination settings via two different strategies and subsequently fuse them using a weight map (w) • In the first strategy, the model predicts the material reflectance parameters (albedo) and illumination/geometry parameters of the scene (shading) for the relit image and constructs the relit image based on the simplified rendering rule Eq. 1. (we refer to this strategy as intrinsic image decomposition (IID).) • The second strategy follows a black box approach, where the model optimizes its weights based on the ground-truth images and the loss terms in the training stage and generates the relit output directly (we refer to this strategy as direct). Our proposed method exploits insights from two different sides of the literature. Moreover, since both approaches have a shared encoder, owing to the virtue of joint optimization, they can benefit the shared features the other one induces the encoder to extract as well. In this work, we are addressing the problem of relighting under two categories: 1) One-to-one: The objective is to change the color temperature and angle of the light source (referred to as illumination parameters) from one specific setting to another one. 2) Any-to-any: The illumination parameters should change from an arbitrary setting according to the illumination of a given guide image. While our proposed method applies to both one-to-one and any-to-any relighting problems, for each case, we propose specific innovations that enrich the model performance:</p><p>• For one-to-one relighting we incorporate normal vectors of the surfaces in the scene to adjust gloss and shadows accordingly in the image. This in particular helps boost the performance of the neural network model in the cases where the complicated geometry of the scene requires removing dense shadows or adding shadows to highly glossed regions. We refer to our network for one-to-one relighting as One-to-one Intrinsic Decomposition-Direct RelightNet (OIDDR-Net). • For any-to-any relighting, we propose an additional multiscale block to the architecture to enhance feature extraction. This block benefits from analyzing the input RGB image (and depth map) in three different dimension levels. Using dense residual blocks and residual global blocks in each level, it provides multiscale features for the subsequent layers. We refer to our network for any-to-any relighting as Any-to-any Multiscale Intrinsic-Direct RelightNet (AMIDR-Net).</p><p>Our experimental results on VIDIT 2020/2021 dataset prove that our proposed method can outperform state-ofthe-art in terms of fidelity metrics and perceptual loss. Our OIDDR-Net ranked second and AMIDR-Net ranked among top five teams in NTIRE 2021 depth guided image relighting challenge <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>The existing works in the area of image relighting can generally be divided into two groups of deep learning-based methods and conventional image processing methods. In the line of conventional methods and starting with models proposed for inverse rendering <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref> and shape estimation <ref type="bibr" target="#b45">[46]</ref>, there have been relighting works based on decomposing the image into its reflectance, illumination, and geometry components. Duchêne et al. <ref type="bibr" target="#b9">[10]</ref> utilize a set of outdoor multiview scenes along with the sunlight direction to achieve albedo and shading decomposition for relighting. Wen et al. <ref type="bibr" target="#b52">[52]</ref> develop a technique in which the estimated radiance environment maps, along with spherical harmonics, are used for face relighting. Other algorithms <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref> treat relighting as approximating the light transport function of the scene to generate the new illumination settings using the input lighting parameters. While these methods construct physically realistic models for relighting, they rely on explicit illumination parameters of the scene or multiview datasets. This is considered a bottleneck for them. While conventional methods mostly focus on physical aspects of the problem, deep learning-based algorithms rely on the capability of neural networks, along with typically large training data set, in developing a mapping function between two image domains. Methods proposed in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref> view the relighting as an image-to-image translation problem and make use of Generative Adversarial Networks (GANs) for image relighting. Xu et al. <ref type="bibr" target="#b46">[47]</ref> use five images to manipulate the illumination under predefined light direction. Inspired by inverse rendering, numerous works incorporate neural networks to estimate image illumination and geometry parameters. Yu et al. <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b33">34]</ref> view relighting as a fruit of regressing the albedo, shading, and light coefficients of the input RGB image using fully convolutional networks. Face relighting methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b53">53]</ref> combine the capabilities of neural networks and the physics of relighting, which is customized for face images. Although introducing neural networks has shown promising results for the relighting problem, the appropriate dataset yet seems to be a challenging aspect. In the past years, IIW <ref type="bibr" target="#b1">[2]</ref> and MIP SINTEL <ref type="bibr" target="#b3">[4]</ref> have been introduced for intrinsic decomposition and optical flow analysis, respectively. More recently, Helou et al. proposed a virtual image dataset for illumination transfer <ref type="bibr" target="#b10">[11]</ref>, which formulates relighting into one-toone and any-to-any problems. Along the line of scene re- lighting and illumination estimation challenge in AIM 2020 <ref type="bibr" target="#b15">[16]</ref>, Puthessery et al. <ref type="bibr" target="#b37">[38]</ref> propose a U-net <ref type="bibr" target="#b40">[41]</ref> model for one-to-one relighting where Discrete Wavelet Transform (DWT) and Inverse Discrete Wavelet Transform (IDWT) are attached to downsampling and upsampling layers, respectively. SA-AE <ref type="bibr" target="#b17">[18]</ref> also develops a U-Net-based architecture for any-to-any relighting, where two auxiliary networks are incorporated for estimating the lighting of the guide image and providing lighting features to the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fusion Strategy</head><p>As mentioned earlier, our model is based on two approaches to the relighting problem: 1) Intrinsic Image Decomposition (IID): From the physical standpoint, every light image can be decomposed into two main parameters <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref>: albedo and shading. Albedo, which is the light independent parameter of the image, preserves the reflectance property of the material in the scene, while shading holds properties corresponding to illumination and the geometry of the image. Based on this, the relit image can be expressed as: I intinsic-relit =Â Ŝ . Here,Â and S denote estimated albedo and shading, respectively. is the element wise product operation. This method has been shown as an effective way to relight the image scene using the input RGB image (and depth map) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b33">34]</ref>. Following this approach, the model is guided toward a systematic way of learning to relight by which it can distinguish between features associated with material reflectance property and features for the illumination and geometry of the scene.</p><p>2) Direct Relighting (DR): In addition to the intrinsic decomposition of the images, we also follow the end-to-end learning method as in state of the art <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b37">38]</ref> for learning a mapping function between the two lighting settings: f (I) = I direct-relit . Where f denotes the mapping function learned by neural network model. This way the model, in addition to the physically inspired insight, constructs an auxiliary insight that complements the other one in terms of the extracted discriminative features. Next, we generate a spatially varying weight map (w) to fuse the estimates:</p><formula xml:id="formula_1">I relit = wI direct-relit + (1 − w)I intrinsic-relit<label>(2)</label></formula><p>This fusion strategy helps the model benefit from both aspects of the problem simultaneously. Furthermore, owing to the usage of a couple of shared structures and the virtue of joint optimization, each approach aids the other one through the insight it lends to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>OIDDR-Net ( <ref type="figure">Fig. 1</ref>) and AMIDR-Net ( <ref type="figure" target="#fig_1">Fig. 3</ref>) are similar in terms of the general architecture which is inspired by U-Net <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b30">31]</ref>. An encoder is shared by three bottlenecks followed by four decoders. Two decoders designed for the IID strategy share a bottleneck, while the other two    <ref type="figure">Figure 5</ref>: The lighting estimation network used for illumination regularization and extraction of illumination features from the guide image. decoders designated for direct strategy and generating the weight map (w) are fed by a bottleneck, each. The details of these components are as follows: 1) Encoder: The encoder construction <ref type="table" target="#tab_0">(Table 1)</ref> follows DenseNet-121 <ref type="bibr" target="#b19">[20]</ref> feature extraction part, which is originally proposed for classification tasks. It consists of a feature extraction part followed by classification layers. We borrow the input convolutional layer, the first three dense layers, and their following transitional blocks in the feature extraction part. The main advantage of using these pretrained layers for our model is that since they're trained over ImageNet dataset <ref type="bibr" target="#b7">[8]</ref>, they provide our model an initial representation capability. This in turn helps for the faster convergence in the training. It is worth mentioning that the first convolutional layer in DenseNet accepts three channels RGB images and inputs to OIDDR-Net and AMIDR-Net are of 4 and 8 channels, respectively. To address this, for OIDDR-Net, we modify the convolutional block by keeping the first three channels and initializing the fourth one as a grayscale transformation of the other three. For AMIDR-Net, we substitute it by an eight channel convolutional block initialized randomly.</p><formula xml:id="formula_2">3 × 3 max-pool   1 × 1 conv. 3 × 3 conv. × 6 1 × 1 conv. 3 × 3 conv. × 12 1 × 1 conv. 3 × 3 conv. × 24 1 × 1 conv. 2 × 2 avg-pool 1 × 1 conv. 2 × 2 avg-pool 1 × 1 conv. 2 × 2 avg-pool</formula><p>2) Bottlenecks: There are three bottlenecks consisting of a dense transitional block and two residual blocks (see <ref type="table" target="#tab_0">Table  1</ref>,2 for details of these blocks). The main function of bottlenecks is to connect the encoder and decoders by compiling the extracted features of the encoder based on the characteristics of the decoders. So, Decoder-A and Decoder-S share a bottleneck as they both contribute on I intrinsic-relit .</p><p>3) Decoders: Our network features four decoders for predicting the components: albedo (Â), shading (Ŝ), weight map (w), and directly relit image (I direct-relit ). <ref type="table" target="#tab_2">Table 2</ref> details the structure of decoders. Each decoder includes four levels of cascaded attention module (Squeeze and excitation <ref type="bibr" target="#b16">[17]</ref> or dilation inception modules <ref type="bibr" target="#b29">[30]</ref>), a dense transitional block, and two residual blocks. It means that there  is an analogy between decoders' structure and the encoder except that the channel attention modules are incorporated midway. This in particular helps each decoder give weight to feature maps based on its functionality, while benefiting the shared encoder. Meanwhile, skip connections from the encoder assist the decoders in reconstructing the scene as in U-net. 4) Lighting Estimation Network: As our main objective in this work is to change the illumination of the images, we need custom blocks for extracting illumination features. Moreover, it is worth mentioning that neural networks usually need either ground-truth or custom loss terms in order to extract our expected features. To this end, we train a lighting estimation network to predict the light angle and color temperature of the images using the training set for the any-to-any problem. We make use of the pretrained feature extraction part of this network <ref type="figure">(Fig. 5</ref>) to compute a perceptual loss for comparing the illumination features of the relit output. Furthermore, in AMIDR-Net ( <ref type="figure" target="#fig_1">Fig. 3)</ref>, this network is incorporated for feeding the decoders with the illumination features of the guide image.</p><formula xml:id="formula_3">3 × 3 conv.   × 2 batch norm 3 × 3 conv. × 7   3 × 3 conv. 3 × 3 conv.   × 2   SE/Dilation (R=3) batch norm 3 × 3 conv.     32 × 32 avg-pool 1 × 1 conv. upsample   1 × 1 conv.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Exploiting Normals for One-to-One Relighting</head><p>While the model is guided toward learning a physicsbased solution for relighting, it may not necessarily be successful in changing the lighting parameters of a given scene. This could be due to the complicated geometry of the scene, which causes the presence of dense shadows needed to be removed or the presence of highly glossed objects needed to be shadowed. This is a challenging aspect of relighting for a neural network model, as neural networks usually fail in regressing outputs that lie on one side of the extreme since their share in training data is typically small. Therefore, the model in order to keep its generalization over the whole data distribution would typically show artifacts on these extreme cases. To address this issue specifically, for the case of one-to-one relighting, we propose to incorpo-rate the information associated with the normal vectors of the surfaces present in the scene. It is shown that the shading of an image can be derived as a nonlinear function of 9-dimensional spherical harmonics coefficients and the normal vectors <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b38">39]</ref>. The normal vectors of a scene indicate the orientation of pixels associated with each surface in the image. <ref type="figure">Fig. 2</ref> shows an example in which the colors red, green, and blue indicate surfaces parallel to x-y, y-z, and x-z plane, respectively. Since our model learns to predict the shading directly from the information provided during the training stage in the form of ground-truth, instead of carrying out the non-linear calculations, we incorporate normal vectors into the problem as weight adjustments. More precisely, in the case of one-to-one relighting in which we know the target lighting direction, the normal vectors are used as adjustment weights for the surfaces facing toward or against the light direction:Ŝ = H(n light-dir ,Ŝ 0 ). Where H, n light-dir , andŜ 0 are the linear adjustment function, the normal vector component corresponding to the light direction of the target, and the shading output by the model, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Any-to-Any Relighting and Multiscale Features</head><p>In any-to-any relighting there is no meaningful pixelwise correspondence between the guide and input RGB image/depth map. Therefore unlike one-to-one relighting, training the network on image patches is not feasible. On the other hand, training the network on whole images limits the representation power of the model as the model may not necessarily extract features from lower fields of view. To prevent that, we equip AMIDR-Net with a multiscale feature extraction block <ref type="bibr" target="#b51">[51]</ref>. <ref type="figure">Fig. 4</ref> shows the details of this block. Using PixelUnShuffle operations, it downsamples the input to three different levels. In each level dense residual and global residual blocks extract the features. Subsequently, the extracted feature maps are customly upsampled and fed to the next level using PixelShuffle operation. Finally, in the highest level the multiscale feature maps are processed and fed to the main pipeline. The main advantage of using this multiscale block over the ones, which make use of traditional upsampling modules, is how it guides the network to learn the upsampling while optimizing the feature maps. Simply put, the model learns how to extract patches from the input while keeping the correspondence between the feature maps from the guide and input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Customized Loss Function</head><p>To train our model so that every part of it functions based on our expectation, we need to define custom loss terms for each part. Our overall loss function is as follows:</p><formula xml:id="formula_4">L = L total + λ1LIID + λ2L direct + λ3LSSIM + λ4L lighting (3) L total = ||Î relit − Y relit || 2 2 (4) LIID = ||Â Ŝ − Y relit || 2 2 + ||Â − A|| 2 2 + ||Ŝ − S|| 2 2 (5) L direct = ||I direct-relit − Y relit || 2 2 (6) LSSIM = 1 − SSIM (Î relit , Y relit ) (7) L lighting = ||g(Î relit ) − g(Y relit )|| 2 2 − 8 i=1 Y i dir-guide log(Ŷ i dir ) − 5 j=1 Y j color-guide log(Ŷ j color ) (8)</formula><p>Where L total , L IID , and L direct are terms to ensure the decoder outputs I intrinsic-relit , I direct-relit , andÎ relit match the ground-truth Y relit . Of note,Î relit is the fused output (eq. 2). To help the model predict physically feasible estimates for albedo and shading, we use a pretrained intrinsic decomposition network <ref type="bibr" target="#b33">[34]</ref>, which is trained on SINTEL dataset <ref type="bibr" target="#b3">[4]</ref>, to generate pseudo ground-truths A and S. L SSIM is used to maximize the structural similarity index (SSIM) of the relit output and ground-truth. We also define L lighting to minimize the difference between the relit output and the groundtruth in terms of illumination parameters. In eq. 8, in the first term, the intermediate features generated by lighting estimation network (denoted by g) are compared for the relit output and ground-truth. The second and third terms (blue) are specifically incorporated for AMIDR-Net, where we minimize the negative log-likelihood of the light direction and color temperature in the relit output (Ŷ dir andŶ color ) based on guide image parameters (Y dir-guide and Y color-guide ). λ 1 , λ 2 , λ 3 , and λ 4 are hyperparameters adjusting the contribution of each term in the overall loss term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We use VIDIT dataset <ref type="bibr" target="#b10">[11]</ref> generated by the Unreal gaming engine <ref type="bibr" target="#b13">[14]</ref> and consisting of two subsets for one-to-one and any-to-any relighting. One-to-One Relighting: VIDIT'21 training set for oneto-one relighting, provides 300 1024 × 1024 images with one particular light direction and color temperature and their corresponding ground-truth with the target particular illumination settings. Additionally, in VIDIT'21, unlike VIDIT'20, depth maps are provided for each image. The validation set includes 45 samples. We augment 1 the training set to a set with about 37000 samples by 1) cropping 256×256 patches. 2) Resizing the whole images to 256 × 256. 3) Rotating the patches and resized images slightly with small angles (0-12 degrees). We don't incorporate flipping or rotation with large angles as the light direction in this problem should be fixed across the training samples. Any-to-Any Relighting: The diversity across the training set is higher in the case of any-to-any relighting. For training VIDIT provides 12000 samples consisting of 300 scenes with a combination of 8 different light angles and 5 different color temperatures (40 for each scene) and 1 depth map for each scene. The validation set includes 90 images. As mentioned earlier, we cannot crop patches in this case; however, training on whole 1024 × 1024 images is not possible due to memory limits. Therefore, we resize the images to 384 × 384. We create the training set following two steps: 1) For each sample in the set, we randomly choose three different guide samples. The guide samples, obviously, are not from the same scene as the original sample. 2) For each of the chosen guide images, we find the version of original scene having the same illumination setting as the guide image. This leads to a training set with 36000 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training</head><p>We use Adam optimizer <ref type="bibr" target="#b23">[24]</ref> with an initial learning rate of 10 −4 , which decreases by a rate of 0.7 every 10 epochs. Owing to pretrained weights of DenseNet and incorporation of pseudo ground-truths, both OIDDR-Net and AMIDR-Net show fast convergence (optimally 20 epochs), but we train the models for 25 epochs (with batch sizes of 8 and 2, respectively) to ensure the complete stability of them. λ 1 , λ 2 , λ 3 , and λ 4 are set to 0.4, 0.4, 0.8, and 0.03, respectively using cross validation <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Testing</head><p>One-to-One Relighting:</p><p>We observe that our model shows better performance by the following ensemble method: i) 1024 × 1024 RGB image and depth map are fed to the model. ii) 384 × 384 RGB image and depth map are input to the model. The output is fed to a bicubic interpolation module (implemented in pytorch <ref type="bibr" target="#b35">[36]</ref>) and scaled to the original size. The final output is the average of the two estimates. Any-to-Any Relighting: In order to get the best performance of AMIDR-Net during the test phase, we resize the input to 384 × 384 so the model has the same observation as in training. The outputs then will be upsampled using bicubic interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>In this section we present experimental results of our proposed OIDDR-Net and AMIDR-Net. We provide ablation studies to show the effect of loss terms and novel network components. We also compare our models with state of the art. Our evaluation metrics are Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM) <ref type="bibr" target="#b54">[54]</ref>, Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b50">[50]</ref>, and Mean Perceptual Score (MPS) which is: M P S = 0.5(SSIM + 1 − LP IP S).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Study</head><p>Effect of Exploiting Normal Vectors and L lighting on OIDDR-Net: To observe how incorporating normal vectors for adjusting the shading estimation of the network would affect its performance, we conduct an experiment on VIDIT'21 dataset through which we train OIDDR-Net with initial shading estimation. Additionally, to study the effect of comparing the illumination in network relit output to illumination in ground-truth (through L lighting ), we train OIDDR-Net without applying L lighting in training loss. <ref type="table" target="#tab_4">Table 3</ref> shows the performance results of the three models. While both factors play an important role in minimizing the fidelity and perceptual loss, we can see how L lighting contributes to structural similarity. Effect of Multiscale Feature Extraction and L lighting on AMIDR-Net: To see the importance of the multiscale block as well as L lighting , we train three models: 1) AMIDR-Net with full architecture and L lighting being activated during training, 2) AMIDR-Net with the multiscale block removed from the architecture, and 3) AMIDR-Net trained with L lighting dropped from the training loss. <ref type="table" target="#tab_5">Table 4</ref> shows the results of our experiments on validation data, whereby one can infer the effect of different components on the network performance. The noticeable drop in SSIM, after removing the multiscale block, proves its impact in helping the network extract more discriminative features for constructing the structural similarity between the output and the ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with State-of-the-art Methods</head><p>All the existing works for image-based relighting (applicable to VIDIT) have been proposed for the dataset without depth information. Therefore, to have a fair comparison, we   <ref type="table" target="#tab_6">Table  5</ref>. While <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b27">[28]</ref> are proposed for deblurring and dehazing, all other methods have been proposed for the same exact problem and dataset. <ref type="table" target="#tab_6">Table 5</ref> shows that our OIDDR-Net outperforms state of the art w.r.t. all metrics as a result of fusing the power of neural networks and the physics of the problem. We can also qualitatively confirm this in <ref type="figure">Fig.  6</ref>, where OIDDR-Net's output successfully mimics the illumination settings of the ground-truth without artifacts. Any-to-any Relighting: We modify our AMIDR-Net by changing the number of input channels and removing the skip connections corresponding to the depth maps and train it on VIDIT 2020 dataset. We compare our modified AMIDR-Net with state of the art in <ref type="table" target="#tab_7">Table 6</ref>, where SA-AE <ref type="bibr" target="#b18">[19]</ref> is the winner of AIM 2020 any-to-any relighting track and <ref type="bibr" target="#b8">[9]</ref> is an encoder-decoder network proposed by another participant of the same challenge. We also compare our method with an adapted version of <ref type="bibr" target="#b53">[53]</ref>, which is originally proposed for portrait relighting. According to <ref type="table" target="#tab_7">Table  6</ref>, AMIDR-Net outperforms others w.r.t. all evaluation metrics. <ref type="figure">Fig. 7</ref> visualizes three outputs from different methods where we see how our AMIDR-Net changes the illumination of the input according to guide image without artifacts. Comparison with NTIRE 2021 Relighting Methods: Additionally, we compare our models with two methods from the top 5 methods of the NTIRE 2021 relighting challenge. As <ref type="table" target="#tab_8">Table 7</ref> confirms, OIDDR-Net and AMIDR-Net are among the top-performing methods. OIDDR-Net ranked second in one-to-one relighting in terms of MPS and AMIDR-Net ranked second in terms of PSNR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We develop a physically inspired dense fusion network for image relighting. Our method benefits from the capabil- <ref type="figure">Figure 6</ref>: Qualitative comparison between different methods on one-to-one relighting. From left to right: SRN <ref type="bibr" target="#b43">[44]</ref>, Dense-GridNet <ref type="bibr" target="#b27">[28]</ref>, DRN <ref type="bibr" target="#b44">[45]</ref>, DMSHN <ref type="bibr" target="#b5">[6]</ref>, WDRN <ref type="bibr" target="#b37">[38]</ref>, OIDDR-Net (ours) and ground-truth. <ref type="figure">Figure 7</ref>: Qualitative comparison between different methods. From left to right: input image, guide image, ground-truth, SA-AE <ref type="bibr" target="#b18">[19]</ref>, DPR <ref type="bibr" target="#b53">[53]</ref>, and AMIDR-Net (ours).  in terms of illumination parameters and therefore less artifacts in the obtained relighted images. Ablation studies explain the role of each component in models proposed both for one-to-one and any-to-any relighting. Comparisons with existing literature on benchmark datasets and competing methods in the NTIRE'21 relighting challenge show our proposal achieves state-of-the-art results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 :Figure 2 :</head><label>12</label><figDesc>Our proposed OIDDR-Net architecture. The visualization of the normal vectors in an example image. Each of RGB channels corresponds to x-y, y-z, and x-z planes, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Our proposed AMIDR-Net architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Encoder Structure for AMIDR-Net. ( * OIDDR-Net doesn't have the multiscale block and guide inputs.)</figDesc><table><row><cell></cell><cell>Multi-Scale block</cell><cell></cell><cell>Base</cell><cell>Dense-Trans.1</cell><cell>Dense-Trans.2</cell><cell>Dense-Trans.3</cell></row><row><cell>Input</cell><cell cols="3">[Input image, Guide image  scale Output</cell><cell>Base</cell><cell>Dense-Trans.1</cell><cell>Dense-Trans.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>7 × 7 conv.</cell><cell></cell></row><row><cell>structure</cell><cell>See Fig. 4</cell><cell></cell><cell></cell><cell></cell></row></table><note>* , Input depth map, Guide depth map* ] Multi-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The multiscale block.</figDesc><table><row><cell cols="2">Output</cell><cell></cell><cell>384 × 384 × 8</cell><cell>96 × 96 × 64</cell><cell>48 × 48 × 128</cell><cell>24 × 24 × 256</cell><cell>24 × 24 × 512</cell></row><row><cell cols="2">PixelUnShuffle/2</cell><cell></cell><cell>PixelShuffle x 2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dense</cell><cell></cell></row><row><cell>PixelShuffle</cell><cell>PixelUnShuffle/4</cell><cell></cell><cell>PixelShuffle x 2</cell><cell>Residual Residual Block</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Global</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Block</cell><cell></cell></row><row><cell></cell><cell cols="2">PixelUnShuffle/8</cell><cell cols="2">PixelShuffle x 2</cell><cell></cell></row><row><cell cols="2">PixelUnShuffle</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Figure 4: Color Temperature</cell><cell></cell></row><row><cell></cell><cell cols="2">DenseNet-121</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Feature Network</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Light Angle</cell><cell></cell></row><row><cell></cell><cell>Avg pool.</cell><cell>Linear+ReLU</cell><cell>Linear+Softmax</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Decoder Structure. (for OIDDR-Net the input to the decoder doesn't include lighting estimation outputs.) C depends on the functionality of the decoder.</figDesc><table><row><cell></cell><cell></cell><cell>Dense-Trans.5</cell><cell>Res.5</cell><cell></cell><cell>Dense-Trans.6</cell><cell></cell><cell></cell><cell>Res.6</cell><cell>Dense-Trans.7</cell></row><row><cell>Input</cell><cell cols="2">[bottleneck output, Dense.Trans.2, Lighting Estimation]</cell><cell cols="2">Dense-Trans.5</cell><cell>[Trans.1, Res.5]</cell><cell></cell><cell></cell><cell>Dense-Trans.6</cell><cell>Res.6</cell></row><row><cell>structure</cell><cell cols="2">  SE/Dilation (R=16)  batch norm 3 × 3 conv.  × 7 1 × 1 conv.</cell><cell>  3 × 3 conv. 3 × 3 conv.</cell><cell>  × 2</cell><cell>SE/Dilation (R=16) batch norm 1 × 1 conv.</cell><cell>× 7</cell><cell> </cell><cell>3 × 3 conv.   × 2 3 × 3 conv.</cell><cell>batch norm 3 × 3 conv. 1 × 1 conv. × 7</cell></row><row><cell></cell><cell></cell><cell>upsample 2</cell><cell></cell><cell></cell><cell>upsample 2</cell><cell></cell><cell></cell><cell></cell><cell>upsample 2</cell></row><row><cell>output</cell><cell></cell><cell>48 × 48 × 128</cell><cell cols="2">48 × 48 × 128</cell><cell>96 × 96 × 64</cell><cell></cell><cell></cell><cell>96 × 96 × 64</cell><cell>192 × 192 × 32</cell></row><row><cell></cell><cell></cell><cell>Res.7</cell><cell cols="2">Dense-Trans.8</cell><cell>Res.8</cell><cell></cell><cell></cell><cell>Refine.9</cell><cell>Refine.10</cell></row><row><cell>Input</cell><cell></cell><cell>Dense-Trans.7</cell><cell>Res.7</cell><cell></cell><cell>Dense-Trans.8</cell><cell></cell><cell></cell><cell>[Input, Res.8]</cell><cell>Refine.9</cell></row><row><cell></cell><cell></cell><cell>3 × 3 conv.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>structure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>One-to-one-VIDIT'21 validation's ablation study.</figDesc><table><row><cell>Model</cell><cell>PSNR</cell><cell>SSIM</cell><cell>LPIPS</cell><cell>MPS</cell></row><row><cell>OIDDR-Net</cell><cell cols="4">18.39 0.6980 0.2591 0.7194</cell></row><row><cell cols="5">w/o Normals 17.49 0.6805 0.2647 0.7079</cell></row><row><cell>w/o Llighting</cell><cell cols="4">17.59 0.6669 0.2741 0.6964</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Any-to-any-VIDIT'21 validation's ablation study. Net 19.83 0.6940 0.3381 0.6779 w/o the multiscale block 19.09 0.6685 0.3421 .6632 w/o L lighting 19.22 0.6721 0.3403 0.6659</figDesc><table><row><cell>Model</cell><cell>PSNR SSIM LPIPS</cell><cell>MPS</cell></row><row><cell>AMIDR-</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison with state of the arts for one-to-one relighting on VIDIT'20 validation set.</figDesc><table><row><cell>Model</cell><cell cols="3">PSNR SSIM LPIPS</cell><cell>MPS</cell><cell>Runtime(s)</cell></row><row><cell>OIDDR-Net (ours)</cell><cell cols="4">17.62 0.6645 0.2733 0.6956</cell><cell>0.53</cell></row><row><cell>WDRN [38]</cell><cell cols="4">17.45 0.6642 0.2771 0.6935</cell><cell>0.05</cell></row><row><cell>DRN [45]</cell><cell>17.59</cell><cell>0.596</cell><cell>0.440</cell><cell>0.578</cell><cell>0.5</cell></row><row><cell>DMSHN [6]</cell><cell cols="4">17.20 0.5696 0.3712 0.5992</cell><cell>0.0058</cell></row><row><cell>SRN [44]</cell><cell cols="4">16.94 0.5660 0.4319 0.5670</cell><cell>0.87</cell></row><row><cell cols="5">Dense-GridNet [28] 16.67 0.2811 0.3691 0.9120</cell><cell>0.9326</cell></row><row><cell>Dong et al. [9]</cell><cell cols="4">17.14 0.6132 0.2764 0.6684</cell><cell>-</cell></row><row><cell cols="6">trained and evaluated our OIDDR-Net and AMIDR-Net on</cell></row><row><cell cols="4">VIDIT'20 training and validation set.</cell><cell></cell><cell></cell></row><row><cell cols="6">One-to-one Relighting: We modify our OIDDR-Net for</cell></row><row><cell cols="6">accepting only the RGB image (so normals are not ex-</cell></row><row><cell cols="6">ploited) and train it over the training set. We compare</cell></row><row><cell cols="6">our modified OIDDR-Net with existing methods in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison with state of the arts for any-to-any relighting on VIDIT'20 validation set. (LPIPS and MPS are not made available by other works. The runtime for<ref type="bibr" target="#b8">[9]</ref> is not reported.) networks in extracting representative features, while simultaneously estimating albedo and shading -key components of the relighting physical model. The simultaneous intrinsic image decomposition and direct relighting help the model refine its feature extraction by joint optimization. This leads to physically more feasible results</figDesc><table><row><cell>Model</cell><cell cols="2">PSNR SSIM Runtime(s)</cell></row><row><cell cols="2">AMIDR-Net (ours) 19.16 0.6621</cell><cell>0.51</cell></row><row><cell>SA-AE [19]</cell><cell>18.06 0.6480</cell><cell>0.15</cell></row><row><cell>DPR [53]</cell><cell>16.40 0.5238</cell><cell>0.095</cell></row><row><cell>Dong et al. [9]</cell><cell>18.07 0.5994</cell><cell>-</cell></row><row><cell>ity of dense</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison with other methods in NTIRE2021 relighting challenge on VIDIT'21 test set.</figDesc><table><row><cell>Track</cell><cell>Model</cell><cell>PSNR SSIM LPIPS</cell><cell>MPS</cell><cell>Runtime(s)</cell></row><row><cell></cell><cell cols="3">OIDDR-Net (ours) 18.83 0.6874 0.1634 0.7620</cell><cell>0.53</cell></row><row><cell>One-to-one</cell><cell>Method 1</cell><cell cols="2">19.14 0.6931 0.1605 0.7663</cell><cell>2.88</cell></row><row><cell></cell><cell>Method 2</cell><cell cols="2">18.27 0.6772 0.1670 0.7551</cell><cell>2.12</cell></row><row><cell></cell><cell cols="3">AMIDR-Net (ours) 20.14 0.6711 0.2028 0.7341</cell><cell>0.51</cell></row><row><cell>Any-to-any</cell><cell>Method 1</cell><cell cols="2">19.22 0.6784 0.1566 0.7609</cell><cell>2.04</cell></row><row><cell></cell><cell>Method 2</cell><cell cols="2">18.60 0.6508 0.1661 0.7423</cell><cell>0.6740</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Please find implementation details and results at: github/Relighting</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lambertian reflectance and linear subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Intrinsic images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;99</title>
		<meeting>the 26th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;99<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">editor, European Conf. on Computer Vision (ECCV), Part IV</title>
		<editor>A. Fitzgibbon et al.</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012-10" />
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple model for intrinsic image decomposition with depth cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dsrn: an efficient deep network for image relighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourya</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisarg</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-02" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dsrn: an efficient deep network for image relighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisarg</forename><forename type="middle">A</forename><surname>Sourya Dipta Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An ensemble neural network for scene relighting with light classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liping</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuolong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 Workshops</title>
		<editor>Adrien Bartoli and Andrea Fusiello</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="581" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiview intrinsic images of outdoors scenes with an application to relighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Sylvain Duchêne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Riant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><forename type="middle">Lopez</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Yves</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Laffont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Bousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drettakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">VIDIT: Virtual image dataset for illumination transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majed</forename><forename type="middle">El</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Barthas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Süsstrunk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05460</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth-guided image relighting challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majed</forename><forename type="middle">El</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Süsstrunk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<idno>2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">2d image relighting with image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Gafton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erick</forename><surname>Maraz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unreal engine | the most powerful real-time 3d creation platform</title>
		<ptr target="https://www.unrealengine.com/en-us/.6" />
		<imprint/>
	</monogr>
	<note>Epic Games</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Low-light image enhancement combined with attention map and u-net network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 3rd International Conference on Information Systems and Computer Aided Education (ICISCAE)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="397" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aim 2020: Scene relighting and illumination estimation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majed</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Süsstrunk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Afifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kele</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengxing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Song</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Tak</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourya</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisarg</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akashdeep</forename><surname>Jassal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongtong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhao</surname></persName>
		</author>
		<idno>09 2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Sabari Nathan, Dr.M.Parisa Beham, and Jian Cheng</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Saae for any-to-any relighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaning</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
		<idno>ing. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 Workshops</title>
		<editor>Adrien Bartoli and Andrea Fusiello</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publish</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="535" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Saae for any-to-any relighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaning</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 Workshops</title>
		<editor>Adrien Bartoli and Andrea Fusiello</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="535" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The rendering equation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Kajiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="143" to="150" />
			<date type="published" when="1986-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fhdr: Hdr image reconstruction from a single ldr image using feedback network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Global Conference on Signal and Information Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page" from="12" to="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lightness and retinex theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mccann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1971-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shadow removal via shadow image decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8577" to="8586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Estimation of intrinsic image sequences from image+depth video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyong Joon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Uk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2012</title>
		<editor>Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, and Cordelia Schmid</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="327" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Griddehazenet: Attention-based multi-scale network for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="7313" to="7322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Progressively-refined reflectance functions from natural illumination. EGSR&apos;04</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurographics Association</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="299" to="308" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nonlocal channel attention for nonhomogeneous image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Metwaly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nonlocal channel attention for nonhomogeneous image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Metwaly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1842" to="1851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention-mask dense merger (attendense) deep hdr for ghost removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kareem</forename><forename type="middle">M</forename><surname>Metwaly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2623" to="2627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Handbook of Convex Optimization Methods in Imaging Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Monga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Direct intrinsics: Learning albedo-shading decomposition by convolutional regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Narihira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning physics-guided face relighting under directional light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nestmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="5123" to="5132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché-Buc, E. Fox, and R. Garnett, editors</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Peers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dhruv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijeet</forename><surname>Lamond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Debevec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Compressive light transport sensing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Wdrn: A wavelet decomposed relightnet for image relighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Densen</forename><surname>Puthussery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Hrishikesh Panikkasseril Sethumadhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiji Charangatt</forename><surname>Kuriakose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Victor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 Workshops</title>
		<editor>Adrien Bartoli and Andrea Fusiello</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="519" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the relationship between radiance and irradiance: Determining the illumination from images of a convex lambertian object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optics, image science, and vision</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2448" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Frequency-space decomposition and acquisition of light transport under spatially varying illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dikpal</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European Conference on Computer Vision -Volume Part VI, ECCV&apos;12</title>
		<meeting>the 12th European Conference on Computer Vision -Volume Part VI, ECCV&apos;12<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="596" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention -MICCAI</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sfsnet: Learning shape, reflectance and illuminance of faces in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lichy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural face editing with intrinsic image disentangling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="8174" to="8182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep relighting networks for image light source manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Chi</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Song</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Tak</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lun</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">08</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From shading to local shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Zickler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep image-based relighting from optimal sparse samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Ensemble dehazing networks for non-homogeneous haze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cherukuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1832" to="1841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Inverserendernet: Learning single image inverse rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A P</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hierarchical regression network for spectral reconstruction from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Po</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1695" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Face relighting with radiance environment maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">158</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep single-image portrait relighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7193" to="7201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Zhou Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
