<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supervised and unsupervised neural approaches to text readability</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Martinc</surname></persName>
							<email>matej.martinc@ijs.si.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jožef</forename><surname>Stefan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Robnik-Šikonja</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Jožef Stefan Institute</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">International Postgraduate School</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country>Slovenia Senja Pollak</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Jožef Stefan Institute</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Computer and Information Science</orgName>
								<orgName type="institution">University of Ljubljana</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Supervised and unsupervised neural approaches to text readability</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The final reviewed publication was published in Computational Linguistics Journal, Volume 47, Issue 1 -March 2021 and is available online at https://doi.org/10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1162/coli_a_00398</head><p>We present a set of novel neural supervised and unsupervised approaches for determining the readability of documents. In the unsupervised setting, we leverage neural language models, whereas in the supervised setting, three different neural classification architectures are tested. We show that the proposed neural unsupervised approach is robust, transferable across languages and allows adaptation to a specific readability task and data set. By systematic comparison of several neural architectures on a number of benchmark and new labelled readability datasets in two languages, this study also offers a comprehensive analysis of different neural approaches to readability classification. We expose their strengths and weaknesses, compare their performance to current state-of-the-art classification approaches to readability, which in most cases still rely on extensive feature engineering, and propose possibilities for improvements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Readability is concerned with the relation between a given text and the cognitive load of a reader to comprehend it. This complex relation is influenced by many factors, such as a degree of lexical and syntactic sophistication, discourse cohesion, and background knowledge <ref type="bibr" target="#b11">(Crossley et al. 2017)</ref>. In order to simplify the problem of measuring readability, traditional readability formulas focused only on lexical and syntactic features expressed with statistical measurements, such as word length, sentence length, and word difficulty <ref type="bibr" target="#b13">(Davison and Kantor 1982)</ref>. These approaches have been criticized because of their reductionism and weak statistical bases <ref type="bibr" target="#b11">(Crossley et al. 2017)</ref>. Another problem is their objectivity and cultural transferability since children from different environments master different concepts at different ages. For example, a word television is quite long and contains many syllables but is well-known to most young children who live in families with a television.</p><p>With the development of novel natural language processing (NLP) techniques, several studies attempted to eliminate deficiencies of traditional readability formulas. These attempts include leveraging high-level textual features for readability modelling, such as semantic and discursive properties of texts. Among them, cohesion and coherence received the most attention, and several readability predictors based on these text features have been proposed (see Section 2). Nevertheless, none of them seems to predict the readability of the text as well as much simpler readability formulas mentioned above <ref type="bibr" target="#b50">(Todirascu et al. 2016)</ref>.</p><p>With the improvements in machine learning, the focus shifted once again, and most newer approaches consider readability as being a classification, regression, or a ranking task. Machine learning approaches build prediction models to predict human assigned readability scores based on several attributes and manually built features that cover as many text dimensions as possible <ref type="bibr" target="#b44">(Schwarm and Ostendorf 2005;</ref><ref type="bibr" target="#b53">Vajjala and Meurers 2012;</ref><ref type="bibr" target="#b42">Petersen and Ostendorf 2009)</ref>. They generally yield better results than the traditional readability formulas and text cohesion based methods but require additional external resources, such as labelled readability datasets, which are scarce. Another problem is the transferability of these approaches between different corpora and languages since the resulting feature sets do not generalize well to different types of texts <ref type="bibr">(Filighera, Steuer, and Rensing 2019;</ref><ref type="bibr" target="#b59">Xia, Kochmar, and Briscoe 2016)</ref>.</p><p>Recently, deep neural networks (Goodfellow, <ref type="bibr" target="#b20">Bengio, and Courville 2016)</ref> have shown impressive performance on many language-related tasks. In fact, they have achieved state-of-the-art performance in all semantic tasks where sufficient amounts of data were available <ref type="bibr" target="#b9">(Collobert et al. 2011;</ref><ref type="bibr" target="#b63">Zhang, Zhao, and LeCun 2015)</ref>. Even though very recently some neural approaches towards readability prediction have been proposed <ref type="bibr">(Filighera, Steuer, and Rensing 2019;</ref><ref type="bibr" target="#b38">Nadeem and Ostendorf 2018)</ref>, these type of studies are still relatively scarce, and further research is required in order to establish what type of neural architectures are the most appropriate for distinct readability tasks and datasets. Furthermore, language model features designed to measure lexical and semantic properties of text, which can be found in many of the readability studies <ref type="bibr" target="#b44">(Schwarm and Ostendorf 2005;</ref><ref type="bibr" target="#b42">Petersen and Ostendorf 2009;</ref><ref type="bibr" target="#b59">Xia, Kochmar, and Briscoe 2016)</ref>, are generated with traditional n-gram language models, even though language modelling has been drastically improved with the introduction of neural language models <ref type="bibr" target="#b36">(Mikolov et al. 2011)</ref>.</p><p>The aim of the present study is two-fold. First, we propose a novel approach to readability measurement that takes into account neural language model statistics. This approach is unsupervised and requires no labelled training set but only a collection of texts from the given domain. We demonstrate that the proposed approach is capable of contextualizing the readability because of the trainable nature of neural networks and that it is transferable across different languages. In this scope, we propose a new measure of readability, RSRS (ranked sentence readability score), with good correlation with true readability scores. Second, we experiment how different neural architectures with automatized feature generation can be used for readability classification and compare their performance to state-of-the-art classification approaches. Three distinct branches of neural architectures -recurrent neural networks (RNN), hierarchical attention networks (HAN), and transfer learning techniques -are tested on four gold standard readability corpora with good results.</p><p>The paper is structured as follows. Section 2 addresses the related work on readability prediction. Section 3 offers a thorough analysis of datasets used in our experiments, while in Section 4, we present the methodology and results for the proposed unsuper-vised approach to readability prediction. The methodology and experimental results for the supervised approach are presented in Section 5. We present conclusions and directions for further work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Approaches to the automated measuring of readability try to find and assess factors that correlate well with human perception of readability. Several indicators, which measure different aspects of readability, have been proposed in the past and are presented in Section 2.1. These measures are used as features in newer approaches, which train machine learning models on texts with human-annotated readability levels so that they can predict readability levels on new unlabeled texts. Approaches, which rely on an extensive set of manually engineered features, are described in Section 2.2. Finally, Section 2.3 covers the approaches that tackle readability prediction with neural classifiers. Besides tackling the readability as a classification problem, several other supervised statistical approaches for readability prediction have been proposed in the past. They include regression <ref type="bibr" target="#b46">(Sheehan et al. 2010)</ref>, SVM ranking <ref type="bibr" target="#b32">(Ma, Fosler-Lussier, and Lofthus 2012)</ref>, and graph-based methods <ref type="bibr" target="#b24">(Jiang, Xun, and Qi 2015)</ref>, among many others. We do not cover these methods in the related work since they are not directly related to the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Readability features</head><p>Classical readability indicators can be roughly divided into five distinct groups: traditional, discourse cohesion, lexico-semantic, syntactic, and language model features. We describe them below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Traditional features.</head><p>Traditionally, readability in texts was measured by statistical readability formulas, which try to construct a simple human-comprehensible formula with a good correlation to what humans perceive as the degree of readability. The simplest of them is average sentence length (ASL), though they take into account various other statistical factors, such as word length, and word difficulty. Most of these formulas were originally developed for English language but are also applicable to other languages with some modifications <ref type="bibr">(Škvorc et al. 2019)</ref>.</p><p>The Gunning fog index <ref type="bibr" target="#b21">(Gunning 1952</ref>) (GFI) estimates the years of formal education a person needs to understand the text on the first reading. It is calculated with the following expression: GFI = 0.4( totalW ords totalSentences + 100 longW ords totalSentences ),</p><p>where longWords are words longer than 7 characters. Higher values of the index indicate lower readability. Flesch reading ease <ref type="bibr" target="#b27">(Kincaid et al. 1975</ref>) <ref type="bibr">(FRE)</ref> assigns higher values to more readable texts. It is calculated in the following way: FRE = 206.835 − 1.015( totalW ords totalSentences ) − 84.6( totalSyllables totalW ords )</p><p>The values returned by the Flesch-Kincaid grade level <ref type="bibr" target="#b27">(Kincaid et al. 1975</ref>) (FKGL) correspond to the number of years of education generally required to understand the text for which the formula was calculated. The formula is defined as follows: FKGL = 0.39( totalW ords totalSentences ) + 11.8( totalSyllables totalW ords ) − 15.59</p><p>Another readability formula that returns values corresponding to the years of education required to understand the text is Automated readability index <ref type="bibr" target="#b49">(Smith and Senter 1967</ref>) (ARI): ARI = 4.71( totalCharacters totalW ords ) + 0.5( totalW ords totalSentences ) − 21.43</p><p>Dale-Chall readability formula <ref type="bibr" target="#b12">(Dale and Chall 1948)</ref> (DCRF) requires a list of 3000 words that fourth-grade US students could reliably understand. Words that do not appear in this list are considered difficult. If the list of words is not available, it is possible to use the GFI approach and consider all the words longer than 7 characters as difficult. The following expression is used in calculation:</p><formula xml:id="formula_0">DCRF = 0.1579( difficultWords totalW ords * 100) + 0.0496( totalW ords totalSentences )</formula><p>The SMOG grade (Simple Measure of Gobbledygook) (Mc Laughlin 1969) is a readability formula originally used for checking health messages. Similar as FKGL and ARI, it roughly corresponds to the years of education needed to understand the text. It is calculated with the following expression:</p><formula xml:id="formula_1">SMOG = 1.0430 numberOfPolysyllables 30 totalSentences + 3.1291,</formula><p>where the numberOfPolysyllables is the number of words with three or more syllables.</p><p>We are aware of one study, which explored the transferability of these formulas across genres <ref type="bibr" target="#b45">(Sheehan, Flor, and Napolitano 2013)</ref>, and one study, which explored transferability across languages (Madrazo Azpiazu and Pera 2020). The study by Sheehan, <ref type="bibr" target="#b45">Flor, and Napolitano (2013)</ref> concludes that mostly due to vocabulary specifics of different genres, traditional readability measures are not appropriate for cross-genre prediction, since they underestimate the complexity levels of literary texts and overestimate that of educational texts. The study by Madrazo Azpiazu and Pera (2020) on the other hand concludes that the readability level predictions for translations of the same text are rarely consistent when using these formulas.</p><p>All of the above-mentioned readability measures were designed for the specific use on English texts. There are some rare attempts to adapt these formulas to other languages <ref type="bibr" target="#b25">(Kandel and Moles 1958)</ref> or to create new formulas that could be used on languages other than English <ref type="bibr" target="#b0">(Anderson 1981)</ref>.</p><p>To show a multilingual potential of our approach, we address two languages in this study, English and Slovenian, a Slavic language with rich morphology and orders of magnitude fewer resources compared to English. For Slovenian, readability studies are scarce. <ref type="bibr">Škvorc et al. (2019)</ref> researched how well the above statistical readability formulas work on Slovenian text by trying to categorize text from three distinct sources: children's magazines, newspapers and magazines for adults, and transcriptions of sessions of the National Assembly of Slovenia. Results of this study indicate that formulas which consider the length of words and/or sentences work better than formulas which rely on word lists. They also noticed that simple indicators of readability, such as percentage of adjectives and average sentence length, work quite well for Slovenian. To our knowledge, the only other study that employed readability formulas on Slovenian texts was done by Zwitter Vitez (2014). Here the readability formulas were used as features in the author recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Discourse cohesion features.</head><p>In the literature, we can find at least two distinct notions of discourse cohesion <ref type="bibr" target="#b50">(Todirascu et al. 2016)</ref>. First is the notion of coherence, defined as the "semantic property of discourse, based on the interpretation of each sentence relative to the interpretation of other sentences" <ref type="bibr" target="#b54">(Van Dijk 1977)</ref>. Previous research which investigates this notion tries to determine whether a text can be interpreted as a coherent message and not just as a collection of unrelated sentences. This can be done by measuring certain observable features of the text, such as the repetition of content words or by analysis of words that explicitly express connectives (e.g., because, consequently, as a result, etc.) <ref type="bibr" target="#b47">(Sheehan et al. 2014)</ref>. A somewhat more investigated, due to its easier operationalization, is the notion of cohesion, defined as "a property of text represented by explicit formal grammatical ties (discourse connectives) and lexical ties that signal how utterances or larger text parts are related to each other".</p><p>According to <ref type="bibr" target="#b50">Todirascu et al. (2016)</ref>, we can divide cohesion features into five distinct classes, outlined below: co-reference and anaphoric chain properties, entity density and entity cohesion features, lexical cohesion measures, and POS tag-based cohesion features. Co-reference and anaphoric chain properties were first proposed by <ref type="bibr" target="#b7">Bormuth (1969)</ref>, who measured various characteristics of anaphora. These features include statistics, such as the average length of reference chains or the proportion of various types of mention (e.g., noun phrases, proper names, etc.) in the chain. Entity density features include statistics such as the total number of all/unique entities per document, the average number of all/unique entities per sentence, etc. These features were first proposed in <ref type="bibr" target="#b17">Huenerfauth (2009) and</ref><ref type="bibr" target="#b18">Feng et al. (2010)</ref> who followed the theoretical line from <ref type="bibr" target="#b22">Halliday and Hasan (1976)</ref> and <ref type="bibr" target="#b58">Williams (2006)</ref>. Entity cohesion features assess relative frequency of possible transitions between syntactic functions played by the same entity in adjacent sentences <ref type="bibr" target="#b43">(Pitler and Nenkova 2008)</ref>. Lexical cohesion measures include features such as the frequency of content word repetition across adjacent sentences <ref type="bibr" target="#b47">(Sheehan et al. 2014</ref>), a Latent Semantic Analysis (LSA) based features for measuring the similarity of words and passages to each other proposed by <ref type="bibr" target="#b29">Landauer (2011)</ref>, or a measure called Lexical Tightness (LT) suggested by <ref type="bibr" target="#b19">Flor, Klebanov, and Sheehan (2013)</ref>, defined as the mean value of the Positive Normalized Pointwise Mutual Information (PMI) for all pairs of content-word tokens in a text. The last category is POS tag-based cohesion features that measure the ratio of pronoun and article parts-of-speech, two crucial elements of cohesion <ref type="bibr" target="#b50">(Todirascu et al. 2016)</ref>. <ref type="bibr" target="#b50">Todirascu et al. (2016)</ref>, who analyzed 65 discourse features found in the readability literature, concluded, that they generally do not contribute much to the predictive power of text readability classifiers when compared to the traditional readability formulas or simple statistics such as sentence length. <ref type="bibr">Collins-Thompson (2014)</ref>, vocabulary knowledge is an important aspect of reading comprehension, and lexico-semantic features measure the difficulty of vocabulary in the text. A common feature is Type-token ratio (TTR), which measures the ratio between the number of unique words and the total number of words in a text. The length of the text influences TTR; therefore, several corrections, which produce a more unbiased representation, such as Root TTR and Corrected TTR, are also used for readability prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Lexico-semantic features. According to</head><p>Other frequently used features in classification approaches to readability are n-gram lexical features, such as word and character n-grams <ref type="bibr" target="#b53">(Vajjala and Meurers 2012;</ref><ref type="bibr" target="#b59">Xia, Kochmar, and Briscoe 2016)</ref>. While part of speech (POS) based lexical features measure lexical variation (i.e. TTR of lexical items such as nouns, adjectives, verbs, adverbs and prepositions) and density (e.g., the percentage of content words and function words), word-list based features use external psycholinguistic and Second Language Acquisition (SLA) resources, which contain information about which words and phrases are acquired at the specific age or English learning class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Syntactic Features.</head><p>Syntactic features measure the grammatical complexity of the text and can be divided into several categories. Parse tree features include features such as an average parse tree height or an average number of noun-or verb-phrases per sentence. Grammatical relations features include measures of grammatical relations between constituents in a sentence, such as the longest/average distance in the grammatical relation sets generated by the parser. Complexity of syntactic unit features measure the length of a syntactic unit at the sentence, clause (any structure with a subject and a finite verb) and T-unit level (one main clause plus any subordinate clause). Finally, coordination and subordination features measure the amount of coordination and subordination in the sentence and include features such as a number of clauses per T-unit or number of coordinate phrases per clause, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5">Language model features.</head><p>The standard task of language modeling can be formally defined as predicting a probability distribution of words from the fixed size vocabulary V , for word w t+1 , given the historical sequence w 1:t = [w 1 , ..., w t ]. To measure its performance, traditionally a metric called perplexity is used. A language model m is evaluated according to how well it predicts a separate test sequence of words w 1:N = [w 1 , ..., w N ]. For this case, the perplexity (PPL) of the language model m() is defined as:</p><formula xml:id="formula_2">PPL = 2 − 1 N N i=1 log 2 m(w i ) ,<label>(1)</label></formula><p>where m(w i ) is the probability assigned to word w i by the language model m, and N is the length of the sequence. The lower the perplexity score, the better the language model predicts the words in a document, i.e. the more predictable and aligned with the training set the text is. All past approaches for readability detection that employ language modeling, leverage older n-gram language models rather than the newer neural language models. <ref type="bibr" target="#b44">Schwarm and Ostendorf (2005)</ref> train one n-gram language model for each readability class c in the training dataset. For each text document d, they calculate the likelihood ratio according to the following formula:</p><formula xml:id="formula_3">LR(d, c) = P (d|c)P (c) c =c P (d|c)P (c) ,</formula><p>where P (d|c) denotes the probability returned by the language model trained on texts labeled with class c, and P (d|c) denotes probability of d returned by the language model trained on the class c. Uniform prior probabilities of classes are assumed. The likelihood ratios are used as features in the classification model along with perplexities achieved by all the models. In <ref type="bibr" target="#b42">Petersen and Ostendorf (2009)</ref>, three statistical language models (unigram, bigram and trigram) are trained on four external data resources: Britannica (adult), Britannica Elementary, CNN (adult) and CNN abridged. The resulting twelve n-gram language models are used to calculate perplexities of each target document. It is assumed that low perplexity scores calculated by language models trained on the adult level texts and high perplexity scores of language models trained on the elementary/abridged levels would indicate a high reading level, and high perplexity scores of language models trained on the adult level texts and low perplexity scores of language models trained on the elementary/abridged levels would indicate a low reading level. <ref type="bibr" target="#b59">Xia, Kochmar, and Briscoe (2016)</ref> train 1-to 5-gram word-based language models on the British National Corpus, and 25 POS-based 1-to 5-gram models on the five classes of the WeeBit corpus. Language models' log-likelihood and perplexity scores are used as features for the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classification approaches based on feature engineering</head><p>The above approaches measure readability in an unsupervised way, using the described features. Alternatively, we can predict the level of readability in a supervised way. These approaches usually require extensive feature engineering and also leverage many of the features described above.</p><p>One of the first classification approaches to readability was proposed by <ref type="bibr" target="#b44">Schwarm and Ostendorf (2005)</ref>. It relies on a Support Vector Machine (SVM) classifier trained on a WeeklyReader corpus 1 , containing articles grouped into four classes according to the age of the target audience. Traditional, syntactic, and language model features are used in the model. This approach was extended and improved upon in <ref type="bibr" target="#b42">Petersen and Ostendorf (2009)</ref>.</p><p>Altogether 155 traditional, discourse cohesion, lexico-semantic and syntactic features were used in an approach proposed by <ref type="bibr" target="#b52">Vajjala and Lučić (2018)</ref>, tested on a recently published OneStopEnglish corpus. Sequential Minimal Optimization (SMO) classifier with the linear kernel achieved the classification accuracy of 78.13% for three readability classes (elementary, intermediate, and advanced reading level).</p><p>A successful classification approach to readability was proposed by <ref type="bibr" target="#b53">Vajjala and Meurers (2012)</ref>. Their multi-layer perceptron classifier is trained on the WeeBit corpus (Vajjala and Meurers 2012) (see Section 3 for more information on WeeBit and other mentioned corpora). The texts were classified into five classes according to the age group they are targeting. For classification, the authors use 46 manually crafted traditional, lexico-semantic and syntactic features. For the evaluation, they trained the classifier on a train set consisting of 500 documents from each class and tested it on a balanced test set of 625 documents (containing 125 documents per each class). They report 93.3% accuracy on the test set 2 .</p><p>Another set of experiments on the WeeBit corpus was conducted by <ref type="bibr" target="#b59">Xia, Kochmar, and Briscoe (2016)</ref> who conducted additional cleaning of the corpus since it contained some texts with broken sentences and additional meta-information about the source of the text, such as copyright declaration and links, strongly correlated with the target labels. They use similar lexical, syntactic, and traditional features as <ref type="bibr" target="#b53">Vajjala and Meurers (2012)</ref> but add language modeling (see Section 2.1.5 for details) and discourse cohesion based features. Their SVM classifier achieves 80.3% accuracy using the 5-fold crossvalidation. This is one of the studies where the transferability of the classification models is tested. Authors used an additional CEFR (Common European Framework of Reference for Languages) corpus. This small dataset of CEFR-graded texts is tailored for learners of English (Council of Europe 2001) and also contains 5 readability classes. The SVM classifier trained on the WeeBit corpus and tested on the CEFR corpus achieved the classification accuracy of 23.3%, hardly beating the majority classifier baseline. This low result was attributed to the differences in readability classes in both corpora, since WeeBit classes are targeting children of different age groups, and CEFR corpus classes are targeting mostly adult foreigners with different levels of English comprehension. However, this result is a strong indication that transferability of readability classification models across different types of texts is questionable.</p><p>Two other studies that deal with the multi-genre prospects of readability prediction were conducted by <ref type="bibr" target="#b45">Sheehan, Flor, and Napolitano (2013)</ref> and <ref type="bibr" target="#b39">Napolitano, Sheehan, and Mundkowsky (2015)</ref>. Both studies describe the problem in the context of the TextEvaluator Tool <ref type="bibr" target="#b46">(Sheehan et al. 2010</ref>), an online system for text complexity analysis. The system supports multi-genre readability prediction with the help of a two-stage prediction workflow, in which first the genre of the text is determined (as being informational, literary or mixed) and after that its readability level is predicted with an appropriate genre-specific readability prediction model. Similarly to the study above, this work also indicates that using classification models for cross-genre prediction is not feasible.</p><p>When it comes to multi-and cross-lingual classification, Madrazo Azpiazu and Pera (2020) explore the possibility of a cross-lingual readability assessment and show that their methodology called CRAS (Cross-lingual Readability Assessment Strategy), which includes building a classifier that employs a set of traditional, lexico-semantic, syntactic and discourse cohesion based features works well in a multilingual setting. They also show that classification for some low resource languages can be improved by including documents from a different language into the train set for a specific language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Neural classification approaches</head><p>Recently, several neural approaches for readability prediction have been proposed. <ref type="bibr" target="#b38">Nadeem and Ostendorf (2018)</ref> tested two different architectures on the WeeBit corpus regression task, namely sequential Gated recurrent unit (GRU) ) based RNN with the attention mechanism and hierarchical RNNs <ref type="bibr" target="#b62">(Yang et al. 2016)</ref> with two distinct attention types: a more classic attention mechanism proposed by <ref type="bibr" target="#b2">Bahdanau, Cho, and Bengio (2014)</ref>, and multi-head attention proposed by <ref type="bibr" target="#b55">Vaswani et al. (2017)</ref>. The results of the study indicate that hierarchical RNNs generally perform better than sequential. <ref type="bibr" target="#b38">Nadeem and Ostendorf (2018)</ref> also show that neural networks can be a good alternative to more traditional feature-based models for readability prediction on texts shorter than 100 words, but do not perform that competitively on longer texts.</p><p>Another version of a hierarchical RNN with the attention mechanism was proposed by <ref type="bibr" target="#b1">Azpiazu and Pera (2019)</ref>. Their system, named Vec2Read, is a multi-attentive RNN capable of leveraging hierarchical text structures with the help of word and sentence level attention mechanisms and a custom-built aggregation mechanism. They employed the network in a multilingual setting (on corpora containing Basque, Catalan, Dutch, English, French, Italian, and Spanish texts). Their conclusion was, that while the number of instances used for training has a strong effect on the overall performance of the system, no language-specific patterns emerged that would indicate that prediction of readability in some languages is harder than in others.</p><p>An even more recent neural approach for readability classification on the cleaned WeeBit corpus <ref type="bibr" target="#b59">(Xia, Kochmar, and Briscoe 2016)</ref> was proposed by Filighera, Steuer, and Rensing (2019), who tested a set of different embedding models, word2vec <ref type="bibr" target="#b37">(Mikolov et al. 2013)</ref>, the uncased Common Crawl GloVe <ref type="bibr" target="#b40">(Pennington, Socher, and Manning 2014)</ref>, ELMo <ref type="bibr" target="#b41">(Peters et al. 2018)</ref>, and BERT <ref type="bibr" target="#b15">(Devlin et al. 2019)</ref>. The embeddings were fed to either a recurrent or a convolutional neural network. The BERT-based approach from their work is somewhat similar to the BERT-based supervised classification approach proposed in this work. However, one main distinction is that no fine-tuning is conducted on the BERT model in their experiments, i.e. the extraction of embeddings is conducted on the pretrained BERT language model. Their best ELMo-based model with a bidirectional LSTM achieved an accuracy of 79.2% on the development set, slightly lower than the accuracy of 80.3% achieved by <ref type="bibr" target="#b59">Xia, Kochmar, and Briscoe (2016)</ref> in the 5-fold cross-validation scenario. However, they did manage to improve on the stateof-the-art by an ensemble of all their models, achieving the accuracy of 81.3%, and the macro averaged F 1 -score of 80.6%.</p><p>A somewhat different neural approach to readability classification was proposed by Mohammadi and Khasteh <ref type="formula" target="#formula_2">(2019)</ref>, who tackled the problem with deep reinforcement learning, or more specifically, with a deep convolutional recurrent double dueling Q network <ref type="bibr" target="#b57">(Wang et al. 2016</ref>) using a limited window of 5 adjacent words. GloVe embeddings and statistical language models were used to represent the input text in order to eliminate the need for sophisticated NLP features. The model was used in a multilingual setting (on English and Persian datasets) and achieved performance comparable to the state-of-the-art on all of the datasets, among them also on the Weebit corpus (accuracy of 91%).</p><p>Finally, a recent study by Deutsch, Jasbi, and Shieber (2020) used predictions of HAN and BERT models as additional features in their SVM model that also employed a set of syntactic and lexico-semantic features. While they did manage to improve the performance of their SVM classifiers with the additional neural features, they concluded that additional syntactic and lexico-semantic features did not generally improve the predictions of the neural models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets</head><p>In this section, we first present the datasets used in the experiments (Section 3.1) and then conduct their preliminary analysis (Section 3.2) in order to assess the feasibility of the proposed experiments. Dataset statistics are presented in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset presentation</head><p>All experiments are conducted on four corpora labelled with readability scores:</p><p>•</p><p>The WeeBit corpus: The articles from WeeklyReader 3 and BBC-Bitesize 4 are classified into five classes according to the age group they are targeting. The classes correspond to age groups between 7-8, 8-9, 9-10, 10-14 and 14-16. Three classes targeting younger audiences consist of articles from WeeklyReader, an educational newspaper that covers a wide range of non-fiction topics, from science to current affairs. Two classes targeting older audiences consist of material from the BBC-Bitesize website, containing educational material categorized into topics that roughly match school subjects in the UK. In the original corpus of <ref type="bibr" target="#b53">Vajjala and Meurers (2012)</ref>, the classes are balanced and the corpus contains altogether 3125 documents, 625 per class. In our experiments, we followed recommendations of <ref type="bibr" target="#b59">Xia, Kochmar, and Briscoe (2016)</ref> to fix broken sentences and remove additional meta information, such as copyright declaration and links, strongly correlated with the target labels. We reextracted the corpus from the HTML files according to the procedure described in <ref type="bibr" target="#b59">Xia, Kochmar, and Briscoe (2016)</ref> and discarded some documents due to the lack of content after the extraction and cleaning process. The final corpus used in our experiments contains altogether 3000 documents, 600 per class.</p><p>• The OneStopEnglish corpus <ref type="bibr" target="#b52">(Vajjala and Lučić 2018)</ref> contains aligned texts of three distinct reading levels (beginner, intermediate, and advanced) that were written specifically for English as Second Language (ESL) learners. The corpus was compiled over the period 2013-2016 from the weekly news lessons section of the language learning resources onestopenglish.com. The section contains articles sourced from the Guardian newspaper, which were rewritten by English teachers to target three levels of adult ESL learners (elementary, intermediate, and advanced). Overall, the document aligned parallel corpus consists of 189 texts, each written in three versions (567 in total). The corpus is freely available 5 .</p><p>• The Newsela corpus <ref type="bibr" target="#b61">(Xu, Callison-Burch, and Napoles 2015)</ref>. We use the version of the corpus from 29 January 2016 consisting of altogether 10 786 documents, out of which we only used 9565 English documents. The corpus contains 1911 original English news articles and up to four simplified versions for every original article, i.e., each original news article has been manually rewritten up to 4 times by editors at Newsela, a company that produces reading materials for pre-college classroom use, in order to target children at different grade levels and help teachers prepare curricula that match the English language skills required at each grade level. The dataset is a document aligned parallel corpus of original and simplified versions corresponding to altogether eleven different imbalanced grade levels (from 2nd to 12th grade).</p><p>•</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus of Slovenian school books (Slovenian SB):</head><p>In order to test the transferability of the proposed approaches to other languages, a corpus of Slovenian school books was compiled. The corpus contains 3 639 665 words in 125 school books for nine grades of primary schools and four grades of secondary school. It was created with several aims, like studying different quality aspects of school books, extraction of terminology, and linguistic analysis. The corpus contains school books for sixteen distinct subjects with very different topics ranging from literature, music and history to math, biology and chemistry, but not in equal proportions, with readers being the largest type of school books included. While some texts were extracted from the Gigafida reference corpus of written Slovene <ref type="bibr" target="#b30">(Logar et al. 2012</ref>), most of the texts were extracted from PDF files. After the extraction, we first conduct some light manual cleaning on the extracted texts (i.e., removal of indices, copyright statements, references, etc.). Next, in order to remove additional noise (e.g., tips, equations, etc.), we apply a filtering script that relies on manually written rules for sentence extraction (e.g., a text is a sentence if it starts with an uppercase and ends with an end of sentence punctuation) to obtain only passages containing sentences. Final extracted texts come without structural information (e.g., where does a specific chapter end or start, which sentences constitute a paragraph, where are questions, etc.), since labelling the document structure would require a large amount of manual effort; therefore we did not attempt it for this research. For supervised classification experiments, we split the school books into chunks twenty-five sentences long, in order to build a train and test set with a sufficient number of documents 6 . The length of twenty-five sentences was chosen due to size limitations of the BERT classifier, which can be fed documents that contain up to 512 byte-pair tokens (Kudo and Richardson 2018) 7 , which on average translates to slightly less than 25 sentences.</p><p>Language models are trained on large corpora of texts. For this purpose, we used the following corpora.</p><p>•</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus of English Wikipedia and Corpus of Simple Wikipedia articles.</head><p>We created three corpora for the use in our unsupervised English experiments 8 : -Wiki-normal contains 130 000 randomly selected articles from the Wikipedia dump, which comprise of 489 976 sentences and 10 719 878 tokens.</p><p>6 Note that this chunking procedure might break the text cohesion and that topical similarities between chunks from the same chapter (or paragraphs) might have a positive effect on the performance of the classification. However, since the corpus does not contain any high-level structural information (e.g., the information about paragraph or chapter structure of a specific school book), no other more refined chunking method is possible. 7 Note that BERT tokenizer employs byte-pair tokenization <ref type="bibr" target="#b28">(Kudo and Richardson 2018)</ref>, which in some cases generates tokens that correspond to sub-parts of words rather than entire words. In case of Slovenian SB, 512 byte-pair tokens correspond to 306 word tokens on average. 8 English Wikipedia and Simple Wikipedia dumps from 26th of January 2018 were used for the corpus construction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset analysis</head><p>Overall, there are several differences between our datasets:</p><p>• Language: As already mentioned before, we have three English (Newsela, OneStopEnglish and WeeBit) and one Slovenian (Slovenian SB) test dataset.</p><p>• Parallel corpora vs unaligned corpora: Newsela and OneStopEnglish datasets are parallel corpora, which means that articles from different readability classes are semantically similar to each other. On the other hand, WeeBit and Slovenian SB datasets contain completely different articles in each readability class. While this might not affect traditional readability measures, which do not take semantic information into account, it might prove substantial for the performance of classifiers and the proposed language model based readability measures. Wikipedia is that no specific target audience is addressed since articles are written by volunteers. In fact, using Simple Wikipedia as a dataset for the training of simplification algorithms has been criticized in the past due to lack of specific simplification guidelines, which are based only on the declarative statement that Simple Wikipedia was created for "children and adults who are learning the English language" <ref type="bibr" target="#b61">(Xu, Callison-Burch, and Napoles 2015)</ref>. This lack of guidelines also contributes to the decrease in the quality of simplification according to <ref type="bibr" target="#b61">Xu, Callison-Burch, and Napoles (2015)</ref>, who found that the corpus can be noisy and that half of its sentences are not actual simplifications but rather copied from the original Wikipedia.</p><p>This diversity of the datasets limits ambitions of the study to offer general conclusions true across genres, languages, or datasets. On the other hand, it offers an opportunity to determine how specifics of each dataset affect each of the proposed readability predictors and also to determine the overall robustness of the applied methods.</p><p>While many aspects differ from one dataset to another, there are also some common characteristics across all the datasets, which allow using the same prediction methods on all of them. These are mostly connected to the common techniques used in the construction of the readability datasets, no matter the language, genre, or target audience of the specific dataset. The creation of parallel simplification corpora (i.e. Newsela, OneStopEnglish, and Simple Wikipedia) generally involves three techniques, splitting (breaking a long sentence into shorter ones), deletion (removing unimportant parts of a sentence), and paraphrasing (rewriting a text into a simpler version via reordering, substitution, and occasionally expansion) <ref type="bibr" target="#b16">(Feng 2008)</ref>. Even though there might be some subtleties involved (since what constitutes simplification for one type of user may not be appropriate for another) how these techniques are applied is rather general. Also, while there is no simplification used in the non-parallel corpora (WeeBit, Slovenian SB), the contributing authors were nevertheless instructed to write the text for a specific target group and adapt the writing style accordingly. In most cases, this leads to the same result, e.g., shorter less complex sentences and simpler vocabulary used in texts intended for younger or less fluently speaking audiences.</p><p>The claim of commonality between datasets can be backed up by the fact, that even traditional readability indicators correlate quite well to human assigned readability, no matter the specific genre, language, or purpose of each dataset. Results in <ref type="table" target="#tab_2">Table  2</ref> demonstrate this point by showcasing readability scores of traditional readability formulas from Section 2.1.1. We can see that the general pattern of increased difficulty on all datasets and for all indicators -larger readability scores (or in case of FRE, smaller) are assigned to those classes of the dataset that contain texts written for older children or more advanced ESL learners. This suggests that multi-dataset, multi-genre and even multi-lingual readability prediction is feasible on the set of chosen datasets, even if only the shallow traditional readability indicators are used.</p><p>However, the results do indicate that cross-genre or even cross-dataset readability prediction might be problematic since the datasets do not cover the same readability range according to the shallow prediction formulas (and also ground truth readability labels). For example, documents in the WeeBit 14-16 age group have scores very similar to the Newsela 6th grade documents, which means that a classifier trained on the WeeBit corpus might have a hard time classifying documents belonging to higher Newsela grades since the readability of these documents is lower than for the most complex documents in the WeeBit corpus according to all of the shallow readability indicators. For this reason, we opted not to perform any supervised cross-dataset or cross-genre experiments. Nevertheless, the problem of cross-genre prediction is important in the context of the proposed unsupervised experiments, since the genre discrepancy between the datasets used for training the language models and the datasets on which the models are employed, might influence the performance of the proposed language model based measures. A more detailed discussion on this topic is presented in Section 4.2. The analysis in <ref type="table" target="#tab_2">Table 2</ref> also confirms the findings by Madrazo Azpiazu and Pera (2020), who have shown that cross-lingual readability prediction with shallow readability indicators is problematic. For example, if we compare the Newsela corpus and Slovenian SB corpus, which both cover roughly the same age group, we can see that for some readability indicators (FRE, FKGL, DCRF, and ASL) the values are on entirely different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Unsupervised neural approach</head><p>In this section, we explore how neural language models can be used for determining the readability of the text in an unsupervised way. In Section 4.1, we present the neural architectures used in our experiments, in Section 4.2, we describe the methodology of the proposed approach, and in Section 4.3, we present the conducted experiments. <ref type="bibr" target="#b36">Mikolov et al. (2011)</ref> have shown that neural language models outperform n-gram language models by a high margin on large and also relatively small (less than 1 million tokens) datasets. The achieved differences in perplexity (see Eq. <ref type="formula" target="#formula_2">(1)</ref>) are attributed to a richer historical contextual information available to neural networks, which are not limited to a small contextual window (usually of up to five previous words) as is the case of n-gram language models. In Section 2.1.5, we mentioned some approaches that use n-gram language models for readability prediction. However, we are unaware of any approach that would employ deep neural network language models for determining the readability of a text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Neural language model architectures</head><p>In this research, we employ three neural architectures for language modelling. First are recurrent neural networks (RNN), which are suitable for modelling sequential data. At each time step t, the input vector x t , and hidden state vector h t-1 are feed into the network, producing the next hidden vector state h t with the following recursive equation:</p><formula xml:id="formula_4">h t = f (W x t + U h t-1 + b),</formula><p>where f is a non-linear activation function, W and U are matrices representing weights of the input layer and hidden layer, and b is the bias vector. Learning long-range input dependencies with plain RNNs is problematic due to vanishing gradients <ref type="bibr" target="#b5">(Bengio, Simard, and Frasconi 1994)</ref>, therefore, in practice, modified recurrent networks, such as Long short-term memory networks (LSTM) are used. In our experiments, we use the LSTM-based language model proposed by <ref type="bibr" target="#b26">Kim et al. (2016)</ref>. This architecture is adapted to language modelling of morphologically rich languages, such as Slovenian, by employing an additional character-level convolutional neural network (CNN). The convolutional level learns a character structure of words and is connected to the LSTMbased layer, which produces predictions at the word level. <ref type="bibr" target="#b3">Bai, Kolter, and Koltun (2018)</ref> introduced a new sequence modelling architecture based on convolution, called temporal convolutional network (TCN), which is also employed in our experiments. TCN uses causal convolution operations, which make sure that there is no information leakage from future time steps to the past. This and the fact that TCN takes a sequence as an input and maps it into an output sequence of the same size makes this architecture appropriate for language modelling. TCNs are capable of leveraging long contexts by using a very deep network architecture and a hierarchy of dilated convolutions. A single dilated convolution operation F on element s of the 1-dimensional sequence x can be defined with the following equation:</p><formula xml:id="formula_5">F (s) = (x * d f )(s) = k−1 i=0 f (i) · x s−d·i ,</formula><p>where f : 0, . . . k − 1 is a filter of size k, d a dilation factor and s − d · i accounts for the direction of the past. In this way, the context taken into account during the prediction can be increased by using larger filter sizes and by increasing the dilation factor. The most common practice is to increase the dilation factor exponentially with the depth of the network.</p><p>Recently, <ref type="bibr" target="#b15">Devlin et al. (2019)</ref> proposed a novel approach to language modelling. Their BERT (Bidirectional Encoder Representations from Transformers) uses both left and right context, which means that a word w t in a sequence is not determined just from its left sequence w 1:t-1 = [w 1 , ..., w t−1 ] but also from its right word sequence w t+1:n = [w t+1 , ..., w t+n ]. This approach introduces a new learning objective, a masked language model, where a predefined percentage of randomly chosen words from the input word sequence is masked, and the objective is to predict these masked words from the unmasked context. BERT uses a transformer neural network architecture <ref type="bibr" target="#b55">(Vaswani et al. 2017)</ref>, which relies on the self-attention mechanism. The distinguishing feature of this approach is the employment of several parallel attention layers, the so-called attention heads, which reduce the computational cost and allow the system to attend to several dependencies at once.</p><p>All types of neural network language models, TCN, LSTM, and BERT, output softmax probability distribution calculated over the entire vocabulary, and present the probabilities for each word given its historical (and in case of BERT also future) sequence. Training of these networks usually minimizes the negative log-likelihood (NLL) of the training corpus word sequence w 1:n = [w 1 , ..., w n ] by backpropagation through time:</p><formula xml:id="formula_6">NLL = − n i=1 log P (w i |w 1:i-1 )<label>(2)</label></formula><p>In case of BERT, the formula for minimizing NLL uses also the right-hand word sequence:</p><formula xml:id="formula_7">NLL = − n i=1 log P (w i |w 1:i-1 , w i+1:n ),</formula><p>where w i are the masked words.</p><p>The following equation, which is used for measuring the perplexity of neural language models, defines the relationship between perplexity (PPL, see Eq. (1)) and NLL (Eq. (2)):</p><formula xml:id="formula_8">PPL = e ( NLL N )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unsupervised methodology</head><p>Two main questions we wish to investigate in the unsupervised approach are the following:</p><p>• Can standalone neural language models be used for unsupervised readability prediction?</p><p>• Can we develop a robust new readability formula that will outperform traditional readability formulas by relying not only on shallow lexical sophistication indicators but also on neural language model statistics?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Language models for unsupervised readability assessment.</head><p>The findings of the related research suggest that a separate language model should be trained for each readability class in order to extract features for successful readability prediction <ref type="bibr" target="#b42">(Petersen and Ostendorf 2009;</ref><ref type="bibr" target="#b59">Xia, Kochmar, and Briscoe 2016)</ref>. On the other hand, we test the possibility of using a neural language model as a standalone unsupervised readability predictor. First two points that support this kind of usage are based on the fact that neural language models tend to capture much more information compared to the traditional n-gram models. First, since n-gram language models used in the previous work on readability detection were in most cases limited to a small contextual window of up to five words, their learning potential was limited to lexico-semantic information (e.g., information about the difficulty of vocabulary and word n-gram structures in the text), and information about the text syntax. We argue that due to much larger contextual information of the neural models (e.g., BERT leverages sequences of up to 512 byte-pair tokens), which spans across sentences, the neural language models also learn high-level textual properties, such as long-distance dependencies <ref type="bibr" target="#b23">(Jawahar, Sagot, and Seddah 2019)</ref>, in order to minimize NLL during training. Secondly, n-gram models in the past readability research have only been trained on the corpora (or more specifically, on parts of the corpora) on which they were later employed. In contrast, by training the neural models on large general corpora, the model also learns semantic information, which can be transferred when the model is employed on a smaller test corpus. The success of this knowledge transfer is, to some extent, dependent on the genre compatibility of the train and test corpora.</p><p>The third point favouring greater flexibility of neural language models relies on the fact that no corpus is a monolithic block of text made out of units (i.e. sentences, paragraphs, and articles) of exactly the same readability level. This means that a language model trained on a large corpus will be exposed to chunks of text with different levels of complexity. We hypothesize that due to this fact, the model will to some extent be able to distinguish between these levels and return a lower perplexity for more standard, predictable (i.e. readable) text. Vice versa, complex and rare language structures and vocabulary of less readable texts would negatively affect the performance of the language model, expressed via larger perplexity score. If this hypothesis is correct, ideally, the average readability of the training corpus should fit somewhere in the middle of the readability spectre of the testing corpus.</p><p>To test the above statements, we train language models on Wiki-normal, Wikisimple, and Wiki-balanced corpora described in Section 3. All three Wiki corpora contain roughly the same amount of text, in order to make sure that the training set size does not influence the results of the experiments. We expect the following results:</p><p>• Hypothesis 1: Training the language models on a corpus with a readability that fits somewhere in the middle of the readability spectre of the testing corpus will yield the best correlation between the language model's performance and readability. According to the preliminary analysis of our corpora conducted in Section 3.2 and results of the analysis in <ref type="table" target="#tab_2">Table 2</ref>, this ideal scenario can be achieved in three cases: i) if a language model trained on the Wiki-simple is employed on the Newsela corpora, ii) if a language model trained on the Wiki-balanced corpus is employed on the OneStopEnglish corpus, and iii) if the model trained on the KRES-balanced corpus is employed on the Slovenian SB corpus, despite the mismatch of genres in these corpora.</p><p>• Hypothesis 2: The language models trained only on texts for adults (Wiki-normal) will show higher perplexity on texts for children (WeeBit and Newsela) since their training set did not contain such texts; this will negatively affect the correlation between the language model's performance and readability.</p><p>• Hypothesis 3: Training the language models only on texts for children (Wiki-simple corpus) will result in a higher perplexity score of the language model when applied to adult texts (OneStopEnglish). This will positively affect the correlation between the language model's performance and readability. However, this language model will not be able to reliably distinguish between texts for different levels of adult ESL learners, which will have a negative effect on the correlation.</p><p>To further test the viability of the unsupervised language models as readability predictors and to test the limits of using a single language model, we also explore the possibility of using a language model trained on a large general corpus. English BERT language model was trained on large corpora (Google Books Corpus <ref type="bibr" target="#b20">(Goldberg and Orwant 2013)</ref> and Wikipedia) of about 3300M words containing mostly texts for adult English speakers. According to hypothesis 2 above, this will have a negative effect on the correlation between the performance of the model and readability.</p><p>Due to the large size of the BERT's model and its huge training corpus, the semantic information acquired during training is much larger than the information acquired by the models we train on our much smaller corpora, which means that there is a greater possibility that the BERT model was trained on some text semantically similar to the content in the test corpora and that this information can be successfully transferred.</p><p>However, the question remains, exactly what type of semantic content does the BERT's training corpus contain. One hypothesis is that its training corpus contains more content specific for adult audiences and less content found in the corpora for children. This would have a negative effect on the correlation between the performance of the model and readability on the WeeBit corpus. Contrarily, since the two highest readability classes in the WeeBit corpus contain articles from different scientific fields used for the education of high school students, which can contain rather specific and technical content that is unlikely to be common in the general training corpus, this might influence a positive correlation between the performance of the model and readability. The Newsela and OneStopEnglish, on the other hand, are parallel corpora, which means that the semantic content in all classes is very similar; therefore the success or failure of semantic transfer will most likely not affect these two corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Ranked sentence readability score.</head><p>Based on the two considerations below, we propose a new Ranked Sentence Readability Score (RSRS) for measuring the readability with language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The shallow lexical sophistication indicators, such as the length of a sentence, correlate well with the readability of a text. Using them besides statistics derived from language models could improve the unsupervised readability prediction.</p><p>• The perplexity score used for measuring the performance of a language model is an unweighted sum of perplexities of words in the predicted sequence. In reality, a small number of unreadable words might drastically reduce the readability of the entire text. Assigning larger weights to such words might improve the correlation of language model scores with the readability.</p><p>The proposed readability score is calculated with the following procedure. First, a given text is split into sentences with the default sentence tokenizer from the NLTK library <ref type="bibr" target="#b6">(Bird and Loper 2004)</ref>. In order to get a readability estimation for each word in a specific context, we compute, for each word in the sentence, the word negative loglikelihood (WNLL) according to the following formula:</p><formula xml:id="formula_9">WNLL = −(y t log y p + (1 − y t ) log (1 − y p )),</formula><p>where y p denotes the probability (from the softmax distribution) predicted by the language model according to the historical sequence, and y t denotes the empirical distribution for a specific position in the sentence, i.e. y t has the value 1 for the word in the vocabulary that actually appears next in the sequence and the value 0 for all the other words in the vocabulary. Next, we sort all the words in the sentence in ascending order according to their WNLL score, and the ranked sentence readability score (RSRS) is calculated with the following expression:</p><formula xml:id="formula_10">RSRS = S i=1 √ i · WNLL(i) S ,<label>(3)</label></formula><p>where S denotes the sentence length and i represents the rank of a word in a sentence according to its WNLL value. The square root of the word rank is used for proportion-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WNLL calculation</head><p>This could make social interactions easier for them . 0.0034</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1</head><p>The RSRS calculation for the sentence This could make social interactions easier for them.</p><p>ally weighting words according to their readability since initial experiments suggested that the use of a square root of a rank represents the best balance between allowing all words to contribute equally to the overall readability of the sentence and allowing only the least readable words to affect the overall readability of the sentence. For out of vocabulary words, square root rank weights are doubled, since these rare words are, in our opinion, good indicators of non-standard text. Finally, in order to get the readability score for the entire text, we calculate the average of all the RSRS scores in the text. An example of how RSRS is calculated for a specific sentence is shown in <ref type="figure">Figure 1</ref>. The main idea behind the RSRS score is to avoid the reductionism of traditional readability formulas. We aim to achieve this by including high-level structural and semantic information through neural language model based statistics. The first assumption is that complex grammatical and lexical structures harm the performance of the language model. Since WNLL score, which we compute for each word, depends on the context in which the word appears in, words appearing in more complex grammatical and lexical contexts will have a higher WNLL. The second assumption is that the semantic information is included in the readability calculation: tested documents with semantics dissimilar to the documents in the language model training set will negatively affect the performance of the language model, resulting in the higher WNLL score for words with unknown semantics. The trainable nature of language models allows for customization and personalization of the RSRS for specific tasks, topics and languages. This means that RSRS shall alleviate the problem of cultural non-transferability of traditional readability formulas.</p><p>On the other hand, the RSRS also leverages shallow lexical sophistication indicators through the index weighting scheme, which makes sure that less readable words contribute more to the overall readability score. This is somewhat similar to the counts of long and difficult words in the traditional readability formulas, such as GFI and DCRF.</p><p>The value of RSRS also increases for texts containing longer sentences, since the square roots of the word rank weights become larger with increased sentence length. This is similar to the behaviour of traditional formulas such as GFI, FRE, FKGL, ARI, and DCRF, where this effect is achieved by incorporating the ratio between the total number of words and the total number of sentences into the equation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Unsupervised experiments</head><p>For the presented unsupervised readability assessment methodology based on neural language models, we first present the experimental design followed by the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Experimental design.</head><p>Three different architectures of language models (described in Section 4.1) are used for experiments: a temporal convolutional network (TCN) proposed by <ref type="bibr" target="#b3">Bai, Kolter, and Koltun (2018)</ref>, a recurrent language model (RLM) using character-level CNN and LSTM proposed by <ref type="bibr" target="#b26">Kim et al. (2016)</ref>, and an attention-based language model BERT <ref type="bibr" target="#b15">(Devlin et al. 2019)</ref>. For the experiments on the English language, we train TCN and RLM on three Wiki corpora.</p><p>To explore the possibility of using a language model trained on a general corpus for the unsupervised readability prediction, we use the bert-base-uncased English language model, a pretrained uncased language model trained on BooksCorpus (0.8G words) <ref type="bibr" target="#b64">(Zhu et al. 2015)</ref> and English Wikipedia (2.5G words). For the experiments on Slovenian language, the corpus containing just school books is too small for efficient training of language models; therefore TCN and RLM were only trained on the KRES-balanced corpus described in Section 3. For exploring the possibility of using a general language model for the unsupervised readability prediction, a pretrained CroSloEngual BERT model trained on corpora from three languages, Slovenian (1.26G words), Croatian (1.95G words), and English (2.69G words) <ref type="bibr" target="#b51">(Ulčar and Robnik-Šikonja 2020)</ref>, is used. The corpora used in training of the model are a mix of news articles and a general web crawl.</p><p>The performance of language models is typically measured with the perplexity (see Eq. <ref type="formula" target="#formula_2">(1)</ref>). To answer the research question if standalone language models can be used for unsupervised readability prediction, we investigate how the measured perplexity of language models correlates with the readability labels in the gold-standard WeeBit, OneStopEnglish, Newsela, and Slovenian SB corpora described in Section 3. The correlation to these ground truth readability labels is also used to evaluate the performance of the RSRS measure. For performance comparison, we calculate the traditional readability formula values (described in Section 2) for each document in the gold-standard corpora and measure the correlation between these values and manually assigned labels. As a baseline, we use the average sentence length (ASL) in each document.</p><p>The correlation is measured with the Pearson correlation coefficient (ρ). Given a pair of distributions X and Y , the covariance cov, and the standard deviation σ, the formula for ρ is:</p><formula xml:id="formula_11">ρ x,y = cov(x, y) σ x σ y</formula><p>A larger positive correlation signifies a better performance for all measures except the FRE readability measure. As this formula assigns higher scores to better readable texts, a larger negative correlation suggests a better performance of the FRE measure. <ref type="table">Table 3</ref>. The ranking of measures on English and Slovenian datasets are presented in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Experimental results. The results of the experiments are presented in</head><p>The correlation coefficients of all measures vary drastically between different corpora. The highest ρ values are obtained on the Newsela corpus, where the best performing measure (surprisingly this is our baseline -the average sentence length) achieves the ρ of 0.906. The highest ρ on the other two English corpora are much lower. On the WeeBit corpus, the best performance is achieved by GFI and FKGL measures (ρ of 0.544), and on the OneStopEnglish corpus, the best performance is achieved with the proposed TCN RSRS-simple (ρ of 0.615). On the Slovenian SB, the ρ values are higher, and the best performing measure is TCN RSRS score-balanced with ρ of 0.789.</p><p>The perplexity-based measures show a much lower correlation with the ground truth readability scores. Overall, they perform the worst of all the measures for both languages (see <ref type="table">Table 4</ref>), but we can observe large differences in their performance across different corpora. While there is either no correlation or low negative correlation between perplexities of all three language models and readability on the WeeBit corpus, there is some correlation between perplexities achieved by RLM and TCN on OneStopEnglish and Newsela corpora (the highest being the ρ of 0.566 achieved by TCN perplexity-simple on the Newsela corpus). The correlation between RLM and TCN perplexity measures and readability classes on the Slovenian SB corpus is low, <ref type="table">Table 3</ref> Pearson correlation coefficient between manually assigned readability labels and the readability scores assigned by different readability measures in the unsupervised setting. The highest correlation for each corpus is marked with the bold typeface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measure/Dataset</head><p>WeeBit for Slovenian since the Slovenian corpus on which the CroSloEngual BERT was trained is smaller than the English corpora used for training of English BERT. While further experiments and data are needed to pinpoint the exact causes for the discrepancies in the results, we can still conclude that using a single language model trained on general corpora for unsupervised readability prediction of texts for younger audiences or English learners is, at least according to our results, not a viable option. Regarding our expectations that performance of the language model trained on a corpus with average readability that fits somewhere in the middle of the readability spectre of the testing corpus would yield the best correlation with manually labelled readability scores, it is interesting to look at the differences in performance between TCN and RLM perplexity measures trained on Wiki-normal, Wiki-simple and Wikibalanced corpora. As expected, the correlation scores are worse on the WeeBit corpus, since all classes in this corpus contain texts that are less complex than texts in any of the training corpora. On the OneStopEnglish corpus, both Wiki-simple perplexity measures perform the best, which is unexpected, since we would expect the balanced measure to perform better. On the Newsela corpus, RLM perplexity-balanced outperforms RLM perplexity-simple by 0.042 (which is unexpected), and TCN perplexity-simple outperforms TCN perplexity-balanced by 0.029, which is according to the expectations. Also, according to the expectation is the fact, that both Wiki-normal perplexity measures are outperformed by a large margin by Wiki-simple and Wiki-balanced perplexity measures on the OneStopEnglish and the Newsela corpora. Similar observations can be made in regards to RSRS, which also leverages language model statistics. On all corpora, the performance of Wiki-simple RSRS measures and Wiki-balanced RSRS measures is comparable, and these measures consistently outperform Wiki-normal RSRS measures.</p><p>These results are not entirely compatible with hypothesis 1 in Section 4.2.1 that Wiki-balanced measures would be most correlated with readability on the On-eStopEnglish corpus and that Wiki-simple measures would be most correlated with readability on the Newsela corpus. Nevertheless, training the language models on the corpora with readability in the middle of the readability spectre of the test corpus seems to be an effective strategy, since the differences in performance between Wiki-balanced and Wiki-simple measures are not large. On the other hand, the good performance of the Wiki-simple measures supports our hypothesis 3 in Section 4.2.1, that training the language models on texts with the readability closer to the bottom of the readability spectrum of the test corpus for children will result in a higher perplexity score of the language model when applied to adult texts, which will have a positive effect on the correlation with readability.</p><p>The fact that positive correlation between readability and both Wiki-simple and Wiki-balanced perplexity measures on the Newsela and OneStopEnglish corpora is quite strong supports the hypothesis that more complex language structures and vocabularies of less readable texts would result in a higher perplexity on these texts. Interestingly, strong correlations also indicate that the genre discrepancies between the language model train and test sets do not appear to have a strong influence on the performance. While the choice of a neural architecture for language modelling does not appear to be that crucial, the readability of the language model training set is of utmost importance. If the training set on average contains more complex texts than the majority of texts in the test set, as in the case of language models trained just on the Wiki-normal corpus (and also BERTs), the correlation between readability and perplexity disappears or even gets reverted, since language models trained on more complex language structures learn how to handle these difficulties.</p><p>The low performance of perplexity measures suggests that neural language model statistics are not good indicators of readability and should therefore not be used alone for readability prediction. Nevertheless, the results of TCN RSRS and RLM RSRS suggest that language models contain quite useful information when combined with other shallow lexical sophistication indicators, especially when readability analysis needs to be conducted on a variety of different datasets.</p><p>As seen in <ref type="table">Table 4</ref>, shallow readability predictors can give inconsistent results on datasets from different genres and languages. For example, the simplest readability measure, the average sentence length, ranked first on Newsela and twelfth on On-eStopEnglish. It also did not do well on the Slovenian SB corpus, where it ranked seventh. SMOG, on the other hand, ranked very well on the Slovenian SB corpus (rank 2) but ranked twice as eleventh and once as eighth on the English corpora. Among the traditional measures, GFI presents the best balance in performance and consistency, ranking first on WeeBit, sixth on OneStopEnglish, tenth on Newsela, and fourth on Slovenian SB.</p><p>On the other hand, RSRS-simple and RSRS-balanced measures offer more robust performance across datasets from different genres and languages according to ranks in <ref type="table">Table 4</ref>. For example, the RLM RSRS-simple measure ranked fourth on all English corpora. The TCN RSRS-balanced measure, which was also employed on Slovenian SB, ranked first on Slovenian SB and second on OneStopEnglish and Newsela. However, it did not do well on WeeBit, where the discrepancy in readability between the language model train and test sets was too large. RLM RSRS-balanced was more consistent, ranking fifth on all English corpora and third on Slovenian SB. These results suggest that language model statistics can improve the consistency of predictions on a variety of different datasets. The robustness of the measure is achieved by training the language model on a specific train set, with which one can optimize the RSRS measure for a specific task and language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Supervised neural approach</head><p>As mentioned in Section 1, recent trends in text classification show the domination of deep learning approaches which internally employ automatic feature construction. Existing neural approaches to readability prediction (see Section 2.3) tend to generalize better across datasets and genres (Filighera, Steuer, and Rensing 2019), and therefore solve the problem of classical machine learning approaches relying on an extensive feature engineering <ref type="bibr" target="#b59">(Xia, Kochmar, and Briscoe 2016)</ref>.</p><p>In this section, we analyze how different types of neural classifiers can predict text readability. In Section 5.1, we describe the methodology, and in Section 5.2 we present experimental scenarios and results of conducted experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Supervised methodology</head><p>We tested three distinct neural network approaches to text classification:</p><p>• Bidirectional Long short-term memory network (BiLSTM). We use the RNN approach proposed by <ref type="bibr" target="#b10">Conneau et al. (2017)</ref> for classification. The bidirectional LSTM layer is a concatenation of forward and backward LSTM layers that read documents in two opposite directions. The max and mean pooling are applied to the LSTM output feature matrix in order to get the maximum and average values of the matrix. The resulting vectors are concatenated and fed to the final linear layer responsible for predictions.</p><p>• Hierarchical attention networks (HAN). We use the architecture of <ref type="bibr" target="#b62">Yang et al. (2016)</ref> that takes hierarchical structure of text into account with the two-level attention mechanism  applied to word and sentence representations encoded by bidirectional LSTMs.</p><p>• Transfer learning. We use the pretrained BERT transformer architecture with 12 layers of size 768 and 12 self-attention heads. A linear classification head was added on top of the pretrained language model, and the whole classification model was fine-tuned on every dataset for 3 epochs. For English datasets, the bert-base-uncased English language model is used, while for the Slovenian SB corpus, we use the CroSloEngual BERT model trained on Slovenian, Croatian and English (Ulčar and Robnik-Šikonja 2020) 9 .</p><p>We randomly shuffle all the corpora, and then Newsela and Slovenian SB corpora are split into a train (80% of the corpus), validation (10% of the corpus) and test (10% of the corpus) sets. Due to the small number of documents in OneStopEnglish and WeeBit corpora (see description in Section 3), we used five-fold stratified cross-validation on these corpora to get more reliable results. For every fold, the corpora were split into the train (80% of the corpus), validation (10% of the corpus) and test (10% of the corpus) sets. We employ Scikit StratifiedKFold 10 , both for train-test splits and five-fold crossvalidation splits, in order to preserve the percentage of samples from each class.</p><p>BiLSTM and HAN classifiers were trained on the train set and tested on the validation set after every epoch (for a maximum of 100 epochs). The best performing model on the validation set was selected as the final model and produced predictions on the test sets. BERT models are fine-tuned on the train set for three epochs, and the resulting model is tested on the test set. The validation sets were used in a grid search to find the best hyperparameters of the models. For BiLSTM, all combinations of the following hyperparameter values were tested before choosing the best combination, which is written in bold in the list below: For BERT fine-tuning, we use the default learning rate of 0.00002. The input sequence length is limited to 512 byte-pair tokens, which is the maximum supported input sequence length.</p><p>We used the same configuration for all the corpora and performed no corpus specific tweaking of classifier parameters. We measured the performance of all the classifiers in terms of accuracy (in order to compare their performance to the performance of the classifiers from the related work), weighted average precision, weighted average recall, and weighted average F 1 -score 11 . Since readability classes are ordinal variables (in our case ranging from 0 to n=number of classes-1), not all mistakes of classifiers are equal; therefore we also employ the Quadratic Weighted Kappa (QWK) measure, which allows for mispredictions to be weighted differently, according to the cost of a specific mistake. Calculation of the QWK involves three matrices containing observed scores, ground truth scores and the weight matrix scores, which in our case correspond to the distance d between the classes c i and c j and is defined as d = |c i − c j |. QWK is therefore calculated as:</p><formula xml:id="formula_12">QWK = 1 − c i=1 c j=1 w ij x ij c i=1 c j=1 w ij m ij ,<label>(4)</label></formula><p>where c is the number of readability classes and w ij , x ij and m ij are elements in the weight, observed, and ground truth matrices, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Supervised experimental results</head><p>The results of supervised readability assessment using different architectures of deep neural networks are presented in <ref type="table">Table 5</ref> together with the state-of-the-art baseline results from the related work <ref type="bibr">(Filighera, Steuer, and Rensing 2019;</ref><ref type="bibr" target="#b59">Xia, Kochmar, and Briscoe 2016;</ref><ref type="bibr" target="#b14">Deutsch, Jasbi, and Shieber 2020)</ref>. We only present the best result reported by each of the baseline studies, the only exception is <ref type="bibr" target="#b14">Deutsch, Jasbi, and Shieber (2020)</ref>, for which we present two results, SVM-BF (SVM with BERT features) and SVM-HF (SVM with HAN features) that proved the best on the WeeBit and Newsela corpora, respectively.</p><p>On the WeeBit corpus, by far the best performance according to all measures was achieved by BERT. In terms of accuracy, BERT outperforms the second-best BiLSTM by about 8 percentage points, achieving the accuracy of 85.73%. HAN performs the worst on the WeeBit corpus according to all measures. BERT also outperforms the accuracy result reported by <ref type="bibr" target="#b59">Xia, Kochmar, and Briscoe (2016)</ref>, who used the five-fold cross-validation setting and the accuracy result on the development set reported by <ref type="bibr">Filighera, Steuer, and Rensing (2019)</ref>  <ref type="bibr">12</ref> . In terms of weighted F 1 -score, both strategies that employ BERT (employing the BERT classifier directly or feeding BERT features to the SVM classifier as in <ref type="bibr" target="#b14">Deutsch, Jasbi, and Shieber (2020)</ref>) seem to return similar results. Finally, in terms of QWK, BERT achieves a very high score of 95.27% and the other two tested classifiers obtain a good QWK score close to 90%.</p><p>The best result on Newsela is achieved by HAN, achieving the F 1 -score of 81.01% and accuracy of 81.38%. This is similar to the baseline SVM-HF result achieved by <ref type="bibr" target="#b14">Deutsch, Jasbi, and Shieber (2020)</ref>, who fed HAN features to the SVM classifier. BERT performs less competitively on the OneStopEnglish and Newsela corpora. On On-eStopEnglish, it is outperformed by the best performing classifier (HAN) by about 10 percentage points, and on Newsela, it is outperformed by about 6 percentage points according to accuracy and F 1 criteria. The most likely reason for the bad performance of 12 For the study by Filighera, Steuer, and Rensing (2019), we report accuracy on the development set instead of the accuracy on the test set, as the authors claim that this result is more comparable to the results achieved in the cross-validation setting. On the test set, Filighera, Steuer, and Rensing (2019) report the best accuracy of 74,4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>The results of the supervised approach to readability in terms of accuracy, weighted precision, weighted recall, and weighted F 1 -score for the three neural network classifiers and methods from the literature. BERT on these two corpora is the length of documents in these two datasets. On average, documents in the OneStopEnglish and Newsela corpora are 677 and 760 words long. On the other hand, BERT only allows input documents of up to 512 byte-pair tokens, which means that documents longer than that need to be truncated. This results in the substantial loss of information on the OneStopEnglish and Newsela corpora but not on the WeeBit and Slovenian SB corpora, which contain shorter documents, 193 and 305 words long.</p><p>The results show that BiLSTM also has problems when dealing with longer texts, even though it does not require input truncation. This suggests that the loss of context is not the only reason for the non-competitive performance of BERT and BiLSTM, and that the key to the successful classification of long documents is the leveraging of hierarchical information in the documents, for which HAN was built for. The assumption is that this is particularly important in parallel corpora, where the simplified versions of the original texts contain the same message as the original texts, which forces the classifiers not to rely as much on semantic differences but rather focus on structural differences.</p><p>While F 1 -scores and accuracies suggest large discrepancies in performance between HAN and other two classifiers on the OneStopEnglish and Newsela corpora, QWK scores draw a different picture. While the discrepancy is still large on OneStopEnglish, all classifiers achieve almost perfect QWK scores on the Newsela dataset. This suggests that even though BERT and BiLSTM make more classification mistakes than HAN, these mistakes are seldom costly on the Newsela corpus (i.e. documents are classified into neighbouring classes of the correct readability class). QWK scores achieved on the Newsela corpus by all classifiers are also much higher than the scores achieved on other corpora (except for the QWK score achieved by BERT on the WeeBit corpus). This is in line with the results in the unsupervised setting, where the ρ values on the Newsela corpus were substantially larger than on other corpora.</p><p>The HAN classifier achieves the best performance on the OneStopEnglish corpus with the accuracy of 78.72% in the five-fold cross-validation setting. This is comparable to the state-of-the-art accuracy of 78.13% achieved by <ref type="bibr" target="#b52">Vajjala and Lučić (2018)</ref> with their SMO classifier using 155 hand-crafted features. BiLSTM and BERT classifiers perform similarly on this corpus, by about 10 percentage points worse than HAN, according to accuracy, F 1 -score, and QWK.</p><p>The results on the Slovenian SB corpus are also interesting. In general, the performance of classifiers is the worst on this corpus, with the F 1 -score of 52.19% achieved by BiLSTM being the best result. BiLSTM performs by about 4 percentage points better than HAN according to F 1 -score and accuracy, while both classifiers achieve roughly the same QWK score of about 80%. On the other hand, BERT achieves lower F 1 -score (about 45.45%) and accuracy (41.57%), but performs much better than the other two classifiers according to QWK, achieving QWK of almost 90%.</p><p>Confusion matrices for classifiers give us a better insight into what kind of mistakes are specific for different classifiers. For the WeeBit corpus confusion matrices show <ref type="figure">(Figure 2</ref>) that all the tested classifiers have the most problems distinguishing between texts for children 8-9 years old and 9-10 years old. The mistakes where the text is falsely classified into an age group that is not neighbouring the correct age group are rare. For example, the best performing BERT classifier misclassified only sixteen documents into non-neighbouring classes. When it comes to distinguishing between neighbouring classes, the easiest distinction for the classifiers was the distinction between texts for children 9-10 years old and 10-14 years old. Besides fitting into two distinct age groups, the documents in these two classes also belong to two different sources (texts for children 9-10 years old consist of articles from WeeklyReader and texts for children 10- 14 years old consist of articles from BBC-Bitesize), which suggests that the semantic and writing style dissimilarities between these two neighbouring classes might be larger than for other neighbouring classes, and that might have a positive effect on the performance of the classifiers. On the OneStopEnglish corpus <ref type="figure">(Figure 3)</ref>, the BERT classifier, which is performing the worst on this corpus according to all criteria but precision, had the most problems correctly classifying documents from the advanced class, misclassifying about half of the documents. HAN had the most problems with distinguishing documents from the advanced and intermediate class, while BiLSTM classifier classified a disproportionate amount of intermediate documents into the beginner class.</p><p>Confusion matrices of all classifiers for the Newsela corpus ( <ref type="figure">Figure 4)</ref> follow a similar pattern. Unsurprisingly, no classifier predicted any documents to be in two minority classes (10th and 11th grade) with minimal training examples. As the QWK score has already shown, all classifiers classified a large majority of misclassified instances into neighbouring classes, and costlier mistakes are rare. Confusion matrices for the Slovenian SB corpus ( <ref type="figure">Figure 5</ref>) are similar for all classifiers. The biggest spread of misclassified documents is visible for the classes in the middle of the readability range (from the 4th-grade primary school to the 1st-grade high school). The mistakes, which cause BERT to have lower F 1 -score and accuracy scores than the other two classifiers, are most likely connected to the misclassification of all but two documents belonging to the school books for the 6th class of the primary school. Nevertheless, a large majority of these documents were misclassified into two neighbouring classes, which explain the high QWK score achieved by the classifier. What negatively affected the QWK scores for HAN and BiLSTM is that the frequency of making costlier mistakes of classifying documents several grades above or below the correct grade is slightly higher for them than for BERT. Nevertheless, even though F 1 -score results are relatively low on this dataset for all classifiers (BiLSTM achieved the best F 1 -score of 52.19%), the QWK scores around or above 80% and the confusion matrices clearly show that a large majority of misclassified examples were put into classes close to the correct one, suggesting that classification approaches to readability prediction can also be reliably used for Slovenian.</p><p>Overall, the classification results suggest that neural networks are a viable option for the supervised readability prediction. Some of the proposed neural approaches man-aged to outperform state-of-the-art machine learning classifiers that leverage feature engineering <ref type="bibr" target="#b59">(Xia, Kochmar, and Briscoe 2016;</ref><ref type="bibr" target="#b52">Vajjala and Lučić 2018;</ref><ref type="bibr" target="#b14">Deutsch, Jasbi, and Shieber 2020)</ref> on all corpora where comparisons are available. However, the gains are not substantial, and the choice of an appropriate architecture depends on the properties of the specific dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented a set of novel unsupervised and supervised approaches for determining the readability of documents using deep neural networks. We tested them on several manually labelled English and Slovenian corpora. We argue that deep neural networks are a viable option both for supervised and unsupervised readability prediction and show that the suitability of a specific architecture for the readability task depends on the dataset specifics.</p><p>We demonstrate that neural language models can be successfully employed in the unsupervised setting, since they, in contrast to n-gram models, capture high-level textual properties and can successfully leverage rich semantic information obtained from the training dataset. However, the results of this study suggest that unsupervised approaches to readability prediction that only take these properties of text into account cannot compete with the shallow lexical sophistication indicators. This is somewhat in line with the findings of the study by <ref type="bibr" target="#b50">Todirascu et al. (2016)</ref>, who also acknowledged the supremacy of shallow lexical indicators when compared to higher-level discourse features. Nevertheless, combining the components of both neural and traditional readability indicators into the new RSRS (ranked sentence readability score) measure does improve the correlation with human readability scores.</p><p>We argue that the RSRS measure is adaptable, robust, and transferable across languages. The results of the unsupervised experiments show the influence of the language model training set on the performance of the measure. While the results indicate that an exact match between the genres of the train and test sets is not necessary, the text complexity of a train set (i.e. its readability), should be in the lower or middle part of the readability spectrum of the test set for the optimal performance of the measure. This indicates that out of the two high-level text properties that the RSRS measure employs for determining readability, semantic information and long-distance structural information, the latter seems to have more effect on the performance. This is further confirmed by the results of using the general BERT language model for the readability prediction, which show a negative correlation between the language model perplexity and readability, even though the semantic information the model possesses is extensive due to the large training set.</p><p>The functioning of the proposed RSRS measure can be customized and influenced by choice of the training set. This is the desired property since it enables personalization and localization of the readability measure according to the educational needs, language, and topic. The usability of this feature might be limited for under-resourced languages since sufficient amount of documents needed to train a language model that can be used for the task of readability prediction in a specific customized setting might not be available. On the other hand, our experiments on the Slovenian language show, that a relatively small 2.4 million word training corpus for language models is sufficient to outperform traditional readability measures.</p><p>The results of the unsupervised approach to readability prediction on the corpus of Slovenian school books are not entirely consistent with the results reported by the previous Slovenian readability study <ref type="bibr">(Škvorc et al. 2019)</ref>, where the authors reported that simple indicators of readability, such as average sentence length, performed quite well. Our results show that the average sentence length performs very competitively on English but ranks badly on Slovenian. This inconsistency in results might be explained by the difference in corpora used for the evaluation of our approaches. While <ref type="bibr">Škvorc et al. (2019)</ref> conducted experiments on a corpus of magazines for different age groups (which we used for language model training), our experiments were conducted on a corpus of school books, which contains school books for sixteen distinct school subjects with very different topics ranging from literature, music and history to math, biology and chemistry. As was already shown in <ref type="bibr" target="#b45">Sheehan, Flor, and Napolitano (2013)</ref>, the variance in genres and covered topics has an important effect on the ranking and performance of different readability measures. Further experiments on other Slovenian datasets are required to confirm this hypothesis.</p><p>In the supervised approach to determining readability, we show that the proposed neural classifiers can either outperform or at least compare with state-of-the-art approaches leveraging extensive feature engineering as well as previously employed neural models on all corpora where comparison data is available. While the improved performance and elimination of work required for manual feature engineering are desirable, on the downside, neural approaches tend to decrease the interpretability and explainability of the readability prediction. Interpretability and explainability are especially important for educational applications <ref type="bibr" target="#b33">(Madnani and Cahill 2018;</ref><ref type="bibr" target="#b47">Sheehan et al. 2014)</ref>, where the users of such technology (educators, teachers, researchers, etc.) often need to understand what causes one text to be judged as more readable than the other and according to which dimensions. Therefore in the future, we will explore the possibilities of explaining the readability predictions of the proposed neural classifier with the help of general explanation techniques such as SHAP (Lundberg and Lee 2017), or the attention mechanism <ref type="bibr" target="#b55">(Vaswani et al. 2017)</ref>, which can be analyzed and visualized and can offer valuable insights into inner workings of the system.</p><p>Another issue worth discussing is the trade-off between performance gains we can achieve by employing computationally demanding neural networks on the one side and the elimination of work on the other. For example, on the OneStopEnglish corpus, we report the accuracy of 78.72% when HAN is employed, while <ref type="bibr" target="#b52">Vajjala and Lučić (2018)</ref> report an accuracy of 78.13% with their classifier employing 155 handcrafted features. While it might be worth opting for a neural network in order to avoid extensive manual feature engineering, on the other hand, the same study by <ref type="bibr" target="#b52">Vajjala and Lučić (2018)</ref> also reports that just by employing generic text classification features, 2-5 character n-grams, they obtained the accuracy of 77.25%. Considering this, one might argue that, depending on the use case, it might not be worth dedicating significantly more time, work or computational resources for an improvement of slightly more than 1%, especially if this also decreases the overall interpretability of the prediction.</p><p>The performance of different classifiers varies across different corpora. The major factor proved to be the length of documents in the datasets. The HAN architecture, which tends to be well equipped to handle long-distance hierarchical text structures, performs the best on these datasets. On the other hand, in terms of QWK measure, BERT offers significantly better performance on datasets that contain shorter documents, such as WeeBit and Slovenian SB. As was already explained in Section 5.2, a large majority of OneStopEnglish and Newsela documents need to be truncated in order to satisfy the BERT's limitation of 512 byte-pair tokens. While it is reasonable to assume that the truncation and the consequential loss of information do have a detrimental effect on the performance of the classifier, the extent of this effect is still unclear. The problem of truncation also raises the question of what is the minimum required length of a text for a reliable assessment of readability and if there exists a length threshold, above which having more text does not influence the performance of a classifier in a significant manner. We plan to assess this in future work thoroughly. Another related line of research we plan to pursue in the future is the employment of novel algorithms, such as Longformer <ref type="bibr" target="#b4">(Beltagy, Peters, and Cohan 2020)</ref> and Linformer <ref type="bibr" target="#b56">(Wang et al. 2020)</ref>, in which the attention mechanism scales linearly with the sequence length, making it feasible to process documents of thousands of tokens. We will check if applying these two algorithms on the readability datasets with longer documents can further improve the state-of-the-art.</p><p>The other main difference between WeeBit and Slovenian SB datasets on the one hand, and Newsela and OneStopEnglish datasets on the other, is that they are not parallel corpora, which means there can be substantial semantic differences between the readability classes in these two corpora. It seems that pretraining BERT as a language model allows for better exploitation of these differences, which leads to better performance. However, this reliance on semantic information might badly affect the performance of transfer learning based models on parallel corpora, since the semantic differences between classes in these corpora are much more subtle. We plan to assess the influence of available semantic information on the performance of different classification models in the future.</p><p>The differences in performance between classifiers on different corpora suggest that tested classifiers take different types of information into account. Provided that this hypothesis is correct, some gains in performance might be achieved if the classifiers are combined. We plan to test a neural ensemble approach for the task of predicting readability in the future.</p><p>While this study mostly focused on multi-lingual and multi-genre readability prediction, in the future, we also plan to test the cross-corpus, cross-genre and crosslanguage transferability of the proposed supervised and unsupervised approaches. This requires new readability datasets for different languages and genres which are currently rare or not publicly available. On the other hand, this type of research will be capable of further determining the role of genre in the readability prediction and might open an opportunity to improve the proposed unsupervised readability score further.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 Figure 3</head><label>23</label><figDesc>Confusion matrices for BERT, HAN, and BiLSTM on the WeeBit corpus. Confusion matrices for BERT, HAN, and BiLSTM on the OneStopEnglish corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 Figure 5</head><label>45</label><figDesc>For example, the best performing HAN classifier altogether misclassified only thirteen examples into non-neighbouring classes. Confusion matrices for BERT, HAN, and BiLSTM on the Newsela corpus. Confusion matrices for BERT, HAN, and BiLSTM on the Slovenian school books corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Readability classes, number of documents, tokens per specific readability class and average tokens per document in each readability corpus. simple contains 130 000 randomly selected articles from the Simple Wikipedia dump, which comprise of 654 593 sentences and 10 933 710 tokens.-Wiki-balanced contains 65 000 randomly selected articles from the Wikipedia dump (dated 26 January 2018) and 65 000 randomly selected articles from the Simple Wikipedia dump. Altogether the corpus comprises of 571 964 sentences and 10 847 108 tokens. KRES corpus<ref type="bibr" target="#b30">(Logar et al. 2012</ref>) is a 100 million word balanced reference corpus of Slovenian language. 35% of its content are books, 40% periodicals, and 20% internet texts. From this corpus we took all the available documents from two children magazines (Ciciban and Cicido), all documents from four teenager magazines (Cool, Frka, PIL plus and Smrklja), and documents from three magazines targeting adult audiences (Življenje in tehnika, Radar, City magazine). With these texts, we built a corpus with approximately 2.4 million words. The corpus is balanced in a sense that about one-third of the sentences come from documents targeting children, one third is targeting teenagers, and the last third is targeting adults.</figDesc><table><row><cell cols="3">Readability class #documents #tokens</cell><cell>#tokens per doc.</cell></row><row><cell></cell><cell cols="2">Wikipedia</cell><cell></cell></row><row><cell>simple</cell><cell>130,000</cell><cell cols="2">10,933,710 84.11</cell></row><row><cell>balanced</cell><cell>130,000</cell><cell cols="2">10,847,108 83.44</cell></row><row><cell>normal</cell><cell>130,000</cell><cell cols="2">10,719,878 82.46</cell></row><row><cell></cell><cell cols="2">OneStopEnglish</cell><cell></cell></row><row><cell>beginner</cell><cell>189</cell><cell>100,800</cell><cell>533.33</cell></row><row><cell>intermediate</cell><cell>189</cell><cell>127,934</cell><cell>676.90</cell></row><row><cell>advanced</cell><cell>189</cell><cell>155,253</cell><cell>820.49</cell></row><row><cell>All</cell><cell>567</cell><cell>383,987</cell><cell>677.23</cell></row><row><cell></cell><cell></cell><cell>WeeBit</cell><cell></cell></row><row><cell>age 7-8</cell><cell>600</cell><cell>77,613</cell><cell>129.35</cell></row><row><cell>age 8-9</cell><cell>600</cell><cell>100,491</cell><cell>167.49</cell></row><row><cell>age 9-10</cell><cell>600</cell><cell>159,719</cell><cell>266.20</cell></row><row><cell>age 10-14</cell><cell>600</cell><cell>89,548</cell><cell>149.25</cell></row><row><cell>age 14-16</cell><cell>600</cell><cell>152,402</cell><cell>254.00</cell></row><row><cell>All</cell><cell>3,000</cell><cell>579,773</cell><cell>193.26</cell></row><row><cell></cell><cell></cell><cell>Newsela</cell><cell></cell></row><row><cell>2nd grade</cell><cell>224</cell><cell>74,428</cell><cell>332.27</cell></row><row><cell>3rd grade</cell><cell>500</cell><cell>197,992</cell><cell>395.98</cell></row><row><cell>4th grade</cell><cell>1,569</cell><cell>923,828</cell><cell>588.80</cell></row><row><cell>5th grade</cell><cell>1,342</cell><cell>912,411</cell><cell>679.89</cell></row><row><cell>6th grade</cell><cell>1,058</cell><cell>802,057</cell><cell>758.09</cell></row><row><cell>7th grade</cell><cell>1,210</cell><cell>979,471</cell><cell>809.48</cell></row><row><cell>8th grade</cell><cell>1,037</cell><cell>890,358</cell><cell>858.59</cell></row><row><cell>9th grade</cell><cell>750</cell><cell>637,784</cell><cell>850.38</cell></row><row><cell>10th grade</cell><cell>20</cell><cell>19,012</cell><cell>950.60</cell></row><row><cell>11th grade</cell><cell>2</cell><cell>1,130</cell><cell>565.00</cell></row><row><cell>12th</cell><cell>1,853</cell><cell>1,833,781</cell><cell>989.63</cell></row><row><cell>All</cell><cell>9,565</cell><cell>7,272,252</cell><cell>760.30</cell></row><row><cell></cell><cell cols="2">KRES-balanced</cell><cell></cell></row><row><cell>balanced</cell><cell>/</cell><cell>2,402,263</cell><cell>/</cell></row><row><cell></cell><cell cols="2">Slovenian SB</cell><cell></cell></row><row><cell>1st-ps</cell><cell>69</cell><cell>12,921</cell><cell>187.26</cell></row><row><cell>2nd-ps</cell><cell>146</cell><cell>30,296</cell><cell>207.51</cell></row><row><cell>3rd-ps</cell><cell>268</cell><cell>62,241</cell><cell>232.24</cell></row><row><cell>4th-ps</cell><cell>1,007</cell><cell>265,242</cell><cell>263.40</cell></row><row><cell>5th-ps</cell><cell>1,186</cell><cell>330,039</cell><cell>278.28</cell></row><row><cell>6th-ps</cell><cell>959</cell><cell>279,461</cell><cell>291.41</cell></row><row><cell>7th-ps</cell><cell>1,470</cell><cell>462,551</cell><cell>314.66</cell></row><row><cell>8th-ps</cell><cell>1,844</cell><cell>540,944</cell><cell>293.35</cell></row><row><cell>9th-ps</cell><cell>2,154</cell><cell>688,149</cell><cell>319.47</cell></row><row><cell>1st-hs</cell><cell>1,663</cell><cell>578,694</cell><cell>347.98</cell></row><row><cell>2nd-hs</cell><cell>590</cell><cell>206,147</cell><cell>349.40</cell></row><row><cell>3rd-hs</cell><cell>529</cell><cell>165,845</cell><cell>313.51</cell></row><row><cell>4th-hs</cell><cell>45</cell><cell>14,313</cell><cell>318.07</cell></row><row><cell>All</cell><cell>11,930</cell><cell>3,636,843</cell><cell>304.85</cell></row></table><note>• KRES-balanced:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Another difference between Newsela and OneStopEnglish datasets on one side, and WeeBit and Slovenian SB dataset on the other is the length of dataset documents. While Newsela and OneStopEnglish datasets contain longer documents, on average about 760 and 677 words long, documents in the WeeBit and Slovenian SB corpora are on average about 193 and 305 words long, respectively. OneStopEnglish and Newsela datasets contain news articles, WeeBit is made of educational articles, and the Slovenian SB dataset is composed of school books. For training of the English language models, we use Wikipedia and Simple Wikipedia, which contain encyclopedia articles, and for Slovene language model training, we use the KRES-balanced corpus, which contains magazine articles. OneStopEnglish is the only test dataset that specifically targets adult ESL learners and not children, as do other test datasets. When it comes to datasets used for language model training, KRES-balanced corpus is made of articles which target both adults and children. The problem with Wikipedia and Simple</figDesc><table /><note>• Length of documents:• Genre:• Target audience:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Scores of traditional readability indicators from Section 2.1.1 for specific classes in the readability datasets.</figDesc><table><row><cell>Class</cell><cell>GFI</cell><cell cols="2">FRE FKGL</cell><cell cols="3">ARI DCRF SMOG</cell><cell>ASL</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Wikipedia</cell><cell></cell><cell></cell></row><row><cell>simple</cell><cell cols="2">11.80 62.20</cell><cell cols="2">8.27 14.08</cell><cell>11.40</cell><cell cols="2">11.40 16.90</cell></row><row><cell>balanced</cell><cell cols="2">13.49 56.17</cell><cell cols="2">9.70 15.86</cell><cell>12.53</cell><cell cols="2">12.53 19.54</cell></row><row><cell>normal</cell><cell cols="2">15.53 49.16</cell><cell cols="2">11.47 18.06</cell><cell>13.89</cell><cell cols="2">13.89 23.10</cell></row><row><cell></cell><cell></cell><cell></cell><cell>WeeBit</cell><cell></cell><cell></cell><cell></cell></row><row><cell>age 7-8</cell><cell cols="2">6.91 83.41</cell><cell>3.82</cell><cell>8.83</cell><cell>7.83</cell><cell cols="2">7.83 10.23</cell></row><row><cell>age 8-9</cell><cell cols="2">8.45 76.68</cell><cell cols="2">5.34 10.33</cell><cell>8.87</cell><cell cols="2">8.87 12.89</cell></row><row><cell>age 9-10</cell><cell cols="2">10.30 69.88</cell><cell cols="2">6.93 12.29</cell><cell>10.01</cell><cell cols="2">10.01 15.69</cell></row><row><cell>age 10-14</cell><cell cols="2">9.94 75.35</cell><cell cols="2">6.34 11.20</cell><cell>9.67</cell><cell cols="2">9.67 16.64</cell></row><row><cell>age 14-16</cell><cell cols="2">11.76 66.61</cell><cell cols="2">8.09 13.56</cell><cell>10.81</cell><cell cols="2">10.81 18.86</cell></row><row><cell></cell><cell></cell><cell cols="3">OneStopEnglish</cell><cell></cell><cell></cell></row><row><cell>beginner</cell><cell cols="2">11.79 66.69</cell><cell cols="2">8.48 13.93</cell><cell>11.05</cell><cell cols="2">11.05 20.74</cell></row><row><cell cols="3">intermediate 13.83 59.68</cell><cell cols="2">10.19 15.98</cell><cell>12.30</cell><cell cols="2">12.30 23.98</cell></row><row><cell>advanced</cell><cell cols="2">15.35 54.84</cell><cell cols="2">11.54 17.65</cell><cell>13.22</cell><cell cols="2">13.22 26.90</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Newsela</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2nd grade</cell><cell cols="2">6.11 85.69</cell><cell>3.27</cell><cell>8.09</cell><cell>7.26</cell><cell>7.26</cell><cell>9.26</cell></row><row><cell>3rd grade</cell><cell cols="2">7.24 80.92</cell><cell>4.27</cell><cell>9.30</cell><cell>7.94</cell><cell cols="2">7.94 10.72</cell></row><row><cell>4th grade</cell><cell cols="2">8.58 76.05</cell><cell cols="2">5.40 10.50</cell><cell>8.88</cell><cell cols="2">8.88 12.72</cell></row><row><cell>5th grade</cell><cell cols="2">9.79 71.76</cell><cell cols="2">6.47 11.73</cell><cell>9.68</cell><cell cols="2">9.68 14.81</cell></row><row><cell>6th grade</cell><cell cols="2">11.00 67.46</cell><cell cols="2">7.53 12.99</cell><cell>10.47</cell><cell cols="2">10.47 16.92</cell></row><row><cell>7th grade</cell><cell cols="2">12.11 62.71</cell><cell cols="2">8.54 14.12</cell><cell>11.26</cell><cell cols="2">11.26 18.46</cell></row><row><cell>8th grade</cell><cell cols="2">13.05 60.37</cell><cell cols="2">9.38 15.19</cell><cell>11.83</cell><cell cols="2">11.83 20.81</cell></row><row><cell>9th grade</cell><cell cols="2">14.20 55.00</cell><cell cols="2">10.46 16.37</cell><cell>12.70</cell><cell cols="2">12.70 22.17</cell></row><row><cell>10th grade</cell><cell cols="2">14.15 55.70</cell><cell cols="2">10.60 16.50</cell><cell>12.83</cell><cell cols="2">12.83 23.33</cell></row><row><cell>11th grade</cell><cell cols="2">15.70 56.41</cell><cell cols="2">11.05 16.96</cell><cell>12.77</cell><cell cols="2">12.77 24.75</cell></row><row><cell>12th grade</cell><cell cols="2">14.52 55.58</cell><cell cols="2">10.71 16.70</cell><cell>12.79</cell><cell cols="2">12.79 23.69</cell></row><row><cell></cell><cell></cell><cell cols="3">KRES-balanced</cell><cell></cell><cell></cell></row><row><cell>balanced</cell><cell cols="2">12.72 29.20</cell><cell cols="2">12.43 14.88</cell><cell>14.08</cell><cell cols="2">14.08 15.81</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Slovenian SB</cell><cell></cell><cell></cell></row><row><cell>1st-ps</cell><cell cols="2">9.54 31.70</cell><cell cols="2">10.38 11.72</cell><cell>11.12</cell><cell>11.12</cell><cell>7.63</cell></row><row><cell>2nd-ps</cell><cell cols="2">9.49 34.90</cell><cell cols="2">10.11 11.34</cell><cell>11.26</cell><cell>11.26</cell><cell>8.37</cell></row><row><cell>3rd-ps</cell><cell cols="2">10.02 32.89</cell><cell cols="2">10.61 11.78</cell><cell>11.80</cell><cell>11.80</cell><cell>9.31</cell></row><row><cell>4th-ps</cell><cell cols="2">10.96 30.29</cell><cell cols="2">11.18 12.84</cell><cell>12.39</cell><cell cols="2">12.39 10.40</cell></row><row><cell>5th-ps</cell><cell cols="2">11.49 28.13</cell><cell cols="2">11.62 13.33</cell><cell>12.79</cell><cell cols="2">12.79 11.02</cell></row><row><cell>6th-ps</cell><cell cols="2">13.20 20.10</cell><cell cols="2">12.84 14.57</cell><cell>13.61</cell><cell cols="2">13.61 11.45</cell></row><row><cell>7th-ps</cell><cell cols="2">12.94 22.97</cell><cell cols="2">12.61 14.52</cell><cell>13.64</cell><cell cols="2">13.64 12.24</cell></row><row><cell>8th-ps</cell><cell cols="2">13.48 18.12</cell><cell cols="2">13.09 14.78</cell><cell>13.71</cell><cell cols="2">13.71 11.32</cell></row><row><cell>9th-ps</cell><cell cols="2">13.69 19.26</cell><cell cols="2">13.13 15.07</cell><cell>13.94</cell><cell cols="2">13.94 12.27</cell></row><row><cell>1st-hs</cell><cell cols="2">15.12 12.66</cell><cell cols="2">14.33 16.22</cell><cell>14.96</cell><cell cols="2">14.96 13.62</cell></row><row><cell>2nd-hs</cell><cell cols="2">15.13 15.13</cell><cell cols="2">13.90 15.83</cell><cell>14.67</cell><cell cols="2">14.67 13.49</cell></row><row><cell>3rd-hs</cell><cell cols="2">14.76 13.09</cell><cell cols="2">14.00 15.62</cell><cell>14.44</cell><cell cols="2">14.44 12.57</cell></row><row><cell>4th-hs</cell><cell cols="2">14.66 14.39</cell><cell cols="2">13.64 15.54</cell><cell>14.03</cell><cell cols="2">14.03 11.62</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.weeklyreader.com 2 A later research by<ref type="bibr" target="#b59">Xia, Kochmar, and Briscoe (2016)</ref> called the validity of the published experimental results into question; therefore, the reported 93.3% accuracy might not be the objective state-of-the-art result for readability classification.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.weeklyreader.com 4 http://www.bbc.co.uk/bitesize 5 https://zenodo.org/record/1219041</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Martinc, Pollak, and Robnik-ŠikonjaNeural approaches to text readability</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Both models are available through the Transformers library https://huggingface.co/transformers/. 10 https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research was financially supported by the European social fund and Republic of Slovenia, Ministry of Education, Science and Sport through project Quality of Slovene textbooks (KaUČ). The work was also supported by the Slovenian Research Agency (ARRS) through core research programmes P6-0411 and P2-0103, and the projects Terminology and knowledge frames across languages (J6-9372) and Quantitative and qualitative analysis of the unregulated corporate financial reporting (J5-2554). This work has also received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 825153 (EMBEDDIA). The results of this publication reflect only the authors' views, and the EC is not responsible for any use that may be made of the information it contains.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>BERT perplexities are negatively correlated with readability, and the negative correlation is relatively strong on Newsela and Slovenian school books corpora (ρ of −0.673 and −0.563, respectively), and weak on WeeBit and OneStopEnglish corpora. As BERT was trained on corpora which are mostly aimed at adults, the strong negative correlation on Newsela and Slovenian SB corpora seem to suggest that BERT language models might actually be less perplexed by the articles aimed at adults than the documents aimed at younger audiences. This is supported by the fact that the negative correlation is weaker on the OneStopEnglish corpus, which is meant for adult audiences, and for which our analysis (see Section 3.2) has shown that it contains more complex texts according to the shallow readability indicators.</p><p>Nevertheless, the weak negative correlation on the WeeBit corpus is difficult to explain as one would expect a stronger negative correlation because the same analysis showed that WeeBit contains least complex texts out of all the tested corpora. If this result is connected with the successful transfer of the semantic knowledge, it supports the hypothesis that the two classes containing most complex texts in the WeeBit corpus contain articles with rather technical content that perplex the BERT model. However, the role of the semantic transfer should also dampen the negative correlation on the Slovenian SB, which is a non-parallel corpus and also contains rather technical educational content meant for high-school children. Perhaps the transfer is less successful</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysing the readability of English and non-English texts in the classroom with LIX</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh Australian Reading Association Conference</title>
		<meeting><address><addrLine>ERIC, Darwin, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiattentive recurrent neural network architecture for multilingual readability assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Azpiazu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Soledad</forename><surname>Madrazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="421" to="436" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Zico Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">NLTK: The natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 on Interactive poster and demonstration sessions</title>
		<meeting>the ACL 2004 on Interactive poster and demonstration sessions<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Development of readability analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Bormuth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
			<publisher>ERIC Clearinghouse</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar. Collins-Thompson, Kevyn</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="97" to="135" />
		</imprint>
	</monogr>
	<note>Computational assessment of text readability: A survey of current and future research</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting text comprehension, processing, and familiarity in adult readers: New approaches to readability formulas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Crossley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Skalicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Dascalu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><forename type="middle">S</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristopher</forename><surname>Kyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discourse Processes</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="340" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeanne</forename><forename type="middle">S</forename><surname>Chall</surname></persName>
		</author>
		<title level="m">A formula for predicting readability: Instructions. Educational research bulletin</title>
		<imprint>
			<date type="published" when="1948" />
			<biblScope unit="page" from="37" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On the failure of readability formulas to define readable texts: A case study from adaptations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">N</forename><surname>Kantor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="page" from="187" to="209" />
		</imprint>
	</monogr>
	<note>Reading research quarterly</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tovly</forename><surname>Deutsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masoud</forename><surname>Jasbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00377</idno>
		<title level="m">Linguistic features for readability assessment</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Council of Europe, Council for Cultural Co-operation. Education Committee. Modern Languages Division</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Common European Framework of Reference for Languages: learning, teaching, assessment</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Text simplification: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>The City University of New York</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cognitively motivated features for readability assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noémie</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Huenerfauth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009)</title>
		<meeting>the 12th Conference of the European Chapter of the ACL (EACL 2009)<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="229" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic text difficulty estimation using embeddings and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jansche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Huenerfauth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noémie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Technology Enhanced Learning</title>
		<meeting><address><addrLine>Beijing, China. Filighera, Anna, Tim Steuer, and Christoph Rensing; Delft, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="335" to="348" />
		</imprint>
	</monogr>
	<note>CoLing 2010: Posters</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lexical tightness and text complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Flor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beata</forename><surname>Beigman Klebanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Sheehan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Natural Language Processing for Improving Textual Accessibility</title>
		<meeting>the Workshop on Natural Language Processing for Improving Textual Accessibility<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A dataset of syntactic-ngrams over time from a very large corpus of English books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Orwant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Joint Conference on Lexical and Computational Semantics</title>
		<meeting><address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="241" to="247" />
		</imprint>
	</monogr>
	<note>Deep Learning</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The technique of clear writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gunning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1952" />
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Alexander</forename><surname>Halliday</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruqaiya</forename><surname>Kirkwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hasan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
		<respStmt>
			<orgName>Cohesion in English. Routledge</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What does BERT learn about the structure of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3651" to="3657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A domain independent approach for extracting terms from research papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Birong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Endong</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Australasian Database Conference</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="155" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Application de l&apos;indice de flesch à la langue française</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilian</forename><surname>Kandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Moles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cahiers Etudes de Radio-Télévision</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="253" to="274" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<meeting><address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Derivation of new readability formulas (Automated readability index, Fog count and Flesch reading ease formula) for Navy enlisted personnel. Institute for Simulation and Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert P Fishburne</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><forename type="middle">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chissom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
		<respStmt>
			<orgName>University of Central Florida</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pearson&apos;s text complexity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>Pearson</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Korpusi slovenskega jezika Gigafida, KRES, ccGigafida in ccKRES: gradnja, vsebina, uporaba. Trojina, zavod za uporabno slovenistiko</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataša</forename><surname>Logar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miha</forename><surname>Grčar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Brakus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaž</forename><surname>Erjavec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Špela Arhar Holdt, Simon Krek, and Iztok Kosem</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su-In</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4768" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ranking-based readability assessment for early primary children&apos;s literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Fosler-Lussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Lofthus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="548" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automated scoring: Beyond natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1099" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Is cross-lingual readability assessment possible</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madrazo</forename><surname>Azpiazu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Soledad</forename><surname>Ion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="644" to="656" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SMOG grading -a new readability formula</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mc</forename><surname>Laughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of reading</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="639" to="646" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Empirical evaluation and combination of advanced language modeling techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Cernockỳ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twelfth Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="605" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05957</idno>
	</analytic>
	<monogr>
		<title level="m">Hamid and Seyed Hossein Khasteh. 2019. Text as environment: A deep reinforcement learning text readability assessment model</title>
		<meeting><address><addrLine>Lake Tahoe, USA. Mohammadi</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Estimating linguistic complexity for science texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farah</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth workshop on innovative use of NLP for building educational applications</title>
		<meeting>the Thirteenth workshop on innovative use of NLP for building educational applications<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="45" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Online readability and text complexity analysis with TextEvaluator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Napolitano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mundkowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="96" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT<address><addrLine>New Orleans, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A machine learning approach to reading level assessment. Computer speech &amp; language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">E</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="89" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Revisiting readability: A unified framework for predicting text quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 conference on empirical methods in natural language processing</title>
		<meeting>the 2008 conference on empirical methods in natural language processing<address><addrLine>Honolulu, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Reading level assessment using support vector machines and statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">E</forename><surname>Schwarm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics<address><addrLine>Ann Arbor, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A two-stage approach for generating unbiased estimates of text complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Flor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Napolitano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Natural Language Processing for Improving Textual Accessibility</title>
		<meeting>the Workshop on Natural Language Processing for Improving Textual Accessibility<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generating automated text complexity classifications that are aligned with targeted text complexity standards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kostin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoko</forename><surname>Futagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Flor</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETS Research Report Series</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The TextEvaluator tool: Helping teachers and test developers select texts for use in instruction and assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kostin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Napolitano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Flor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Elementary School Journal</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="209" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Predicting Slovene text complexity using readability measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadej</forename><surname>Škvorc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Krek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senja</forename><surname>Pollak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contributions to Contemporary History (Spec. issue on Digital humanities and Language technologies</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="198" to="220" />
		</imprint>
	</monogr>
	<note>Špela Arhar Holdt, and Marko Robnik-Šikonja</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Automated readability index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Senter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aerospace Medical Research Laboratories (US)</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
	<note>AMRL-TR</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Are cohesive features relevant for text readability evaluation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amalia</forename><surname>Todirascu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>François</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delphine</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Núria</forename><surname>Gala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne-Laure</forename><surname>Ligozat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="987" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">FinEst BERT and CroSloEngual BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Ulčar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Robnik-Šikonja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Text, Speech, and Dialogue</title>
		<meeting><address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">OneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sowmya</forename><surname>Vajjala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Lučić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>New Orleans, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On improving the accuracy of readability classification using insights from second language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sowmya</forename><surname>Vajjala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detmar</forename><surname>Meurers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh workshop on building educational applications using NLP</title>
		<meeting>the Seventh workshop on building educational applications using NLP<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="163" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Text and context: Explorations in the semantics and pragmatics of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teun</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adrianus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<publisher>Longman London</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hado</forename><surname>Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1995" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Michael Hoey. Lexical priming: A new theory of words and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="327" to="335" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Text readability assessment for second language learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Kochmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 11th Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<meeting><address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Problems in current text simplification research: New data can help</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="283" to="297" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language technologies: Proceedings of the 17th International Multiconference Information Society -IS 2014</title>
		<meeting><address><addrLine>Santiago, Chile. Zwitter Vitez, Ana; Ljubljana, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="131" to="134" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE international conference on computer vision</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
