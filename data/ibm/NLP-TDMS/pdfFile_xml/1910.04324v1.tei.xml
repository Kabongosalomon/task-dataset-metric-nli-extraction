<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Practical License Plate Recognition in Unconstrained Surveillance Systems with Adversarial Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younkwan</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Learning and Vision Laboratory</orgName>
								<orgName type="institution">Gwangju Institute of Science and Technology</orgName>
								<address>
									<settlement>Gwangju</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Jun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Learning and Vision Laboratory</orgName>
								<orgName type="institution">Gwangju Institute of Science and Technology</orgName>
								<address>
									<settlement>Gwangju</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoojin</forename><surname>Hong</surname></persName>
							<email>yoojinhong@gist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Learning and Vision Laboratory</orgName>
								<orgName type="institution">Gwangju Institute of Science and Technology</orgName>
								<address>
									<settlement>Gwangju</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moongu</forename><surname>Jeon</surname></persName>
							<email>mgjeon@gist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Learning and Vision Laboratory</orgName>
								<orgName type="institution">Gwangju Institute of Science and Technology</orgName>
								<address>
									<settlement>Gwangju</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Practical License Plate Recognition in Unconstrained Surveillance Systems with Adversarial Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Intelligent Transportation Systems</term>
					<term>Visual Surveillance</term>
					<term>License Plate Recognition</term>
					<term>Super-Resolution</term>
					<term>Generative Adversarial Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although most current license plate (LP) recognition applications have been significantly advanced, they are still limited to ideal environments where training data are carefully annotated with constrained scenes. In this paper, we propose a novel license plate recognition method to handle unconstrained real world traffic scenes. To overcome these difficulties, we use adversarial super-resolution (SR), and one-stage character segmentation and recognition. Combined with a deep convolutional network based on VGG-net, our method provides simple but reasonable training procedure. Moreover, we introduce GIST-LP, a challenging LP dataset where image samples are effectively collected from unconstrained surveillance scenes. Experimental results on AOLP and GIST-LP dataset illustrate that our method, without any scene-specific adaptation, outperforms current LP recognition approaches in accuracy and provides visual enhancement in our SR results that are easier to understand than original data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>License plate recognition (LPR) is a fundamental and essential process of identifying vehicles and can be extended to a variety of real-world applications. LPR methods have been widely studied over the last decade, and are especially of big interest in intelligent transport systems (ITS) applications such as access control <ref type="bibr" target="#b3">(Chinomi et al., 2008)</ref>, road traffic monitoring <ref type="bibr" target="#b24">(Noh et al., 2016;</ref><ref type="bibr" target="#b27">Pu et al., 2013;</ref><ref type="bibr" target="#b37">Song and Jeon, 2016;</ref><ref type="bibr" target="#b18">Lee et al., 2017;</ref><ref type="bibr" target="#b40">Yoon et al., 2018)</ref> and traffic law enforcement <ref type="bibr" target="#b42">(Zhang et al., 2011)</ref>. Since all license plate recognition methods always deal with the letters and numbers in images, they are closely related to image classification <ref type="bibr" target="#b35">(Simonyan and Zisserman, 2014;</ref><ref type="bibr" target="#b33">Russakovsky et al., 2015)</ref> and text localization <ref type="bibr" target="#b0">(Anagnostopoulos et al., 2006)</ref>.</p><p>Conventional LPR methods typically include two stages: character localization and character recognition. Those methods are widely designed for unrealistically most constrained scenarios: a high-quality resolution and an unrotated frontal or rear image. However, unlike the ideal situation, many traffic surveillance cameras scattered around the world are operating in a number of unconstrained scenarios: they produce poor-resolution images and tilted license plates as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Although considerable progress of computer vision technology has been made, existing methods may fail to recognize license plates in such an environment without considering any unconstrained conditions. As a consequence, we find its limitations in three aspects: first, many license plate samples only constitute incomplete text search space; second, the projection angle of the sample is tilted with respect to the image plane at an angle of up to 30 degrees, interfering character exploitation; third, bad text localization often results in erroneous outputs.</p><p>Based on this finding, we propose a novel deep convolutional neural network based method for better LPR.</p><p>Adversarial Super-Resolution We suggest an adversarial super-resolution (SR) method including a generator and a discriminator networks over an image area. Modern SR method <ref type="bibr" target="#b5">(Dong et al., 2014)</ref> commonly targets the pixel-wise average as optimization goal, minimizing the mean squared error (MSE) between the super-resolved image and the ground truth, which leads to the smoothing effect, especially across text. Instead, we follow <ref type="bibr" target="#b17">(Ledig et al., 2017)</ref>'s generator network, which solves minimax game as optimization goal, avoiding a smoothing effect, which provide a sharpening effect. Combined with SR in generator, we introduce a new loss function that encourages the discriminator to count characters and distinguish whether SR or high-resolution(HR) sample concurrently. Character counting results from the discriminator network help improve character recognition performance in one-stage recognition module as a conditional term.</p><p>Reconstruction Auto-Encoder We always reconstruct the samples to straighten when the horizontally or vertically tilted license plate is projected onto the image plane. To address this issue, we utilize the convolutional auto-encoder network with the objective function as the difference between the tilted image and the straightened image. By doing so, it serves as a preprocessing for correct character exploitation.</p><p>One-Stage Recognition We do use the commonly used character segmentation and localization process. Instead, we propose a unified character localization and recognition approach as one-stage. One-stage recognition is not only more intuitive, but also more accurate than segmentation that requires precise an estimate of each pixel's class. Our One-stage method divides the input image into a 1*S grid, and detects LP at three different scales which includes a conditional term. The result of our character localization using each grid cell is naturally unified with character classification.</p><p>In summary, our key contributions are:</p><p>• We show that adversarial SR module and AE based reconstruction module in the real world for unconstrained surveillance cameras can improve the recognition performance greatly by (2.57% (AOLP) and 8.06% (GIST-LP)) compared with the state-of-the-art methods.</p><p>• The One-stage method combined with the conditional term, instead of the two-stage method (character detection and classification), reduced the localization and classification error.</p><p>• We collected a dataset of challenging license plate samples from unconstrained conditions accompanied by the text annotations (1,800 samples, 50 different license plates).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">License Plate Recognition</head><p>Traditionally, numerous LPR methods proposed consists of the two stages: semantic segmentation of the exact character region and recognition of the characters. The related methods generally utilize discriminate features, such as edge, color, shape and texture but does not show good results. Edge-based methods <ref type="bibr" target="#b13">(Kim et al., 2000;</ref><ref type="bibr" target="#b41">Zhang et al., 2006;</ref><ref type="bibr" target="#b38">Wang and Lee, 2003;</ref><ref type="bibr" target="#b13">Kim et al., 2000;</ref><ref type="bibr" target="#b41">Zhang et al., 2006)</ref> and geometrical features <ref type="bibr" target="#b38">(Wang and Lee, 2003)</ref> assume the presence of characters in the license plate. Many color-based methods <ref type="bibr" target="#b34">(Shi et al., 2005;</ref><ref type="bibr" target="#b2">Chen et al., 2009</ref>) usually use the combination of the license plate and the characters. However, since the two-stage methods are not only slow to run, but also take more time to converge for optimized training due to the double networks, one-stage pipeline based methods, segmentation-free approach <ref type="bibr" target="#b43">(Zherzdev and Gruzdev, 2018;</ref><ref type="bibr" target="#b1">Cheang et al., 2017;</ref><ref type="bibr" target="#b20">Li and Shen, 2016;</ref>, including segmentation and recognition at once, are proposed. Most segmentation-free models take advantage of deeply learned features which outperforms traditional methods on the task of classification by deep convolutional neural networks (DCNN) <ref type="bibr" target="#b35">(Simonyan and Zisserman, 2014;</ref><ref type="bibr" target="#b9">He et al., 2016)</ref> and data-driven approaches <ref type="bibr" target="#b33">(Russakovsky et al., 2015)</ref>. The core underlying assumption of these methods extracts features directly without sliding window for LPR. As examples of these models, Sergey et al. <ref type="bibr" target="#b43">(Zherzdev and Gruzdev, 2018</ref>) adopted a lightweight convolutional neural network to learn end-to-end way. In another work that use RNN module, Teik Koon et al. <ref type="bibr" target="#b1">(Cheang et al., 2017)</ref> proposed CNN-RNN unification model that feed the entire image as input. It is assumed that the context of the entire image is further evaluated for exact classification than the sliding window approaches being. Also, Hui et al. <ref type="bibr" target="#b20">(Li and Shen, 2016</ref>) utilized a cascade framework using DCNN and LSTM and Xinlong et al. <ref type="bibr">(Wang et al., )</ref> proposed DCNN and a bidirectional LSTM to use sequence labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adversarial Learning</head><p>The generative adversarial network (GAN) <ref type="bibr" target="#b8">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b28">Radford et al., 2015;</ref><ref type="bibr" target="#b28">Radford et al., 2015)</ref> is an amazing solution for training deep neural network of generative models, which aim to learn the probability distributions of the input data. Originally, GAN is suggested to yield the more realistic-fake im- ages <ref type="bibr" target="#b6">(Frid-Adar et al., 2018)</ref>, but recent researches show that this adversarial technique can be utilized to produce the specific training algorithms. e.g,. generative focused tasks; super-resolution <ref type="bibr" target="#b23">(Nguyen et al., ;</ref><ref type="bibr" target="#b17">Ledig et al., 2017;</ref><ref type="bibr" target="#b19">Lee et al., 2018)</ref>, style transfer <ref type="bibr" target="#b44">(Zhu et al., 2017;</ref><ref type="bibr" target="#b21">Li et al., 2017)</ref>, natural-language processing <ref type="bibr" target="#b29">(Rajeswar et al., 2017)</ref> and discriminative focused tasks; human pose estimation <ref type="bibr" target="#b4">(Chou et al., 2017;</ref><ref type="bibr" target="#b26">Peng et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head><p>In this section, we describe the details of the proposed end-to-end pipeline for LPR. The schematics of the method is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. We first introduce the adversarial network to super-resolve the input image, and reconstruct its output. Then, the details of the proposed one-stage character recognition network are presented for recognizing characters on the license plate and locating individual text regions without character segmentation. Finally, we describe a training process to find optimal parameters of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adversarial Network Architecture</head><p>Adversarial learning techniques have been widely used in many tasks <ref type="bibr" target="#b6">(Frid-Adar et al., 2018;</ref><ref type="bibr" target="#b44">Zhu et al., 2017;</ref><ref type="bibr" target="#b29">Rajeswar et al., 2017;</ref><ref type="bibr" target="#b4">Chou et al., 2017)</ref>, providing boosted performance through adversarial data or features. In vanilla GAN <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref>, a minimax game is trained by alternately updating a generator sub-network G and a discriminator subnetwork D simultaneously. The value function of the generator G and the discriminator D is defined as:</p><formula xml:id="formula_0">min θ G max θ D V (D, G) =E x∼p real (x) [logD(x)] + E z∼p f ake (z) [log(1 − D(G(z)))]</formula><p>( <ref type="formula">1)</ref> where p real is the real data distribution observation from x and p f ake is the fake data distribution observation from a random distribution z. These subnetworks have conflicting goals to minimize their own cost and maximize the opposite's cost. Therefore, the conclusion to play the minimax game can be that the probability distribution (p f ake ) generated by the generator G exactly matches the data distribution (p real ). After all, the discriminator D will not be able to distinguish between sampling distribution from the generator G and real data distribution. At this time, for the fixed generator, the optimal discriminator function is as follows:</p><formula xml:id="formula_1">D * G (x) = p real (x) p real (x) + p f ake (x) .<label>(2)</label></formula><p>In a similar way, we modified the minimax value function in the vanilla GAN for solving SR so that the generator G consisting of a HR generator G SR and a reconstruction network G recon creates an HR image from LP image, while the discriminator D trains to distinguish the HR fake image obtained by the generator from the actual LR image. This adversarial SR process can be defined as follows:</p><formula xml:id="formula_2">min θ G max θ D V (D, G) = E I HR ∼p train (I HR ) [log D θ D (I HR )] + E I LR ∼p G (I LR ) [log(1 − D θ D (G θ G I LR ))],<label>(3)</label></formula><p>where I HR is the high-resolution image, I LR is the lowresolution image, θ G and θ D denote the parameters trained by a feed-forward CNN G θ G and D θ D respectively. Generator Network. Different from <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref>, our generator network is composed of two sub-networks: (1) HR Generator G SR and <ref type="formula" target="#formula_1">(2)</ref> Convolutional Auto-encoder for reconstruction G recon as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The former is a series of convolutional layers and fractionally-strided convolution layers (i.e. upsample layer) inspired by <ref type="bibr" target="#b17">(Ledig et al., 2017)</ref>. We use two upsample layers(2 times upsampling) as proposed by <ref type="bibr" target="#b28">Radford et al. (Radford et al., 2015)</ref>, and acquire a 4 times enhanced image image from them.</p><p>In addition to its network, we include a reconstruction sub-network for the refinement task of image with enhanced resolution. Given the output of 4 times super-resolved image, our proposed network aims at discovering that it corrects slightly distorted image through denoising learning manner. Basically, we employ a convolutional neural network (CNN) as encoder and decoder, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Although both encoder and decoder consist of the same number of convolutional layers, the former adds MaxPool-ing2D layers for spatial down-sampling, while the latter adds UpSampling2D layers, with the BatchNormalization <ref type="bibr" target="#b11">(Ioffe and Szegedy, 2015)</ref>.</p><p>Discriminator Network. <ref type="figure" target="#fig_1">Figure 2</ref> shows the architecture of the discriminator network and its output components. Inspired by VGG19 <ref type="bibr" target="#b35">(Simonyan and Zisserman, 2014)</ref>, we follow the same network structure. To discriminate exact object regions, we design all the fully-connected layers to split into two parallel branches to obtain two outputs: (1) how many characters are in the image as counting result f count and (2) the HR vs. SR f GAN .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Character Recognition Network Architecture</head><p>In this section, we describe the details of the proposed character recognition approach where localiza-tion and recognition are integrated into one-stage. We employ YOLO v3 <ref type="bibr" target="#b31">(Redmon and Farhadi, 2018)</ref> as our detection network. To achieve scale-invariance, it detects characters at three scales, which are given by diminished dimensions of the image by 32, 16 and 8 each other, without the MaxPooling2D layer. Unlike previous model <ref type="bibr" target="#b30">(Redmon and Farhadi, 2017)</ref>, this allows better detection performance of small size character, which is optimized for character on a license plate that is mostly expressed in small size localization and recognition with residual skip connections. The shape of detection kernel denoted as 1 × 1 × (B × (5 + C)), where B is the number of bounding boxes, 5 is the sum of the four attributs of bounding boxes (coordinates (x, y), width and height) and one object confidence score and C is the number of classes. In our method, we define the detection kernel size as B = 3 and C is 66 (10 numbers (0-9), 26 English letters and 30 Korean letters), result in 1 × 1 × 213.</p><p>Furthermore, we add the counting information output f count from the discriminator as a conditional term in our character recognition model. The last layer of recognition model has the previous layer's output and f count as inputs. We demonstrate that our recognition model can be extended to the sophisticated model where it can accurately count and localize any character in any input. These are further discussed later in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>In this section, we discuss the objective to optimize our adversarial network and one-stage recognition network.</p><p>Let I LR i , I HR i and I SR i denote a low-resolution image, high-resolution image and SR image, respectively. Given a training dataset {I LR i , I HR i ,text j=1 , ...,text j=count i , count i } N i=1 , our goal is to learn the adversarial model that predicts SR im-age from low-resolution image and recognition model that predicts character's class and location from SR image.</p><p>Pixel-wise loss To force the generated plate image to high-resolution ground truth, our generator network is optimized for the MSE loss in each pixel values between the generated image sets and the small and blurry plate image sets calculated as follows:</p><formula xml:id="formula_3">L MSE = 1 N N ∑ i=1 ( G S1 (I LR i − I HR i ) 2 + G S2 (G S1 (I LR i ) − I HR i ) 2 ),<label>(4)</label></formula><p>where G S1 means HR generator, G S2 denotes the reconstruction network, and w are the parameters of generator network. Adversarial loss In order to provide a sharpening effect to the generated image different from the MSE loss that gives the smoothing effect, we define adversarial loss as:</p><formula xml:id="formula_4">L adv = 1 N N ∑ i=1 (log(1 − D θ (G w (I LR i ))) + log(D θ (I HR )))</formula><p>(5) Adversarial loss amplifies the photo-realistic effect and is trained in the direction of deception of the discriminator.</p><p>Reconstruction loss In order to let the quality of generated images by the S1 to be more photorealistic, we propose the reconstruction loss that corrects changes in the generated image topology that interfere with the detection and is defined as follows:</p><formula xml:id="formula_5">L const = 1 N N ∑ i=1 G S1 (I LR i ) − G S2 (G S1 (I LR i )) . (6)</formula><p>The reconstruction loss is calculated as L1 loss, the difference between the output of G S1 and G S2 . Classification loss The classification loss is playing both the roles of an character counting task as well as the discrimination task. To be more specific, the discriminator takes an image as input and classified it into two outputs: the HR real natural image or the SR fake image and the numbers of characters respectively. The loss of this multi-task is calculated as follows:</p><formula xml:id="formula_6">L clc = 1 N N ∑ i=1 (log((y i ∧ count i ) − D θ (G w (I LR i ))) + log((y i ∧ count i ) − D θ (I HR i ))),<label>(7)</label></formula><p>where y n represents prediction value of the number of characters and the operations ∧ with y n and count i output 1 if it predicts correctly or 0 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>All the reported implementations are based on Ten-sorFlow as learning framework, and our method has done on the NVIDIA TITAN X GPU. First of all, we use the YOLO-v3 for the pre-trained model on COCO <ref type="bibr" target="#b22">(Lin et al., 2014)</ref> as our one-stage recognition model so that we trained license plate images by fine-tuning their network parameters. Also, to avoid the premature convergence of the discriminator network, the generator network is updated more frequently than original one. In addition, higher learning rate is applied to the training of the generator. For stable training, we use a technique called gradient clipping trick <ref type="bibr" target="#b25">(Pascanu et al., 2013)</ref> and the Adam optimizer <ref type="bibr" target="#b14">(Kingma and Ba, 2014)</ref> with a high momentum term. For the discriminator network, we use the VGG-19 <ref type="bibr" target="#b35">(Simonyan and Zisserman, 2014)</ref> model pre-trained on ImageNet as our backbone network and we divide all the fully connected layers into two parallel f count and f GAN layers. The weights in all parallel fully connected layers are initialized from the standard Gaussian distribution with zero-mean, a standard deviation of 0.01 and the constant 0 as the bias in all layers. All models are trained on loss function for first 10 epochs with initial learning rate of 10 −4 . After that, we set the learning rate to a further reduced 10 −5 for the remaining epochs. Finally, batch normalization <ref type="bibr" target="#b11">(Ioffe and Szegedy, 2015)</ref> is used in all layers of generator and discriminator, except the last layer of the G and the first layer of the D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset</head><p>AOLP : This dataset <ref type="bibr" target="#b10">(Hsu et al., 2013)</ref> includes 2,049 images of Taiwan license plates, which are collected from the unconstrained surveillance scenes. AOLP dataset is divided into three subsets: access control (AC) with 681 samples, traffic law enforcement (LE) with 757 samples, and road patrol (RP) with 611 samples, based on diverse application parameters. 100 samples per subset are used for the training, and the rest of the 581(AC)/657(LE)/511(RP) samples are used for testing. More specifically, AC has a narrow range of variation conditions, while LE/RP have a wider range of variation conditions. Therefore, compared to the AC subset, LE/RP are more challenging subsets because they require a wider range of search conditions on the experiments. Besides, the RP samples collected via mobile have more challenging conditions because of the larger pan and orientation changes compared to the LE samples collected at road cameras with fixed viewing angles.</p><p>GIST-LP : We collected and annotated a new dataset GIST-LP for LPR. Our dataset is targeted on images captured from surveillance cameras under unconstrained scenes. We do not limit the license plate always to be large and front. We used traffic surveillance cameras which has 1920 x 1080 pixels of spatial resolution. We annotated the characters, including Korean (30 categories) and numbers (0-9, 10 categories) for all of the license plate images. In total, there are 1,800 license plates that appear in 1,569 frames. For license plate images, the characters are usually small-sized, blurred or tilted without occlusion. The dataset include information about bounding box for each character and text class (Koreans and numbers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Other Methods</head><p>In the experiment with AOLP, we compared our method with the state-of-the-are license plate recognition approaches <ref type="bibr" target="#b0">(Anagnostopoulos et al., 2006;</ref><ref type="bibr" target="#b12">Jiao et al., 2009;</ref><ref type="bibr" target="#b36">Smith, 2007;</ref><ref type="bibr" target="#b10">Hsu et al., 2013)</ref>. The results are listed in <ref type="table">Table 1</ref>, which are provided with accuracy of recognition to evaluate both text localization and classification are all performed well at the same time. We see that our method obtained the highest performance (i.e. 96.74%) on the all subsets, and outperformed the state-of-the-are LPR approaches by more than 2.5%. Also, it is important to note that, under the fairly tilted conditions, our method operated consistently robust and successfully detects the characters, while the baseline fail to detect. Furthermore, one interesting finding of these results is that, based on <ref type="figure">Figure 6 (b,c)</ref>, the addition of adversarial loss lead to the highlighting of the positive features, <ref type="figure">Figure 5</ref>: Example in GIST-LP dataset <ref type="bibr" target="#b16">(Laroca et al., 2018)</ref>. Qualitative sample images of recognition results. The first column shows the original plates, the second column shows the character localization results and the third indicates the recognitionm results.</p><p>while decimating of other irrelevant features. By doing so, it was further improved when detecting under night or confusing conditions. Based on these observations, our proposed method operated at least as well as others, which outperformed all other methods in most cases.</p><p>To show the results of experiment of LPR with GIST-LP, we compared our method with <ref type="bibr" target="#b7">(Girshick et al., 2014;</ref><ref type="bibr" target="#b32">Ren et al., 2015)</ref> and followed the standard metrics (i.e. accuracy of recognition) of the GIST-LP. There were many tiny license plates in <ref type="table">Table 1</ref>: Comparison of our method with other state-of-the-art method on the AOLP dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Performance AC LE RP Avg <ref type="bibr" target="#b0">(Anagnostopoulos et al., 2006)</ref> 92.00% 88.00% 91.00% 86.34% <ref type="bibr" target="#b12">(Jiao et al., 2009)</ref> 90.00% 86.00% 90.00% 88.51% <ref type="bibr" target="#b36">(Smith, 2007)</ref> 96.00% 83.00% 83.00% 87.31% <ref type="bibr" target="#b10">(Hsu et al., 2013)</ref> 95.00% 93.00% 94.00% 94.17% Baseline (YOLO v3) <ref type="bibr" target="#b31">(Redmon and Farhadi, 2018)</ref> 94  Method Performance RCNN based on VGG-16 <ref type="bibr" target="#b7">(Girshick et al., 2014)</ref> 74.44% RCNN based on ZFNET <ref type="bibr" target="#b7">(Girshick et al., 2014)</ref> 72.11% Faster-RCNN et al. <ref type="bibr" target="#b32">(Ren et al., 2015)</ref> 86.77% Baseline (YOLO v3) <ref type="bibr" target="#b31">(Redmon and Farhadi, 2018)</ref> 84.16% Ours without pixel-wise MSE loss 91.78% Ours without reconstruction loss 89.00% Ours without adversarial loss 87.72% Ours without classification loss 90.78% Ours 93.83%</p><p>GIST-LP, making character detection not be accurate. Hence, we found that the state-of-the-art method <ref type="bibr" target="#b31">(Redmon and Farhadi, 2018)</ref> that performed without considering the tiny size and blurred condition recorded on the inferior performance. However, our method mitigated the influence of these conditions and indicated these license plates successfully. Under such a challenging condition, our LPR performance still achieved a comparable performance (93.83%) over all other state-of-the-art LPR approaches, as shown <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In the proposed method, the loss functions of adversarial networks locate different regions, each with their unique roles. In order to inspect its influence on character recognition performance, we removed one loss function from the objective function at a time and performed an ablation study with it to compare the complete objective function. Most extremely, we perform experiments that compare the baseline and overall objective function, which obtain the superior performance by a considerable gap (5.84% / 9.67%) from <ref type="table" target="#tab_1">Table 1 and 2.</ref> Also, when removing one loss function from the overall objective function our method shows a considerable performance drop. First of all, even if the MSE loss is not suitable for tiny objects due to the smoothing effect, if there is no MSE loss, the performance degradation is up to 1.14% (in AOLP) / 2.05% (in GIST-LP), affecting the image up-scaling superresolution. Then the reconstruction loss affects the correct converting of the tilted plate, because the SR performance of the generator is somewhat dependent on the degree of tilted angle of the license plate, and it leads to about 3.83% (in AOLP), 4.83%(in GIST-LP) improvement in performance. In another step, we observe that adversarial loss leads to the sharpened super-resolved result of minimax game. Thus it has a great influence on the detection performance as shown in <ref type="figure">Figure 6</ref>. The GIST-LP dataset which has relatively more tiny plates than AOLP dataset has found a performance improvement of almost 4.74% as shown <ref type="table" target="#tab_1">Table 2</ref>, and the AOLP dataset also achieves performance improvement of nearly 6.11% as shown <ref type="table">Table 1</ref>. Finally removing classification loss in the objective function shows a significant impact on the character recognition performance, which observes an impressive improvement of 0.86% (in AOLP) and <ref type="figure">Figure 6</ref>: Example in AOLP dataset <ref type="bibr" target="#b10">(Hsu et al., 2013)</ref>. Poor-resolution and background clutter are common challenging issues on character recognition problem.</p><p>3.15% (in GIST-LP). This proves that our two parallel fully-connected layers for classification affect the classification performance for our text localization of the detector as well as the SR performance of the generator. Also, we demonstrate that the counting term as conditional data benefits to better explore the space of the character localization as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Results</head><p>As shown in <ref type="figure">Figure 6</ref>., we give additional examples of the clear LP generated by the proposed generator network from the tiny ones. Upon thorough investigation of the generated images, we find that our method learn strong priors using the proposed new loss functions of GAN by focusing on images of plate contour, certain letters and numbers as shown in <ref type="figure">Figure 6 (a)</ref>. It implies that the proposed loss significantly allows visually clearer LP and can be used to solve the illposed problem. Thus, SR module can capture the tiny LP without hallucination and it implies the proposed architecture has an impact on reducing the false negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose a new method based on GAN to recognize characters in unconstrained license plates. We design a novel network to directly generate a clear SR image from a blurry small one, and our up-sampling sub-network and reconstruction sub-network are trained in an end-to-end way. Moreover, we introduce an extra classification branch to the discriminator network, which can distinguish the HR/SR and the character counting probability simultaneously. Furthermore, the adversarial loss brings to generator network to restore a clearer SR image. Our experiment on AOLP and GIST-LP datasets demonstrate the substantial improvements, when compared to previous state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example in GIST-LP dataset. Poor-resolution and plate variation are common challenging issues on license plate recognition problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The proposed license plate recognition pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The proposed Auto-Encoder based reconstruction sub-network structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Samples from the unconstrained surveillance cameras in GIST-LP dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our method with other state-of-the-art method on the GIST-LP dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A license plate-recognition algorithm for intelligent transportation system applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N E</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Loumos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kayafas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Intelligent transportation systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="377" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Cheang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06439</idno>
		<title level="m">Segmentation-free vehicle license plate recognition using convnet-rnn</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic license-plate location and recognition based on feature salience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on vehicular technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">3781</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Prisurv: privacy protected video surveillance system using adaptive visual abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chinomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Babaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="144" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Self adversarial training for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02439</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gan-based synthetic medical image augmentation for increased cnn performance in liver lesion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frid-Adar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Diamant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amitai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01229</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Application-oriented license plate recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on vehicular technology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="552" to="561" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A configurable method for multi-style license plate recognition. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="358" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning-based approach for license plate recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 IEEE Signal Processing Society Workshop</title>
		<meeting>the 2000 IEEE Signal Processing Society Workshop</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="614" to="623" />
		</imprint>
	</monogr>
	<note>Neural Networks for Signal Processing X</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A robust real-time automatic license plate recognition based on the YOLO detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Severo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Zanlorensi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gonçalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<idno>abs/1802.09567</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic part localization using 3d cuboid box for vehicle subcategory recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Control, Automation and Information Sciences (ICCAIS), 2017 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="175" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate license plate recognition and super-resolution using a generative adversarial networks on traffic surveillance video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Reading car license plates using deep convolutional neural networks and lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.05610</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01036</idno>
		<title level="m">Demystifying neural style transfer</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive slidingwindow strategy for vehicle detection in highway environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="323" to="335" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Jointly optimize data augmentation and network training: Adversarial data augmentation in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09707</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Twatcher: A new visual analytic system for effective traffic surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 14th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
	<note>Mobile Data Management (MDM)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dutil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10929</idno>
		<title level="m">Adversarial generation of natural language</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolo9000: better, faster, stronger. arXiv preprint</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic license plate recognition system based on color image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Science and Its Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An overview of the tesseract ocr engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="629" to="633" />
		</imprint>
	</monogr>
	<note>Ninth International Conference on</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Online multiple object tracking with the hierarchically adopted gmphd filter using motion and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Consumer Electronics-Asia (ICCE-Asia)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Detection and recognition of license plate characters with different appearances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Transportation Systems, 2003. Proceedings. 2003 IEEE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="979" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Adversarial generation of training examples: Applications to moving vehicle license plate recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Online multi-object tracking with historical appearance matching and scene adaptive detection filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boragule</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learningbased license plate detection using global and local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition, 2006. ICPR 2006. 18th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1102" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Data-driven intelligent transportation systems: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1624" to="1639" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zherzdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gruzdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10447</idno>
		<title level="m">Lprnet: License plate recognition via deep neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
