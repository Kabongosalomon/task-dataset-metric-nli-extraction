<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>White</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University and Petuum Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abacus</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University and Petuum Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin@abacus</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University and Petuum Inc</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
							<email>neiswanger@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University and Petuum Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Savani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University and Petuum Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abacus</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University and Petuum Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash@abacus</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University and Petuum Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over the past half-decade, many methods have been considered for neural architecture search (NAS). Bayesian optimization (BO), which has long had success in hyperparameter optimization, has recently emerged as a very promising strategy for NAS when it is coupled with a neural predictor. Recent work has proposed different instantiations of this framework, for example, using Bayesian neural networks or graph convolutional networks as the predictive model within BO. However, the analyses in these papers often focus on the full-fledged NAS algorithm, so it is difficult to tell which individual components of the framework lead to the best performance.</p><p>In this work, we give a thorough analysis of the "BO + neural predictor" framework by identifying five main components: the architecture encoding, neural predictor, uncertainty calibration method, acquisition function, and acquisition optimization strategy. We test several different methods for each component and also develop a novel path-based encoding scheme for neural architectures, which we show theoretically and empirically scales better than other encodings. Using all of our analyses, we develop a final algorithm called BANANAS, which achieves state-of-the-art performance on NAS search spaces. We adhere to the NAS research checklist (Lindauer and Hutter 2019) to facilitate best practices, and our code is available at https://github.com/naszilla/naszilla.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since the deep learning revolution in 2012, neural networks have been growing increasingly more complex and specialized <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b56">57]</ref>. Developing new state-of-the-art architectures often takes a vast amount of engineering and domain knowledge. A rapidly developing area of research, neural architecture search (NAS), seeks to automate this process. Since the popular work by Zoph and Le <ref type="bibr" target="#b78">[79]</ref>, there has been a flurry of research on NAS <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19]</ref>. Many methods have been proposed, including evolutionary search, reinforcement learning, Bayesian optimization (BO), and gradient descent. In certain settings, zeroth-order (non-differentiable) algorithms such as BO are of particular interest over first-order (one-shot) techniques, due to advantages such as simple parallelism, joint optimization with other hyperparameters, easy implementation, portability to diverse architecture spaces, and optimization of other/multiple non-differentiable objectives.</p><p>BO with Gaussian processes (GPs) has had success in deep learning hyperparameter optimization <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11]</ref>, and is a leading method for efficient zeroth order optimization of expensive-to-evaluate functions in Euclidean spaces. However, initial approaches for applying GP-based BO to NAS came with challenges that limited its ability to achieve state-of-the-art results. For example, initial approaches required specifying a distance function between architectures, which involved cumbersome hyperparameter tuning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b18">19]</ref>, and required a time-consuming matrix inversion step.</p><p>Recently, Bayesian optimization with a neural predictor has emerged as a high-performing framework for NAS. This framework avoids the aforementioned problems with BO in NAS: there is no need to construct a distance function between architectures, and the neural predictor scales far better than a GP model. Recent work has proposed different instantiations of this framework, for example, Bayesian neural networks with BO <ref type="bibr" target="#b52">[53]</ref>, and graph neural networks with BO <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b35">36]</ref>. However, the analyses often focus on the full-fledged NAS algorithm, making it challenging to tell which components of the framework lead to the best performance.</p><p>In this work, we start by performing a thorough analysis of the "BO + neural predictor" framework. We identify five major components of the framework: architecture encoding, neural predictor, uncertainty calibration method, acquisition function, and acquisition optimization strategy. For example, graph convolutional networks, variational autoencoder-based networks, or feedforward networks can be used for the neural predictor, and Bayesian neural networks or different types of ensembling methods can be used for the uncertainty calibration method. After conducting experiments on all components of the BO + neural predictor framework, we use this analysis to define a high-performance instantiation of the framework, which we call BANANAS: Bayesian optimization with neural architectures for NAS.</p><p>In order for the neural predictor to achieve the highest accuracy, we also define a novel path-based architecture encoding, which we call the path encoding. The motivation for the path encoding is as follows. Each architecture in the search space can be represented as a labeled directed acyclic graph (DAG) -a set of nodes and directed edges, together with a list of the operations that each node (or edge) represents. However, the adjacency matrix can be difficult for the neural network to interpret <ref type="bibr" target="#b77">[78]</ref>, since the features are highly dependent on one another. By contrast, each feature in our path encoding scheme represents a unique path that the tensor can take from the input layer to the output layer of the architecture. We show theoretically and experimentally that this encoding scales better than the adjacency matrix encoding, and allows neural predictors to achieve higher accuracy.</p><p>We compare BANANAS to a host of popular NAS algorithms including random search <ref type="bibr" target="#b29">[30]</ref>, DARTS <ref type="bibr" target="#b34">[35]</ref>, regularized evolution <ref type="bibr" target="#b43">[44]</ref>, BOHB <ref type="bibr" target="#b10">[11]</ref>, NASBOT <ref type="bibr" target="#b20">[21]</ref>, local search <ref type="bibr" target="#b65">[66]</ref>, TPE <ref type="bibr" target="#b3">[4]</ref>, BONAS <ref type="bibr" target="#b47">[48]</ref>, BOHAMIANN <ref type="bibr" target="#b52">[53]</ref>, REINFORCE <ref type="bibr" target="#b66">[67]</ref>, GP-based BO <ref type="bibr" target="#b49">[50]</ref>, AlphaX <ref type="bibr" target="#b61">[62]</ref>, ASHA <ref type="bibr" target="#b29">[30]</ref>, GCN Predictor <ref type="bibr" target="#b63">[64]</ref>, and DNGO <ref type="bibr" target="#b51">[52]</ref>. BANANAS achieves state-of-the-art performance on NASBench-101 and is competitive on all NASBench-201 datasets. Subsequent work has also shown that BANANAS is competitive on NASBench-301 <ref type="bibr" target="#b48">[49]</ref>, even when compared to first-order methods such as DARTS <ref type="bibr" target="#b34">[35]</ref>, PC-DARTS <ref type="bibr" target="#b68">[69]</ref>, and GDAS <ref type="bibr" target="#b7">[8]</ref>.</p><p>Finally, to promote reproducibility, in Appendix E we discuss how our experiments adhere to the NAS best practices checklist <ref type="bibr" target="#b32">[33]</ref>. In particular, we experiment on well-known search spaces and NAS pipelines, run enough trials to reach statistical significance, and release our code. Our contributions. We summarize our main contributions.</p><p>• We analyze a simple framework for NAS: Bayesian optimization with a neural predictor, and we thoroughly test five components: the encoding, neural predictor, calibration, acquisition function, and acquisition optimization. • We propose a novel path-based encoding for architectures, which improves the accuracy of neural predictors. We give theoretical and experimental results showing that the path encoding scales better than the adjacency matrix encoding.</p><p>• We use our analyses to develop BANANAS, a high performance instantiation of the above framework. We empirically show that BANANAS is state-of-the-art on popular NAS benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Societal Implications</head><p>Our work gives a new method for neural architecture search, with the aim of improving the performance of future deep learning research. Therefore, we have much less control over the net impact of our work on society. For example, our work may be used to tune a deep learning optimizer for reducing the carbon footprint of large power plants, but it could just as easily be used to improve a deep fake generator. Clearly, the first example would have a positive impact on society, while the second example may have a negative impact. Our work is one level of abstraction from real applications, but our algorithm, and more generally the field of NAS, may become an important step in advancing the field of artificial intelligence. Because of the recent push for explicitly reasoning about the impact of research in AI <ref type="bibr" target="#b15">[16]</ref>, we are hopeful that neural architecture search will be used to benefit society.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>NAS has been studied since at least the 1990s and has gained significant attention in the past few years <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b78">79]</ref>. Some of the most popular recent techniques for NAS include evolutionary algorithms <ref type="bibr" target="#b36">[37]</ref>, reinforcement learning <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b41">42]</ref>, BO <ref type="bibr" target="#b20">[21]</ref>, and gradient descent <ref type="bibr" target="#b34">[35]</ref>. For a survey of neural architecture search, see <ref type="bibr" target="#b9">[10]</ref>.</p><p>Initial BO approaches defined a distance function between architectures <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b18">19]</ref>. There are several works that predict the validation accuracy of neural networks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b0">1]</ref>. A few recent papers have used Bayesian optimization with a graph neural network as a predictor <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b47">48]</ref>, however, they do not conduct an ablation study of all components of the framework. In this work, we do not claim to invent the BO + neural predictor framework, however, we give the most in-depth analysis that we are aware of, which we use to design a high-performance instantiation of this framework.</p><p>There is also prior work on using neural network models in BO for hyperparameter optimization <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>, The explicit goal of these papers is to improve the efficiency of Gaussian process-based BO from cubic to linear time, not to develop a different type of prediction model in order to improve the performance of BO with respect to the number of iterations.</p><p>Recent papers have called for fair and reproducible experiments <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b71">72]</ref>. In this vein, the NASBench-101 <ref type="bibr" target="#b71">[72]</ref>, -201 <ref type="bibr" target="#b8">[9]</ref>, and -301 <ref type="bibr" target="#b48">[49]</ref> datasets were created, which contain tens of thousands of pretrained neural architectures. We provide additional related work details in Appendix A.</p><p>Subsequent work. Since its release, several papers have included BANANAS in new experiments, further showing that BANANAS is a competitive NAS algorithm <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b62">63]</ref>. Finally, a recent paper conducted a study on several encodings used for NAS <ref type="bibr" target="#b64">[65]</ref>, concluding that neural predictors perform well with the path encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BO + Neural Predictor Framework</head><p>In this section, we give a background on BO, and we describe the BO + neural predictor framework. In applications of BO for deep learning, the typical goal is to find a neural architecture and/or set of hyperparameters that lead to an optimal validation error. Formally, BO seeks to compute a * = arg min a∈A f (a), where A is the search space, and f (a) denotes the validation error of architecture a after training on a fixed dataset for a fixed number of epochs. In the standard BO setting, over a sequence of iterations, the results from all previous iterations are used to model the topology of {f (a)} a∈A using the posterior distribution of the model (often a GP). The next architecture is then chosen by optimizing an acquisition function such as expected improvement (EI) <ref type="bibr" target="#b37">[38]</ref> or Thompson sampling (TS) <ref type="bibr" target="#b58">[59]</ref>. These functions balance exploration with exploitation during the search. The chosen architecture is then trained and used to update the model of {f (a)} a∈A . Evaluating f (a) in each iteration is the bottleneck of BO (since a neural network must be trained). To mitigate this, parallel BO methods typically output k architectures to train in each iteration, so that the k architectures can be trained in parallel.</p><p>BO + neural predictor framework. In each iteration of BO, we train a neural network on all previously evaluated architectures, a, to predict the validation accuracy f (a) of unseen architectures. The architectures are represented as labeled DAGs <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b8">9]</ref>, and there are different methods of encoding the DAGs before they are passed to the neural predictor <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b64">65]</ref>, which we describe in the next section. Choices for the neural predictor include feedforward networks, graph convolutional networks (GCN), and variational autoencoder (VAE)-based networks. In order to evaluate an acquisition function, we also compute an uncertainty estimate for each input datapoint. This can be accomplished by using, for example, a Bayesian neural network or an ensemble of neural predictors. Given the acquisition function, an acquisition optimization routine is then carried out, which returns the next architecture to be evaluated. In the next section, we give a thorough analysis of the choices that must be made when instantiating this framework.  In this section, we give an extensive study of the BO + neural predictor framework. First, we discuss architecture encodings, and we define a novel featurization called the path encoding. Then we conduct an analysis of different choices of neural predictors. Next, we analyze different methods for achieving calibrated uncertainty estimates from the neural predictors. After that, we conduct experiments on different acquisition functions and acquisition optimization strategies. Finally, we use these analyses to create our algorithm, BANANAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis of the Framework</head><p>Throughout this section, we run experiments on the NASBench-101 dataset (experiments on additional search spaces are given in Section 6). The NASBench-101 dataset <ref type="bibr" target="#b71">[72]</ref> consists of over 423,000 neural architectures from a cell-based search space, and each architecture comes with precomputed validation and test accuracies on CIFAR-10. The search space consists of a DAG with 7 nodes that can each take on three different operations, and there can be at most 9 edges between the nodes. We use the open source version of the NASBench-101 dataset <ref type="bibr" target="#b71">[72]</ref>. We give the full details about the use of NASBench-101 in Appendix D. Our code is available at https://github.com/naszilla/naszilla. Architecture encodings. The majority of existing work on neural predictors use an adjacency matrix representation to encode the neural architectures. The adjacency matrix encoding gives an arbitrary ordering to the nodes, and then gives a binary feature for an edge between node i and node j, for all i &lt; j. Then a list of the operations at each node must also be included in the encoding. This is a challenging data structure for a neural predictor to interpret because it relies on an arbitrary indexing of the nodes, and features are highly dependent on one another. For example, an edge from the input to node 2 is useless if there is no path from node 2 to the output. And if there is an edge from node 2 to the output, this edge is highly correlated with the feature that describes the operation at node 2 (conv 1x1, pool 3x3, etc.). A continuous-valued variant of the adjacency matrix encoding has also been tested <ref type="bibr" target="#b71">[72]</ref>.</p><p>We introduce a novel encoding which we term the path encoding, and we show that it substantially increases the performance of neural predictors. The path encoding is quite simple to define: there is a binary feature for each path from the input to the output of an architecture cell, given in terms of the operations (e.g., input→conv 1x1→pool 3x3→output). To encode an architecture, we simply check which paths are present in the architecture, and set the corresponding features to 1s. See <ref type="figure">Figure 5</ref>.1. Intuitively, the path encoding has a few strong advantages. The features are not nearly as dependent on one another as they are in the adjacency matrix encoding, since each feature represents a unique path that the data tensor can take from the input node to the output node. Furthermore, there is no longer an arbitrary node ordering, which means that each neural architecture maps to only one encoding (which is not true for the adjacency matrix encoding). On the other hand, it is possible for multiple architectures to map to the same path encoding (i.e., the encoding is well-defined, but it is not one-to-one). However, subsequent work showed that architectures with the same path encoding also have very similar validation errors <ref type="bibr" target="#b64">[65]</ref>, which is beneficial in NAS algorithms.</p><p>The length of the path encoding is the total number of possible paths in a cell, n i=0 q i , where n denotes the number of nodes in the cell, and q denotes the number of operations for each node. However, we present theoretical and experimental evidence that substantially truncating the path encoding, even to length smaller than the adjacency matrix encoding, does not decrease its performance. Many NAS algorithms sample architectures by randomly sampling edges in the DAG subject to a maximum edge constraint <ref type="bibr" target="#b71">[72]</ref>. Intuitively, the vast majority of paths have a very low probability of occurring in a cell returned by this procedure. Therefore, by simply truncating the least-likely paths, our encoding scales linearly in the size of the cell, with an arbitrarily small amount of information loss. In the following theorem, let G n,k,r denote a DAG architecture with n nodes, r choices of operations on each node, and where each potential forward edge ( n(n−1) 2 total) was chosen with probability 2k n(n−1) (so that the expected number of edges is k).</p><p>Theorem 5.1 (informal). Given integers r, c &gt; 0, there exists an N such that ∀ n &gt; N , there exists a set of n paths P such that the probability that G n,n+c,r contains a path not in P is less than 1 n 2 . For the formal statement and full proof, see Appendix C. This theorem says that when n is large enough, with high probability, we can truncate the path encoding to a size of just n without losing information. Although the asymptotic nature of this result makes it a proof of concept, we empirically show in <ref type="figure">Figure 5</ref>.1 that in BANANAS running on NASBench-101, the path encoding can be truncated from its full size of 5 i=0 3 i = 364 bits to a length of just twenty bits, without a loss in performance. (The exact experimental setup for this result is described later in this section.) In fact, the performance after truncation actually improves up to a certain point. We believe this is because with the full-length encoding, the neural predictor overfits to very rare paths. In Appendix D, we show a similar result for NASBench-201 <ref type="bibr" target="#b8">[9]</ref>: the full path encoding length of 3 i=0 5 i = 156 can be truncated to just 30, without a loss of performance.</p><p>Neural predictors. Now we study the neural predictor, a crucial component in the BO + neural predictor framework. Recall from the previous section that a neural predictor is a neural network that is repeatedly trained on the current set of evaluated neural architectures and predicts the accuracy of unseen neural architectures. Prior work has used GCNs <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b35">36]</ref> or VAE-based architectures <ref type="bibr" target="#b74">[75]</ref> for this task. We evaluate the performance of standard feedfoward neural networks with either the adjacency matrix or path-based encoding, compared to VAEs and GCNs in predicting the validation accuracy of neural architectures. The feedforward neural network we use is a sequential fully-connected network with 10 layers of width 20, the Adam optimizer with a learning rate of 0.01, and the loss function set to mean absolute error (MAE). We use open-source implementations of the GCN <ref type="bibr" target="#b75">[76]</ref> and VAE <ref type="bibr" target="#b74">[75]</ref>. See Appendix D for a full description of our implementations.</p><p>In <ref type="figure">Figure 5</ref>.2 (left), we compare the different neural predictors by training them on a set of neural architectures drawn i.i.d. from NASBench-101, along with validation accuracies, and then computing the MAE on a held-out test set of size 1000. We run 50 trials for different training set sizes and average the results. The best-performing neural predictors are the feedforward network with the path encoding (with and without truncation) and the GCN. The feedforward networks also had shorter runtime compared to the GCN and VAE, however, the runtime of the full NAS algorithm is dominated by evaluating neural architectures, not by training neural predictors.</p><p>Uncertainty calibration. In the previous section, we evaluated standalone neural predictors. To incorporate them within BO, for any datapoint, neural predictors need to output both a prediction and an uncertainty estimate for that prediction. Two popular ways of achieving uncertainties are by using a Bayesian neural network (BNN), or by using an ensemble of neural predictors. In a BNN, we infer a posterior distribution over network weights. It has been demonstrated recently that accurate prediction and uncertainty estimates in neural networks can be achieved using Hamiltonian Monte Carlo <ref type="bibr" target="#b52">[53]</ref>. In the ensemble approach, we train m neural predictors using different random weight   initializations and training set orders. Then for any datapoint, we can can compute the mean and standard deviation of these m predictions. Ensembles of neural networks, even of size three and five, have been shown in some cases to give more reliable uncertainty estimates than other leading approaches such as BNNs <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b72">73]</ref>.</p><p>We compare the uncertainty estimate of a BNN with an ensemble of size five for each of the neural predictors described in the previous section. We use the BOHAMIANN implementation for the BNN <ref type="bibr" target="#b52">[53]</ref>, and to ensure a fair comparison with the ensembles, we train it for five times longer. The experimental setup is similar to the previous section, but we compute a standard measure of calibration: root mean squared calibration error (RMSCE) on the test set <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b59">60]</ref>. See <ref type="figure">Figure 5</ref>.2 (middle). Intuitively, the RMSCE is low if a method yields a well-calibrated predictive estimate (i.e. predicted coverage of intervals equals the observed coverage). All ensemble-based predictors yielded better uncertainty estimates than the BNN, consistent with prior work. Note that RMSCE only measures the quality of uncertainty estimates, agnostic to prediction accuracy. We must therefore look at prediction ( <ref type="figure">Figure 5</ref> Finally, we evaluate the performance of each neural predictor within the full BO + neural predictor framework. We use the approach described in Section 4, using independent Thompson sampling and mutation for acquisition optimization (described in more detail in the next section). Each algorithm is given a budget of 47 TPU hours, or about 150 neural architecture evaluations on NASBench-101. That is, there are 150 iterations of training a neural predictor and choosing a new architecture to evaluate using the acquisition function. The algorithms output 10 architectures in each iteration of BO for better parallelization, as described in the previous section. After each iteration, we return the test error of the architecture with the best validation error found so far. We run 200 trials of each algorithm and average the results. This is the same experimental setup as in <ref type="figure">Figure 5</ref>.1, as well as experiments later in this section and the next section. See <ref type="figure">Figure 5</ref>.2 (right). The two best-performing neural predictors are an ensemble of GCNs, and an ensemble of feedforward neural networks with the path encoding, with the latter having a slight edge. The feedforward network is also desirable because it requires less hyperparameter tuning than the GCN.</p><p>Acquisition functions and optimization. Now we analyze the BO side of the framework, namely, the choice of acquisition function and acquisition optimization. We consider four common acquisition functions that can be computed using a mean and uncertainty estimate for each input datapoint: expected improvement (EI) <ref type="bibr" target="#b37">[38]</ref>, probability of improvement (PI) <ref type="bibr" target="#b26">[27]</ref>, upper confidence bound (UCB) <ref type="bibr" target="#b53">[54]</ref>, and Thompson sampling (TS) <ref type="bibr" target="#b58">[59]</ref>. We also consider a variant of TS called independent Thompson sampling. First we give the formal definitions of each acquisition function.</p><p>Suppose we have trained an ensemble of M predictive models, {f m } M m=1 , where f m : A → R. Let y min denote the lowest validation error of an architecture discovered so far. Following previous work <ref type="bibr" target="#b38">[39]</ref>, we use the following acquisition function estimates for an input architecture a ∈ A:</p><formula xml:id="formula_0">φ EI (a) = E [1 [f m (a) &gt; y min ] (y min − f m (a))] (5.1) = y min −∞ (y min − y) N f ,σ 2 dy φ PI (x) = E [1 [f m (x) &gt; y min ]] (5.2) = y min −∞ N f ,σ 2 dy φ UCB (x) =f − βσ (5.3) φ TS (x) = fm(x),m ∼ Unif (1, M ) (5.4) φ ITS (x) =f x (x),f x (x) ∼ N (f ,σ 2 ) (5.5)</formula><p>In these acquisition function definitions, 1(x) = 1 if x is true and 0 otherwise, and we are making a normal approximation for our model's posterior predictive density, where we estimate parameterŝ</p><formula xml:id="formula_1">f = 1 M M m=1 f m (x), andσ = M m=1 (f m (x) −f ) 2 M − 1 .</formula><p>In the UCB acquisition function experiments, we set the tradeoff parameter β = 0.5. We tested each acquisition function within the BO + neural predictor framework, using mutation for acquisition optimization and the best neural predictor from the previous section -an ensemble of feedforward networks with the path encoding. The experimental setup is the same as in previous sections. See <ref type="figure">Figure 6</ref>.1 (left). We see that the acquisition function does not have as big an effect on performance as other components, though ITS performs the best overall. Note also that both TS and ITS have advantages when running parallel experiments, since they are stochastic acquisition functions that can be directly applied in the batch BO setting <ref type="bibr" target="#b19">[20]</ref>.</p><p>Next, we test different acquisition optimization strategies. In each iteration of BO, our goal is to find the neural architecture from the search space which minimizes the acquisition function. Evaluating the acquisition function for every neural architecture in the search space is computationally infeasible. Instead, we create a set of 100-1000 architectures (potentially in an iterative fashion) and choose the architecture with the value of the acquisition function in this set.</p><p>The simplest strategy is to draw 1000 random architectures. However, it can be beneficial to generate a set of architecture that are close in edit distance to architectures in the training set, since the neural predictor is more likely to give accurate predictions to these architectures. Furthermore, local optimization methods such as mutation, evolution, and local search have been shown to be effective for acquisition optimization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b67">68]</ref>. In "mutation", we simply mutate the architectures with the best validation accuracy that we have found so far by randomly changing one operation or one edge. In local search, we iteratively take the architectures with the current highest acquisition function value, and compute the acquisition function of all architectures in their neighborhood. In evolution, we iteratively maintain a population by mutating the architectures with the highest acquisition function value and killing the architectures with the lowest values. We give the full details of these methods in Appendix D. The experimental setup is the same as in the previous sections. See <ref type="figure">Figure 6</ref>.1 (middle). We see that mutation performs the best, which indicates that it is better to consider architectures with edit distance closer to the set of already evaluated architectures.</p><p>BANANAS: Bayesian optimization with neural architectures for NAS. Using the best components from the previous sections, we construct our full NAS algorithm, BANANAS, composed of an ensemble of feedforward neural networks using the path encoding, ITS, and a mutation acquisition function. See Algorithm 1 and <ref type="figure">Figure 5</ref>.3. Note that in the previous sections, we conducted experiments on each component individually while keeping all other components fixed. In Appendix D, we give further analysis varying all components at once, to ensure that BANANAS is indeed the optimal instantiation of this framework.</p><p>For the loss function in the neural predictors, we use mean absolute percentage error (MAPE) because it gives a higher weight to architectures with lower validation losses:</p><formula xml:id="formula_2">L(y pred , y true ) = 1 n n i=1 y (i) pred − y LB y (i) true − y LB − 1 ,<label>(5.6)</label></formula><p>where y </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">BANANAS Experiments</head><p>In this section, we compare BANANAS to many other popular NAS algorithms on three search spaces. To promote reproducibility, we discuss our adherence to the NAS research checklist <ref type="bibr" target="#b32">[33]</ref> in</p><formula xml:id="formula_3">Algorithm 1 BANANAS Input: Search space A, dataset D, parameters t 0 , T, M, c, x, acquisition function φ, function f (a)</formula><p>returning validation error of a after training. 1. Draw t 0 architectures a 0 , . . . , a t 0 uniformly at random from A and train them on D.</p><p>2. For t from t 0 to T , i. Train an ensemble of neural predictors on {(a 0 , f (a 0 )), . . . , (a t , f (a t ))} using the path encoding to represent each architecture.</p><p>ii. Generate a set of c candidate architectures from A by randomly mutating the x architectures a from {a 0 , . . . , a t } that have the lowest value of f (a).</p><p>iii. For each candidate architecture a, evaluate the acquisition function φ(a).</p><p>iv. Denote a t+1 as the candidate architecture with minimum φ(a), and evaluate f (a t+1 ).</p><p>Output: a * = argmin t=0,...,T f (a t ).  Appendix E. In particular, we release our code, we use a tabular NAS dataset, and we run many trials of each algorithm.</p><p>We run experiments on NASBench-101 described in the previous section, as well as NASBench-201 and the DARTS search space. The NASBench-201 dataset <ref type="bibr" target="#b70">[71]</ref> consists of 15625 neural architectures with precomputed validation and test accuracies for 200 epochs on CIFAR-10, CIFAR-100, and ImageNet-16-120. The search space consists of a complete directed acyclic graph on 4 nodes, and each edge can take on five different operations. The DARTS search space <ref type="bibr" target="#b34">[35]</ref> is size 10 18 . It consists of two cells: a convolutional cell and a reduction cell. Each cell has four nodes that have two incoming edges which take on one of eight operations.</p><p>Performance on NASBench search spaces. We compare BANANAS to the most popular NAS algorithms from a variety of paradigms: random search <ref type="bibr" target="#b29">[30]</ref>, regularized evolution <ref type="bibr" target="#b43">[44]</ref>, BOHB <ref type="bibr" target="#b10">[11]</ref>, NASBOT <ref type="bibr" target="#b20">[21]</ref>, local search <ref type="bibr" target="#b65">[66]</ref>, TPE <ref type="bibr" target="#b3">[4]</ref>, BOHAMIANN <ref type="bibr" target="#b52">[53]</ref>, BONAS <ref type="bibr" target="#b47">[48]</ref>, REINFORCE <ref type="bibr" target="#b66">[67]</ref>, GP-based BO <ref type="bibr" target="#b49">[50]</ref>, AlphaX <ref type="bibr" target="#b61">[62]</ref>, GCN Predictor <ref type="bibr" target="#b63">[64]</ref>, and DNGO <ref type="bibr" target="#b51">[52]</ref>. As much as possible, we use the code directly from the open-source repositories, without changing the hyperparameters (but with a few exceptions). For a description of each algorithm and details of the implementations we used, see Appendix D.</p><p>The experimental setup is the same as in the previous section. For results on NASBench-101,  <ref type="figure">Figure 6</ref>.1 (right). The top three algorithms in order, are BANANAS, local search, and BONAS.</p><p>In Appendix D, we also show that BANANAS achieves strong performance on the three datasets in NASBench-201.</p><p>Performance on the DARTS search space. We test BANANAS on the search space from DARTS. Since the DARTS search space is not a tabular dataset, we cannot fairly compare to other methods which use substantially different training and testing pipelines <ref type="bibr" target="#b32">[33]</ref>. We use a common test evaluation pipeline which is to train for 600 epochs with cutout and auxiliary tower <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b69">70]</ref>, where the state of the art is around 2.6% on CIFAR-10. Other papers use different test evaluation settings (e.g., training for many more epochs) to achieve lower error, but they cannot be fairly compared to other algorithms.</p><p>In our experiments, BANANAS is given a budget of 100 evaluations. In each evaluation, the chosen architecture is trained for 50 epochs and the average validation error of the last 5 epochs is recorded. To ensure a fair comparison by controlling all hyperparameter settings and hardware, we re-trained the architectures from prior work when they were available. In this case, we report the mean test error over five random seeds of the best architecture found for each method. We compare BANANAS to DARTS <ref type="bibr" target="#b34">[35]</ref>, random search <ref type="bibr" target="#b34">[35]</ref>, local search <ref type="bibr" target="#b65">[66]</ref>, and ASHA <ref type="bibr" target="#b29">[30]</ref>. See <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Note that a new surrogate benchmark on the DARTS search space <ref type="bibr" target="#b48">[49]</ref>, called NASBench-301 was recently introduced, allowing for fair and computationally feasible experiments. Initial experiments showed <ref type="bibr" target="#b48">[49]</ref> that BANANAS is competitive with nine other popular NAS algorithms, including DARTS <ref type="bibr" target="#b34">[35]</ref> and two improvements of DARTS <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b7">8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We conduct an analysis of the BO + neural predictor framework, which has recently emerged as a high-performance framework for NAS. We test several methods for each main component: the encoding, neural predictor, calibration method, acquisition function, and acquisition optimization strategy. We also propose a novel path-based encoding scheme, which improves the performance of neural predictors. We use all of this analysis to develop BANANAS, an instantiation of the BO + neural predictor framework which achieves state-of-the-art performance on popular NAS search spaces. Interesting follow-up ideas are to develop multi-fidelity or successive halving versions of BANANAS. Incorporating these approaches with BANANAS could result in a significant decrease in the runtime without sacrificing accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work Continued</head><p>Bayesian optimization. Bayesian optimization is a leading technique for zeroth order optimization when function queries are expensive <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b12">13]</ref>, and it has seen great success in hyperparameter optimization for deep learning <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31]</ref>. The majority of Bayesian optimization literature has focused on Euclidean or categorical input domains, and has used a GP model <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b49">50]</ref>. There are techniques for parallelizing Bayesian optimization <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>There is also prior work on using neural network models in Bayesian optimization for hyperparameter optimization <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>. The goal of these papers is to improve the efficiency of Gaussian Process-based Bayesian optimization from cubic to linear time, not to develop a different type of prediction model in order to improve the performance of BO with respect to the number of iterations. In our work, we present techniques which deviate from Gaussian Process-based Bayesian optimization and see a performance boost with respect to the number of iterations.</p><p>Neural architecture search. Neural architecture search has been studied since at least the 1990s <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b55">56]</ref>, but the field was revitalized in 2017 <ref type="bibr" target="#b78">[79]</ref>. Some of the most popular techniques for NAS include evolutionary algorithms <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b36">37]</ref>, reinforcement learning <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b60">61]</ref>, Bayesian optimization <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b76">77]</ref>, gradient descent <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b28">29]</ref>, tree search <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b60">61]</ref>, and neural predictors <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b63">64]</ref>. For a survey on NAS, see <ref type="bibr" target="#b9">[10]</ref>.</p><p>Recent papers have called for fair and reproducible experiments <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b71">72]</ref>. In this vein, the NASBench-101 <ref type="bibr" target="#b71">[72]</ref>, -201 <ref type="bibr" target="#b8">[9]</ref>, and -301 <ref type="bibr" target="#b48">[49]</ref> datasets were created, which contain tens of thousands of pretrained neural architectures.</p><p>Initial BO approaches for NAS defined a distance function between architectures <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b18">19]</ref>. A few recent papers have used Bayesian optimization with a graph neural network as a predictor <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b47">48]</ref>, however, they do not conduct an ablation study of all components of the framework. In this work, we do not claim to invent the BO + neural predictor framework, however, we give the most in-depth analysis that we are aware of, which we use to design a high-performance instantiation of this framework.</p><p>Predicting neural network accuracy. There are several approaches for predicting the validation accuracy of neural networks, such as a layer-wise encoding of neural networks with an LSTM algorithm <ref type="bibr" target="#b5">[6]</ref>, and a layer-wise encoding and dataset features to predict the accuracy for neural network and dataset pairs <ref type="bibr" target="#b17">[18]</ref>. There is also work in predicting the learning curve of neural networks for hyperparameter optimization <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b6">7]</ref> or NAS <ref type="bibr" target="#b0">[1]</ref> using Bayesian techniques. None of these methods have predicted the accuracy of neural networks drawn from a cell-based DAG search space such as NASBench or the DARTS search space. Another recent work uses a hypernetwork for neural network prediction in NAS <ref type="bibr" target="#b73">[74]</ref>. Other recent works for predicting neural network accuracy include AlphaX <ref type="bibr" target="#b61">[62]</ref>, and three papers which use GCN's to predict neural network accuracy <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>Ensembling of neural networks is a popular approach for uncertainty estimates, shown in many settings to be more effective than all other methods such as Bayesian neural networks even for an ensemble of size five <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Subsequent work. Since its release, several papers have independently shown that BANANAS is a competitive algorithm for NAS <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b62">63]</ref>. For example, one paper shows that BANANAS outperforms other algorithms on NASBench-101 when given a budget of 3200 evaluations <ref type="bibr" target="#b23">[24]</ref>, and one paper shows that BANANAS outperforms many popular NAS algorithms on NASBench-301 <ref type="bibr" target="#b48">[49]</ref>. Finally, a recent paper conducted a study on several encodings used for NAS <ref type="bibr" target="#b64">[65]</ref>, concluding that neural predictors perform well with the path encoding, and also improved upon the theoretical results presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Preliminaries Continued</head><p>We give background information on three key ingredients of NAS algorithms.</p><p>Search space. Before deploying a NAS algorithm, we must define the space of neural networks that the algorithm can search through. Perhaps the most common type of search space for NAS is a cell-based search space <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b71">72]</ref>. A cell consists of a relatively small section of a neural network, usually 6-12 nodes forming a directed acyclic graph (DAG). A neural architecture is then built by repeatedly stacking one or two different cells on top of each other sequentially, possibly separated by specialized layers. The layout of cells and specialized layers is called a hyper-architecture, and this is fixed, while the NAS algorithm searches for the best cells. The search space over cells consists of all possible DAGs of a certain size, where each node can be one of several operations such as 1 × 1 convolution, 3 × 3 convolution, or 3 × 3 max pooling. It is also common to set a restriction on the number of total edges or the in-degree of each node <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b34">35]</ref>. In this work, we focus on NAS over convolutional cell-based search spaces, though our method can be applied more broadly.</p><p>Search strategy. The search strategy is the optimization method that the algorithm uses to find the optimal or near-optimal neural architecture from the search space. There are many varied search strategies, such as Bayesian optimization, evolutionary search, reinforcement learning, and gradient descent. In Section 5, we introduced the search strategy we study in this paper: Bayesian optimization with a neural predictor.</p><p>Evaluation method. Many types of NAS algorithms consist of an iterative framework in which the algorithm chooses a neural network to train, computes its validation error, and uses this result to guide the choice of neural network in the next iteration. The simplest instantiation of this approach is to train each neural network in a fixed way, i.e., the algorithm has black-box access to a function that trains a neural network for x epochs and then returns the validation error. Algorithms with black-box evaluation methods can be compared by returning the architecture with the lowest validation error after a certain number of queries to the black-box function. There are also multi-fidelity methods, for example, when a NAS algorithm chooses the number of training epochs in addition to the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Path Encoding Theory</head><p>In this section, we give the full details of Theorem 5.1 from Section 5, which shows that with high probability, the path encoding can be truncated significantly without losing information.</p><p>Recall that the size of the path encoding is equal to the number of unique paths, which is i=0 n r i , where n is the number of nodes in the cell, and r is the number of operations to choose from at each node. This is at least r n . By contrast, the adjacency matrix encoding scales quadratically in n. However, the vast majority of the paths rarely show up in any neural architecture throughout a full run of a NAS algorithm. This is because many NAS algorithms can only sample architectures from a random procedure or mutate architectures drawn from the random procedure. Now we will give the full details of Theorem 5.1, showing that the vast majority of paths have a very low probability of occurring in a cell outputted from random spec(), a popular random procedure used by many NAS algorithms <ref type="bibr" target="#b71">[72]</ref>, including BANANAS. Our results show that by simply truncating the least-likely paths, the path encoding scales linearly in the size of the cell, with an arbitrarily small amount of information loss. We back this up with experimental evidence in <ref type="figure" target="#fig_8">Figures 5.1 and D.4</ref>, and <ref type="table" target="#tab_7">Table 3</ref>.</p><p>We start by defining random spec(), the procedure to output a random neural architecture.</p><p>Definition C.1. Given integers n, r, and k &lt; n(n−1) /2, a random graph G n,k,r is generated as follows: (1) Denote n nodes by 1 to n.</p><p>(2) Label each node randomly with one of r operations.</p><p>(3) For all i &lt; j, add edge (i, j) with probability 2k n(n−1) . (4) If there is no path from nodes 1 to n, goto (1). <ref type="formula">(3)</ref> is chosen so that the expected number of edges after this step is exactly k. Recall that we use 'path' to mean a path from node 1 to node n. We restate the theorem formally. Denote P as the set of all possible paths from node 1 to node n that could occur in G n,k,r . Theorem 5.1 (formal). Given integers r, c &gt; 0, there exists N such that for all n &gt; N , there exists a set of n paths P ⊆ P such that P (∃p ∈ G n,n+c,r ∩ P \ P ) ≤ 1 n 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The probability value in step</head><p>This theorem says that when k = n + c, and when n is large enough compared to c and r, then we can truncate the path encoding to a set P of size n, because the probability that random spec() outputs a graph G n,k,r with a path outside of P is very small.</p><p>Note that there are two caveats to this theorem. First, BANANAS may mutate architectures drawn from Definition C.1, and Theorem 5.1 does not show the probability of paths from mutated architectures is small. However, our experiments ( <ref type="figure" target="#fig_8">Figures 5.1 and D.4)</ref> give evidence that the mutated architectures do not change the distribution of paths too much. Second, the most common paths in Definition C.1 are not necessarily the paths whose existence or non-existence give the most entropy in predicting the validation accuracy of a neural architecture. Again, while this is technically true, our experiments back up Theorem 5.1 as a reasonable argument that truncating the path encoding does not sacrifice performance.</p><p>Denote by G n,k,r the random graph outputted by Definiton C.1 without step <ref type="bibr" target="#b3">(4)</ref>. In other words, G n,k,r is a random graph that could have no path from node 1 to node n. Since there are n(n−1) 2 pairs (i, j) such that i &lt; j, the expected number of edges of G n,k,r is k. For reference, in the NASBench-101 dataset, there are n = 7 nodes and r = 3 operations, and the maximum number of edges is 9.</p><p>We choose P as the n shortest paths from node 1 to node n. The argument for Theorem 5.1 relies on a simple concept: the probability that G n,k,r contains a long path (length &gt; log r n) is much lower than the probability that it contains a short path. For example, the probability that G n,k,r contains a path of length n − 1 is very low, because there are Θ(n 2 ) potential edges but the expected number of edges is n + O(1). We start by upper bounding the length of the n shortest paths.</p><p>Lemma C.2. Given a graph with n nodes and r node labels, there are fewer than n paths of length less than or equal to log r n − 1.</p><p>Proof. The number of paths of length is r , since there are r choices of labels for each node. Then</p><formula xml:id="formula_4">1 + r + · · · + r log r n −1 = r log r n − 1 r − 1 = n − 1 r − 1 &lt; n.</formula><p>To continue our argument, we will need the following well-known bounds on binomial coefficients, e.g. <ref type="bibr" target="#b54">[55]</ref>.</p><p>Theorem C.3. Given 0 ≤ ≤ n, we have n ≤ n ≤ en .</p><p>Now we define a n,k, as the expected number of paths from node 1 to node n of length in G n,k,r . Formally, a n,k, = E [|p ∈ P| | |p| = ] .</p><p>The following lemma, which is the driving force behind Theorem 5.1, shows that the value of a n,k, for small is much larger than the value of a n,k, for large .</p><p>Lemma C.4. Given integers r, c &gt; 0, then there exists n such that for k = n + c, we have n−1 =log r n a n,k, &lt; 1 n 3 and a n,k,1 &gt; 1 n .</p><p>Proof. We have that a n,k, = n − 2 − 1 2k n(n − 1) . This is because on a path from node 1 to n of length , there are n−2 −1 choices of intermediate nodes from 1 to n. Once the nodes are chosen, we need all edges between the nodes to exist, and each edge exists independently with probability 2 n(n−1) · k. When = 1, we have n−2 −1 = 1. Therefore, a n,k,1 = 2k n(n − 1) ≥ 1 n , for sufficiently large n. Now we will derive an upper bound for a n,k, using Theorem C.3. a n,k, =</p><formula xml:id="formula_5">n − 2 − 1 2k n(n − 1) ≤ e(n − 2) − 1 −1 2k n(n − 1) ≤ 2k n(n − 1) 2ek(n − 2) ( − 1)n(n − 1) −1 ≤ 4 n 4e − 1 −1</formula><p>The last inequality is true because k/(n − 1) = (n + c)/(n − 1) ≤ 2 for sufficiently large n. Now we have n−1 =log r n a n,k, ≤ n−1</p><formula xml:id="formula_6">=log r n 4 n 4e − 1 −1 ≤ n−1 =log r n 4e − 1 −1 ≤ n−1 =log r n 4e log r n −1 ≤</formula><p>4e log r n log r n n−log r n =0 4e log r n ≤ (e) 3 log r n 1 log r n log r n · 2 (C.1)</p><formula xml:id="formula_7">≤ 2 (n) 3 1 n log r log r n (C.2) ≤ 1 n log r log r n−4 ≤ 1 n 3 .</formula><p>In inequality C.1, we use the fact that for large enough n, 4e log r n &lt; 1 2 , therefore,</p><formula xml:id="formula_8">n−log r n =0 4e log r n ≤ n−log r n =0 1 2 ≤ 2</formula><p>In inequality C.2, we use the fact that (log n) log n = e log log n log n = e log n log log n = n log log n Now we can prove Theorem 5.1.</p><p>Proof of Theorem 5.1. Recall that P denotes the set of all possible paths from node 1 to node n that could be present in G n,k,r , and let P = {p | |p| &lt; log r n − 1}. Then by Lemma C.2, |P| &lt; n.</p><p>In Definition C.1, the probability that we return a graph in step (4) is at least the probability that there exists an edge from node 1 to node n. This probability is ≥ 1 n from Lemma C.4. Now we will compute the probability that there exists a path in P \ P in G n,k,r by conditioning on returning a graph in step <ref type="bibr" target="#b3">(4)</ref>. The penultimate inequality is due to Lemma C.4. P (∃p ∈ G n,k,r ∩ P \ P ) = P (∃p ∈ G n,k,r ∩ P \ P | ∃q ∈ G n,k,r ∩ P) = P (∃p ∈ G n,k,r ∩ P \ P ) P (∃q ∈ G n,k,r ∩ P)</p><formula xml:id="formula_9">≤ 1 n 3 / 1 n ≤ 1 n 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Experiments and Details</head><p>In this section, we present details and supplementary experiments from Sections 5 and 6. In the first subsection, we give a short description and implementation details for all 15 of the NAS algorithms we tested in Section 6, as well as additional details from Section 6. Next, we give an exhaustive experiment on the BO + neural predictor framework. After that, we evaluate BANANAS on the three datasets in NASBench-201. Then, we discuss the NASBench-101 API and conduct additional experiments. Finally, we study the effect of the length of the path encoding on the performance of BANANAS on NASBench-201.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Details from Section 6</head><p>Here, we give more details on the NAS algorithms we compared in Section 6. Regularized evolution. This algorithm consists of iteratively mutating the best achitectures out of a sample of all architectures evaluated so far <ref type="bibr" target="#b43">[44]</ref>. We used the <ref type="bibr" target="#b71">[72]</ref> implementation although we changed the population size from 50 to 30 to account for fewer total queries. We also found that in each round, removing the architecture with the worst validation accuracy performs better than removing the oldest architecture, so this is the algorithm we compare to. (Technically this would make the algorithm closer to standard evolution.)</p><p>Local search. Another simple baseline, local search iteratively evaluates all architectures in the neighborhood of the architecture with the lowest validation error found so far. For NASBench-101, the "neighborhood" means all architectures which differ from the current architecture by one operation or one edge. We used the implementation from White et al. <ref type="bibr" target="#b65">[66]</ref>, who showed that local search is a state-of-the-art approach on NASBench-101 and NASBench-201.</p><p>Bayesian optimization with a GP model. We set up Bayesian optimization with a Gaussian process model and UCB acquisition. In the Gaussian process, we set the distance function between two neural networks as the sum of the Hamming distances between the adjacency matrices and the list of operations. We use the ProBO implementation <ref type="bibr" target="#b38">[39]</ref>.</p><p>NASBOT. Neural architecture search with Bayesian optimization and optimal transport (NASBOT) <ref type="bibr" target="#b20">[21]</ref> works by defining a distance function between neural networks by computing the similarities between layers and then running an optimal transport algorithm to find the minimum earth-mover's distance between the two architectures. Then Bayesian optimization is run using this distance function. The NASBOT algorithm is specific to macro NAS, and we put in a good-faith effort to implement it in the cell-based setting. Specifically, we compute the distance between two cells by taking the earth-mover's distance between the set of row-sums, column-sums, and node operations. This is a version of the OTMANN distance <ref type="bibr" target="#b20">[21]</ref>, defined for the cell-based setting.</p><p>Random search. The simplest baseline, random search, draws n architectures at random and outputs the architecture with the lowest validation error. Despite its simplicity, multiple papers have concluded that random search is a competitive baseline for NAS algorithms <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b45">46]</ref>. In <ref type="table" target="#tab_2">Table 1</ref>, we also compared to Random Search with Weight-Sharing, which uses shared weights to quickly compare orders of magnitude more architectures.</p><p>AlphaX. AlphaX casts NAS as a reinforcement learning problem, using a neural network to guide the search <ref type="bibr" target="#b61">[62]</ref>. Each iteration, a neural network is trained to select the best action, such as making a small change to, or growing, the current architecture. We used the open-source implementation of AlphaX as is <ref type="bibr" target="#b61">[62]</ref>.</p><p>BOHAMIANN. Bayesian Optimization with Hamiltonian Monte Carlo Artificial Neural Networks (BOHAMIANN) <ref type="bibr" target="#b52">[53]</ref> is an approach which fits in to the "BO + neural predictor" framework. It uses a Bayesian neural network (implemented using Hamiltonian Monte Carlo) as the neural predictor. We used the BOHAMIANN implementation of the Bayesian neural network <ref type="bibr" target="#b52">[53]</ref> with our own outer BO wrapper, so that we could accurately compare different neural predictors within the framework.</p><p>REINFORCE. We use the NASBench-101 implementation of REINFORCE <ref type="bibr" target="#b66">[67]</ref>. Note that this was the best reinforcement learning-based NAS algorithm released by NASBench-101, outperforming other popular approaches such as a 1-layer LSTM controller trained with PPO <ref type="bibr" target="#b71">[72]</ref>.</p><p>GCN Predictor. We implemented a GCN predictor <ref type="bibr" target="#b63">[64]</ref>. Although the code is not opensourced, we found an open-source implementation online <ref type="bibr" target="#b75">[76]</ref>. We used this implementation, keeping the hyperparameters the same as in the original paper <ref type="bibr" target="#b63">[64]</ref>.</p><p>BONAS. We implemented BONAS <ref type="bibr" target="#b47">[48]</ref>. Again, the code was not open-sourced, so we used the same GCN implementation as above <ref type="bibr" target="#b75">[76]</ref>, using our own code for the outer BO wrapper.</p><p>DNGO. Deep Networks for Global Optimization (DNGO) is an implementation of Bayesian optimization using adaptive basis regression using neural networks instead of Gaussian processes to avoid the cubic scaling. We used the open-source code <ref type="bibr" target="#b51">[52]</ref>.</p><p>BOHB. Bayesian Optimization HyperBand (BOHB) combines multi-fidelity Bayesian optimization with principled early-stopping from Hyperband <ref type="bibr" target="#b10">[11]</ref>. We use the NASBench implementation <ref type="bibr" target="#b71">[72]</ref>.</p><p>TPE. Tree-structured Parzen estimator (TPE) is a BO-based hyperparameter optimization algorithm based on adaptive Parzen windows. We use the NASBench implementation <ref type="bibr" target="#b71">[72]</ref>.</p><p>DARTS. DARTS <ref type="bibr" target="#b34">[35]</ref> is a popular first-order (sometimes called "one-shot") NAS algorithm. In DARTS, the neural network parameters and the architecture hyperparameters are optimized simultaneously using alternating steps of gradient descent. In <ref type="table" target="#tab_2">Table 1</ref>, we reported the published numbers from the paper, and then we retrained the architecture published by the DARTS paper, five times, to account for differences in hardware.</p><p>ASHA. Asyncrhonous Successive Halving Algorithm (ASHA) is an algorithm that uses asynchronous parallelization and early-stopping. As with DARTS, we reported both the published  number and the numbers we achieved by retraining the published architecture on our hardware. Additional notes from Section 6. We give the results from <ref type="figure">Figure 6</ref>.1 (right) into a table <ref type="table" target="#tab_3">(Table 2</ref>).</p><p>In the main NASBench-101 experiments, <ref type="figure">Figure 6</ref>.1, we added an isomorphism-removing subroutine to any algorithm that uses the adjacency matrix encoding. This is because multiple adjacency matrices can map to the same architecture. With the path encoding, this is not necessary. Note that without the isomorphism-removing subroutine, algorithms using the adjacency matrix encoding may perform significantly worse (e.g., we found this to be true for BANANAS with the adjacency matrix encoding). This is another strength of the path encoding.</p><p>In Section 6, we described the details of running BANANAS on the DARTS search space, which resulted in an architecture. We show this architecture in <ref type="figure">Figure D</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Exhaustive Framework Experiment</head><p>In Section 5, we conducted experiments on each component individually while keeping all other components fixed. However, this experimental setup implicitly assumes that all components are linear with respect to performance. For example, we showed GCN performs worse than the path encoding with ITS, and UCB performs worse than ITS using the path encoding, but we never tested GCN together with UCB -what if it outperforms ITS with the path encoding?</p><p>In this section we run a more exhaustive experiment by testing the 18 most promising configurations. We take all combinations of the highest-performing components from Section 5). Specifically, we test all combinations of {UCB, EI, ITS}, {mutation, mutation+random}, and {GCN, path enc., trunc. path enc.)} as acquisition function, acquisition optimization strategy, and neural predictor. We use the same experimental setup as in Section 5, and we run 500 trials of each algorithm. See <ref type="figure">Figure D.</ref>2. The overall best-performing algorithm was Path-ITS-Mutation, which was the same conclusion reached in Section 5. The next best combinations are Path-ITS-Mut+Rand and Trunc-ITS-Mut+Rand. Note that there is often very little difference between the path encoding and truncated path encoding, all else being equal. The results show that each component has a fairly linear relationship with respect to performance: mutation outperforms mutation+random; ITS outperforms UCB which outperforms EI; and both the path and truncated path encodings outperform GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Results on NASBench-201</head><p>We described the NASBench-201 dataset in Section 6. The NASBench-201 dataset is similar to NASBench-101. Note that NASBench-201 is much smaller even than NASBench-101: it is originally size 15625, but it only contains 6466 unique architectures after all isomorphisms are removed <ref type="bibr" target="#b8">[9]</ref>. By contrast, NASBench-101 has about 423,000 architectures after removing isomorphisms. Some papers have claimed that NASBench-201 may be too small to effectively benchmark NAS algorithms <ref type="bibr" target="#b65">[66]</ref>. However, one upside of NASBench-201 is that it contains three image datasets instead of just one: CIFAR-10, CIFAR-100, and ImageNet-16-120.</p><p>Our experimental setup is the same as for NASBench-101 in Section 6. See <ref type="figure">Figure D.</ref>  Due to the extremely small size of the search space as described above, several algorithms tie for the best performance. We see that BANANAS ties for the best performance on CIFAR-10 and CIFAR-100. On ImageNet16-120, it ties for the best performance after 40 GPU hours, but NASBOT and BO w. GP reach top performance more quickly. We stress that we did not change any hyperparameters or any other part of the code of BANANAS, when moving from NASBench-101 to all three NASBench-201 datasets, which shows that BANANAS does not need to be tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 NASBench-101 API</head><p>In the NASBench-101 dataset, each architecture was trained to 108 epochs three separate times with different random seeds. The original paper conducted experiments by (1 ) choosing a random validation error when evaluating each architecture, and then reporting the mean test error at the conclusion of the NAS algorithm. The most realistic setting is: (2 ) choosing a random validation error when evaluating each architecture, and then reporting the corresponding test error, and an approximation of this is (3 ) using the mean validation error in the search, and reporting the mean test error at the end. However, (2 ) is currently not possible with the NASBench-101 API, so our options are <ref type="bibr" target="#b0">(1 )</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Path encoding length</head><p>In Section 5, we gave theoretical results which suggested that truncating the path encoding may not decrease performance of NAS algorithms such as BANANAS. We backed this up by plotting performance of BANANAS vs. truncation length of the path encoding ( <ref type="figure">Figure 5</ref>.1. Specifically, we ran BANANAS up to 150 evaluations for truncation lengths of 3 0 , 3 1 , . . . , 3 5 and plotted the results. Now, we conduct the same experiment for NASBench-201. We run BANANAS up to 150 evaluations on CIFAR-10 on NASBench-201 for truncation lengths of 1, 2, 5, 7, 10, 15, 20, 30, 60, and 155 (where 155 is the total number of paths for NASBench-201). See <ref type="figure">Figure D</ref>.4. We see that truncating from 155 down to just 30 has no decrease in performance. In fact, similar to NASBench-101, the performance after truncation actually improves up to a certain point. We believe this is because with the full-length encoding, the neural predictor overfits to very rare paths.</p><p>Next, we give a table of the probabilities of paths by length from NASBench-101 generated from random spec() (i.e., Definition C.1). These probabilities were computed experimentally by making 100000 calls to random spec(). See <ref type="table" target="#tab_7">Table 3</ref>. This table gives further experimental evidence to support Theorem 5.1, because it shows that the longest paths are exceedingly rare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Best practices checklist for NAS research</head><p>The area of NAS has seen problems with reproducibility, as well as fair empirical comparisons. Following calls for fair and reproducible NAS research <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b71">72]</ref>, a best practices checklist was recently created <ref type="bibr" target="#b32">[33]</ref>. In order to promote fair and reproducible NAS research, we address all points on the checklist, and we encourage future papers to do the same. Our code is available at https://github.com/naszilla/naszilla.</p><p>• Code for the training pipeline used to evaluate the final architectures. We used three of the most • Code for the search space. We used the popular and publicly avaliable NASBench and DARTS search spaces with no changes.</p><p>• Hyperparameters used for the final evaluation pipeline, as well as random seeds. We left all hyperparameters unchanged. We trained the architectures found by BANANAS, ASHA, and DARTS five times each, using random seeds 0, 1, 2, 3, 4.</p><p>• For all NAS methods you compare, did you use exactly the same NAS benchmark, including the same dataset, search space, and code for training the architectures and hyperparameters for that code? Yes, we did this by virtue of the NASBench-101 and 201 datasets. For the DARTS experiments, we used the reported architectures (found using the same search space and dataset as our method), and then we trained the final architectures using the same code, including hyperparameters. We compared different NAS methods using exactly the same NAS benchmark.</p><p>• Did you control for confounding factors? Yes, we used the same setup for all of our NASBench-101 and 201 experiments. For the DARTS search space, we compared our algorithm to two other algorithms using the same setup (pytorch version, CUDA version, etc). Across training over 5 seeds for each algorithm, we used different GPUs, which we found to have no greater effect than using a different random seed.</p><p>• Did you run ablation studies? Yes, in fact, ablation studies guided our entire decision process in constructing BANANAS. Section 5 is devoted entirely to ablation studies.</p><p>• Did you use the same evaluation protocol for the methods being compared? Yes, we used the same evaluation protocol for all methods and we tried multiple evaluation protocols.</p><p>• Did you compare performance over time? Yes, all of our plots are performance over time.</p><p>• Did you compare to random search? Yes.</p><p>• Did you perform multiple runs of your experiments and report seeds? We ran 200 trials of our NASBench-101 and 201 experiments. Since we ran so many trials, we did not report random seeds. We ran four total trials of BANANAS on the DARTS search space. Currently we do not have a fully deterministic version of BANANAS on the DARTS search space (which would be harder to implement as the algorithm runs on 10 GPUs). However, the average final error across trials was within 0.1%.</p><p>• Did you use tabular or surrogate benchmarks for in-depth evaluations Yes, we used NASBench-101 and 201.</p><p>• Did you report how you tuned hyperparameters, and what time and resources this required?</p><p>We performed light hyperparameter tuning at the start of this project, for the number of layers, layer size, learning rate, and number of epochs of the meta neural network. We did not perform any hyperparameter tuning when we ran the algorithm on NASBench-201 for all three datasets, or the DARTS search space. This suggests that the current hyperparameters work well for most new search spaces.</p><p>• Did you report the time for the entire end-to-end NAS method? We reported time for the entire end-to-end NAS method.</p><p>• Did you report all details of your experimental setup? We reported all details of our experimental setup.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>150 evaluations BANANAS with path encoding of different lengths</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 . 1 :</head><label>51</label><figDesc>Example of the path encoding (left). Performance of BANANAS with the path encoding truncated to different lengths (right). Since each node has 3 choices of operations, the "natural" cutoffs are at powers of 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 . 2 :</head><label>52</label><figDesc>Performance of neural predictors on NASBench-101: predictive ability (left), accuracy of uncertainty estimates (middle), performance in NAS when combined with BO (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.2 left) and RMSCE (Figure 5.2 middle) together when evaluating the neural predictors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 . 3 :</head><label>53</label><figDesc>Diagram of the BANANAS neural predictor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>true are the predicted and true values of the validation error for architecture i, and y LB is a global lower bound on the minimum true validation error. To parallelize Algorithm 1, in step iv. we simply choose the k architectures with the smallest values of the acquisition function and evaluate the architectures in parallel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 . 1 :</head><label>61</label><figDesc>Performance of different acquisition functions (left). Performance of different acquisition optimization strategies (middle). Performance of BANANAS compared to other NAS algorithms (right). See Appendix D for the same results in a table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure D. 1 :</head><label>1</label><figDesc>The best neural architecture found by BANANAS in the DARTS space. Normal cell (left) and reduction cell (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>25</head><label>25</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure D. 2 :</head><label>2</label><figDesc>A more exhaustive study of the different components in the BO + neural predictor framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure D. 3 :</head><label>3</label><figDesc>Results on NASBench-201. Top row is validation error, bottom row is test error. CIFAR-10 (left), CIFAR-100 (middle), and ImageNet-16-120 (right). validation error found so far (and then we averaged this over 200 trials). However, on NASBench-201, the validation and test errors are not as highly correlated as on NASBench-101, which makes it possible for the NAS algorithms to overfit to the validation errors. Specifically for ImageNet16-120, the lowest validation error out of all 15625 architectures is 53.233, and the corresponding test error is 53.8833. However, there are 23 architectures which have a higher validation loss but lower test loss (and the lowest overall test loss is 53.1556). Coupled with the small size of NASBench-201, this can cause NAS performance over time to not be strictly decreasing (see Figure D.3 bottom left). Therefore, we focus on the plots of the validation error over time (Figure D.3 top row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure D. 4 :</head><label>4</label><figDesc>NAS experiments with random validation error (left). Performance of BANANAS on NASBench-201 with CIFAR-10 with the path encoding truncated to different lengths (right). random validation errors and mean test errors. We used (3 ) forFigure 6.1, and now we use (1 ) inFigure D.4. We found the overall trends to be the same inFigure D.4 (in particular, BANANAS still distinctly outperforms all other algorithms after 40 iterations), but the Bayesian optimization-based methods performed better at the very start.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of NAS algorithms on the DARTS search space. The runtime unit is total GPU-days on a Tesla V100.</figDesc><table><row><cell>NAS Algorithm</cell><cell>Source</cell><cell>Avg. Test error</cell><cell>Runtime</cell><cell>Method</cell></row><row><cell>Random search</cell><cell>[35]</cell><cell>3.29</cell><cell>4</cell><cell>Random</cell></row><row><cell>Local search</cell><cell>[66]</cell><cell>3.49</cell><cell>11.8</cell><cell>Local search</cell></row><row><cell>DARTS</cell><cell>[35]</cell><cell>2.76</cell><cell>5</cell><cell>Gradient-based</cell></row><row><cell>ASHA</cell><cell>[30]</cell><cell>3.03</cell><cell>9</cell><cell>Successive halving</cell></row><row><cell>Random search WS</cell><cell>[30]</cell><cell>2.85</cell><cell>9.7</cell><cell>Random</cell></row><row><cell>DARTS</cell><cell>Ours</cell><cell>2.68</cell><cell>5</cell><cell>Gradient-based</cell></row><row><cell>ASHA</cell><cell>Ours</cell><cell>3.08</cell><cell>9</cell><cell>Successive halving</cell></row><row><cell>BANANAS</cell><cell>Ours</cell><cell>2.64</cell><cell>11.8</cell><cell>BO + neural predictor</cell></row><row><cell>see</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the architectures with the lowest test error (averaged over 200 trials) returned by NAS algorithms after 150 architecture evaluations on NASBench-101.</figDesc><table><row><cell>NAS Algorithm</cell><cell>Source</cell><cell>Method</cell><cell>Test Error</cell></row><row><cell>REINFORCE</cell><cell>[67]</cell><cell>Reinforcement learning</cell><cell>6.436</cell></row><row><cell>TPE</cell><cell>[4]</cell><cell>BO (Parzen windows)</cell><cell>6.415</cell></row><row><cell>BOHB</cell><cell>[11]</cell><cell>BO (successive halving)</cell><cell>6.356</cell></row><row><cell>Random search</cell><cell>[30]</cell><cell>Random search</cell><cell>6.341</cell></row><row><cell>GCN Pred.</cell><cell>[64]</cell><cell>GCN</cell><cell>6.331</cell></row><row><cell>BO w. GP</cell><cell>[50]</cell><cell>BO (Gaussian process)</cell><cell>6.267</cell></row><row><cell>NASBOT</cell><cell>[21]</cell><cell>BO (Gaussian process)</cell><cell>6.250</cell></row><row><cell>AlphaX</cell><cell>[62]</cell><cell>Monte Carlo tree search</cell><cell>6.233</cell></row><row><cell>Reg. Evolution</cell><cell>[44]</cell><cell>Evolution</cell><cell>6.109</cell></row><row><cell>DNGO</cell><cell>[52]</cell><cell>BO (neural networks)</cell><cell>6.085</cell></row><row><cell>BOHAMIANN</cell><cell>[53]</cell><cell>BO (Bayesian NN)</cell><cell>6.010</cell></row><row><cell>BONAS</cell><cell>[48]</cell><cell>BO (GCN)</cell><cell>5.954</cell></row><row><cell>Local search</cell><cell>[66]</cell><cell>Local search</cell><cell>5.932</cell></row><row><cell>BANANAS</cell><cell>Ours</cell><cell>BO (path encoding)</cell><cell>5.923</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc><ref type="bibr" target="#b2">3</ref>. As with NASBench-101, at each point in time, we plotted the test error of the architecture with the best</figDesc><table><row><cell>val error of best neural net</cell><cell>8.4 8.6 8.8 9.0 9.2 9.4</cell><cell></cell><cell></cell><cell></cell><cell>CIFAR-10 BO w. GP NASBOT BOHAMIANN BONAS GCN Pred. BO w. GP NASBOT BOHAMIANN BONAS GCN Pred.</cell><cell cols="2">BANANAS Local Search Random DNGO Reg. Evolution</cell><cell>val error of best neural net</cell><cell>26.5 27.0 27.5 28.0 28.5 29.0 29.5 30.0</cell><cell></cell><cell></cell><cell>CIFAR-100 BO w. GP NASBOT BOHAMIANN BONAS GCN Pred. BO w. GP NASBOT BOHAMIANN BONAS GCN Pred.</cell><cell cols="2">BANANAS Local Search Random DNGO Reg. Evolution</cell><cell>val error of best neural net</cell><cell>53.5 54.0 54.5 55.0 55.5</cell><cell></cell><cell cols="2">ImageNet-16-120 BO w. GP NASBOT BOHAMIANN BONAS GCN Pred. BO w. GP NASBOT BOHAMIANN BONAS GCN Pred.</cell><cell>BANANAS Local Search Random DNGO Reg. Evolution</cell></row><row><cell></cell><cell></cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30 time in GPU hours 35</cell><cell>40</cell><cell>45</cell><cell></cell><cell></cell><cell>15</cell><cell>20</cell><cell>25 time in GPU hours 30 35</cell><cell>40</cell><cell>45</cell><cell></cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30 time in GPU hours 35</cell><cell>40</cell><cell>45</cell></row><row><cell>test error of best neural net</cell><cell>5.8 6.0 6.2 6.4 6.6</cell><cell></cell><cell></cell><cell></cell><cell>BO w. GP NASBOT BOHAMIANN BONAS GCN Pred. BO w. GP NASBOT BOHAMIANN BONAS GCN Pred.</cell><cell cols="2">BANANAS Local Search Random DNGO Reg. Evolution</cell><cell>test error of best neural net</cell><cell>27.0 27.5 28.0 28.5 29.0 29.5 30.0</cell><cell></cell><cell></cell><cell>BO w. GP NASBOT BOHAMIANN BONAS GCN Pred. BO w. GP NASBOT BOHAMIANN BONAS GCN Pred.</cell><cell cols="2">BANANAS Local Search Random DNGO Reg. Evolution</cell><cell>test error of best neural net</cell><cell>54.0 54.5 55.0 55.5</cell><cell></cell><cell></cell><cell>BO w. GP NASBOT BOHAMIANN BONAS GCN Pred. BO w. GP NASBOT BOHAMIANN BONAS GCN Pred.</cell><cell>BANANAS Local Search Random DNGO Reg. Evolution</cell></row><row><cell></cell><cell>5.6</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30 time in GPU hours 35</cell><cell>40</cell><cell>45</cell><cell></cell><cell>26.5</cell><cell>15</cell><cell>20</cell><cell>25 time in GPU hours 30 35</cell><cell>40</cell><cell>45</cell><cell></cell><cell>53.5</cell><cell>15</cell><cell>20</cell><cell>25 time in GPU hours 30 35</cell><cell>40</cell><cell>45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>or (3 ), neither of which is perfect. (3 ) does not capture the uncertainty in real NAS experiments, while (1 ) does not give as accurate results because of the differences between</figDesc><table><row><cell>test error of best neural net</cell><cell>6.2 6.4 6.6 6.8 7.0 7.2</cell><cell>DNGO NASBOT DNGO NASBOT</cell><cell>BANANAS Random search Reg. evolution BO w. GP prior REINFORCE</cell><cell>test error after 150 evaluations</cell><cell>5.7 5.8 5.9 6.0 6.1 6.2</cell><cell cols="2">BANANAS with path encoding of different lengths</cell></row><row><cell></cell><cell>6.0</cell><cell></cell><cell></cell><cell></cell><cell>5.6</cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>20 time in TPU hours 30</cell><cell>40</cell><cell></cell><cell>3 1</cell><cell>3 2 length of path encoding 3 3</cell><cell>3 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Probabilities of path lengths in NASBench-101 using random spec(). popular search spaces in NAS research, the NASBench-101 and NASBench-201 search spaces, and the DARTS search space. For NASBench-101 and 201, the accuracy of all architectures were precomputed. For the DARTS search space, we released our fork of the DARTS repo, which is forked from the DARTS repo designed specifically for reproducible experiments<ref type="bibr" target="#b29">[30]</ref>, making trivial changes to account for pytorch 1.2.0.</figDesc><table><row><cell>Path Length</cell><cell>Probability</cell><cell>Total num. paths</cell><cell>Expected num. paths</cell></row><row><cell>1</cell><cell>0.200</cell><cell>1</cell><cell>0.200</cell></row><row><cell>2</cell><cell>0.127</cell><cell>3</cell><cell>0.380</cell></row><row><cell>3</cell><cell>3.36 ×10 −2</cell><cell>9</cell><cell>0.303</cell></row><row><cell>4</cell><cell>3.92 ×10 −3</cell><cell>27</cell><cell>0.106</cell></row><row><cell>5</cell><cell>1.50 ×10 −4</cell><cell>81</cell><cell>1.22 ×10 −2</cell></row><row><cell>6</cell><cell>6.37 ×10 −7</cell><cell>243</cell><cell>1.55 ×10 −4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Jeff Schneider, Naveen Sundar Govindarajulu, and Liam Li for their help with this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Accelerating neural architecture search using performance prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10823</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Balandat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Daulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Letham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bakshy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06403</idno>
		<title level="m">Botorch: Programmable bayesian optimization in pytorch</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The power of ensembles for active learning in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Beluch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">M</forename><surname>Nürnberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Köhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Algorithms for hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balázs</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ensemble of deep convolutional neural networks for prognosis of ischemic stroke</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchan</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><surname>Beom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myunghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joong-Ho</forename><surname>Cho Paik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Won</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Peephole: Predicting network performance before training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03351</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer vision and pattern recognition</title>
		<meeting>the IEEE Conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05377</idno>
		<title level="m">Neural architecture search: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bohb: Robust and efficient hyperparameter optimization at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neuroevolution: from architectures to learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Floreano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dürr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Mattiussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="62" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frazier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02811</idno>
		<title level="m">A tutorial on bayesian optimization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Google vizier: A service for black-box optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Solnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhodeep</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Karro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1487" to="1495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch bayesian optimization via local penalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwen</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">It&apos;s time to do something: Mitigating the negative impacts of computing through a change to the peer review process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Hecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauren</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bigham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Schöning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Hoque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">De</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lana</forename><surname>Russis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bushra</forename><surname>Yarosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danish</forename><surname>Anjum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cathy</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Future of Computing Blog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tapas: Train-less accuracy predictor for architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Istrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Nikolopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Costas</forename><surname>Bekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cristiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Malossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Auto-keras: Efficient neural architecture search with network morphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10282</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parallelised bayesian optimisation via thompson sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural architecture search with bayesian optimisation and optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2016" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Designing neural networks using genetic algorithms with graph generation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Kitano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="461" to="476" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning curve prediction with bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforce and masked attention autoregressive density estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Chepuri Shri Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swarnim</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00939</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Accurate uncertainties for deep learning using calibrated regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Fenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00263</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kushner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Basic Engineering</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="106" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6402" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Alexander Laube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07528</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Prune and replace nas. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07638</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Desalvo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06560</idno>
		<title level="m">Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingqiu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kechen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06035</idno>
		<title level="m">Darts+: Improved differentiable architecture search with early stopping</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Lindauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02453</idno>
		<title level="m">Best practices for scientific research on neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep neural architecture search with deep graph bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/WIC/ACM International Conference on Web Intelligence (WI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="500" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Evolutionary-neural hybrid agents for architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Khorlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09828</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On bayesian methods for seeking the extremum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Močkus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization Techniques IFIP Technical Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1975" />
			<biblScope unit="page" from="400" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Probo: a framework for using probabilistic programming in bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11515</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Optimal transport kernels for sequential and parallel neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam</forename><surname>Vu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael A</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Osborne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07593</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The parallel bayesian optimization algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Očenášek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The State of the Art in Computational Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gaussian processes in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Summer School on Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
		<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binxin</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Osborne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07556</idno>
		<title level="m">Neural architecture search using bayesian optimisation with weisfeiler-lehman kernel</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08142</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Amoebanet: An sdn-enabled network service for big data science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raza</forename><surname>Syed Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenji</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajith</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Sasidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin</forename><surname>Demar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Guok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Macauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Pouyoul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Network and Computer Applications</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="70" to="82" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Multi-objective neural architecture search via predictive network performance optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09336</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Nas-bench-301 and the case for surrogate benchmarks for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jovita</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09777</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Can you trust your model&apos;s uncertainty? evaluating predictive uncertainty under dataset shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Ovadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scalable bayesian optimization using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadathur</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mr</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2171" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Bayesian optimization with robust bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4134" to="4142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Gaussian process optimization in the bandit setting: No regret and experimental design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seeger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0912.3995</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Good lower and upper bounds on binomial coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pantelimon</forename><surname>Stanica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Inequalities in Pure and Applied Mathematics</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="127" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On the likelihood that one unknown probability exceeds another in view of the evidence of two samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William R Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="285" to="294" />
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Methods for comparing uncertainty quantifications for material property predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwoong</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">W</forename><surname>Ulissi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning: Science and Technology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">25006</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Sample-efficient neural architecture search by learning action space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06832</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Alphax: exploring neural architectures with deep neural networks and monte carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuu</forename><surname>Jinnai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Fonseca</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07440</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Npenas: Neural predictor guided evolution for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12857</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00848</idno>
		<title level="m">Neural predictor for neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A study on encodings for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Nolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Savani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Local search is state of the art for nas benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Nolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Savani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02960</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Maximizing acquisition functions for bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9884" to="9895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Pc-darts: Partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Does unsupervised architecture representation learning help neural architecture search?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06936</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Nas evaluation is frustratingly hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Esperança</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09635</idno>
		<title level="m">Nas-bench-101: Towards reproducible neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheheryar</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08573</idno>
		<title level="m">Neural ensemble search for performant and calibrated predictions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Graph hypernetworks for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">D-vae: A variational autoencoder for directed acyclic graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shali</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Neural predictor for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GitHub repository ultmaster/neuralpredictor.pytorch</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Bayesnas: A bayesian approach for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongpeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04919</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
