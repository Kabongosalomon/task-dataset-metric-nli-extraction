<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feedback Network for Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sichuan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglei</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sichuan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwanggil</forename><surname>Jeon</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Incheon National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sichuan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Feedback Network for Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in image super-resolution (SR) explored the power of deep learning to achieve a better reconstruction performance. However, the feedback mechanism, which commonly exists in human visual system, has not been fully exploited in existing deep learning based image SR methods. In this paper, we propose an image super-resolution feedback network (SRFBN) to refine lowlevel representations with high-level information. Specifically, we use hidden states in an RNN with constraints to achieve such feedback manner. A feedback block is designed to handle the feedback connections and to generate powerful high-level representations. The proposed SRFBN comes with a strong early reconstruction ability and can create the final high-resolution image step by step. In addition, we introduce a curriculum learning strategy to make the network well suitable for more complicated tasks, where the low-resolution images are corrupted by multiple types of degradation. Extensive experimental results demonstrate the superiority of the proposed SRFBN in comparison with the state-of-the-art methods. Code is avaliable at https: //github.com/Paper99/SRFBN_CVPR19.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image super-resolution (SR) is a low-level computer vision task, which aims to reconstruct a high-resolution (HR) image from its low-resolution (LR) counterpart. It is inherently ill-posed since multiple HR images may result in an identical LR image. To address this problem, numerous image SR methods have been proposed, including interpolation-based methods <ref type="bibr" target="#b44">[45]</ref>, reconstruction-based methods <ref type="bibr" target="#b41">[42]</ref>, and learning-based methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Since Dong et al. <ref type="bibr" target="#b5">[6]</ref> firstly introduced a shallow Convolutional Neural Network (CNN) to implement image SR, deep learning based methods have attracted extensive attention in recent years due to their superior reconstruction * Corresponds to: {arielyang,wuwei}@scu.edu.cn performance. The benefits of deep learning based methods mainly come from its two key factors, i.e., depth and skip connections (residual or dense) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b36">37]</ref>. The first one provides a powerful capability to represent and establish a more complex LR-HR mapping, while preserving more contextual information with larger receptive fields. The second factor can efficiently alleviate the gradient vanishing/exploding problems caused by simply stacking more layers to deepen networks. As the depth of networks grows, the number of parameters increases. A large-capacity network will occupy huge storage resources and suffer from the overfitting problem.</p><p>To reduce network parameters, the recurrent structure is often employed. Recent studies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b9">10]</ref> have shown that many networks with recurrent structure (e.g. DRCN <ref type="bibr" target="#b18">[19]</ref> and DRRN <ref type="bibr" target="#b30">[31]</ref>) can be extrapolated as a single-state Recurrent Neural Network (RNN). Similar to most conventional deep learning based methods, these networks with recurrent structure can share the information in a feedforward manner. However, the feedforward manner makes it impossible for previous layers to access useful information from the following layers, even though skip connections are employed.</p><p>In cognition theory, feedback connections which link the cortical visual areas can transmit response signals from higher-order areas to lower-order areas <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b8">9]</ref>. Motivated by this phenomenon, recent studies <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40]</ref> have applied the feedback mechanism to network architectures. The feedback mechanism in these architectures works in a topdown manner, carrying high-level information back to previous layers and refining low-level encoded information.</p><p>In this paper, we propose a novel network for image SR, namely the Super-Resolution Feedback Network (SRFBN), in order to refine low-level information using high-level one through feedback connections. The proposed SRFBN is essentially an RNN with a feedback block (FB), which is specifically designed for image SR tasks. The FB is constructed by multiple sets of up-and down-sampling layers with dense skip connections to generate powerful high-level representations. Inspired by <ref type="bibr" target="#b39">[40]</ref>, we use the output of the FB, i.e., a hidden state in an unfolded RNN, to achieve the feedback manner (see <ref type="figure" target="#fig_0">Fig. 1(a)</ref>). The hidden state at each iteration flows into the next iteration to modulate the input. To ensure the hidden state contains the information of the HR image, we connect the loss to each iteration during the training process. The principle of our feedback scheme is that the information of a coarse SR image can facilitate an LR image to reconstruct a better SR image (see <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). Furthermore, we design a curriculum for the case, in which the LR image is generated by a complex degradation model. For each LR image, its target HR images for consecutive iterations are arranged from easy to hard based on the recovery difficulty. Such curriculum learning strategy well assists our proposed SRFBN in handling complex degradation models. Experimental results demonstrate the superiority of our proposed SRFBN against other state-of-the-art methods.</p><p>In summary, our main contributions are as follows:</p><p>• Proposing an image super-resolution feedback network (SRFBN), which employs a feedback mechanism. High-level information is provided in top-down feedback flows through feedback connections. Meanwhile, such recurrent structure with feedback connections provides strong early reconstruction ability, and requires only few parameters.</p><p>• Proposing a feedback block (FB), which not only efficiently handles feedback information flows, but also enriches high-level representations via up-and downsampling layers, and dense skip connections. • Proposing a curriculum-based training strategy for the proposed SRFBN, in which HR images with increasing reconstruction difficulty are fed into the network as targets for consecutive iterations. This strategy enables the network to learn complex degradation models step by step, while the same strategy is impossible to settle for those methods with only one-step prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep learning based image super-resolution</head><p>Deep learning has shown its superior performance in various computer vision tasks including image SR. Dong et al. <ref type="bibr" target="#b6">[7]</ref> firstly introduced a three-layer CNN in image SR to learn a complex LR-HR mapping. Kim et al. <ref type="bibr" target="#b17">[18]</ref> increased the depth of CNN to 20 layers for more contextual information usage in LR images. In <ref type="bibr" target="#b17">[18]</ref>, a skip connection was employed to overcome the difficulty of optimization when the network became deeper. Recent studies have adopted different kind of skip connections to achieve remarkable improvement in image SR. SRResNet <ref type="bibr" target="#b20">[21]</ref> and EDSR <ref type="bibr" target="#b22">[23]</ref> applied residual skip connections from <ref type="bibr" target="#b12">[13]</ref>. SRDenseNet <ref type="bibr" target="#b35">[36]</ref> applied dense skip connections from <ref type="bibr" target="#b13">[14]</ref>. Zhang et al. <ref type="bibr" target="#b46">[47]</ref> combined local/global residual and dense skip connections in their RDN. Since the skip connections in these network architectures use or combine hierarchical features in a bottom-up way, the low-level features can only receive the information from previous layers, lacking enough contextual information due to the limitation of small receptive fields. These low-level features are reused in the following layers, and thus further restrict the reconstruction ability of the network. To fix this issue, we propose a superresolution feedback network (SRFBN), in which high-level information flows through feedback connections in a topdown manner to correct low-level features using more contextual information.</p><p>Meanwhile, with the help of skip connections, neural networks go deeper and hold more parameters. Such largecapacity networks occupy huge amount of storage resources and suffer from overfitting. To effectively reduce network parameters and gain better generalization power, the recurrent structure was employed <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. Particularly, the recurrent structure plays an important role to realize the feedback process in the proposed SRFBN (see <ref type="figure" target="#fig_0">Fig. 1(b)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feedback mechanism</head><p>The feedback mechanism allows the network to carry a notion of output to correct previous states. Recently, the feedback mechanism has been adopted by many network  architectures for various vision tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28]</ref>.</p><formula xml:id="formula_0">F 1 in F 1 out F t in F t out F t out F 1 t out F  T out F T in F 1 T out F  3x3 Conv</formula><p>For image SR, a few studies also showed efforts to introduce the feedback mechanism. Based on back-projection, Haris et al. <ref type="bibr" target="#b10">[11]</ref> designed up-and down-projection units to achieve iterative error feedback. Han et al. <ref type="bibr" target="#b9">[10]</ref> applied a delayed feedback mechanism which transmits the information between two recurrent states in a dual-state RNN. However, the flow of information from the LR image to the final SR image is still feedforward in their network architectures unlike ours.</p><p>The most relevant work to ours is <ref type="bibr" target="#b39">[40]</ref>, which transfers the hidden state with high-level information to the information of an input image to realize feedback in an convolutional recurrent neural network. However, it aims at solving high-level vision tasks, e.g. classification. To fit a feedback mechanism in image SR, we elaborately design a feedback block (FB) as the basic module in our SRFBN, instead of using ConvLSTM as in <ref type="bibr" target="#b39">[40]</ref>. The information in our FB efficiently flows across hierarchical layers through dense skip connections. Experimental results indicate our FB has superior reconstruction performance than ConvLSTM 1 and thus is more suitable for image SR tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Curriculum learning</head><p>Curriculum learning <ref type="bibr" target="#b1">[2]</ref>, which gradually increases the difficulty of the learned target, is well known as an efficient strategy to improve the training procedure. Early work of curriculum learning mainly focuses on a single task. Pentina et al. <ref type="bibr" target="#b26">[27]</ref> extended curriculum learning to multiple tasks in a sequential manner. Gao et al. <ref type="bibr" target="#b7">[8]</ref> utilized curriculum learning to solve the fixation problem in image restoration. Since their network is limited to a one-time prediction, they enforce a curriculum through feeding different training data in terms of the complexity of tasks as epoch increases during the training process. In the context of image SR, Wang et al. <ref type="bibr" target="#b37">[38]</ref> designed a curriculum for the pyramid <ref type="bibr" target="#b0">1</ref> Further analysis can be found in our supplementary material. structure, which gradually blends a new level of the pyramid in previously trained networks to upscale an LR image to a bigger size.</p><p>While previous works focus on a single degradation process, we enforce a curriculum to the case, where the LR image is corrupted by multiple types of degradation. The curriculum containing easy-to-hard decisions can be settled for one query to gradually restore the corrupted LR image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Feedback Network for Image SR</head><p>Two requirements are contained in a feedback system: (1) iterativeness and (2) rerouting the output of the system to correct the input in each loop. Such iterative cause-andeffect process helps to achieve the principle of our feedback scheme for image SR: high-level information can guide an LR image to recover a better SR image (see <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). In the proposed network, there are three indispensable parts to enforce our feedback scheme: (1) tying the loss at each iteration (to force the network to reconstruct an SR image at each iteration and thus allow the hidden state to carry a notion of high-level information), (2) using recurrent structure (to achieve iterative process) and (3) providing an LR input at each iteration (to ensure the availability of low-level information, which is needed to be refined). Any absence of these three parts will fail the network to drive the feedback flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network structure</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, our proposed SRFBN can be unfolded to T iterations, in which each iteration t is temporally ordered from 1 to T . In order to make the hidden state in SRFBN carry a notion of output, we tie the loss for every iteration. The description of the loss function can be found in Sec. 3.3. The sub-network placed in each iteration t contains three parts: an LR feature extraction block (LRFB), a feedback block (FB) and a reconstruction block (RB). The weights of each block are shared across time. The global residual skip connection at each iteration t delivers an upsampled image to bypass the sub-network. Therefore, the purpose of the sub-network at each iteration t is to recover a residual image I t Res while input a low-resolution image I LR . We denote Conv(s, n) and Deconv(s, n) as a convolutional layer and a deconvolutional layer respectively, where s is the size of the filter and n is the number of filters.</p><p>The LR feature extraction block consists of Conv(3, 4m) and Conv <ref type="bibr">(1, m)</ref>. m denotes the base number of filters. We provide an LR input I LR for the LR feature extraction block, from which we obtain the shallow features F t in containing the information of an LR image:</p><formula xml:id="formula_1">F t in = f LRF B (I LR ),<label>(1)</label></formula><p>where f LRF B denotes the operations of the LR feature extraction block. F t in are then used as the input to the FB. In addition, F 1 in are regarded as the initial hidden state F 0 out . The FB at the t-th iteration receives the hidden state from previous iteration F t−1 out through a feedback connection and shallow features F t in . F t out represents the output of the FB. The mathematical formulation of the FB is:</p><formula xml:id="formula_2">F t out = f F B (F t−1 out , F t in ),<label>(2)</label></formula><p>where f F B denotes the operations of the FB and actually represents the feedback process as shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. More details of the FB can be found in Sec. 3.2. The reconstruction block uses Deconv(k, m) to upscale LR features F t out to HR ones and Conv(3, c out ) to generate a residual image I t Res . The mathematical formulation of the reconstruction block is:</p><formula xml:id="formula_3">I t Res = f RB (F t out ),<label>(3)</label></formula><p>where f RB denotes the operations of the reconstruction block.</p><p>The output image I t SR at the t-th iteration can be obtained by:</p><formula xml:id="formula_4">I t SR = I t Res + f U P (I LR ),<label>(4)</label></formula><p>where f U P denotes the operation of an upsample kernel. The choice of the upsample kernel is arbitrary. We use a bilinear upsample kernel here. After T iterations, we will get totally T SR images (I 1 SR , I 2 SR , ..., I T SR ).   <ref type="figure">Figure 3</ref>. Feedback block (FB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feedback block</head><formula xml:id="formula_5">F  t out F t in F 0 t L 1 t L t g L T G L t G H 1 t H t g H</formula><p>As shown in <ref type="figure">Fig. 3</ref>, the FB at the t-th iteration receives the feedback information F t−1 out to correct low-level representations F t in , and then passes more powerful high-level representations F t out to the next iteration and the reconstruction block. The FB contains G projection groups sequentially with dense skip connections among them. Each projection group, which can project HR features to LR ones, mainly includes an upsample operation and a downsample operation.</p><p>At the beginning of the FB, F t in and F t−1 out are concatenated and compressed by Conv(1, m) to refine input features F t in by feedback information F t−1 out , producing the refined input features L t 0 :</p><formula xml:id="formula_6">L t 0 = C 0 ([F t−1 out , F t in ]),<label>(5)</label></formula><p>where C 0 refers to the initial compression operation and</p><formula xml:id="formula_7">[F t−1 out , F t in ]</formula><p>refers to the concatenation of F t−1 out and F t in . Let H t g and L t g be the HR and LR feature maps given by the g-th projection group in the FB at the t-th iteration. H t g can be obtained by:</p><formula xml:id="formula_8">H t g = C ↑ g ([L t 0 , L t 1 , ..., L t g−1 ]),<label>(6)</label></formula><p>where C ↑ g refers to the upsample operation using Deconv(k, m) at the g-th projection group. Correspondingly, L t g can be obtained by</p><formula xml:id="formula_9">L t g = C ↓ g ([H t 1 , H t 2 , ..., H t g ]),<label>(7)</label></formula><p>where C ↓ g refers to the downsample operation using Conv(k, m) at the g-th projection group. Except for the first projection group, we add Conv(1, m) before C ↑ g and C ↓ g for parameter and computation efficiency.</p><p>In order to exploit useful information from each projection group and map the size of input LR features F t+1 in at the next iteration, we conduct the feature fusion (green arrows in <ref type="figure">Fig. 3</ref>) for LR features generated by projection groups to generate the output of FB:</p><formula xml:id="formula_10">F t out = C F F ([L t 1 , L t 2 , ..., L t G ]),<label>(8)</label></formula><p>where C F F represents the function of Conv(1, m).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Curriculum learning strategy</head><p>We choose L1 loss to optimize our proposed network.</p><p>T target HR images (I 1 HR , I 2 HR , ..., I T HR ) are placed to fit in the multiple output in our proposed network. (I 1 HR , I 2 HR , ..., I T HR ) are identical for the single degradation model. For complex degradation models, (I 1 HR , I 2 HR , ..., I T HR ) are ordered based on the difficulty of tasks for T iterations to enforce a curriculum. The loss function in the network can be formulated as:</p><formula xml:id="formula_11">L(Θ) = 1 T T t=1 W t I t HR − I t SR 1 ,<label>(9)</label></formula><p>where Θ denotes to the parameters of our network. W t is a constant factor which demonstrates the worth of the output at the t-th iterations. As <ref type="bibr" target="#b39">[40]</ref> do, we set the value to 1 for each iteration, which represents each output has equal contribution. Details about settings of target HR images for complex degradation models will be revealed in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation details</head><p>We use PReLU <ref type="bibr" target="#b11">[12]</ref> as the activation function following all convolutional and deconvolutional layers except the last layer in each sub-network. Same as <ref type="bibr" target="#b10">[11]</ref>, we set various k in Conv(k, m) and Deconv(k, m) for different scale factors to perform up-and down-sampling operations. For ×2 scale factor, we set k in Conv(k, m) and Deconv(k, m) as 6 with two striding and two padding. Then, for ×3 scale factor, we set k = 7 with three striding and two padding. Finally, for ×4 scale factor, we set k = 8 with four striding and two padding. We take the SR image I T SR at the last iteration as our final SR result unless we specifically analysis every output image at each iteration. Our network can process both gray and color images, so c out can be 1 or 3 naturally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>Datasets and metrics. We use DIV2K <ref type="bibr" target="#b0">[1]</ref> and Flickr2K as our training data. To make full use of data, we adopt data augmentation as <ref type="bibr" target="#b22">[23]</ref> do. We evaluate SR results under PSNR and SSIM <ref type="bibr" target="#b38">[39]</ref> metrics on five standard benchmark datasets: Set5 <ref type="bibr" target="#b2">[3]</ref>, Set14 <ref type="bibr" target="#b40">[41]</ref>, B100 <ref type="bibr" target="#b23">[24]</ref>, Urban100 <ref type="bibr" target="#b14">[15]</ref>, and Manga109 <ref type="bibr" target="#b24">[25]</ref>. To keep consistency with previous works, quantitative results are only evaluated on luminance (Y) channel.</p><p>Degradation models. In order to make fair comparison with existing models, we regard bicubic downsampling as our standard degradation model (denoted as BI) for generating LR images from ground truth HR images. To verify the effectiveness of our curriculum learning strategy, we further conduct two experiments involving two other multidegradation models as <ref type="bibr" target="#b46">[47]</ref> do in Sec. 4.4 and 4.5.3. We define BD as a degradation model which applies Gaussian blur followed by downsampling to HR images. In our experiments, we use 7x7 sized Gaussian kernel with standard deviation 1.6 for blurring. Apart from the BD degradation model, DN degradation model is defined as bicubic downsampling followed by adding Gaussian noise, with noise level of 30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale factor</head><p>×2 ×3 ×4 Input patch size 60 × 60 50 × 50 40 × 40 Training settings. We train all networks with the batch-size of 16. To fully exploit contextual information from LR images, we feed RGB image patches with different patch size based on the upscaling factor. The settings of input patch size are listed in Tab. 1. The network parameters are initialized using the method in <ref type="bibr" target="#b11">[12]</ref>. Adam <ref type="bibr" target="#b19">[20]</ref> is employed to optimize the parameters of the network with initial learning rate 0.0001. The learning rate multiplies by 0.5 for every 200 epochs. We implement our networks with Pytorch framework and train them on NVIDIA 1080Ti GPUs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Study of T and G</head><p>In this subsection, we explore the influence of the number of iterations (denoted as T) and the number of projection groups in the feedback block (denoted as G). The base number of filters m is set to 32 in subsequent experiments. We first investigate the influence of T by fixing G to 6. It can be observed from <ref type="figure" target="#fig_3">Fig. 4</ref>(a) that with the help of feedback connection(s), the reconstruction performance is significantly improved compared with the network without feedback connections (T=1). Besides, as T continues to increase, the reconstruction quality keeps rising. In other words, our feedback block surely benefits the information flow across time. We then study the influence of G by fixing T to 4. <ref type="figure" target="#fig_3">Fig. 4</ref>(b) shows that larger G leads to higher accuracy due to stronger representative ability of deeper networks. In conclusion, choosing larger T or G contributes to better results. It is worth noticing that small T and G still outperform VDSR <ref type="bibr" target="#b17">[18]</ref>. In the following discussions, we use SRFBN-L (T=4, G=6) for analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Feedback vs. feedforward</head><p>To investigate the nature of the feedback mechanism in our network, we compare the feedback network with feedforward one in this subsection.</p><p>We first demonstrate the superiority of the feedback mechanism over its feedforward counterpart. By simply disconnecting the loss to all iterations except the last one, the network is thus impossible to reroute a notion of output to low-level representations and is then degenerated to a feedforward one (however still retains its recurrent property), denoted as SRFBN-L-FF. SRFBN-L and SRFBN-L-FF both have four iterations, producing four intermediate output. We then compare the PSNR values of all intermediate SR images from both networks. The results are shown in Tab. 2. SRFBN-L outperforms SRFBN-L-FF at every iteration, from which we conclude that the feedback network is capable of producing high quality early predictions in contrast to feedforward network. The experiment also indicates that our proposed SRFBN does benefit from the feedback mechanism, instead of only rely on the power of the recurrent structure. Except for the above discussions about the necessity of early losses, we also conduct two more abalative experiments to verify other parts (discussed in Sec. 3) which form our feedback system. By turning off weights sharing across iterations, the PSNR value in the proposed network is decreased from 32.11dB to 31.82dB on Set5 with scale factor ×4. By disconnecting the LR input at each iteration except the first iteration, the PSNR value is decreased by 0.17dB.</p><p>We first demonstrate the superiority of the feedback mechanism over its feedforward counterpart. By simply disconnecting the loss to all iterations except the last one, the network is thus impossible to reroute a notion of output to low-level representations and is then degenerated to a feedforward one (however still retains its recurrent property), denoted as SRFBN-L-FF. SRFBN-L and SRFBN-L-FF both have four iterations, producing four intermediate output. We then compare the PSNR values of all intermediate SR images from both networks. The results are shown in Tab. 2. SRFBN-L outperforms SRFBN-L-FF at every iteration, from which we conclude that the feedback network is capable of producing high quality early predictions in contrast to feedforward network. The experiment also indicates that our proposed SRFBN does benefit from the feedback mechanism, instead of only rely on the power of the recurrent structure. Except the above discussions about the necessity of early losses, we also conduct two more abalative experiments to verify other parts (discussed in Sec. 3) determining the feedback process. By turning off weights sharing across iterations, the PSNR value in the proposed network is decreased from 32.11dB to 31.82dB on Set5 with scale factor ×4. By disconnecting the LR input at each iteration except the first iteration, the PSNR value is decreased by 0.17dB.  To dig deeper into the difference between feedback and feedforward networks, we visualize the average feature map of every iteration in SRFBN-L and SRFBN-L-FF, illustrated in <ref type="figure" target="#fig_5">Fig. 5</ref>. Each average feature map is the mean of F t out in channel dimension, which roughly represents the output of the feedback block at the t-th iteration. Our network with global residual skip connections aims at recovering the residual image. In other words, the tasks of our network are to suppress the smooth area of the original input image <ref type="bibr" target="#b15">[16]</ref> and to predict high-frequency components (i.e. edges and contours). From <ref type="figure" target="#fig_5">Fig. 5</ref>, we have two observations. First, compared with the feedforward network at early iterations, feature maps acquired from the feedback network contain more negative values, showing a stronger effect of suppressing the smooth area of the input image, which further leads to a more accurate residual image. To some extent, this illustration reflects the reason why the feedback network has more powerful early reconstruction ability than the feedforward one. The second observation is that the feedback network learns different representations in contrast to feedforward one when handling the same task. In the feedforward network, feature maps vary significantly from the first iteration (t=1) to the last iteration (t=4): the edges and contours are outlined at early iterations and then the smooth areas of the original image are suppressed at latter iterations. The distinct patterns demonstrate that the feedforward network forms a hierarchy of information through layers, while the feedback network is allowed to devote most of its efforts to take a self-correcting process, since it can obtain well-developed feature representations at the initial iteration. This further indicates that F t out containing high-level information at the t-th iteration in the feedback network will urge previous layers at subsequent iterations to generate better representations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Study of curriculum learning</head><p>As mentioned in Sec. 4.1, we now present our results for two experiments on two different degradation models, i.e. BD and DN, to show the effectiveness of our curriculum learning strategy.</p><p>We formulate the curriculum based on the recovery difficulty. For example, to guide the network to learn recovering a BD operator corrupted image step by step, we provide a Gaussian blurred HR image as (intermediate) ground truth To dig deeper into the difference between feedback and feedforward networks, we visualize the average feature map of every iteration in SRFBN-L and SRFBN-L-FF, illustrated in <ref type="figure" target="#fig_5">Fig. 5</ref>. Each average feature map is the mean of F t out in channel dimension, which roughly represents the output of the feedback block at the t-th iteration. Our network with global residual skip connections aims at recovering the residual image. In other words, the tasks of our network are to suppress the smooth area of the original input image <ref type="bibr" target="#b15">[16]</ref> and to predict high-frequency components (i.e. edges and contours). From <ref type="figure" target="#fig_5">Fig. 5</ref>, we have two observations. First, compared with the feedforward network at early iterations, feature maps acquired from the feedback network contain more negative values, showing a stronger effect of suppressing the smooth area of the input image, which further leads to a more accurate residual image. To some extent, this illustration reflects the reason why the feedback network has more powerful early reconstruction ability than the feedforward one. The second observation is that the feedback network learns different representations in contrast to feedforward one when handling the same task. In the feedforward network, feature maps vary significantly from the first iteration (t=1) to the last iteration (t=4): the edges and contours are outlined at early iterations and then the smooth areas of the original image are suppressed at latter iterations. The distinct patterns demonstrate that the feedforward network forms a hierarchy of information through layers, while the feedback network is allowed to devote most of its efforts to take a self-correcting process, since it can obtain well-developed feature representations at the initial iteration. This further indicates that F t out containing high-level information at the t-th iteration in the feedback network will urge previous layers at subsequent iterations to generate better representations.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Study of curriculum learning</head><p>As mentioned in Sec. 4.1, we now present our results for two experiments on two different degradation models, i.e. BD and DN, to show the effectiveness of our curriculum learning strategy.</p><p>We formulate the curriculum based on the recovery difficulty. For example, to guide the network to learn recovering a BD operator corrupted image step by step, we provide a Gaussian blurred HR image as (intermediate) ground truth so that the network only needs to learn the inversion of a single downsampling operator at early iterations. Original HR image is provided at latter iterations as a senior challenge. Specifically, we empirically provide blurred HR images at first two iterations and original HR images at remaining two iterations for experiments with the BD degradation model. For experiments with the DN degradation model, we instead use noisy HR images at first two iterations and HR images without noise at last two iterations.</p><p>We also examine the compatibility of this strategy with two common training processes, i.e. training from scratch and fine-tuning on a network pretrained on the BI degradation model. The results shown in Tab. 3 infer that the curriculum learning strategy well assists our proposed SRFBN in handling BD and DN degradation models under both circumstances. We also observe that fine-tuning on a network pretrained on the BI degradation model leads to higher PSNR values than training from scratch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with the state-of-the-arts</head><p>The SRFBN with a larger base number of filters (m=64), which is derived from the SRFBN-L, is implemented for comparison. A self-ensemble method <ref type="bibr" target="#b34">[35]</ref> is also used to further improve the performance of the SRFBN (denoted as SRFBN+). A lightweight network SRFBN-S (T=4, G=3, m=32) is provided to compare with the state-of-the-art methods, which are carried only few parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Network parameters</head><p>The state-of-the-art methods considered in this experiment include SRCNN <ref type="bibr" target="#b6">[7]</ref>, VDSR <ref type="bibr" target="#b17">[18]</ref>, DRRN <ref type="bibr" target="#b30">[31]</ref>, MemNet <ref type="bibr" target="#b35">[36]</ref>, EDSR <ref type="bibr" target="#b22">[23]</ref>, DBPN-S <ref type="bibr" target="#b10">[11]</ref> and D-DBPN <ref type="bibr" target="#b10">[11]</ref>. The comparison results are given in <ref type="figure" target="#fig_6">Fig. 6</ref> in terms of the network param-eters and the reconstruction effects (PSNR). The SRFBN-S can achieve the best SR results among the networks with parameters fewer than 1000K. This demonstrates our method can well balance the number of parameters and the reconstruction performance. Meanwhile, in comparison with the networks with a large number of parameters, such as D-DBPN and EDSR, our proposed SRFBN and SRFBN+ can achieve competitive results, while only needs the 35% and 7% parameters of D-DBPN and EDSR, respectively. Thus, our network is lightweight and more efficient in comparison with other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Results with BI degradation model</head><p>For BI degradation model, we compare the SRFBN and SRFBN+ with seven state-of-the-art image SR methods: SRCNN <ref type="bibr" target="#b6">[7]</ref>, VDSR <ref type="bibr" target="#b17">[18]</ref>, DRRN <ref type="bibr" target="#b30">[31]</ref>, SRDenseNet <ref type="bibr" target="#b35">[36]</ref>, MemNet <ref type="bibr" target="#b35">[36]</ref>, EDSR <ref type="bibr" target="#b22">[23]</ref>, D-DBPN <ref type="bibr" target="#b10">[11]</ref>. The quantitative results in Tab. 4 are re-evaluated from the corresponding public codes. Obviously, our proposed SRFBN can outperform almost all comparative methods. Compared with our method, EDSR utilizes much more number of filters (256 vs. 64), and D-DBPN employs more training images (DIV2K+Flickr2K+ImageNet vs. DIV2K+Flickr2K). However, our SRFBN can earn competitive results in contrast to them. In addition, it also can be seen that our SRFBN+ outperforms almost all comparative methods.</p><p>We show SR results with scale factor ×4 in <ref type="figure" target="#fig_9">Fig. 7</ref>. In general, the proposed SRFBN can yield more convincing results. For the SR results of the 'BokuHaSitatakaKun' image from Manga109, DRRN and MemNet even split the 'M' letter. VDSR, EDSR and D-DBPN fail to recover the clear image. The proposed SRFBN produces a clear image which is very close to the ground truth. Besides, for the 'img 092' from Urban100, the texture direction of the SR images from all comparative methods is wrong. However, our proposed SRFBN makes full use of the high-level information to take a self-correcting process, thus a more faithful SR image can be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Results with BD and DN degradation models</head><p>As aforementioned, the proposed SRFBN is trained using curriculum learning strategy for BD and DN degradation models, and fine-tuned based on BI degradation model using DIV2K. The proposed SRFBN and SRFBN+ are compared with SRCNN <ref type="bibr" target="#b6">[7]</ref>, VDSR <ref type="bibr" target="#b17">[18]</ref>, IRCNN G <ref type="bibr" target="#b42">[43]</ref>, IR-CNN C <ref type="bibr" target="#b42">[43]</ref>, SRMD(NF) <ref type="bibr" target="#b43">[44]</ref>, and RDN <ref type="bibr" target="#b46">[47]</ref>. Because of degradation mismatch, SRCNN and VDSR are re-trained for BD and DN degradation models. As shown in Tab. 5, The proposed SRFBN and SRFBN+ achieve the best on almost all quantative results over other state-of-the-art methods.  In <ref type="figure" target="#fig_10">Fig. 8</ref>, we also show two sets of visual results with BD and DN degradation models from the standard benchmark datasets. Compared with other methods, the proposed SRFBN could alleviate the distortions and generate more accurate details in SR images. From above comparisions, we further indicate the robustness and effectiveness of SRFBN in handling BD and DN degradation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel network for image SR called super-resolution feedback network (SRFBN) to faithfully reconstruct a SR image by enhancing low-level representations with high-level ones. The feedback block (FB) in the network can effectively handle the feedback information flow as well as the feature reuse. In addition, a curriculum learning strategy is proposed to enable the network to well suitable for more complicated tasks, where the low-resolution images are corrupted by complex degradation models. The comprehensive experimental results have demonstrated that the proposed SRFBN could deliver the comparative or better performance in comparison with the state-of-the-art methods by using very fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>The following items are contained in the supplementary material:</p><p>1. Discussions on the feedback block. 2. More insights on the feedback mechanism. 3. Quantitative results using DIV2K training images. 4. Running time comparison. 5. More qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Study of Feedback Block</head><p>More effective basic block could generate finer highlevel representations and then benefit our feedback process. Thus, we explore the design of the basic block in this section. We still use SRFBN-L (T=4, G=6), which has a small base number of fiters (m=32) for analysis.</p><p>Ablation study mainly focuses on two components of our feedback block (FB): (1) up-and down-sampling layers (UDSL), (2) dense skip connecitons (DSC). To analysis the effect of UDSL in our proposed FB, we replace the up-and down-sampling layers with 3 × 3 sized convolutional layers (with one padding and one stridding). In Tab. 6, when UDSL is replaced with 3 × 3 sized convolutional layers in the FB, the PSNR value dramatically decreases. This indicates that up-and down-sampling operations carrying large kernel size can expliot abundant contextual information and are effective for image super-resolution (SR). After adding DSC to the FB, the reconstruction performance can be further improved, because the information efficiently flows through DSC across hierarchy layers and even across time.  Other basic blocks are considered in this experiment in comparison with our FB. We choose two superior basic blocks (i.e. projection units <ref type="bibr" target="#b10">[11]</ref> and RDB <ref type="bibr" target="#b46">[47]</ref>), which were designed for image SR task recently, and ConvLSTM from <ref type="bibr" target="#b39">[40]</ref> for comparison. To keep consistency, the number of convolutional layers 2 and filters in each basic block are set to 12 and 32, respectively. In Tab. 7, we first see that all SR custom basic blocks outperform ConvLSTM by a large margin. The gate mechanisms in ConvLSTM influence the distribution and intensity of original images and thus are difficult to meet high fidelity needs in image SR tasks. Besides, high-level information is directly added to low-level <ref type="bibr" target="#b1">2</ref> 1 × 1 sized convolutional layers are omitted. <ref type="figure">Figure 9</ref>. Average feature maps of refined low-level features from different iterations in the propose SRFBN (zoom for a better view). All average feature maps use the same colormap for better visualization. <ref type="figure" target="#fig_0">Figure 10</ref>. The spectral densities of the average feature map at each iteration t (zoom for a better view). From left to right of the horizontal axis, frequency is normalized and ranged from low to high for better visualization.</p><formula xml:id="formula_12">t = 1 t = 2 t = 3 t = 4</formula><p>information in ConvLSTM, causing the loss of enough contextual information for the next iteration. Noticeably, our proposed FB obtains the best quantitative results in comparison with other basic blocks. This further demonstrates the powerful representation ability of our proposed FB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Insights on Feedback Mechanism</head><p>For better understanding the feedback mechanism in the proposed network, we visualize the average feature map of L t 0 at each iteration t in <ref type="figure">Fig. 9</ref>. L t 0 actually represents the low-level representations refined by high-level features F t−1 out from last iteration (see the main paper's Eq. 5). The initial state F 0 out is set to F 1 in , hence the first iteration in the proposed network can not receive the feedback information. From <ref type="figure">Fig. 9</ref>, we observe that, except the first iteration (t=1), these average feature maps show bright activations in the contours and outline edges of the original image. It seems that the feedback connection adds the high-level representations to the initial feature maps. This further indicates that initial low-level features, which lack enough contextual information, surely are corrected using high-level information through the feedback mechanism in the proposed network.</p><p>To further conduct differences between feedforward and feedback networks, we plot 1-D spectral densities of the average feature map at each iteration t in SRFBN-L (feedback) and SRFBN-L-FF (feedforward). As shown in <ref type="figure" target="#fig_5">Fig.5</ref> of the main paper, each average feature map is the mean of F t out . To acquire 1-D spectral densities of the aver-  age feature map at each iteration t, we get the 2-D spectrum map through discrete Fourier transform, center the low-frequency component of the spectrum map, and place concentric annular regions to compute the mean of spectral densities for continuous frequency ranges. From <ref type="figure" target="#fig_0">Fig. 10</ref>, we can conclude that the feedback network can estimate more mid-frequency and high-frequency information than the feedforward network at early iterations. With the iteration t grows, the feedforward network gradually recovers mid-frequency and high-frequency components, while the feedback network pays attention to refine the welldeveloped information. For the feedback network, we also observe that, because of the help of the feedback mechanism (t &gt;1), mid-frequency and high-frequency information of the average feature map at the second iteration (t=2) is more similiar to the final representations (t=4) in contrast to the first iteration (t=1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sanity Check</head><p>To purely investigate the effect of the network architecture design, we compare the quantitative results obtained from different networks using the same training dataset (DIV2K training images <ref type="bibr" target="#b0">[1]</ref>). The choices of networks for comparison include D-DBPN (which is a state-of-theart network with moderate parameters) and MemNet <ref type="bibr" target="#b31">[32]</ref> (which is the leading network with recurrent structure). Because MemNet only reveals the results trained using 291 images, we re-train it using DIV2K on Pytorch framework. The results of D-DBPN are cited from their supplementary materials. Our SRFBN-S (T=4, G=3, m=32) and final SRFBN (T=4, G=6, m=64) are provided for this comparison. In Tab. 8, our SRFBN-S shows better quantitative results than MemNet with 71% fewer parameters. Moreover, the final SRFBN also gains competitive results in contrast to D-DBPN especially on Urban100 and Manga109 datasets, which mainly contain images with a large size. This comparison shows the effectiveness of the proposed SRFBN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Running </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Running Time Comparison</head><p>We compare running time of our proposed SRFBN-S and SRFBN with five state-of-the-art networks: MemNet <ref type="bibr" target="#b31">[32]</ref>, EDSR <ref type="bibr" target="#b22">[23]</ref>, D-DBPN <ref type="bibr" target="#b10">[11]</ref>, RDN <ref type="bibr" target="#b46">[47]</ref> and RCAN <ref type="bibr" target="#b45">[46]</ref> on Ur-ban100 with scale factor ×4. Because the large memory consumption in Caffe, we re-implement MemNet on Pytorch for fair comparison. The running time of all networks is evaluated on the same machine with 4.2GHz Intel i7 CPU (16G RAM) and an NVIDIA 1080Ti GPU using their official codes. Tab. 9 shows that our SRFBN-S and SRFBN have the fastest evaluation time in comparison with other networks. This further reflects the effectiveness of our proposed networks. The quantitative results of our proposed networks are less comparable with RCAN, but RCAN mainly focuses on much deeper networks design (about 400 convolutional layers) to purchase more accurate SR results. In contrast, our SRFBN only has about 100 convolutional layers with 77% fewer parameters (3,631K vs. 15,592K) than RCAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Qualitative Results</head><p>In <ref type="figure" target="#fig_0">Fig. 11-22</ref>, we provide more visual results of different degradation models to prove the superiority of the proposed network.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The illustrations of the feedback mechanism in the proposed network. Blue arrows represent the feedback connections. (a) Feedback via the hidden state at one iteration. The feedback block (FB) receives the information of the input Fin and hidden state from last iteration F t−1 out , and then passes its hidden state F t out to the next iteration and output. (b) The principle of our feedback scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of our proposed super-resolution feedback network (SRFBN). Blue arrows represent feedback connections. Green arrows represent global residual skip connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Conv</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Convergence analysis of T and G on Set5 with scaling factor ×4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Average feature maps of feedforward and feedback networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Performance and number of parameters. Results are evaluated on Set5 with scale factor ×4. Red points represent our proposed networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Average feature maps of feedforward and feedback networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Performance and number of parameters. Results are evaluated on Set5 with scale factor ×4. Red points represent our proposed networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Visual results of BI degradation model with scale factor ×4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Visual results of BD and DN degradation models with scale factor ×3. The first set of images shows the results obtained from BD degradation model. The second set of images shows the results from DN degradation model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .Figure 12 .Figure 13 .</head><label>111213</label><figDesc>Visual results of BI degradation model with scale factor ×4. Visual results of BI degradation model with scale factor ×4. Visual results of BI degradation model with scale factor ×4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 .Figure 15 .Figure 16 .Figure 17 .</head><label>14151617</label><figDesc>Visual results of BI degradation model with scale factor ×4. Visual results of BD degradation model with scale factor ×4. Visual results of BD degradation model with scale factor ×4. Visual results of BD degradation model with scale factor ×4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 18 .Figure 19 .</head><label>1819</label><figDesc>Visual results of BD degradation model with scale factor ×4. Visual results of DN degradation model with scale factor ×4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 20 .</head><label>20</label><figDesc>Visual results of DN degradation model with scale factor ×4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 21 .</head><label>21</label><figDesc>Visual results of DN degradation model with scale factor ×4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 22 .</head><label>22</label><figDesc>Visual results of DN degradation model with scale factor ×4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>The settings of input patch size.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>FF 30.69 31.74 32.00 32.09 SRFBN-L 31.85 32.06 32.11 32.11 The impact of feedback on Set5 with scale factor ×4.</figDesc><table><row><cell>No. Prediction</cell><cell>1st</cell><cell>2nd</cell><cell>3rd</cell><cell>4th</cell></row><row><cell>SRFBN-L-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>Model</cell><cell cols="4">from scratch w/o CL with CL w/o CL with CL from pretrained</cell></row><row><cell></cell><cell>BD DN</cell><cell>29.78 26.92</cell><cell>29.96 26.93</cell><cell>29.98 26.96</cell><cell>30.03 26.98</cell></row><row><cell></cell><cell cols="2">65)%1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>''%31</cell><cell></cell><cell></cell><cell>('65</cell></row><row><cell></cell><cell cols="2">65)%1</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">65)%16</cell><cell></cell><cell></cell></row><row><cell>3615G%</cell><cell cols="2">'551 9'65 0HP1HW '%316</cell><cell></cell><cell></cell></row><row><cell></cell><cell>65&amp;11</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">1XPEHURI3DUDPHWHUV.</cell></row></table><note>. The investigation of curriculum learning (CL) on BD and DN degradation models with scale factor ×4. The average PSNR values are evaluated on Set5.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>The investigation of curriculum learning (CL) on BD and DN degradation models with scale factor ×4. The average PSNR values are evaluated on Set5.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>.9299 36.66/0.9542 37.53/0.9590 37.74/0.9591 37.78/0.9597 37.78/0.9597 38.11/0.9602 38.09/0.9600 38.11/0.9609 38.18/0.9611 ×3 30.39/0.8682 32.75/0.9090 33.67/0.9210 34.03/0.9244 34.09/0.9248 34.20/0.9255 34.65/0.9280 -/-34.70/0.9292 34.77/0.9297 ×4 28.42/0.8104 30.48/0.8628 31.35/0.8830 31.68/0.8888 31.74/0.8893 31.98/0.8923 32.46/0.8968 32.47/0.8980 32.47/0.8983 32.56/0.8992 Set14 ×2 30.24/0.8688 32.45/0.9067 33.05/0.9130 33.23/0.9136 33.28/0.9142 33.35/0.9156 33.92/0.9195 33.85/0.9190 33.82/0.9196 33.90/0.9203 ×3 27.55/0.7742 29.30/0.8215 29.78/0.8320 29.96/0.8349 30.00/0.8350 30.10/0.8372 30.52/0.8462 -/-30.51/0.8461 30.61/0.8473 ×4 26.00/0.7027 27.50/0.7513 28.02/0.7680 28.21/0.7721 28.26/0.7723 28.45/0.7779 28.80/0.7876 28.82/0.7860 28.81/0.7868 28.87/0.7881 Average PSNR/SSIM values for scale factors ×2, ×3and ×4 with BI degradation model. The best performance is shown in red and the second best performance is shown in blue. Average PSNR/SSIM values for scale factor ×3 with BD and DN degradation models. The best performance is shown in red and the second best performance is shown in blue.</figDesc><table><row><cell>Dataset</cell><cell>Scale</cell><cell>Bicubic</cell><cell>SRCNN [7]</cell><cell></cell><cell>VDSR [18]</cell><cell cols="2">DRRN [31]</cell><cell cols="2">MemNet [32]</cell><cell cols="2">SRFBN-S (Ours)</cell><cell>EDSR [23]</cell><cell>D-DBPN [11]</cell><cell>SRFBN (Ours)</cell><cell>SRFBN+ (Ours)</cell></row><row><cell cols="13">Set5 33.66/0B100 ×2 ×2 29.56/0.8431 31.36/0.8879 31.90/0.8960 32.05/0.8973 32.08/0.8978 32.00/0.8970 32.32/0.9013 32.27/0.9000 32.29/0.9010 32.34/0.9015 ×3 27.21/0.7385 28.41/0.7863 28.83/0.7990 28.95/0.8004 28.96/0.8001 28.96/0.8010 29.25/0.8093 -/-29.24/0.8084 29.29/0.8093 25.96/0.6675 26.90/0.7101 27.29/0.7260 27.38/0.7284 27.40/0.7281 27.44/0.7313 27.71/0.7420 27.72/0.7400 27.72/0.7409 27.77/0.7419 ×4 Urban100 ×2 26.88/0.8403 29.50/0.8946 30.77/0.9140 31.23/0.9188 31.31/0.9195 31.41/0.9207 32.93/0.9351 32.55/0.9324 32.62/0.9328 32.80/0.9341 ×3 24.46/0.7349 26.24/0.7989 27.14/0.8290 27.53/0.8378 27.56/0.8376 27.66/0.8415 28.80/0.8653 -/-28.73/0.8641 28.89/0.8664 23.14/0.6577 24.52/0.7221 25.18/0.7540 25.44/0.7638 25.50//0.7630 25.71/0.7719 26.64/0.8033 26.38/0.7946 26.60/0.8015 26.73/0.8043 ×4 Manga109 ×2 30.30/0.9339 35.60/0.9663 37.22/0.9750 37.60/0.9736 37.72/0.9740 38.06/0.9757 39.10/0.9773 38.89/0.9775 39.08/0.9779 39.28/0.9784 ×3 26.95/0.8556 30.48/0.9117 32.01/0.9340 32.42/0.9359 32.51/0.9369 33.02/0.9404 34.17/0.9476 -/-34.18/0.9481 34.44/0.9494 ×4 24.89/0.7866 27.58/0.8555 28.83/0.8870 29.18/0.8914 29.42/0.8942 29.91/0.9008 31.02/0.9148 30.91/0.9137 31.15/0.9160 31.40/0.9182</cell></row><row><cell>Dataset</cell><cell>Model</cell><cell>Bicubic</cell><cell cols="2">SRCNN [7]</cell><cell cols="2">VDSR [18]</cell><cell cols="2">IRCNN G [43]</cell><cell cols="2">IRCNN C [43]</cell><cell cols="2">SRMD(NF) [44]</cell><cell>RDN [47]</cell><cell>SRFBN (Ours)</cell><cell>SRFBN+ (Ours)</cell></row><row><cell>Set5</cell><cell>BD DN</cell><cell cols="11">28.34/0.8161 31.63/0.8888 33.30/0.9159 33.38/0.9182 29.55/0.8246 34.09/0.9242 34.57/0.9280 34.66/0.9283 34.77/0.9290 24.14/0.5445 27.16/0.7672 27.72/0.7872 24.85/0.7205 26.18/0.7430 27.74/0.8026 28.46/0.8151 28.53/0.8182 28.59/0.8198</cell></row><row><cell>Set14</cell><cell>BD DN</cell><cell cols="11">26.12/0.7106 28.52/0.7924 29.67/0.8269 29.73/0.8292 27.33/0.7135 30.11/0.8364 30.53/0.8447 30.48/0.8439 30.64/0.8458 23.14/0.4828 25.49/0.6580 25.92/0.6786 23.84/0.6091 24.68/0.6300 26.13/0.6974 26.60/0.7101 26.60/0.7144 26.67/0.7159</cell></row><row><cell>B100</cell><cell>BD DN</cell><cell cols="11">26.02/0.6733 27.76/0.7526 28.63/0.7903 28.65/0.7922 26.46/0.6572 28.98/0.8009 29.23/0.8079 29.21/0.8069 29.28/0.8080 22.94/0.4461 25.11/0.6151 25.52/0.6345 23.89/0.5688 24.52/0.5850 25.64/0.6495 25.93/0.6573 25.95/0.6625 25.99/0.6636</cell></row><row><cell>Urban100</cell><cell>BD DN</cell><cell cols="11">23.20/0.6661 25.31/0.7612 26.75/0.8145 26.77/0.8154 24.89/0.7172 27.50/0.8370 28.46/0.8581 28.48/0.8581 28.68/0.8613 21.63/0.4701 23.32/0.6500 23.83/0.6797 21.96/0.6018 22.63/0.6205 24.28/0.7092 24.92/0.7362 24.99/0.7424 25.10/0.7458</cell></row><row><cell>Manga109</cell><cell>BD DN</cell><cell cols="11">25.03/0.7987 28.79/0.8851 31.66/0.9260 31.15/0.9245 28.68/0.8574 32.97/0.9391 33.97/0.9465 34.07/0.9466 34.43/0.9483 23.08/0.5448 25.78/0.7889 26.41/0.8130 23.18/0.7466 24.74/0.7701 26.72/0.8424 28.00/0.8590 28.02/0.8618 28.17/0.8643</cell></row><row><cell></cell><cell></cell><cell></cell><cell>HR</cell><cell>Bicubic</cell><cell>SRCNN</cell><cell>VDSR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">butterfly from Set5</cell><cell>IRCNN_G</cell><cell>SRMDNF</cell><cell>RDN</cell><cell cols="2">SRFBN (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>HR</cell><cell>Bicubic</cell><cell>SRCNN</cell><cell>VDSR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">img_044 from Urban100</cell><cell>IRCNN_C</cell><cell>SRMD</cell><cell>RDN</cell><cell cols="2">SRFBN (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell>The investigation of up-and down-sampling layers</cell></row><row><cell>(UDSL), and dense skip connection (DSC) with scale factor ×4 on Set5.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>The investigation of other block design with scale factor ×4 on Set5. Average PSNR/SSIM values for scaling factor ×4 using BI degradation model. The networks used for comparison are all trained using DIV2K training images. The best performance is highlighted.</figDesc><table><row><cell></cell><cell>Params.</cell><cell>Set5</cell><cell>Set14</cell><cell>B100</cell><cell>Urban100</cell><cell>Manga109</cell></row><row><cell>MemNet-Pytorch D-DBPN [11] SRFBN-S (Ours) SRFBN (Ours)</cell><cell cols="6">677K 10,426K 32.40/0.897 28.75/0.785 27.67/0.738 26.38/0.793 30.89/0.913 31.75/0.889 28.31/0.775 27.37/0.729 25.54/0.766 29.65/0.897 483K 31.98/0.892 28.45/0.778 27.44/0.731 25.71/0.772 29.91/0.901 3,631K 32.39/0.897 28.77/0.786 27.68/0.740 26.47/0.798 30.96/0.914</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 .</head><label>9</label><figDesc>Average running time comparison on Urban100 with scale factor 4 on an NVIDIA 1080Ti GPU.</figDesc><table><row><cell></cell><cell cols="2">time (s) PSNR</cell></row><row><cell>MemNet-Pytorch</cell><cell>0.481</cell><cell>25.54</cell></row><row><cell>EDSR</cell><cell>1.218</cell><cell>26.64</cell></row><row><cell>D-DBPN</cell><cell>0.015</cell><cell>26.38</cell></row><row><cell>RDN</cell><cell>1.268</cell><cell>26.61</cell></row><row><cell>RCAN</cell><cell>1.130</cell><cell>26.82</cell></row><row><cell>SRFBN-S (Ours)</cell><cell>0.006</cell><cell>25.71</cell></row><row><cell>SRFBN (Ours)</cell><cell>0.011</cell><cell>26.60</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Line Alberi-Morel</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunshui</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On-demand learning for deep image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Brain states: topdown influences in sensory processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image super-resolution via dual-state recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Van Der Maaten Laurens, and Kilian Q Weinberger. Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast and accurate single image super-resolution via information distillation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiumei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cortical feedback improves discrimination between figure and background by v1, v2 and v3 neurons. Nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Lomber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bullier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeplyrecursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bridging the gaps between residual learning, recurrent neural networks and visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianli</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03640</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sketch-based manga retrieval using manga109 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azuma</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A statistical prediction model based on sparse representations for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Curriculum learning of multiple tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Pentina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktoriia</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Top-down feedback for crowd counting convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak Babu</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep networks with internal selective attention through feedback connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Stollenga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno>NIPS. 2014. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image superresolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Desmet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Vangool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Desmet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Vangool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Seven ways to improve example-based single image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiejie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinquan</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Alexander Sorkinehornung, Olga Sorkinehornung, and Christopher Schroers. A fully progressive approach to single-image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPRW</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Feedback networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Te-Lin</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><forename type="middle">E</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Curves and Surfaces</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Single image super-resolution with non-local means and steering kernel regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaibing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning deep CNN denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An edge-guided image interpolation algorithm via directional filtering and data fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
