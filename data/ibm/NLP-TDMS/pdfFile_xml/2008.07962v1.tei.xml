<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relational Reflection Entity Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Virtual Event, Ireland. ACM</publisher>
				<availability status="unknown"><p>Copyright Virtual Event, Ireland. ACM</p>
				</availability>
				<date type="published" when="2020-10-19">2020. October 19-23, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
							<email>wenting.wang@lazada.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Xu</surname></persName>
							<email>hmxu@stu.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
							<email>mlan@cs.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Relational Reflection Entity Alignment</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM &apos;20)</title>
						<meeting>the 29th ACM International Conference on Information and Knowledge Management (CIKM &apos;20) <address><addrLine>New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Virtual Event, Ireland. ACM</publisher>
							<date type="published" when="2020-10-19">2020. October 19-23, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3340531.3412001</idno>
					<note>10 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Knowledge representation and reasoning</term>
					<term>Natural language processing</term>
					<term>Supervised learning KEYWORDS Graph Neural Networks</term>
					<term>Knowledge Graph</term>
					<term>Entity Alignment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Entity alignment aims to identify equivalent entity pairs from different Knowledge Graphs (KGs), which is essential in integrating multi-source KGs. Recently, with the introduction of GNNs into entity alignment, the architectures of recent models have become more and more complicated. We even find two counter-intuitive phenomena within these methods: (1) The standard linear transformation in GNNs is not working well.</p><p>(2) Many advanced KG embedding models designed for link prediction task perform poorly in entity alignment. In this paper, we abstract existing entity alignment methods into a unified framework, Shape-Builder &amp; Alignment, which not only successfully explains the above phenomena but also derives two key criteria for an ideal transformation operation. Furthermore, we propose a novel GNNs-based method, Relational Reflection Entity Alignment (RREA). RREA leverages Relational Reflection Transformation to obtain relation specific embeddings for each entity in a more efficient way. The experimental results on real-world datasets show that our model significantly outperforms the state-of-the-art methods, exceeding by 5.8%-10.9% on Hits@1.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With more and more KGs emerging, integrating multi-source KGs becomes necessary and beneficial to not only complement information but also improve downstream tasks such as recommendation system and search engine. One of the key steps to integrating KGs is to identify equivalent entity pairs. Therefore, the task of entity alignment attracts increasing attention in recent years. Existing entity alignment methods can be divided into two main categories: Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM '20, October 19-23, 2020, Virtual Event, Ireland © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6859-9/20/10. . . $15.00 https://doi.org/10.1145/3340531.3412001 <ref type="bibr" target="#b0">(1)</ref> Translation-based. Inspired by cross-lingual word embedding task, these methods presume that embeddings of different KGs have similar distributions, so the entity pairs who are aligned between KGs would also have relatively similar positions in their own vector spaces. These methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> first use translation-based KGs embedding models (e.g., TransE <ref type="bibr" target="#b0">[1]</ref>) on every single KG to get its embeddings of entities and relations, and then align entities from two vector spaces into a unified one based on some pre-aligned entity pairs. (2) GNNs-based. Different from translation-based methods where the relation is a translation from one entity to another, Graph Neural Networks (GNNs) generate node-level embeddings through aggregating information from the neighboring nodes. Inspired by Siamese Neural Networks <ref type="bibr" target="#b4">[5]</ref> which are widely used in computer vision, a typical architecture of GNNs-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref> consists of two multi-layer GNNs with the contrastive loss <ref type="bibr" target="#b8">[9]</ref> or triplet loss <ref type="bibr" target="#b21">[22]</ref>.</p><p>With the introduction of GNNs into entity alignment task, recent model architectures have become more and more complicated which are hard to interpret the effectiveness of individual components. Despite the success in empirical results, we observe two counter-intuitive phenomena in these complicated methods that need to be further clarified and studied: Q1: Why the standard linear transformation of GNNs is not working well in entity alignment? GNNs are originally designed with a standard linear transformation matrix, however, many GNNs-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref> 1 constrain it to be unit (i.e., removing this matrix from GNNs) or diagonal with unit initialization. All previous methods just treat it as parameter reduction but do not explore nor explain about this setting. When we try to undo this setting in GCN-Align <ref type="bibr" target="#b28">[29]</ref>, the performances significantly drop by ⩾ 10% on Hits@1. So we believe this should be related to some more fundamental issues.</p><p>Q2: Why many advanced KG embedding models are not working well in entity alignment? In other tasks that also need KG modeling, such as link prediction, many advanced KG embedding models are proposed and proved to be very effective. Strangely, a lot of these advanced embedding models designed for link prediction do not show success in entity alignment. Sun et al. <ref type="bibr" target="#b25">[26]</ref> experiments with many advanced KG embedding models, such as TransR <ref type="bibr" target="#b15">[16]</ref>, ConvE <ref type="bibr" target="#b5">[6]</ref> and etc., but performances are even worse than TransE. The authors conclude with "not all embedding models designed for link prediction are suitable for entity alignment" but not giving any further exploration or explanation.</p><p>To analyze these two issues from a global and unified perspective, we propose an abstract entity alignment framework, named as Shape-Builder &amp; Alignment. In this framework, both translationbased and GNNs-based methods are just special cases under respective special settings. With this framework, we successfully derive the answers to address the above questions: (Q1) Entity alignment presumes similarity between distributions, so in order to avoid destroying the shape, the norms and the relative distances of entities should remain unchanged after transformation. Thus, it is mandatory that the transformation matrix is orthogonal. (Q2) Many advanced KG embedding models share one key idea -transforming entity embeddings into relation specific ones. However, their transformation matrix is difficult to comply with the orthogonal property. This is the fundamental reason why they perform poorly in entity alignment.</p><p>Inspired by the above findings, we propose two key criteria of an ideal transformation operation for entity alignment: Relational Differentiation and Dimensional Isometry. Then, we design a new transformation operation, Relational Reflection Transformation, which fulfills these two criteria. This new operation is able to reflect entity embeddings along different relational hyperplanes to construct relation specific embeddings. Meanwhile, the reflection matrix is orthogonal which is easy to prove, so reflection transformation could keep the norms and the relative distances unchanged. By integrating this proposed transformation into GNNs, we further present a novel GNNs-based entity alignment method, Relational Reflection Entity Alignment (RREA). The experimental results on real-world public datasets validate that our model greatly exceeds existing state-of-the-art methods by 5.8%-10.9% on Hits@1 across all datasets. We summarize the main contributions of this paper as follows:</p><p>• To our best knowledge, this is the first work to abstract existing entity alignment methods into a unified framework. Through this framework, we successfully derive two key criteria for an ideal transformation operation: relational differentiation and dimensional isometry. • To our best knowledge, this is the first work to design a new transformation operation, Relational Reflection Transformation, which fulfills the above two criteria. By integrating this operation into GNNs, we further propose a novel GNNsbased method Relational Reflection Entity Alignment (RREA). • The extensive experimental results show that our model is ranked consistently as the best across all real-world datasets and outperforms the state-of-the-art methods by 5.8%-10.9% on Hits@1. In addition, we also carry ablation experiments to demonstrate that each component of our model is effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Existing entity alignment methods can be divided into two categories according to their motivations. In this section, we will give a detailed illustration of these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Translation-based Methods</head><p>Translation-based methods are originated from cross-lingual word embedding task. So they also have a core assumption that the entity embeddings of different KGs have similar distributions, just like the word embeddings of different languages. As shown in <ref type="figure" target="#fig_0">Figure   1</ref>(a), translation-based methods usually consist of two modules: translation module and alignment module. Translation Module: The major function of the translation module is to constrain the randomly initialized embeddings into a fixed distribution through translation-based KGs embedding models. Due to its solid theoretical foundation and minimum implementation effort, the majority of translation-based methods adopt TransE <ref type="bibr" target="#b0">[1]</ref> as the translation module (e.g., MtransE <ref type="bibr" target="#b3">[4]</ref>, JAPE <ref type="bibr" target="#b23">[24]</ref> and BootEA <ref type="bibr" target="#b24">[25]</ref>). Inspired by Word2Vec <ref type="bibr" target="#b17">[18]</ref>, TransE interprets a relation as the translation from its head to its tail (h + r ≈ t), so that entity embeddings also have the property of translation invariance. Theoretically, any KG embedding model could act as a translation module. However, as mentioned in Section 1, many advanced embedding models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref> which perform well in link prediction do not show success in entity alignment.</p><p>Alignment Module: By taking pre-aligned entities as seeds, the alignment module is responsible for aligning the embeddings of different KGs into a unified vector space. At present, there are two types of alignment modules:</p><p>(1) mapping: Similar to its counterparts in cross-lingual word embedding, this approach embeds different KGs into a unified vector space through a linear transformation matrix. For example, MtransE <ref type="bibr" target="#b3">[4]</ref>, KDCoE <ref type="bibr" target="#b2">[3]</ref>, and OTEA <ref type="bibr" target="#b19">[20]</ref> minimize the distances between the pre-aligned pairs by optimizing one or two linear transformation matrices (i.e., W e 1 ≈ e 2 or W 1 e 1 ≈ W 2 e 2 ).</p><p>(2) sharing: The sharing approach embeds different KGs into a unified vector space by letting each pre-aligned pair directly share the same embedding, which is more straightforward compared to the mapping approaches. There are three different implementations about sharing: (a) MTransE <ref type="bibr" target="#b3">[4]</ref> proposes to minimize the equation ∥e 1 − e 2 ∥ for each pre-aligned pairs 2 . (b) JAPE <ref type="bibr" target="#b23">[24]</ref> and RSNs <ref type="bibr" target="#b7">[8]</ref> directly configure e 1 and e 2 to share a common embedding when the model is built. (c) BootEA <ref type="bibr" target="#b24">[25]</ref> and TransEdge <ref type="bibr" target="#b25">[26]</ref> swap the pre-aligned entities in their triples to generate extra triples for supervision, e.g., given (e 1 , e 2 ) is a pre-aligned pair and a triple ⟨e 1 , r 1 , e 3 ⟩ in KGs, the model will produce a new triple ⟨e 2 , r 1 , e 3 ⟩.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GNNs-based Methods</head><p>Due to the fact that TransE is only trained on individual triples, it may lack the ability to exploit the global view of entities and relations. Therefore, many recent studies introduce GNNs into entity alignment task, which is originated with the ability to model global information of graphs.</p><p>Inspired by Siamese Neural Networks <ref type="bibr" target="#b4">[5]</ref>, a typical GNNs-based method has a simple and intuitive architecture (as shown in <ref type="figure" target="#fig_0">Figure  1</ref>(b)) -two multi-layer GNNs encoders with a loss function, either contrastive loss <ref type="bibr" target="#b8">[9]</ref> or triplet loss <ref type="bibr" target="#b21">[22]</ref>. The first GNNs-based method is proposed by GCN-Align <ref type="bibr" target="#b28">[29]</ref> using multi-layer vanilla GCN as the encoder and successfully applies GNNs to entity alignment task. However, due to the disability of vanilla GCN in modeling heterogeneous graphs, GCN-Align is unable to effectively utilize the rich relation information in KGs.</p><p>Many more recent studies attempt to incorporate relation information into GNNs and build relation-aware models to better represent KGs. HMAN <ref type="bibr" target="#b33">[34]</ref> concatenates the entity embeddings obtained by GCN with the average of the neighboring relation and attribute embeddings. MuGNN <ref type="bibr" target="#b1">[2]</ref>, NAEA <ref type="bibr" target="#b34">[35]</ref> and MRAEA <ref type="bibr" target="#b16">[17]</ref> assign different weight coefficients to entities according to relation types between them, which empowers the model to distinguish the importance between different entities. RDGCN <ref type="bibr" target="#b29">[30]</ref> establishes a dual relation graph for KGs which regards relation as node and entity as edge. Strangely, many GNNs-methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref> adopt counter-intuitive constraint in their transformation matrix design, i.e., forcing the matrix to be unit or diagonal. All previous methods just treat it as parameter reduction but do not explore nor explain about this setting.</p><p>In addition, there are also some other GNNs-based models proposed for modeling KGs in link prediction task. By assigning different transformation matrices to different relations, RGCN <ref type="bibr" target="#b20">[21]</ref> maps entities to corresponding relational vector spaces before convolution. KBAT <ref type="bibr" target="#b18">[19]</ref> converts the triple embeddings into new entity embeddings with a linear transformation matrix and assigns different weight coefficients to the new embeddings via attention mechanism. However, according to our experimental results in Table 5, these advanced models perform even worse than vanilla GCN in entity alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY 3.1 Problem Formulation</head><p>KGs store the real-world information in the form of triples, ⟨entity 1 , relation, entity 2 ⟩, which describe the relations between two entities. A KG could be defined as G = (E, R,T ), where E and R represent the sets of entities and relations respectively, T represents the set of triples. Although different KGs are constructed from different sources, there are still many entity pairs referring  to the same real-world object. Entity alignment aims to find these aligned entity pairs from multi-source KGs, which is the key step of knowledge integration. Formally, G 1 and G 2 are two multi-source</p><formula xml:id="formula_0">KGs, P = (e i 1 , e i 2 )|e i 1 ∈ E 1 , e i 2 ∈ E 2 p i=1</formula><p>represents the set of prealigned seed pairs. The aim of entity alignment is to find new aligned entity pairs based on these pre-aligned seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets</head><p>In order to make the comparison with previous methods reliable and fair, we experiment on two widely used open-source datasets:</p><p>• DBP15K [24] which contains three cross-lingual datasets constructed from the multilingual version of DBpedia, including DBP ZH−EN (Chinese to English), DBP JA−EN (Japanese to English), and DBP FR−EN (French to English). • DWY100K <ref type="bibr" target="#b24">[25]</ref> are extracted from DBpedia, Wikidata, and YAGO3. It has two monolingual datasets: DWY WD (DBpedia-Wikidata) and DWY YG (DBpedia-YAGO3). Each dataset has 100, 000 reference entity alignments and more than nine hundred thousand triples. <ref type="table" target="#tab_1">Table 1</ref> shows the statistics of these datasets. Following the setting of previous studies, we randomly split 30% of aligned pairs for training and keep 70% of them for testing. The reported performance is the average of five independent training runs and the train/test datasets are shuffled in every round.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A UNIFIED ENTITY ALIGNMENT FRAMEWORK</head><p>In this section, we model GNNs-based methods and translationbased methods into an abstract but unified entity alignment framework. Then this framework successfully leads to not only the answers regarding the two questions raised in Section 1 but also the key criteria of an ideal transformation operation for entity alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Shape-Builder &amp; Alignment</head><p>The motivation behind translation-based entity alignment methods is cross-lingual word embedding (word alignment). So naturally, they all can be abstracted into a unified framework composed of Shape-Builder and Alignment as shown in <ref type="figure" target="#fig_1">Figure 2</ref>: Shape-Builder: The main function of shape-builder is to constrain the random initialized distribution to a specific distribution which we define as shape. Obviously, the translation module mentioned in Section 2 is a shape-builder. In fact, besides TransE, any embedding model can be used as a shape-builder. The only prerequisite is that the obtained embeddings from two KGs should have Shape Similarity in-between. In other words, equivalent elements (such as word or entity) have relatively similar positions in their own vector spaces.</p><p>Alignment: When the Shape Similarity holds, different shapes can be matched by pre-aligned seeds. As mentioned in Section 2, mapping is one of the alignment modules in translation-based methods which trains a matrix W to minimize the distances between the pre-aligned seeds <ref type="bibr" target="#b3">[4]</ref> as follow:</p><formula xml:id="formula_1">min W (e i ,e j )∈P W h e i − h e j<label>(1)</label></formula><p>where (e i , e j ) is a pre-aligned pair, h e i represents the embedding vector of entity e i . However, if matrix W has no constraint, then there is no guarantee that the norms and the relative distances of embeddings will be reserved after transformation, which in turn could destroy the original shape similarity. The seed pairs are well fitted, but the rest of entities could be misaligned (as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(a)). On the other hand, ifW is constrained to be orthogonal, it becomes a rotation operation and then shape similarity will not be destroyed. This is why many word alignment methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32]</ref> use orthogonal constraint. In entity alignment, OTEA <ref type="bibr" target="#b19">[20]</ref> also proposes to constrain the transformation matrix to be orthogonal (as illustrated in <ref type="figure" target="#fig_1">Figure 2(b)</ref>). In addition, in another alignment module sharing, pre-aligned entities are treated as anchors and then the rest of the entities can be gradually aligned during the optimization process of shape-builder (as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(c)). Compared to mapping, sharing abandons the transformation matrix at all which reduces parameters and simplifies the architecture. So far, all translation-based methods could be abstracted into this framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GNNs-based Methods Are Also Subject to Our Unified Framework</head><p>Many GNNs in entity alignment task contains the following equations <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_2">h l N e e i ← Aддreдate({h l e k , ∀e k ∈ {e i } ∪ N e e i }) (2) h l +1 e i ← σ W l · h l N e e i<label>(3)</label></formula><p>where N e e i represents the set of neighboring nodes around e i , W l is the transformation matrix of layer l. Equation 2 is responsible for aggregating information from the neighboring nodes while Equation 3 transforms the node embeddings into better ones. There are many operations available that can serve the purpose of Aддreдate, such as normalized mean pooling (vanilla GCN <ref type="bibr" target="#b12">[13]</ref>) and attentional weighted summation (GAT <ref type="bibr" target="#b27">[28]</ref>).</p><p>After generating the embeddings, GNNs-based methods often use triplet loss to make the equivalent entities close to each other:</p><formula xml:id="formula_3">L = (e i ,e j )∈P (e ′ i ,e ′ j )∈P ′ max ∥h e i − h e j ∥ aliдnment − ∥h e ′ i − h e ′ j ∥ + λ apar t , 0<label>(4)</label></formula><p>where λ represents the margin hyper-parameter, (e ′ i , e ′ j ) represents the negative pair by randomly replacing one of (e i , e j ). Interestingly, the first half of the loss function (i.e., ∥h e i − h e j ∥) is exactly the same as the sharing alignment module. The same finding is even more obvious if looking at the contrastive loss used in AliNet <ref type="bibr" target="#b26">[27]</ref>:</p><formula xml:id="formula_4">L = (e i ,e j )∈P ∥h e i − h e j ∥ aliдnment + (e ′ i ,e ′ j )∈P ′ max ∥h e ′ i − h e ′ j ∥ + λ apar t , 0 (5)</formula><p>So the losses in GNNs all can be broken down into two sub-parts: the 1st half, i.e. alignment loss, acts as an alignment module; while the 2nd half, i.e. apart loss, acts as part of a shape-builder. Therefore, we propose a hypothesis: GNNs-based methods are also subject to our unified framework, Shape-Builder &amp; Alignment. More specifically, we believe the Aддreдate operation of GNNs and the apart loss function together compose a potential shape-builder. The Aддreдate operation makes similar entities close to each other, and the apart loss keeps dissimilar entities away from each other. So the combination of them builds a distribution which possess the property of Shape Similarity. Visual Experiment: If our hypothesis is correct, distributions of different KGs should have visual similarity. Thus, to verify our hypothesis, we retain the apart loss from triplet loss in GCN-Align <ref type="bibr" target="#b28">[29]</ref> 3 which has the simplest architecture:</p><formula xml:id="formula_5">L apar t = (e ′ i ,e ′ j )∈P ′ max λ − h e i ′ − h e j ′ 1 , 0<label>(6)</label></formula><p>Then GCN-Align is transformed from a supervised model into a selfsupervised model. We train the model on DBP FR−EN and extract 100 embeddings of aligned pairs, then map them to 2-dimensional space by t-SNE <ref type="bibr" target="#b11">[12]</ref>. The distributions are shown in <ref type="figure" target="#fig_2">Figure 3</ref> and   we observe that there indeed are similarities between the two distributions. For instance, both of them have a large amount of entities scattered in the right portion while having a small amount of entities located closely in the left bottom corner. Quantitative Experiment: If the distributions have shape similarity, the relative distances between entities in one KG should be equal to that of the counterparts in another KGs. To further quantify the similarity between the two distributions, we design shape similarity metric as follows:</p><formula xml:id="formula_6">SS = (e i , e i )∈P (e j , e j )∈P dist(e i , e j ) − dist( e i , e j ) (e ′ i , e i ′ )∈P ′ (e ′ j , e j ′ )∈P ′ dist(e ′ i , e ′ j ) − dist( e i ′ , e j ′ )<label>(7)</label></formula><p>where e i , e j ∈ G 1 represent an arbitrarily entity pair in one KG and e i , e j ∈ G 2 represent the counterparts in another KG. Then (e ′ i , e i ′ , e ′ j , e j ′ ) represents a negative quadruple obtained by randomly replacing one entity from (e i , e i , e j , e j ), dist(e i , e j ) represents the distance between two entities where any distance metrics such as L2 or cosine is applicable. All the embeddings are normalized by L2-normalization. In <ref type="figure">Equation 7</ref>, the numerator represents the difference of distances between aligned entities, while the denominator represents that of random pairs. Ideally, the SS between the distributions should be as small as possible and the SS between the random distributions should be close to1. <ref type="table" target="#tab_3">Table 2</ref> shows the SS between the distributions obtained by random initialization, GCN-Align, and TransE under two different distance metrics. The experimental results are in line with our expectation: <ref type="bibr" target="#b0">(1)</ref> The SS between the random embeddings is almost 1. (2) Although the untrained GCN-Align has some minimum clustering ability, it is still close to the random initialization.</p><p>(3) Both TransE and GCN-Align successfully reduce the SS of the distributions and GCN-Align is slightly better than TransE.</p><p>These two experiments prove that the Aggregate operation of GNNs and the apart loss compose a shape-builder together. Notice  that our hypothesis is applicable to the alignment methods purely based on structural information (i.e., triples). Some methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref> take entity names and pre-align them by machine translation or cross-lingual word embeddings. In these methods, GNNs play a role as noise smoothing rather than actual alignment. Therefore, these methods are not in the scope of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Why Linear Transformation Not Work</head><p>As mentioned in Section 1, many GNNs-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34]</ref> constrain their transformation matrix to be unit (i.e., removing W ) or diagonal with unit initialization. With our hypothesis verified in Section 4.2, it is easy to explain why these methods adopt such a counter-intuitive constraint. In fact, if transformation matrix W of GNNs is a unit matrix, it is equivalent to sharinд alignment in translation-based methods; If W is unconstrained, it is equivalent to mappinд alignment in translation-based methods. As explained in Section 4.1, the unconstrained transformation could destroy Shape Similarity and degrade performances. Therefore, the orthogonal constraint should be adopted to reserve the norm and relative distance during transformation. In fact, unit matrix is not only a special case of orthogonal but also the simplest implementation. In order to verify our answer to Q1, we design two experiments:</p><p>(1) Experiment on GCN-Align: To prove that keeping transformation matrix orthogonal is necessary, we test different constraints on GCN-Align which is the simplest GNN-based method <ref type="bibr" target="#b3">4</ref> . To keep W orthogonal in the training process, we adopt the following constraint:</p><formula xml:id="formula_7">L o = W T W − I 2 2<label>(8)</label></formula><p>From <ref type="table" target="#tab_5">Table 3</ref>, it's not surprising to see that the unconstrained method is the worst. Although diagonal constraint with unit initialization shows a great improvement, both unit and orthogonal W achieve the best and very close performances. This indicates that diagonal constraint is only a temporary solution under incomplete understanding. Orthogonal initialization with unconstrained W slightly improves the performance compared to He initialization, but the large gap between unconstrained W and orthogonal W demonstrates that orthogonal constraint is an essential factor impacting performance.</p><p>(2) Experiment on Complex GNNs: To further verify orthogonal is also necessary for complex methods, we test orthogonal and   <ref type="table" target="#tab_7">Table 4</ref>. It is obvious that both orthogonal and unit constraints improve the performances on all datasets compared to each method's original constraint setting. The unit constraint is slightly better than orthogonal constraint. This may be due to the fact that more transformation matrices are in complex methods, which make the orthogonal constraint slightly harder to optimize. In summary, we believe that the transformation matrix W in GNNs should be constrained to be orthogonal to ensure that the norms and the relative distances of entities remain unchanged after transformation. Unit matrix is not only a special case of orthogonal but also the simplest implementation. The experimental results prove that our conclusion is universal to both the simplest and complex GNNs-based methods. Many existing GNNs-based methods could be further improved by adopting this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Why Advanced KG Embedding Not Work</head><p>Many advanced KG embedding models are proposed and proven to be successful in link prediction task. But a lot of them have very poor performances in entity alignment task as shown in <ref type="table" target="#tab_9">Table 5</ref>. For translation-based methods, they are at least 17% worse than TransE, while for GNNs-based methods they are at least 3% worse than GCN. Why they are not working in with entity alignment? To compare these KG embedding models clearly, we summarize their core functions in <ref type="table" target="#tab_10">Table 6</ref>. From the table, we observe that all these advanced methods share one key idea: transform universal entity embeddings into relation specific ones. In particular, RGCN is a combination of GCN and TransR while KBAT references the ConvE and applies it to GAT. However, in their original design, all of them do not put any constraint on their transformation matrix. This violates our conclusion in Section 4.3. Such unconstrained transformation destroys the shape similarity and results in their poor performances for entity alignment task <ref type="table" target="#tab_9">(Table 5)</ref>.</p><p>Theoretically, based on our conclusion in Section 4.3, if the transformation matrix in these advanced methods could comply to orthogonal, then the shape similarity would be reserved. But such constraint is very difficult to adopt in practice. For TransR and RGCN, because there are usually thousands of relations in KGs, <ref type="bibr" target="#b4">5</ref> AliNet only releases part of the source code (w/o rel).   <ref type="bibr" target="#b25">[26]</ref>. Other results are produced by ourselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Embedding Function φ(·) constraining all the relational matrices is not feasible. For ConvE and KBAT, the dimension of transformed embeddings must be kept consistent with that of input embeddings. Otherwise, it will cause dimension mismatch in ConvE or dimension explosion when stacking multiple layers in KBAT. Therefore, the transformation matrix of ConvE and KBAT cannot be a square matrix, let alone an orthogonal matrix. But their successes in linked prediction bring one insight that constructing relation specific entity embedding is more effective in modeling relations, compared to just assigning relation-based to entities.</p><formula xml:id="formula_8">GCN [13] σ j ∈Ni 1 √ di dj φ(h ej ) W h TransR [16] ∥φ(h, r ) + r − φ(t, r )∥ W r h RGCN [21] σ ( r ∈R j ∈N r i 1 |N r i | φ(h ej , r ) + W l 0 h l ei ) W r h GAT [28] σ j ∈Ni α i j φ(h ej ) W h ConvE [6] σ (φ(h, r ) ⊙ t) W vec([h∥r ] * ω) KBAT [19] σ j ∈Ni k ∈Ri j α i jk φ(h ei , h r k , h ej ) W [h∥r ∥t]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Key Criteria for Transformation Operation</head><p>Therefore, the ideal transformation operation in entity alignment should satisfy the following two key criteria:</p><p>(1) Relational Differentiation: Corresponding to different relation types, the operation could transform embedding of the same entity into different relational spaces.</p><formula xml:id="formula_9">φ(h e , h r 1 ) φ(h e , h r 2 ), ∀e ∈ E, ∀r 1 , r 2 ∈ R<label>(9)</label></formula><p>(2) Dimensional Isometry: When two entities in the same KG are transformed into the same relational space, their norms and relative distance should be retained.</p><formula xml:id="formula_10">∥h e ∥ = ∥φ(h e , h r )∥, ∀e ∈ E, ∀r ∈ R<label>(10)</label></formula><p>h T e 1 h e 2 = φ(h e 1 , h r ) T φ(h e 2 , h r ), ∀e 1 , e 2 ∈ E, ∀r ∈ R (11) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE PROPOSED METHOD</head><p>In this section, we propose a novel GNNs-based method, Relational Reflection Entity Alignment (RREA), which incorporates Relational Reflection Transformation in GNNs to fulfill both relational differentiation and dimensional isometry criteria at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Relational Reflection Transformation</head><p>To meet the key criteria, we design a new transformation operation, Relational Reflection Transformation. Let relation embedding h r be a normal vector, there is one and only one hyperplane P r and only one corresponding reflection matrix M r such that:</p><formula xml:id="formula_11">M r = I − 2h r h T r<label>(12)</label></formula><p>Here h r should be normalized to ensure ∥h r ∥ 2 = 1. It is easy to derive that the reflection of entity embedding h e along the relational hyperplane P r can be computed by M r h e . It is also easy to prove that M r is orthogonal:</p><formula xml:id="formula_12">M T r M r = (I − 2h r h T r ) T (I − 2h r h T r ) = I − 4h r h T r + 4h r h T r h r h T r = I<label>(13)</label></formula><p>Therefore, as long as {h r i h r j , ∀r i , r j ∈ R}, our Relational Reflection Transformation satisfies the two key criteria (illustrated as <ref type="figure" target="#fig_3">Figure 4</ref> (a) and (b)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Relational Reflection Entity Alignment</head><p>In this section, we describe our proposed model Relational Reflection Entity Alignment (RREA). The inputs are two matrices: H e ∈ R |E |×d represents the entity embeddings and H r ∈ R |R |×d represents the relation embeddings. Both H e and H r are randomly initialized by He_initializer <ref type="bibr" target="#b10">[11]</ref>. RREA consists of the following four major components: Relational Reflection Aggregate Layer: The output feature of e i from the l-th layer is obtained as follow:</p><formula xml:id="formula_13">h l +1 e i = ReLU e j ∈N e e i r k ∈R i j α l i jk M r k h l e j<label>(14)</label></formula><p>where N e e i represents the neighboring entity set of e i , R i j represents the set of relations between e i and e j , M r k ∈ R d ×d is the relational reflection matrix of r k . Compared with RGCN which assigns different W r to different relations, the number of trainable parameters of relational reflection is much less because the degrees of freedom of M r is only d rather than d 2 . Similar to GAT, α l i jk represents the weight coefficient of M r k h l e j which is computed by the following equations:</p><formula xml:id="formula_14">β l i jk = v T [h l e i ∥M r k h l e j ∥h r k ]<label>(15)</label></formula><formula xml:id="formula_15">α l i jk = exp(β l i jk ) e j ∈N e e i r k ∈R i j exp(β l i jk ))<label>(16)</label></formula><p>where v ∈ R 2d is a trainable vector for calculating the weight coefficient. To create a global-aware graph representation, we stack multiple layers of GNNs to capture multi-hop neighborhood information. The embeddings from different layers are concatenated together to get the final output feature h out e i of entity e i :</p><formula xml:id="formula_16">h out e i = [ h 0 e i ∥...∥h l e i ]<label>(17)</label></formula><p>where h 0 e i represents the initial embedding of e i . Dual-Aspect Embedding: Some recent studies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34]</ref> believe that the entity embeddings generated by GNNs only contain the topological information, lack the relational information around entities. Therefore, they concatenate the summation of the relation embeddings with entity embeddings to get dual-aspect embeddings. In this paper, we adopt dual-aspect embeddings with the following equation:</p><formula xml:id="formula_17">h Mul e i =       h out e i 1 |N r e i | r j ∈N r e i h r j      <label>(18)</label></formula><p>where N r e i represents the set of the relations around entity e i . Alignment Loss Function for Training: In order to make the equivalent entities close to each other in the unified vector space, we adopt the following triplet loss function:</p><formula xml:id="formula_18">L = (ei,ej )∈P max dist e i , e j − dist e ′ i , e ′ j + λ, 0<label>(19)</label></formula><p>Here, e ′ i and e ′ j represent the negative pair of e i and e j which are generated by nearest neighbor sampling <ref type="bibr" target="#b24">[25]</ref>. In the training process, we take the same setting with GCN-Align <ref type="bibr" target="#b28">[29]</ref> which uses Manhattan distance as the distance metric. </p><p>CSLS Metric for Testing: We notice that Lample et al. <ref type="bibr" target="#b13">[14]</ref> propose Cross-domain Similarity Local Scaling (CSLS) to solve the hubness problem existing in cross-lingual word embedding task. Inspired by their study, we adopt CSLS as the distance metric during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Further Data Enhancement</head><p>Semi-supervised Learning: In practice, the aligned seeds are often inadequate due to the high cost of manual annotations and the huge size of KG. To expand training data, some recent studies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref> adopt iterative or bootstrapping strategies to build semisupervised models. In this paper, we use the iterative strategy proposed by MRAEA <ref type="bibr" target="#b16">[17]</ref> to generate semi-supervised data. Unsupervised Textual Framework: The methods we have discussed before only focus on the structural information of KGs. In some KGs, rich textual information are also available such as the entity names. Therefore, some recent methods propose to combine textual information and structural information. Among these methods, the unsupervised textual framework proposed by MRAEA <ref type="bibr" target="#b16">[17]</ref> does not require labeled data, which is more practical. In this paper, we adopt the unsupervised textual framework from MRAEA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section, we conduct a series of experiments on two public datasets to prove that our model not only outperforms all existing methods but also is robust. The code is now available on GitHub 5 . 5}. For all of the datasets, we use a same config: d = 100, λ = 3, l = 2, µ = 0.3, γ = 0.005. RMSprop is adopted to optimize the model and the number of epochs is set to 3, 000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Baselines</head><p>As an emerging task, entity alignment attracts a lot of attention in a short time. Many studies believe that the information of existing datasets is insufficient, so they try to introduce extra data into datasets. For example, GMNN <ref type="bibr" target="#b32">[33]</ref> and RDGCN <ref type="bibr" target="#b29">[30]</ref> use the name of entities as input features, BootEA <ref type="bibr" target="#b24">[25]</ref> introduces semi-supervision to extend the datasets. We believe that the introduction of extra data may lead to unfair comparisons between methods. Therefore, we divide existing methods into three categories according to the data they use:</p><p>• Basic: This kind of methods only uses original structural data (i.e., triples) from the datasets: JAPE <ref type="bibr" target="#b23">[24]</ref>, GCN-Align <ref type="bibr" target="#b28">[29]</ref>, RSN <ref type="bibr" target="#b7">[8]</ref>, MuGNN <ref type="bibr" target="#b1">[2]</ref>, TransEdge <ref type="bibr" target="#b25">[26]</ref>, AliNet <ref type="bibr" target="#b26">[27]</ref> and MRAEA <ref type="bibr" target="#b16">[17]</ref>. • Semi-supervised: This kind of methods introduces semisupervision to generate extra structural data: Boot-EA <ref type="bibr" target="#b24">[25]</ref>, NAEA <ref type="bibr" target="#b34">[35]</ref>, TransEdge (semi), MRAEA (semi). • Textual: Besides the structural data, textual methods introduce entity names as additional input features: GMNN <ref type="bibr" target="#b32">[33]</ref>, RDGCN <ref type="bibr" target="#b29">[30]</ref>, HGCN <ref type="bibr" target="#b30">[31]</ref>, MRAEA (text) and DGMC <ref type="bibr" target="#b6">[7]</ref>.</p><p>Correspondingly, in order to make fair comparisons with all kinds of methods, our RREA also has three versions: RREA (basic), RREA (semi), and RREA (text).   <ref type="table" target="#tab_15">Table 8</ref> shows the performance comparisons for basic and semi-supervised methods. Obviously, the performances of our model are consistently ranked as the best over all competing basic methods and semisupervised methods on all the evaluation metrics. Especially, compared with the state-of-the-art methods TransEdge and MRAEA, RREA (basic) exceeds by at least 6% on Hits@1 and RREA(semi) exceeds by more than 5% on Hits@1 respectively. The main reason is that our reflection transformation builds relation specific embeddings for entities which could capture the relation information better. In addition, it is clear that semi-supervision could significantly improve the performances of all the methods on all datasets. Compared to RREA (basic), RREA (semi) iteratively generates extra training data via semi-supervision which improves the performance by an average of 6% on Hits@1. In summary, RREA breaks the performance ceiling of purely structural-based entity alignment methods, which proves that our designs are effective. RREA vs. Textual Methods. Since all the datasets of DWY100K are sampled from English KGs, the textual information is highly similar. Therefore, we only conduct the experiments of textual methods on DBP15K. <ref type="table" target="#tab_13">Table 7</ref> shows the results of the compared methods. Our model beats MRAEA and achieves the best on all datasets. Since we use the unsupervised textual framework proposed by MRAEA, the performance improvement is totally contributed by the better modelling of structural data. Compared with other supervised models (e.g., DGMC, GMNN), RREA (text) even achieves better performance while using the same datasets. We observe that the performance gap of textual methods between different datasets is far bigger than that of structural methods. All methods perform much better in French than in the other two languages. That is because the difference between French and English is much smaller than the others. So French words are easier to be mapped to English by cross-lingual word embedding or machine translation. In addition, although the performances of textual methods are significantly better than that of structural methods, the structural methods are more universal in practice. Because the current datasets are all sampled from Wikipedia, the textual information such as entity names is too simple for Google translation or cross-lingual embedding whose training corpus are also sampled from Wikipedia. In reality, textual information often is not available,   <ref type="table">Table 9</ref>: Ablation experiment of RREA (basic) on DBP15K.</p><p>or it is very hard to get a high quality translation. Therefore, we believe that the textual methods should be compared separately in studies, rather than with the structural methods together. Ablation Studies. In the above experiments, we have shown the overall success of RREA. In this part, we want to demonstrate the effectiveness of each component in RREA (basic). As mentioned in Section 5, RREA (basic) has three designs compared with GCN-Align: (1) Cross-domain Similarity Local Scaling; (2) Relational Reflection Aggregate Layer; (3) Dual-Aspect Embedding. Starting from GCN-Align baseline, we gradually adopt these components and report the results with Means ±stds. in <ref type="table">Table 9</ref>. Obviously, all of these three designs significantly improve performance. Compared to GCN-Align, the introduction of CSLS improve performance by about 4%. That shows the high correlation between entity alignment task and cross-lingual word embedding. Adding Relational Reflection Aggregate Layer and Dual-Aspect Embedding to the model further brings about 15% and 7% improvement on Hits@1 respectively. This means that both of the two designs introduce unique information into the model. These ablation experiments show that our designs are meaningful and bring significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Robustness Analysis</head><p>Robustness on Pre-aligned Ratio. Generally speaking, building pre-aligned seeds is a high resource-consuming operation. Especially when practicing in the real-world, the KGs usually have millions of entities, relations, and triples. Therefore, we hope that the model could perform well in a lower pre-aligned resource situation. To investigate the robustness of RREA in different pre-aligned ratios, we compare the performance of three GNN-based methods on DBP15K (MuGNN, MRAEA, and RREA (basic)) with different ratios of pre-aligned pairs. <ref type="figure">Figure 5</ref> reports their performance when reserving 10% to 40% of pre-aligned pairs as training data on each of three cross-lingual datasets. Obviously, RREA significantly outperforms compared methods in all pre-aligned ratios of training data. With only 10% pre-aligned pairs, RREA (basic) still achieves more than 52% Hits@1 on DBP15K, which even better than the performance of MuGNN in 40% pre-aligned ratio. Robustness on Hyper-parameter. In order to investigate the robustness of RREA on hyper-parameters, we evaluate the performance on DBP15K varying the number of layer l and the margin λ while keeping the other hyper-parameters consistent with the default setting. The experiment results are shown in <ref type="figure" target="#fig_5">Figure 6</ref>. For layer depth l, RREA with 2 layers achieves the best performance on all datasets. When stacking more layers, the performance begins to decrease slightly. Stacking more layers only results in slower speed, not better performance. For margin λ, when λ is set to 2.0∼4.0, the performance gap is less than 1%. In general, the impact of l and λ on performance is limited and the model is relatively stable during the varying of hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we raise the counter-intuitive phenomena in entity alignment, which are neglected by previous studies. By abstracting existing entity alignment methods into a unified framework, we successfully explain the questions and derive two key criteria for transformation operation in entity alignment: relational differentiation and dimensional isometry. Inspired by these findings, we propose a novel GNNs-based method, Relational Reflection Entity Alignment (RREA) which leverages a new transformation operation called relational reflection. The experimental results show that our model is ranked consistently as the best across all real-world datasets and outperforms the state-of-the-art method more than 5.8% on Hits@1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Decomposition of existing alignment methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The unified framework of entity alignment and representative alignment methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The distributions obtained by GCN-Align on DBP F R−E N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The illustration of relational reflection operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>dist e i , e j = h Mul e i − h Mul e j 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Hyper-parameter studies on DBP15K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistical data of DBP15K and DWY100K.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>The SS on different datasets and metrics. "w/o T" represents without training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Performances on DBP15K with different constraints and initializations. "Unconst." represents unconstrained.</figDesc><table /><note>"Orth. Init." represents orthogonal initialization.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation experiment on complex methods 5 .</figDesc><table /><note>unit constraint settings with MuGNN, KECG, and AliNet. Origi- nally, MuGNN and KECG adopt diagonal constraint while AliNet is unconstrained. The experimental results are shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Performance of different KGs embedding models in entity alignment. * represents the result is taken from Sun et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>A summary of some representative KGs embedding models. ∥ represents the concatenate operation. * and ω rep- resent the convolution operation and kernel. d i represents the degree of entity e i .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Data Split and Metrics: Following previous studies, we randomly split 30% of the pre-aligned entity pairs as training data and left the remaining data for testing. The reported performance is the average of five independent training runs and the train/test datasets are shuffled in every round. We also use Hits@k and Mean Reciprocal Rank (MRR) to be the evaluation metrics as previous works. Hits@k represents the percentage of correctly aligned entities to the top-k potential entities. The higher the Hits@k and MRR, the better the performance.</figDesc><table><row><cell>Hyper-parameters Selection: We select the hyper-parameters</cell></row><row><cell>with the following candidate sets: embedding dimension d ∈ {75,</cell></row><row><cell>100, 150, 200}, margin λ ∈ {1.0, 2.0, 3.0, 4.0}, learning rate γ ∈</cell></row><row><cell>{0.001, 0.005, 0.01}, GNN's depth l ∈ {1, 2, 3, 4}, dropout rate µ ∈</cell></row><row><cell>{0.2, 0.3, 0.4, 0.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Experimental results of textual methods.</figDesc><table><row><cell>6.3 Main Results and Ablation Studies</cell></row></table><note>RREA vs. Basic and Semi-supervised Methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10 MRR Basic △JAPE 0.411 0.744 0.490 0.362 0.685 0.476 0.323 0.666 0.430 0.318 0.589 0.411 0.236 0.484 0.320 GCN-Align 0.412 0.743 0.549 0.399 0.744 0.546 0.372 0.744 0.532 0.506 0.772 0.600 0.597 0.838 0.682</figDesc><table><row><cell></cell><cell cols="8">DBP ZH−EN H@1 H@10 △RSN Method 0.508 0.745 0.591 0.507 0.737 0.590 0.516 0.768 0.605 0.607 0.793 0.673 0.689 0.878 0.756 DBP JA−EN DBP FR−EN DWY WD DWY YG</cell></row><row><cell></cell><cell>MuGNN</cell><cell cols="7">0.494 0.844 0.611 0.501 0.857 0.621 0.495 0.870 0.621 0.616 0.897 0.714 0.741 0.937 0.810</cell></row><row><cell></cell><cell>KECG</cell><cell cols="7">0.477 0.835 0.598 0.489 0.844 0.610 0.486 0.851 0.610 0.632 0.899 0.726 0.728 0.915 0.795</cell></row><row><cell></cell><cell>AliNet</cell><cell cols="7">0.539 0.826 0.628 0.549 0.831 0.645 0.552 0.852 0.657 0.690 0.908 0.766 0.786 0.943 0.841</cell></row><row><cell></cell><cell cols="8">△TransEdge 0.659 0.903 0.748 0.646 0.907 0.741 0.649 0.921 0.746 0.692 0.898 0.770 0.726 0.909 0.792</cell></row><row><cell></cell><cell>MRAEA</cell><cell>0.638 0.886 0.736 0.646 0.891 0.735 0.666 0.912 0.765</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>RREA</cell><cell cols="7">0.715 0.929 0.794 0.713 0.933 0.793 0.739 0.946 0.816 0.753 0.945 0.824 0.839 0.968 0.887</cell></row><row><cell></cell><cell>Improv.</cell><cell cols="7">8.49% 2.88% 6.15% 10.4% 2.87% 7.02% 10.9% 3.73% 6.67% 9.13% 4.07% 7.57% 6.74% 2.65% 5.47%</cell></row><row><cell></cell><cell>△BootEA</cell><cell cols="7">0.629 0.847 0.703 0.622 0.853 0.701 0.653 0.874 0.731 0.747 0.898 0.801 0.761 0.894 0.808</cell></row><row><cell></cell><cell>NAEA</cell><cell cols="7">0.650 0.867 0.720 0.641 0.872 0.718 0.673 0.894 0.752 0.767 0.917 0.817 0.778 0.912 0.821</cell></row><row><cell>Semi</cell><cell cols="8">△TransEdge 0.735 0.919 0.801 0.719 0.932 0.795 0.710 0.941 0.796 0.788 0.938 0.824 0.792 0.936 0.832 MRAEA 0.757 0.930 0.827 0.758 0.934 0.826 0.781 0.948 0.849 ------</cell></row><row><cell></cell><cell>RREA</cell><cell cols="7">0.801 0.948 0.857 0.802 0.952 0.858 0.827 0.966 0.881 0.854 0.966 0.877 0.874 0.976 0.913</cell></row><row><cell></cell><cell>Improv.</cell><cell cols="7">5.81% 1.94% 3.63% 5.80% 1.93% 3.87% 5.89% 1.90% 3.77% 8.37% 2.99% 6.43% 10.3% 4.27% 9.73%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Experimental results of basic and semi-supervised methods. "Improv." represents the percentage increase compared with SOTA. △ represents translation-based methods. Hits@1 performances of different pre-aligned ratios on DBP15K. ±.002 .574 ±.002 .464 ±.003 .588 ±.002 .463 ±.004 .596 ±.005 +CSLS .487 ±.002 .601 ±.002 .507 ±.003 .620 ±.002 .503 ±.004 .487 ±.005 +Rel. Refl. .631 ±.002 .724 ±.002 .644 ±.005 .738 ±.003 .667 ±.004 .761 ±.003 +D-A Emb. .715 ±.002 .794 ±.001 .713 ±.001 .793 ±.002 .739 ±.002 .816 ±.001</figDesc><table><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell></row><row><cell>Hits@1</cell><cell>50 60</cell><cell></cell><cell></cell><cell></cell><cell>Hits@1</cell><cell>50 60</cell><cell></cell><cell></cell><cell>Hits@1</cell><cell>50 60</cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell></row><row><cell></cell><cell>30</cell><cell cols="2">MuGNN</cell><cell>MRAEA</cell><cell>RREA</cell><cell>30</cell><cell>MuGNN</cell><cell>MRAEA</cell><cell>RREA</cell><cell>30</cell><cell>MuGNN</cell><cell>MRAEA</cell><cell>RREA</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell>20%</cell><cell></cell><cell>30%</cell><cell>40%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell></row><row><cell></cell><cell></cell><cell cols="3">(a) DBP ZH-EN</cell><cell></cell><cell></cell><cell cols="2">(b) DBP JA-EN</cell><cell></cell><cell></cell><cell cols="2">(b) DBP FR-EN</cell></row><row><cell cols="6">Figure 5: Method DBP ZH−EN DBP JA−EN</cell><cell cols="2">DBP FR−EN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Hits@1</cell><cell>MRR</cell><cell>Hits@1</cell><cell>MRR</cell><cell>Hits@1</cell><cell>MRR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GCN-Align</cell><cell>.449</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Hereafter, ∥ * ∥ means L1 or L2 norm unless explicitly specified.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Although Wang et al.<ref type="bibr" target="#b28">[29]</ref> retain W in the paper, it is actually removed from the released code.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In our experiment, dropout rate is set to 30%. There's no dropout in original code of GCN-Align, so our experiment results are higher than that in origin paper.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multi-Channel Graph Neural Network for Entity Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-lingual Entity Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Zaniolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Zaniolo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning a Similarity Metric Discriminatively, with Application to Face Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep Graph Matching Consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<idno>abs/2001.09621</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingbing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dimensionality Reduction by Learning an Invariant Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Word translation without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised Entity Alignment via Joint Knowledge Embedding Model and Cross-graph Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">MRAEA: An Efficient and Robust Entity Alignment Approach for Cross-lingual Knowledge Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exploiting Similarities among Languages for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving Cross-lingual Entity Alignment via Optimal Transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael Sejr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cross-Lingual Entity Alignment via Joint Attribute-Preserving Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Bootstrapping Entity Alignment with Knowledge Graph Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">TransEdge: Translating Relation-Contextualized Embeddings for Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingbing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08936</idno>
		<title level="m">Knowledge Graph Alignment Network with Gated Multi-hop Neighborhood Aggregation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Cross-lingual Knowledge Graph Alignment via Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingsong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relation-Aware Entity Alignment for Heterogeneous Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Jointly Learning Entity and Relation Representations for Entity Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiye</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cross-lingual Knowledge Graph Alignment via Graph Matching Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiu-Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06575</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Aligning Cross-Lingual Entities with Multi-Aspect Information. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neighborhood-Aware Attentional Representation for Multilingual Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiannan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
