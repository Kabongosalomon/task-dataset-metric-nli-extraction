<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Universal Dependency Parsing from Scratch</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
							<email>pengqi@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
							<email>tdozat@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
							<email>yuhaozhang@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<email>manning@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Universal Dependency Parsing from Scratch</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes Stanford's system at the CoNLL 2018 UD Shared Task. We introduce a complete neural pipeline system that takes raw text as input, and performs all tasks required by the shared task, ranging from tokenization and sentence segmentation, to POS tagging and dependency parsing. Our single system submission achieved very competitive performance on big treebanks. Moreover, after fixing an unfortunate bug, our corrected system would have placed the 2 nd , 1 st , and 3 rd on the official evaluation metrics LAS, MLAS, and BLEX, and would have outperformed all submission systems on lowresource treebank categories on all metrics by a large margin. We further show the effectiveness of different model components through extensive ablation studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dependency parsing is an important component in various natural langauge processing (NLP) systems for semantic role labeling <ref type="bibr" target="#b12">(Marcheggiani and Titov, 2017)</ref>, relation extraction <ref type="bibr" target="#b20">(Zhang et al., 2018)</ref>, and machine translation <ref type="bibr" target="#b3">(Chen et al., 2017)</ref>. However, most research has treated dependency parsing in isolation, and largely ignored upstream NLP components that prepare relevant data for the parser, e.g., tokenizers and lemmatizers <ref type="bibr" target="#b19">(Zeman et al., 2017)</ref>. In reality, however, these upstream systems are still far from perfect.</p><p>To this end, in our submission to the CoNLL 2018 UD Shared Task, we built a raw-textto-CoNLL-U pipeline system that performs all tasks required by the Shared Task (Zeman et al., * These authors contributed roughly equally. 2018). <ref type="bibr">1</ref> Harnessing the power of neural systems, this pipeline achieves competitive performance in each of the inter-linked stages: tokenization, sentence and word segmentation, partof-speech (POS)/morphological features (UFeats) tagging, lemmatization, and finally, dependency parsing. Our main contributions include:</p><p>• New methods for combining symbolic statistical knowledge with flexible, powerful neural systems to improve robustness; • A biaffine classifier for joint POS/UFeats prediction that improves prediction consistency; • A lemmatizer enhanced with an edit classifier that improves the robustness of a sequenceto-sequence model on rare sequences; and • Extensions to our parser from <ref type="bibr" target="#b5">(Dozat et al., 2017)</ref> to model linearization. Our system achieves competitive performance on big treebanks. After fixing an unfortunate bug, the corrected system would have placed the 2 nd , 1 st , and 3 rd on the official evaluation metrics LAS, MLAS, and BLEX, and would have outperformed all submission systems on low-resource treebank categories on all metrics by a large margin. We perform extensive ablation studies to demonstrate the effectiveness of our novel methods, and highlight future directions to improve the system. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>In this section, we present detailed descriptions for each component of our neural pipeline system, namely the tokenizer, the POS/UFeats tagger, the lemmatizer, and finally the dependency parser. <ref type="bibr">1</ref> We chose to develop a pipeline system mainly because it allows easier parallel development and faster model tuning in a shared task context. <ref type="bibr">2</ref> To facilitate future research, we make our implementation public at: https://github.com/stanfordnlp/ stanfordnlp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tokenizer</head><p>To prepare sentences in the form of a list of words for downstream processing, the tokenizer component reads raw text and outputs sentences in the CoNLL-U format. This is achieved with two subsystems: one for joint tokenization and sentence segmentation, and the other for splitting multiword tokens into syntactic words.</p><p>Tokenization and sentence segmentation. We treat joint tokenization and sentence segmentation as a unit-level sequence tagging problem. For most languages, a unit of text is a single character; however, in Vietnamese orthography, the most natural units of text are single syllables. <ref type="bibr">3</ref> We assign one out of five tags to each of these units: end of token (EOT), end of sentence (EOS), multi-word token (MWT), multi-word end of sentence (MWS), and other (OTHER). We use bidirectional LSTMs (BiLSTMs) as the base model to make unit-level predictions. At each unit, the model predicts hierarchically: it first decides whether a given unit is at the end of a token with a score s (tok) , then classifies token endings into finer-grained categories with two independent binary classifiers: one for sentence ending s (sent) , and one for MWT s <ref type="bibr">(MWT)</ref> .</p><p>Since sentence boundaries and MWTs usually require a larger context to determine (e.g., periods following abbreviations or the ambiguous word "des" in French), we incorporate token-level information into a two-layer BiLSTM as follows (see also <ref type="figure">Figure 1</ref>). The first layer BiLSTM operates directly on raw units, and makes an initial prediction over the categories. To help capture local unit patterns more easily, we also combine the firstlayer BiLSTM with 1-D convolutional networks, by using a one hidden layer convolutional network (CNN) with ReLU nolinearity at its first layer, giving an effect a little like a residual connection <ref type="bibr" target="#b9">(He et al., 2016)</ref>. The output of the CNN is simply added to the concatenated hidden states of the Bi-LSTM for downstream computation:</p><formula xml:id="formula_0">h RNN 1 = [ − → h 1 , ← − h 1 ] = BiLSTM 1 (x),<label>(1)</label></formula><formula xml:id="formula_1">h CNN 1 = CNN(x),<label>(2)</label></formula><formula xml:id="formula_2">h 1 = h RNN 1 + h CNN 1 ,<label>(3)</label></formula><formula xml:id="formula_3">[s (tok) 1 , s (sent) 1 , s (MWT) 1 ] = W 1 h 1 ,<label>(4)</label></formula><p>where x is the input character representations, <ref type="bibr">3</ref> In this case, we define a syllable as a consecutive run of alphabetic characters, numbers, or individual symbols, together with any leading white spaces before them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BiLSTM 1</head><p>Step t Step t <ref type="figure">Figure 1</ref>: Illustration of the tokenizer/sentence segmenter model. Components in blue represent the gating mechanism between the two layers.</p><formula xml:id="formula_4">Input Unit t 1-D CNN ! ",$ (&amp;'()) ! ",$ (+,-) ! ",$ ()./) ! 0,$ (&amp;'()) ! 0,$ (+,-) ! 0,$ ()./) ! $ (&amp;'()) ! $ (+,-) ! $ (</formula><p>and W 1 contains the weights and biases for a linear classifier. 4 For each unit, we concatenate its trainable embedding with a four-dimensional binary feature vector as input, each dimension corresponding to one of the following feature functions: (1) does the unit start with whitespace; (2) does it start with a capitalized letter; (3) is the unit fully capitalized; and (4) is it purely numerical.</p><p>To incorporate token-level information at the second layer, we use a gating mechanism to suppress representations at non-token boundaries before propagating hidden states upward:</p><formula xml:id="formula_5">g 1 = h 1 σ(s (tok) 1 ) (5) h 2 = [ − → h 2 , ← − h 2 ] = BiLSTM 2 (g 1 ), (6) [s (tok) 2 , s (sent) 2 , s (MWT) 2 ] = W 2 h 2 ,<label>(7)</label></formula><p>where is an element-wise product broadcast over all dimensions of h 1 for each unit. This can be viewed as a simpler alternative to multiresolution RNNs <ref type="bibr" target="#b16">(Serban et al., 2017)</ref>, where the first-layer BiLSTM operates at the unit level, and the second layer operates at the token level. Unlike multi-resolution RNNs, this formulation is end-toend differentiable, and can more easily leverage efficient off-the-shelf RNN implementations.</p><p>To combine predictions from both layers of the BiLSTM, we simply sum the scores to obtain s (X) = s </p><formula xml:id="formula_6">p EOT = p +−− p EOS = p ++− ,<label>(8)</label></formula><formula xml:id="formula_7">p MWT = p +−+ p MWS = p +++ ,<label>(9)</label></formula><p>where p ±±± = σ(±s (tok) )σ(±s (sent) )σ(±s (MWT) ), and σ(·) is the logistic sigmoid function. p OTHER is simply σ(−s <ref type="bibr">(tok)</ref> ). The model is trained to minimize the standard cross entropy loss.</p><p>Multi-word Token Expansion. The tokenizer/ sentence segmenter produces a collection of sentences, each being a list of tokens, some of which are labeled as multi-word tokens (MWTs). We must expand these MWTs into the underlying syntactic words they correspond to (e.g., "im" to "in dem" in German), in order for downstream systems to process them properly. To achieve this, we take a hybrid approach to combine symbolic statistical knowledge with the power of neural systems. The symbolic statistical side is a frequency lexicon. Many languages, like German, have only a handful of rules for expanding a few MWTs. We leverage this information by simply counting the number of times a MWT is expanded into different sequences of words in the training set, and retaining the most frequent expansion in a dictionary to use at test time. When building this dictionary, we lowercase all words in the expansions to improve robustness. However, this approach would fail for languages with rich clitics, a large set of unique MWTs, and/or complex rules for MWT expansion, such as Arabic and Hebrew. We capture this by introducing a powerful neural system.</p><p>Specifically, we train a sequence-to-sequence model using a BiLSTM encoder with an attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> in the form of a multi-layer perceptron (MLP). Formally, the input multi-word token is represented by a sequence of characters x 1 , . . . , x I , and the output syntactic words are represented similarly as a sequence of characters y 1 , . . . , y J , where the words are separated by space characters. Inputs to the RNNs are encoded by a shared matrix of character embeddings E. Once the encoder hidden states h enc are obtained with a single-layer BiLSTM, each decoder step is unrolled as follows:</p><formula xml:id="formula_8">h dec j = LSTM dec (E y j−1 , h dec j−1 ),<label>(10)</label></formula><formula xml:id="formula_9">α ij ∝ exp(u α tanh(W α [h dec j , h enc i ])), (11) c j = i α ij h enc i ,<label>(12)</label></formula><formula xml:id="formula_10">P (y j = w|y &lt;j ) ∝ u w tanh(W [h dec j , c j ]).</formula><p>(13) Here, w is a character index in the output vocabulary, y 0 a special start-of-sequence symbol in the vocabulary, and h dec 0 the concatenation of the last hidden states of each direction of the encoder.</p><p>To bring the symbolic and neural systems together, we train them separately and use the following protocol during evaluation: for each MWT, we first look it up in the dictionary, and return the expansion recorded there if one can be found. If this fails, we retry by lowercasing the incoming token. If that fails again, we resort to the neural system to predict the final expansion. This allows us to not only account for languages with flexible MWTs patterns (Arabic and Hebrew), but also leverage the training set statistics to cover both languages with simpler MWT rules, and MWTs in the flexible languages seen in the training set without fail. This results in a high-performance, robust system for multi-word token expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">POS/UFeats Tagger</head><p>Our tagger follows closely that of <ref type="bibr" target="#b5">(Dozat et al., 2017)</ref>, with a few extensions. As in that work, the core of the tagger is a highway BiLSTM <ref type="bibr" target="#b17">(Srivastava et al., 2015)</ref> with inputs coming from the concatenation of three sources: (1) a pretrained word embedding, from the word2vec embeddings provided with the task when available <ref type="bibr" target="#b13">(Mikolov et al., 2013)</ref>, and from fastText embeddings otherwise <ref type="bibr" target="#b1">(Bojanowski et al., 2017)</ref>; (2) a trainable frequent word embedding, for all words that occurred at least seven times in the training set; and (3) a character-level embedding, generated from a unidirectional LSTM over characters in each word. UPOS is predicted by first transforming each word's BiLSTM state with a fully-connected (FC) layer, then applying an affine classifier:</p><formula xml:id="formula_11">h i = BiLSTM (tag) i (x 1 , . . . , x n ), (14) v (u) i = FC (u) (h i ),<label>(15)</label></formula><formula xml:id="formula_12">P y (u) ik |X = softmax k W (u) v (u) i .<label>(16)</label></formula><p>To predict XPOS, we similarly start with transforming the BiLSTM states with an FC layer. In order to further ensure consistency between the different tagsets (e.g., to avoid a VERB UPOS with an NN XPOS), we use a biaffine classifier, conditioned on a word's XPOS state as well as an embedding for its gold (at training time) or predicted (at inference time) UPOS tag y</p><formula xml:id="formula_13">(u) i * : v (x) i = FC (x) (h i ),<label>(17)</label></formula><formula xml:id="formula_14">s (x) i = [E (u) y (u) i * , 1] U (x) [v (x) i , 1], (18) P y (x) ik |y (u) i * , X = softmax k s (x) i .<label>(19)</label></formula><p>UFeats is predicted analogously with separate parameters for each individual UFeat tag. The tagger is also trained to minimize the cross entropy loss. Some languages have composite XPOS tags, yielding a very large XPOS tag space (e.g., Arabic and Czech). For these languages, the biaffine classifier requires a prohibitively large weight tensor U <ref type="bibr">(x)</ref> . For languages that use XPOS tagsets with a fixed number of characters, we classify each character of the XPOS tag in the same way we classify each UFeat. For the rest, instead of taking the biaffine approach, we simply share the FC layer between all three affine classifiers, hoping that the learned features for one will be used by another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Lemmatizer</head><p>For the lemmatizer, we take a very similar approach to that of the multi-word token expansion component introduced in Section 2.1 with two key distinctions customized to lemmatization.</p><p>First, we build two dictionaries from the training set, one from a (word, UPOS) pair to the lemma, and the other from the word itself to the lemma. During evaluation, the predicted UPOS is used. When the UPOS-augmented dictionary fails, we fall back to the word-only dictionary before resorting to the neural system. In looking up both dictionaries, the word is never lowercased, because case information is more relevant in lemmatization than in MWT expansion.</p><p>Second, we enhance the neural system with an edit classifier that shortcuts the prediction process to accommodate rare, long words, on which the decoder is more likely to flounder. The concatenated encoder final states are put through an FC layer with ReLU nonlinearity and fed into a 3way classifier, which predicts whether the lemma is (1) exactly identical to the word (e.g., URLs and emails), (2) the lowercased version of the word (e.g., capitalized rare words in English that are not proper nouns), or (3) in need of the sequence-tosequence model to make more complex edits to the character sequence. During training time, we assign the labels to each word-lemma pair greedily in the order of identical, lowercase, and sequence decoder, and train the classifier jointly with the sequence-to-sequence lemmatizer. At evaluation time, predictions are made sequentially, i.e., the classifier first determines whether any shortcut can be taken, before the sequence decoder model is used if needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Dependency Parser</head><p>The dependency parser also follows that of <ref type="bibr" target="#b5">(Dozat et al., 2017)</ref> with a few augmentations. The highway BiLSTM takes as input pretrained word embeddings, frequent word and lemma embeddings, character-level word embeddings, summed XPOS and UPOS embeddings, and summed UFeats embeddings. In <ref type="bibr" target="#b5">(Dozat et al., 2017)</ref>, unlabeled attachments are predicted by scoring each word i and its potential heads with a biaffine transformation</p><formula xml:id="formula_15">h t = BiLSTM (parse) t (x 1 , . . . , x n ), (20) v (ed) i , v (eh) j = FC (ed) (h i ), FC (eh) (h j ),<label>(21)</label></formula><formula xml:id="formula_16">s (e) ij = [v (eh) j , 1] U (e) [v (ed) i , 1],<label>(22)</label></formula><formula xml:id="formula_17">= Deep-Biaff (e) (h i , h j ),<label>(23)</label></formula><formula xml:id="formula_18">P y (e) ij |X = softmax j s (e) i ,<label>(24)</label></formula><p>where</p><formula xml:id="formula_19">v (ed) i is word i's edge-dependent repre- sentation and v (eh) i its edge-head representation.</formula><p>This approach, however, does not explicitly take into consideration relative locations of heads and dependents during prediction; instead, such predictive location information must be implicitly learned by the BiLSTM. Ideally, we would like the model to explicitly condition on (i − j), namely the dependent i and its potential head j's location relative to each other, in modeling p(y ij ). <ref type="bibr">5</ref> Here, we motivate one way to build this into the model. First we factorize the relative location of word i and head j into their linear order and the distance between them, i.e., P (y ij |sgn(i − j), abs(i − j)), where sgn(·) is the sign function. Applying Bayes' rule and assuming conditional independence, we arrive at the following</p><formula xml:id="formula_20">P (y ij |sgn(i − j), abs(i − j)) ∝ (25) P (y ij )P (sgn(i − j)|y ij )P (abs(i − j)|y ij ).</formula><p>In a language where heads always follow their dependents, P (sgn(i − j) = 1|y ij ) would be extremely low, heavily penalizing rightward attachments. Similarly, in a language where dependencies are always short, P (abs(i−j) 0|y ij ) would be extremely low, penalizing longer edges.</p><p>P (y ij ) can remain the same as computed in Eq. (24). P (sgn(i − j)|y ij ) can be computed similarly with a deep biaffine scorer (cf. Eqs. (20)-(23)) over the recurrent states. This results in the score of j preceding i; flipping the sign wherever i precedes j turns this into the log odds of the ob-served linearization. Applying the sigmoid function then turns it into a probability:</p><formula xml:id="formula_21">s (l) ij = Deep-Biaff (l) (h i , h j ), (26) s (l) ij = sgn(i − j)s (l) ij ,<label>(27)</label></formula><formula xml:id="formula_22">P (sgn(i − j)|y ij ) = σ s (l) ij .<label>(28)</label></formula><p>This can be effortlessly incorporated into the edge score by adding in the log of this probability − log(1 + exp(−s (l) ij )). Error is not backpropagated to this submodule through the final attachment loss; instead, it is trained with its own cross entropy, with error only computed on gold edges. This ensures that the model learns the conditional probability given a true edge, rather than just learning to predict the linear order of two words.</p><p>For P (abs(i − j)|y ij ), we use another deep biaffine scorer to generate a distance score. Distances are always no less than 1, so we apply 1 + softplus to predict the distance between i and j when there's an edge between them:</p><formula xml:id="formula_23">s (d) ij = Deep-Biaff (d) (h i , h j ),<label>(29)</label></formula><formula xml:id="formula_24">s (d) ij = 1 + softplus s (d) ij .<label>(30)</label></formula><p>where softplus(x) = log(1 + exp(x)). The distribution of edge lengths in the treebanks roughly follows a Zipfian distribution, to which the Cauchy distribution is closely related, only the latter is more stable for values at or near zero. Thus, rather than modeling the probability of an arc's length, we can use the Cauchy distribution to model the probability of an arc's error in predicted length, namely how likely it is for the predicted distance and the true distance to have a difference of δ</p><formula xml:id="formula_25">(d) ij : Zipf(k; α, β) ∝ (k α /β) −1 ,<label>(31)</label></formula><formula xml:id="formula_26">Cauchy(x; γ) ∝ (1 + x 2 /γ) −1 (32) δ (d) ij = abs(i − j) − s (d) ij , (33) P (abs(i − j)|y ij ) ∝ (1 + δ 2(d) ij /2) −1 .<label>(34)</label></formula><p>When the difference δ</p><p>ij is small or zero, there will be effectively no penalty; but when the model expects a significantly longer or shorter arc than the observed distance between i and j, it is discouraged from assigning an edge between them. As with the linear order probability, the log of the distance probability is added to the edge score, and trained with its own cross-entropy on gold edges. 6 6 Note that the penalty assigned to the edge score in this way is proportional to ln δ At inference time, the Chu-Liu/Edmonds algorithm <ref type="bibr" target="#b4">(Chu and Liu, 1965;</ref><ref type="bibr">Edmonds, 1967)</ref> is used to ensure a maximum spanning tree. Dependency relations are assigned to gold (at training time) or predicted (at inference time) edges y (e) i * using another deep biaffine classifier, following <ref type="bibr" target="#b5">(Dozat et al., 2017)</ref> with no augmentations:</p><formula xml:id="formula_28">s (r) i = Deep-Biaff (r) h i , h y (e) i * , (35) P y (r) ik |y (e) i * = softmax k s (r) i .<label>(36)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training Details</head><p>Except where otherwise stated, our system is a pipeline: given a document of raw text, the tokenizer/sentence segmenter/MWT expander first splits it into sentences of syntactic words; the tagger then assigns UPOS, XPOS and UFeat tags to each word; the lemmatizer takes the predicted word and UPOS tag and outputs a lemma; finally, the parser takes all annotations as input and predicts the head and dependency label for each word.</p><p>All components are trained with early stopping on the dev set when applicable. When a dev set is unavailable, we split the training set into an approximately 7-to-1 split for training and development. All components (except the dependency parser) are trained and evaluated on the development set assuming all related components had oracle implementations. This means the tokenizer/sentence segmenter assumes all correctly predicted MWTs will be correctly expanded, the MWT expander assumes gold word segmentation, and all downstream tasks assume gold word segmentation, along with gold annotations of all prerequisite tasks. The dependency parser is trained with predicted tags and morphological features from the POS/UFeats tagger.</p><p>Treebanks without training data. For treebanks without training data, we adopt a heuristic approach for finding replacements. Where a larger treebank in the same language is available (i.e., all PUD treebanks and Japanese-Modern), we used the models from the largest treebank available in that language. Where treebanks in related languages are available (as determined by language families from Wikipedia), we use models from the largest treebank in that related language. We or Poisson distribution to model the distance directly, or using a normal distribution instead of Cauchy, respectively, assigns penalties roughly proportional to δ ended up choosing the models from English-EWT for Naija (an English-based pidgin), Irish-IDT for Breton (both are Celtic), and Norwegian-Nynorsk for Faroese (both are West Scandinavian). For Thai, since it uses a different script from all other languages, we use UDPipe 1.2 for all components.</p><p>Hyperparameters. The tokenizer/sentence segmenter uses BiLSTMs with 64d hidden states in each direction and takes 32d character embeddings as input. During training, we employ dropout to the input embeddings and hidden states at each layer with p = .33. We also randomly replace the input unit with a special &lt;UNK&gt; unit with p = .33, which would be used in place of any unseen input at test time. We add noise to the gating mechanism in Eq. (6) by randomly setting the gates to 1 with p = .02 and setting its temperature to 2 to make the model more robust to tokenization errors at test time. Optimization is performed with Adam (Kingma and Ba, 2015) with an initial learning rate of .002 for up to 20,000 steps, and whenever dev performance deteriorates, as is evaluated every 200 steps after the 2,000 th step, the learning rate is multiplied by .999. For the convolutional component we use filter sizes of 1 and 9, and for each filter size we use 64 channels (same as one direction in the BiLSTM). The convolutional outputs are concatenated in the hidden layer, before an affine transform is applied to serve as a residual connection for the BiLSTM. For the MWT expander, we use BiLSTMs with 256d hidden states in each direction as the encoder, a 512d LSTM decoder, 64d character embeddings as input, and dropout rate p = .5 for the inputs and hidden states. Models are trained up to 100 epochs with the standard Adam hyperparameters, and the learning rate is annealed similarly every epoch after the 15 th epoch by a factor of 0.9. Beam search of beam size 8 is employed in evaluation.</p><p>The lemmatizer uses BiLSTMs with 100d hidden states in each direction of the encoder, 50d character embeddings as input, and dropout rate p = .5 for the inputs and hidden states. The decoder is an LSTM with 200d hidden states. During training we jointly minimize (with equal weights) the cross-entropy loss of the edit classifier and the negative log-likelihood loss of the seq2seq lemmatizer. Models are trained up to 60 epochs with standard Adam hyperparameters.</p><p>The tagger and parser share most of their hyperparameters. We use 75d uncased frequent word and lemma embeddings, and 50d POS tag and UFeat embeddings. Pretrained embeddings and character-based word representations are both transformed to be 125d. During training, all embeddings are randomly replaced with a &lt;drop&gt; symbol with p = .33. We use 2-layer 200d BiL-STMs for the tagger and 3-layer 400d BiLSTMs for the parser. We employ dropout in all feedforward connections with p = .5 and all recurrent connections <ref type="bibr" target="#b8">(Gal and Ghahramani, 2016)</ref> with p = .25 (except p = .5 in the tagger BiLSTM). All classifiers use 400d FC layers (except 100d for UFeats) with the ReLU nonlinearity. We train the systems with Adam (α = .003, β 1 = .9, β 2 = .95) until dev accuracy decreases, at which point we switch to <ref type="bibr">AMSGrad (Reddi et al., 2018)</ref> until 3,000 steps pass with no dev accuracy increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The main results are shown in <ref type="table">Table 1</ref>. As can be seen from the table, our system achieves competitive performance on nearly all of the metrics when macro-averaged over all treebanks. Moreover, it achieves the top performance on several metrics when evaluated only on big treebanks, showing that our systems can effectively leverage statistical patterns in the data. Where it is not the top performing system, our system also achieved competitive results on each of the metrics on these treebanks. This is encouraging considering that our system is comprised of single-system components, whereas some of the best performing teams used ensembles (e.g., HIT-SCIR <ref type="bibr" target="#b2">(Che et al., 2018)</ref>).</p><p>When taking a closer look, we find that our UFeats classifier is very accurate on these treebanks as well. Not only did it achieve the top performance on UFeats F 1 , but also it helped the parser achieve top MLAS as well on big treebanks, even when the parser is not the best-performing as evaluated by other metrics. We also note the contribution from our consistency modeling in the POS tagger/UFeats classifier: in both settings the individual metrics (UPOS, XPOS, and UFeats) achieve a lower advantage margin over the reference systems when compared to the AllTags metric, showing that these reference systems, though sometimes more accurate on each individual task, are not as consistent as our system overall.</p><p>The biggest disparity between the all-treebanks and big-treebanks results comes from sentence  segmentation. After inspecting the results on smaller treebanks and double-checking our implementation, we noticed issues with how we processed data in the tokenizer that negatively impacted generalization on these treebanks. 7 This is devastating for these treebanks, as all downstream components process words at the sentence level. We fixed this issue, and trained new tokenizers with all hyperparameters identical to our system at submission. We further built an unofficial evaluation pipeline, which we verified achieves the same evaluation results as the official system, and eval-7 Specifically, our tokenizer was originally designed to be aware of newlines (\n) in double newline-separated paragraphs, but we accidentally prepared training and dev sets for low resource treebanks by putting each sentence on its own line in the text file. This resulted in the sentence segmenter overfitting to relying on newlines. In later experiments, we replaced all in-paragraph whitespaces with space characters. uated our entire pipeline by only replacing the tokenizer. As is shown in Table 1, the resulting system (Stanford+) is much more accurate overall, and we would have ranked 2 nd , 1 st , and 3 rd on the official evaluation metrics LAS, MLAS, and BLEX, respectively. 8 On big treebanks, all metrics changed within only 0.02% F 1 and are thus not included. On small treebanks, however, this effect is more pronounced: as is shown in Table 2, our corrected system outperforms all submission systems on all official evaluation metrics on all low-resource treebanks by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section, we perform ablation studies on the new approaches we proposed for each component, and the contribution of each component to the final pipeline. For each component, we assume access to an oracle for all other components in the analysis, and show their efficacy on the dev sets. 9 For the ablations on the pipeline, we report macroaveraged F 1 on the test set.   Tokenizer. We perform ablation studies on the less standard components in the tokenizer, namely the gating mechanism in Eq. (6) (gating), the convolutional residual connections (conv), and the seq2seq model in the MWT expander (seq2seq), on all 61 big treebanks. As can be seen in <ref type="table" target="#tab_4">Table 3</ref>, all but the gating mechanism make noticeable differences in macro F 1 . When taking a closer look, we find that both gating and conv show a mixed contribution to each treebank, and we could have improved overall performance further through treebank-level component selection. One surprising discovery is that conv greatly helps identify MWTs in Hebrew (+34.89 Words F 1 ) and sentence breaks in Ancient Greek-PROIEL (+18.77 Sents F 1 ). In the case of seq2seq, although the overall macro difference is small, it helps with the word segmentation performance on all treebanks where it makes any meaningful difference, most notably +10.08 on Hebrew and +4.19 on Arabic in Words F 1 (see also <ref type="figure" target="#fig_3">Figure 2</ref>). Finally, we note that dropout plays an important role in safeguarding the tokenizer from overfitting.  large, composite XPOS tagsets would incur prohibitive memory requirements. We therefore exclude treebanks that either have more than 250 XPOS tags or don't use them, leaving 36 treebanks for this analysis. We also measure consistency between tags by their pointwise mutual information</p><formula xml:id="formula_29">PMI = log p c (AllTags) p c (UPOS)p c (XPOS)p c (UFeats) ,</formula><p>where p c (X) is the accuracy of X. This quantifies (in nats) how much more likely it is to get all tags right than we would expect given their individual accuracies, if they were independent. As can be seen in <ref type="table">Table 4</ref>, the added parameters do not affect UPOS performance significantly, but do help improve XPOS and UFeats prediction. Moreover, the biaffine classifier is markedly more consistent than the affine one with shared representations.</p><p>Lemmatizer. We perform ablation studies on three individual components in our lemmatizer: the edit classifier (edit), the sequence-to-sequence module (seq2seq) and the dictionaries (dictionaries). As shown in <ref type="table" target="#tab_6">Table 5</ref>, we find that our lemmatizer with all components achieves the best overall performance. Specifically, adding the neural components (i.e., edit &amp; seq2seq) drastically improves overall lemmatization performance over a simple dictionary-based approach (+6.77 F 1 ), and the gains are consistent over different treebank groups. While adding the edit classifier slightly decreases the F 1 score on small treebanks, it improves the performance on lowresource languages substantially (+0.91 F 1 ), and therefore leads to an overall gain of 0.11 F 1 . Tree-  banks where the largest gains are observed include Upper Sorbian-UFAL (+4.55 F 1 ), Kurmanji-MG (+2.27 F 1 ) and English-LinES (+2.16 F 1 ). Finally, combining the neural lemmatizer with dictionaries helps capture common lemmatization patterns seen during training, leading to substantial improvements on all treebank groups. To further understand the behavior of the edit classifier, for each treebank we present the ratio of all predicted edit types on dev set words in <ref type="figure">Figure</ref> 3. We find that the behavior of the edit classifier aligns well with linguistic knowledge. For example, while Ancient Greek, Arabic and Korean require a lot of complex edits in lemmatization, the vast majority of operations in Chinese and Japanese are simple identity mappings.</p><p>Dependency Parser. The main innovation for the parsing module is terms that model locations of a dependent word relative to possible head words in the sentence. Here we examine the impact of these terms, namely linearization (Eq. (28)) and distance (Eq. (34)). For this analysis, we exclude six treebanks with very small dev sets. As can be seen in <ref type="table" target="#tab_7">Table 6</ref>, both terms contribute significantly to the final parser performance, with the distance term contributing slightly more.</p><p>Pipeline Ablation. We analyze the contribution of each pipeline component by incrementally replacing them with gold annotations and observing performance change. As shown in <ref type="figure" target="#fig_5">Figure 4</ref>, most downstream systems benefit moderately from gold sentence and word segmentation, while the parser largely only benefits from improved POS/UFeats tagger performance (aside from BLEX, which is directly related to lemmatization performance and benefits notably). Finally, we note that the parser still is far from perfect even given gold annotations from all upstream tasks, but our components in the pipeline are very effective at closing the gap between predicted and gold annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion &amp; Future Directions</head><p>In this paper, we presented Stanford's submission to the CoNLL 2018 UD Shared Task. Our submission consists of neural components for each stage of a pipeline from raw text to dependency parses.</p><p>The final system was very competitive on big treebanks; after fixing our preprocessing bug, it would have outperformed all official systems on all metrics for low-resource treebank categories. One of the greatest opportunities for further gains is through the use of context-sensitive word embeddings, such as ELMo <ref type="bibr" target="#b14">(Peters et al., 2018)</ref> and ULMfit <ref type="bibr" target="#b10">(Howard and Ruder, 2018)</ref>. Although this requires a large resource investment, HIT-SCIR <ref type="bibr" target="#b2">(Che et al., 2018)</ref> has shown solid improvements from incorporating these embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>X ∈ {tok, sent, MWT}. The final probability over the tags is then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(d) ij , ln Γ(δ (d) ij ), and δ 2(d) ij .Thus, the Cauchy is more numerically stable during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Effect of the seq2seq component for MWT expansion in the tokenizer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Edit operation types as output by the edit classifier on the official dev set. Due to space limit only treebanks containing over 120k dev words are shown and sorted by the ratio of seq2seq operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>SFigure 4 :</head><label>4</label><figDesc>ta n fo rd + + g o ld to k + g o ld ta g + g o ld le m m a + g o ld p a rs e Pipeline ablation results. Dashed, dotted, and solid lines represent tagger, lemmatizer, and parser metrics, respectively. Official evaluation metrics are highlighted with thickened lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>) on the test set, on all treebanks and big treebanks only. For each set of results on all metrics, we compare it against results from reference systems. A reference system is the top performing system on that metric if we are not top, or the second-best performing system on that metric. Reference systems are identified by superscripts ( †: HIT-SCIR, ‡: Uppsala, : TurkuNLP, * : UDPipe Future). Shaded columns in the table indicate the three official evaluation metrics. "Stanford+" is our system after a bugfix evaluated unofficially; for more details please see the main text.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(a) Results on all treebanks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>System</cell><cell>Tokens</cell><cell>Sent</cell><cell>Words</cell><cell>Lemmas</cell><cell>UPOS</cell><cell>XPOS</cell><cell>UFeats</cell><cell>AllTags</cell><cell>UAS</cell><cell>CLAS</cell><cell>LAS</cell><cell>MLAS</cell><cell>BLEX</cell></row><row><cell>Stanford</cell><cell>96.19</cell><cell>76.55</cell><cell>95.99</cell><cell>88.32</cell><cell>89.01</cell><cell>85.51</cell><cell>85.47</cell><cell>79.71</cell><cell>76.78</cell><cell>68.73</cell><cell>72.29</cell><cell>60.92</cell><cell>64.04</cell></row><row><cell>Reference</cell><cell>98.42  †</cell><cell>83.87  †</cell><cell>98.18  ‡</cell><cell>91.24</cell><cell>90.91  ‡</cell><cell>86.67  *</cell><cell>87.59  ‡</cell><cell>80.30  *</cell><cell>80.51  †</cell><cell>72.36  †</cell><cell>75.84  †</cell><cell>61.25  *</cell><cell>66.09</cell></row><row><cell>∆</cell><cell>-2.23</cell><cell>-7.32</cell><cell>-2.19</cell><cell>-2.92</cell><cell>-1.90</cell><cell>-1.16</cell><cell>-2.12</cell><cell>-0.59</cell><cell>-3.73</cell><cell>-3.63</cell><cell>-3.55</cell><cell>-0.33</cell><cell>-2.05</cell></row><row><cell>Stanford+</cell><cell>97.42</cell><cell>85.46</cell><cell>97.23</cell><cell>89.17</cell><cell>89.95</cell><cell>86.50</cell><cell>86.20</cell><cell>80.36</cell><cell>79.04</cell><cell>70.39</cell><cell>74.16</cell><cell>62.08</cell><cell>65.28</cell></row><row><cell>∆</cell><cell>-1.00</cell><cell>+1.59</cell><cell>-0.95</cell><cell>-2.07</cell><cell>-0.96</cell><cell>-0.17</cell><cell>-1.39</cell><cell>+0.06</cell><cell>-1.47</cell><cell>-1.97</cell><cell>-1.68</cell><cell>+0.83</cell><cell>-0.81</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) Results on big treebanks only</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>System</cell><cell>Tokens</cell><cell>Sent</cell><cell>Words</cell><cell>Lemmas</cell><cell>UPOS</cell><cell>XPOS</cell><cell>UFeats</cell><cell>AllTags</cell><cell>UAS</cell><cell>CLAS</cell><cell>LAS</cell><cell>MLAS</cell><cell>BLEX</cell></row><row><cell>Stanford</cell><cell>99.43</cell><cell>89.52</cell><cell>99.21</cell><cell>95.25</cell><cell>95.93</cell><cell>94.95</cell><cell>94.14</cell><cell>91.50</cell><cell>86.56</cell><cell>79.60</cell><cell>83.03</cell><cell>72.67</cell><cell>75.46</cell></row><row><cell>Reference</cell><cell>99.51  †</cell><cell>87.73  †</cell><cell>99.16  †</cell><cell>96.08</cell><cell>96.23  †</cell><cell>95.16  †</cell><cell>94.11  *</cell><cell>91.45  *</cell><cell>87.61  †</cell><cell>81.29  †</cell><cell>84.37  †</cell><cell>71.71  *</cell><cell>75.83</cell></row><row><cell>∆</cell><cell>-0.08</cell><cell>+1.79</cell><cell>+0.05</cell><cell>-0.83</cell><cell>-0.30</cell><cell>-0.21</cell><cell>+0.03</cell><cell>+0.05</cell><cell>-1.05</cell><cell>-1.69</cell><cell>-1.34</cell><cell>+0.96</cell><cell>-0.37</cell></row><row><cell cols="4">Table 1: Evaluation results (F 1 Treebanks System LAS</cell><cell cols="2">MLAS BLEX</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Small</cell><cell cols="4">Stanford+ 83.90 Reference 69.53  † 49.24  ‡ 72.75</cell><cell>77.30 54.89  ‡</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Low-Res</cell><cell cols="3">Stanford+ 63.20 Reference 27.89</cell><cell>51.64 6.13</cell><cell>53.58 13.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PUD</cell><cell cols="4">Stanford+ 82.25 Reference 74.20  † 58.75  74.20</cell><cell>74.37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* 63.25 •</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Evaluation results (F 1 ) on low-resource</cell></row><row><cell>treebank test sets. Reference systems are identi-</cell></row><row><cell>fied by symbol superscripts ( †: HIT-SCIR,  ‡: ICS</cell></row><row><cell>PAS, : CUNI x-ling,</cell></row></table><note>* : Stanford, •: TurkuNLP).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation results for the tokenizer. All metrics in the table are macro-averaged dev F 1 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation results for the lemmatizer, split by different groups of treebanks. All metrics in the table are macro-averaged dev F 1 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Ablation results for the parser. All metrics in the table are macro-averaged dev F 1 .</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We will omit bias terms in affine transforms for clarity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Henceforth we omit the (e) superscript and X.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We note that the only system that is more accurate than ours on LAS is HIT's ensemble system, and we achieve very close performance to their system on MLAS (only 0.05% F1 lower, which is likely within the statistical variation reported in the official evaluation).9  We perform treebank-level paired bootstrap tests for each ablated system against the top performing system in ablation with 10 5 bootstrap samples, and indicate statistical significance in tables with symbol superscripts (*:p &lt; 0.05, **:p &lt; 0.01, ***:p &lt; 0.001).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">POS/UFeats Tagger. The main novelty in our tagger is the explicit conditioning of XPOS and UFeats predictions on the UPOS prediction. We compare this against a tagger that simply shares the hidden features between the UPOS, XPOS, and UFeats classifiers. Since we used full-rank tensors in the biaffine classifier, treebanks withSystem UPOS XPOS UFeats AllTags PMI Stanford 96.50 95.87 95.01 92.52 .0514 − biaff 96.47 95.71 * 94.13 * * * 91.32 * * * .0497 *Table 4: Ablation results for the tagger. All metrics are macro-averaged dev F 1 , except PMI, which is explained in detail in the main text. System Big Small LowRes All</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/Q17-1010" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards better UD parsing: Deep contextualized word embeddings, ensemble, and treebank concatenation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan Zheng Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved neural machine translation with a syntax-aware encoder and decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the shortest arborescence of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoeng-Jin</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tseng-Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientia Sinica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1396" to="1400" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stanford&apos;s graph-based neural dependency parser at the CoNLL 2017 Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL</title>
		<meeting>the CoNLL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<ptr target="http://www.aclweb.org/anthology/K/K17/K17-3002.pdf" />
		<imprint>
			<biblScope unit="page" from="20" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Journal of Research of the</title>
	</analytic>
	<monogr>
		<title level="j">National Bureau of Standards</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="233" to="240" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
	<note>Optimum branchings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods for Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the convergence of Adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyen</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiresolution recurrent neural networks: An application to dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Deep Learning Workshop at the International Conference on Machine Learning</title>
		<meeting>the Deep Learning Workshop at the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Badmaeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Memduh</forename><surname>Gokirmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Nedoluzhko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvie</forename><surname>Cinkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaroslava</forename><surname>Hlavacova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Václava</forename><surname>Kettnerová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zdenka</forename><surname>Uresova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenna</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stina</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Missilä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Taji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Sanguinetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Simi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeria</forename><surname>De-Paiva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics</title>
		<editor>Kira Droganova, Héctor Martínez Alonso, Ç agr Çöltekin, Umut Sulubacak, Hans Uszkoreit, Vivien Macketanz, Aljoscha Burchardt, Kim Harris, Katrin Marheinecke, Georg Rehm, Tolga Kayadelen, Mohammed Attia, Ali Elkahky, Zhuoran Yu, Emily Pitler, Saran Lertpradit, Michael Mandl, Jesse Kirchner, Hector Fernandez Alcalde, Jana Strnadová, Esha Banerjee, Ruli Manurung, Antonio Stella, Atsuko Shimada, Sookyoung Kwak, Gustavo Mendonca, Tatiana Lando, Rattima Nitisaroj, and Josie Li</editor>
		<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods for Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
