<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RECLOR: A READING COMPREHENSION DATASET REQUIRING LOGICAL REASONING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
							<email>weihaoyu6@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RECLOR: A READING COMPREHENSION DATASET REQUIRING LOGICAL REASONING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent powerful pre-trained language models have achieved remarkable performance on most of the popular datasets for reading comprehension. It is time to introduce more challenging datasets to push the development of this field towards more comprehensive reasoning of text. In this paper, we introduce a new Reading Comprehension dataset requiring logical reasoning (ReClor) extracted from standardized graduate admission examinations. As earlier studies suggest, human-annotated datasets usually contain biases, which are often exploited by models to achieve high accuracy without truly understanding the text. In order to comprehensively evaluate the logical reasoning ability of models on ReClor, we propose to identify biased data points and separate them into EASY set while the rest as HARD set. Empirical results show that state-of-the-art models have an outstanding ability to capture biases contained in the dataset with high accuracy on EASY set. However, they struggle on HARD set with poor performance near that of random guess, indicating more research is needed to essentially enhance the logical reasoning ability of current models. 1 * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Machine reading comprehension (MRC) is a fundamental task in Natural Language Processing, which requires models to understand a body of text and answer a particular question related to the context. With success of unsupervised representation learning in NLP, language pre-training based models such as GPT-2 <ref type="bibr" target="#b30">(Radford et al., 2019)</ref>, BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>, XLNet <ref type="bibr" target="#b46">(Yang et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b21">(Liu et al., 2019)</ref> have achieved nearly saturated performance on most of the popular MRC datasets <ref type="bibr" target="#b31">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b20">Lai et al., 2017;</ref><ref type="bibr" target="#b32">Rajpurkar et al., 2018;</ref><ref type="bibr" target="#b41">Wang et al., 2018)</ref>. It is time to challenge state-of-the-art models with more difficult reading comprehension tasks and move a step forward to more comprehensive analysis and reasoning over text <ref type="bibr" target="#b10">(Dua et al., 2019)</ref>.</p><p>In natural language understanding, logical reasoning is an important ability to examine, analyze and critically evaluate arguments as they occur in ordinary language according to the definition from Law School Admission Council (2019a). It is a significant component of human intelligence and is essential in negotiation, debate and writing etc. However, existing reading comprehension datasets have none or merely a small amount of data requiring logical reasoning, e.g., 0% in MCTest dataset <ref type="bibr" target="#b33">(Richardson et al., 2013</ref>) and 1.2% in SQuAD <ref type="bibr" target="#b31">(Rajpurkar et al., 2016)</ref> according to <ref type="bibr" target="#b37">Sugawara &amp; Aizawa (2016)</ref>. One related task is natural language inference, which requires models to label the logical relationships of sentence pairs. However, this task only considers three types of simple logical relationships and only needs reasoning at sentence-level. To push the development of models in logical reasoning from simple logical relationship classification to multiple complicated logical reasoning and from sentence-level to passage-level, it is necessary to introduce a reading comprehension dataset targeting logical reasoning.</p><p>A typical example of logical reasoning questions is shown in <ref type="table" target="#tab_0">Table 1</ref>. Similar to the format of multiple-choice reading comprehension datasets <ref type="bibr" target="#b33">(Richardson et al., 2013;</ref><ref type="bibr" target="#b20">Lai et al., 2017)</ref>, it contains a context, a question and four options with only one right answer. To answer the question in this example, readers need to identify the logical connections between the lines to pinpoint the conflict, then understand each of the options and select an option that solves the conflict. Human minds need extensive training and practice to get used to complex reasoning, and it will take immense efforts for crowdsourcing workers to design such logical reasoning questions. Inspired by the datasets extracted from standardized examinations <ref type="bibr" target="#b20">(Lai et al., 2017;</ref>, we build a dataset by selecting such logical reasoning questions from standardized exams such as GMAT 2 and LSAT 3 . We finally collect 6,138 pieces of logical reasoning questions, which constitute a Reading Comprehension dataset requiring logical reasoning (ReClor).</p><p>Human-annotated datasets usually contain biases <ref type="bibr" target="#b35">(Schwartz et al., 2017;</ref><ref type="bibr" target="#b4">Cai et al., 2017;</ref><ref type="bibr" target="#b3">Bugert et al., 2017;</ref><ref type="bibr" target="#b28">Poliak et al., 2018;</ref><ref type="bibr" target="#b12">Gururangan et al., 2018;</ref><ref type="bibr" target="#b47">Zellers et al., 2019)</ref>, which are often exploited by neural network models as shortcut solutions to achieve high testing accuracy. For data points whose options can be selected correctly without knowing the contexts and questions, we classify them as biased ones. In order to fully assess the logical reasoning ability of the models, we propose to identify the biased data points and group them as EASY set, and put the rest into HARD set. Based on our experiments on these separate sets, we find that even the state-of-the-art models can only perform well on EASY set and struggle on HARD set as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. This phenomenon shows that current models can well capture the biases in the dataset but lack the ability to understand the text and reason based on connections between the lines. On the other hand, human beings perform similarly on both the EASY and HARD set. It is thus observed that there is still a long way to go to equip models with true logical reasoning ability.</p><p>The contributions of our paper are two-fold. First, we introduce ReClor, a new reading comprehension dataset requiring logical reasoning. We use option-only-input baselines trained with different random seeds to identify the data points with biases in the testing set, and group them as EASY set, with the rest as HARD set to facilitate comprehensive evaluation. Second, we evaluate several stateof-the-art models on ReClor and find these pre-trained language models can perform well on EASY set but struggle on the HARD set. This indicates although current models are good at exploiting biases in the dataset, they are far from capable of performing real logical reasoning yet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Reading Comprehension Datasets. A variety of reading comprehension datasets have been introduced to promote the development of this field. MCTest <ref type="bibr" target="#b33">(Richardson et al., 2013</ref>) is a dataset with 2,000 multiple-choice reading comprehension questions about fictional stories in the format similar to ReClor. <ref type="bibr" target="#b31">Rajpurkar et al. (2016)</ref> proposed SQuAD dataset, which contains 107,785 questionanswer pairs on 536 Wikipedia articles. The authors manually labeled 192 examples of the dataset and found that the examples mainly require reasoning of lexical or syntactic variation. In an analysis of the above-mentioned datasets, <ref type="bibr" target="#b37">Sugawara &amp; Aizawa (2016)</ref> found that none of questions requiring logical reasoning in MCTest dataset <ref type="bibr" target="#b33">(Richardson et al., 2013)</ref> and only 1.2% in SQuAD dataset <ref type="bibr" target="#b31">(Rajpurkar et al., 2016)</ref>. <ref type="bibr" target="#b20">Lai et al. (2017)</ref> introduced RACE dataset by collecting the English exams for middle and high school Chinese students in the age range between 12 to 18. They hired crowd workers on Amazon Mechanical Turk to label the reasoning type of 500 samples in the dataset and show that around 70 % of the samples are in the category of word matching, paraphrasing or single-sentence reasoning. To encourage progress on deeper comprehension of language,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context:</head><p>In jurisdictions where use of headlights is optional when visibility is good, drivers who use headlights at all times are less likely to be involved in a collision than are drivers who use headlights only when visibility is poor. Yet Highway Safety Department records show that making use of headlights mandatory at all times does nothing to reduce the overall number of collisions. Question: Which one of the following, if true, most helps to resolve the apparent discrepancy in the information above?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. In jurisdictions where use of headlights is optional when visibility is good, one driver in four uses headlights for daytime driving in good weather. B. Only very careful drivers use headlights when their use is not legally required. C. The jurisdictions where use of headlights is mandatory at all times are those where daytime visibility is frequently poor. D. A law making use of headlights mandatory at all times is not especially difficult to enforce. Answer: B more reading comprehension datasets requiring more complicated reasoning types are introduced, such as iterative reasoning about the narrative of a story <ref type="bibr" target="#b19">(Kočiskỳ et al., 2018)</ref>, multi-hop reasoning across multiple sentences <ref type="bibr" target="#b18">(Khashabi et al., 2018)</ref> and multiple documents <ref type="bibr" target="#b42">(Welbl et al., 2018)</ref>, commonsense knowledge reasoning <ref type="bibr" target="#b23">(Mihaylov et al., 2018;</ref><ref type="bibr" target="#b48">Zhang et al., 2018;</ref><ref type="bibr" target="#b15">Huang et al., 2019)</ref> and numerical discrete reasoning over paragraphs <ref type="bibr" target="#b10">(Dua et al., 2019)</ref>. However, to the best of our knowledge, although there are some datasets targeting logical reasoning in other NLP tasks mentioned in the next section, there is no dataset targeting evaluating logical reasoning in reading comprehension task. This work introduces a new dataset to fill this gap.</p><p>Logical Reasoning in NLP. There are several tasks and datasets introduced to investigate logical reasoning in NLP. The task of natural language inference, also known as recognizing textual entailment <ref type="bibr" target="#b11">(Fyodorov et al., 2000;</ref><ref type="bibr" target="#b7">Condoravdi et al., 2003;</ref><ref type="bibr" target="#b1">Bos &amp; Markert, 2005;</ref><ref type="bibr" target="#b8">Dagan et al., 2005;</ref><ref type="bibr" target="#b22">MacCartney &amp; Manning, 2009</ref>) requires models to take a pair of sentence as input and classify their relationship types, i.e., <ref type="bibr">ENTAILMENT, NEUTRAL, or CONTRADICTION. SNLI (Bowman et al., 2015)</ref> and MultiNLI <ref type="bibr" target="#b43">(Williams et al., 2018)</ref> datasets are proposed for this task. However, this task only focuses on sentence-level logical relationship reasoning and the relationships are limited to only a few types. Another task related to logical reasoning in NLP is argument reasoning comprehension task introduced by <ref type="bibr" target="#b13">Habernal et al. (2018)</ref> with a dataset of this task. Given an argument with a claim and a premise, this task aims to select the correct implicit warrant from two options. Although the task is on passage-level logical reasoning, it is limited to only one logical reasoning type, i.e., identifying warrants. ReClor and the proposed task integrate various logical reasoning types into reading comprehension, with the aim to promote the development of models in logical reasoning not only from sentence-level to passage-level, but also from simple logical reasoning types to the complicated diverse ones.</p><p>Datasets from Examinations. There have been several datasets extracted from human standardized examinations in NLP, such as RACE dataset <ref type="bibr" target="#b20">(Lai et al., 2017)</ref> mentioned above. Besides, NTCIR QA Lab <ref type="bibr" target="#b36">(Shibuki et al., 2014)</ref> offers comparative evaluation for solving real-world university entrance exam questions; The dataset of CLEF QA Entrance Exams Task <ref type="bibr" target="#b34">(Rodrigo et al., 2015)</ref> is extracted from standardized English examinations for university admission in Japan; ARC dataset  consists of 7,787 science questions targeting student grade level, ranging from 3rd grade to 9th; The dialogue-based multiple-choice reading comprehension dataset DREAM <ref type="bibr" target="#b39">(Sun et al., 2019)</ref> contains 10,197 questions for 6,444 multi-turn multi-party dialogues from English language exams that are designed by human experts to assess the comprehension level of Chinese learners of English. Compared with these datasets, ReClor distinguishes itself by targeting logical reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RECLOR DATA COLLECTION AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DATA COLLECTION</head><p>The format of data in ReClor is similar to other multiple-choice reading comprehension datasets <ref type="bibr" target="#b33">(Richardson et al., 2013;</ref><ref type="bibr" target="#b20">Lai et al., 2017)</ref>, where a data point contains a context, a question and four  answer options, among which only one option is right/most suitable. We collect reading comprehension problems that require complicated logical reasoning. However, producing such data requires the ability to perform complex logical reasoning, which makes it hard for crowdsourcing workers to generate such logical questions. Fortunately, we find the reading comprehension problems in some standardized tests, such as GMAT and LSAT, are highly in line with our expectation.</p><p>We construct a dataset containing 6,138 logical reasoning questions sourced from open websites and books. In the original problems, there are five answer options in which only one is right. To comply with fair use of law 4 , we shuffle the order of answer options and randomly delete one of the wrong options for each data point, which results in four options with one right option and three wrong options. Furthermore, similar to ImageNet dataset 5 , ReClor is available for non-commercial research purpose only. We are also hosting a public evaluation server on EvalAI <ref type="bibr" target="#b45">(Yadav et al., 2019)</ref> to benchmark progress on Reclor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DATA ANALYSIS</head><p>As mentioned above, we collect 6,138 data points, in which 91.22% are from actual exams of GMAT and LSAT while others are from high-quality practice exams. They are divided into training set, validation set and testing set with 4,638, 500 and 1,000 data points respectively. The overall statistics of ReClor and comparison with other similar multiple-choice MRC datasets are summarized in Table 2. As shown, ReClor is of comparable size and relatively large vocabulary size. Compared with RACE, the length of the context of ReCor is much shorter. In RACE, there are many redundant sentences in context to answer a question. However, in ReClor, every sentence in the context passages is important, which makes this dataset focus on evaluating the logical reasoning ability of models rather than the ability to extract relevant information from a long context. The length of answer options of ReClor is largest among these datasets. We analyze and manually annotate the types of questions on the testing set and group them into 17 categories, whose percentages and descriptions are shown in <ref type="table">Table 3</ref>. The percentages of different types of questions reflect those in the logical reasoning module of GMAT and LSAT. Some examples of different types of logical reasoning are listed in <ref type="figure" target="#fig_2">Figure 2</ref>, and more examples are listed in the Appendix C.</p><p>Taking two examples, we further express how humans would solve such questions in <ref type="table">Table 4</ref>, showing the challenge of ReClor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DATA BIASES IN THE DATASET</head><p>The dataset is collected from exams devised by experts in logical reasoning, which means it is annotated by humans and may introduce biases in the dataset. Recent studies have shown that models can utilize the biases in a dataset of natural language understanding to perform well on the task without truly understanding the text <ref type="bibr" target="#b35">(Schwartz et al., 2017;</ref><ref type="bibr" target="#b4">Cai et al., 2017;</ref><ref type="bibr" target="#b3">Bugert et al., 2017;</ref><ref type="bibr" target="#b28">Poliak et al., 2018;</ref><ref type="bibr" target="#b12">Gururangan et al., 2018;</ref><ref type="bibr" target="#b47">Zellers et al., 2019)</ref>. It is necessary to analyze such data biases to help evaluate models. In the ReClor dataset, the common context and question are shared across the four options for each data point, so we focus on the analysis of the difference in lexical choice and sentence length of the right and wrong options without contexts and questions. We first investigate the biases of lexical choice. We lowercase the options and then use WordPiece tokenization <ref type="bibr" target="#b44">(Wu et al., 2016)</ref> of BERT BASE <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> to get the tokens. Similar to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type Description</head><p>Necessary Assumptions (11.4%) identify the claim that must be true or is required in order for the argument to work. Sufficient Assumptions (3.0%) identify a sufficient assumption, that is, an assumption that, if added to the argument, would make it logically valid. Strengthen (9.4%) identify information that would strengthen an argument Weaken (11.3%) identify information that would weaken an argument Evaluation (1.3%) identify information that would be useful to know to evaluate an argument Implication (4.6%) identify something that follows logically from a set of premises Conclusion/Main Point (3.6%) identify the conclusion/main point of a line of reasoning Most Strongly Supported (5.6%) find the choice that is most strongly supported by a stimulus Explain or Resolve (8.4%) identify information that would explain or resolve a situation Principle (6.5%) identify the principle, or find a situation that conforms to a principle, or match the principles Dispute (3.0%) identify or infer an issue in dispute Technique (3.6%) identify the technique used in the reasoning of an argument Role (3.2%) describe the individual role that a statement is playing in a larger argument Identify a Flaw (11.7%) identify a flaw in an arguments reasoning Match Flaws (3.1%) find a choice containing an argument that exhibits the same flaws as the passages argument Match the Structure (3.0%) match the structure of an argument in a choice to the structure of the argument in the passage Others (7.3%) other types of questions which are not included by the above <ref type="table">Table 3</ref>: The percentage and description of each logical reasoning type. The descriptions are adapted from those specified by Khan Academy (2019). <ref type="bibr" target="#b28">Poliak et al. (2018)</ref>, for the tokens in options, we analyze their conditional probability of label l ∈ {right, wrong} given by the token t by p(l|t) = count(t, l)/count(t). The larger the correlation score is for a particular token, the more likely it contributes to the prediction of related option. <ref type="table" target="#tab_4">Table  5</ref> reports tokens in training set which occur at least twenty times with the highest scores since many of the tokens with the highest scores are of low frequency. We further analyze the lengths of right and wrong options <ref type="bibr" target="#b12">(Gururangan et al., 2018)</ref> in training set. We notice a slight difference in the distribution of sentence length for right and wrong options. The average length for wrong options is around 21.82 whereas that for right options is generally longer with an average length of 23.06.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context:</head><p>If the purpose of laws is to contribute to people's happiness, we have a basis for criticizing existing laws as well as proposing new laws. Hence, if that is not the purpose, then we have no basis for the evaluation of existing laws, from which we must conclude that existing laws acquire legitimacy simply because they are the laws Question: The reasoning in the argument is flawed in that the argument Options:</p><p>A. takes a sufficient condition for a state of affairs to be a necessary condition for it B. draws a conclusion about how the world actually is on the basis of claims about how it should be C. infers a causal relationship from the mere presence of a correlation D. trades on the use of a term in one sense in a premise and in a different sense in the conclusion Answer: A Reasoning Process of Humans:</p><p>We may first look at the question to understand the specific task of the question -identify a flaw. We then analyze the argument in the context. The conclusion 'existing laws acquire legitimacy simply because they are the laws.' is based on the argument (purpose is NOT happiness) → (NOT basis for criticizing laws), which is obtained from the first statement: (purpose is happiness) → (basis for criticizing laws). However, we know ¬A → ¬B cannot be obtained from A → B. Therefore, we should choose option A that describes this flaw. The distractors here are different types of reasoning flaws. Prior knowledge of basic logical rules is needed to correctly answer this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context:</head><p>Psychologist: Phonemic awareness, or the knowledge that spoken language can be broken into component sounds, is essential for learning to read an alphabetic language. But one also needs to learn how sounds are symbolically represented by means of letters; otherwise, phonemic awareness will not translate into the ability to read an alphabetic language. Yet many children who are taught by the whole-language method, which emphasizes the ways words sound, learn to read alphabetic languages. Question: Which one of the following can be properly inferred from the psychologist's statements?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. The whole-language method invariably succeeds in teaching awareness of how spoken language can be broken into component sounds. B. Some children who are taught by the whole-language method are not prevented from learning how sounds are represented by means of letters. C. The whole-language method succeeds in teaching many children how to represent sounds symbolically by means of letters. D. When the whole-language method succeeds in teaching someone how to represent sounds by means of letters, that person acquires the ability to read an alphabetic language. Answer: B Reasoning Process of Humans: Looking at the question and we know that it is asking about implication. From the first two sentences in context, we know that there are two necessary conditions to read an alphabetic language: phonemic awareness and symbolic letters. We also learn [(NOT symbolic letters) AND (phonemic awareness)] → read an alphabetic language (denoted as Formula 1). The last sentence in the context says that many children are taught by the whole-language method to learn a language. As for option A, from the context, we only know the whole language method works for 'many' children, which cannot be inferred to 'invariably' works. As for option B, combing three sentences in the context, we know that the whole-language method meets the two necessary conditions to learn a language, especially the last sentence mentions 'learn to read alphabetic languages'. Children learn to read alphabetic languages means that they must recognize symbolic letters that represent sound because symbolic letters is a necessary condition of read an alphabetic language; otherwise, they cannot read because of Formula 1 mentioned above. Therefore, option B is correct. As for option C, from the context we only know the whole-language method teaches phonemic awareness and read an alphabetic language. Symbolic letters may be taught by other methods, so C is wrong. As for D, similar to C, symbolic letters may be taught by other methods and we also cannot obtain: symbolic letters → read an alphabetic language. <ref type="table">Table 4</ref>: Two examples to show how humans would solve the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Necessary Assumptions</head><p>Context: The current pattern of human consumption of resources, in which we rely on nonrenewable resources, for example metal ore, must eventually change. Since there is only so much metal ore available, ultimately, we must either do without or turn to renewable resources to take its place.</p><p>Q: Which one of the following is an assumption required by the argument?</p><p>A. We cannot indefinitely replace exhausted nonrenewable resources with other nonrenewable resources. B. Consumption of nonrenewable resources will not continue to increase in the near future. C. There are renewable resource replacements for all of the nonrenewable resources currently being consumed. D. Ultimately we cannot do without nonrenewable resources.</p><p>Context: Some theorists argue that literary critics should strive to be value-neutral in their literary criticism. These theorists maintain that by exposing the meaning of literary works without evaluating them, critics will enable readers to make their own judgments about the works' merits. But literary criticism cannot be completely value-neutral. Thus, some theorists are mistaken about what is an appropriate goal for literary criticism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q:</head><p>The argument's conclusion follows logically if which one of the following is assumed?</p><p>A. Any critic who is able to help readers make their own judgments about literary works' merits should strive to produce value-neutral criticism. B. If it is impossible to produce completely value-neutral literary criticism, then critics should not even try to be value-neutral. C. The less readers understand the meaning of a literary work, the less capable they will be of evaluating that work's merits.. D. Critics are more likely to provide criticisms of the works they like than to provide criticisms of the works they dislike.</p><p>Context: The television show Henry was not widely watched until it was scheduled for Tuesday evenings immediately after That' s Life, the most popular show on television. During the year after the move, Henry was consistently one of the ten most-watched shows on television. Since Henry' s recent move to Wednesday evenings, however, it has been watched by far fewer people. We must conclude that Henry was widely watched before the move to Wednesday evenings because it followed That' s Life and not because people especially liked it.</p><p>Q: Which one of the following, if true, most strengthens the argument?</p><p>A. The show that now follows That's Life on Tuesdays has double the number of viewers it had before being moved. B. Henry has been on the air for three years, but That's Life has been on the air for only two years. C. After its recent move to Wednesday, Henry was aired at the same time as the second most popular show on television. D. That's Life was not widely watched during the first year it was aired.</p><p>Context: When a certain gland becomes cancerous in humans, it produces high levels of a particular protein. A blood test can determine the level of this protein well before a cancer of the gland could be detected by other means. Some doctors recommend that aggressive anticancer treatment should be begun as early as possible for anyone who is tested and is found to have high levels of the protein.</p><p>Q: Which one of the following, if true, most seriously weakens the doctors' recommendation?</p><p>A. The blood test for the protein has been in use for some time to monitor the condition of patients who have been diagnosed as having cancer of the gland. B. Before the blood test became available, about one third of all cases of cancer of the gland were detected in early stages. C. So far, no patients whose protein levels were found to be normal have subsequently developed cancer of the gland. D. Enlargement of the gland, a common condition infrequently associated with cancer, results in high levels of the protein.</p><p>Context: In a study, pairs of trained dogs were placed side by side and given a command such as "sit". After both obeyed the command, one dog was given a treat while its partner was given no reward at all. Over time, the dogs who went unrewarded began to disobey the command. This shows that dogs have an aversion to being treated unfairly.</p><p>Q: Which one of the following would be most useful to know in order to evaluate the argument? A. Were dogs who were accustomed to receiving regular rewards prior to the study more inclined to obey the command? B. How many repetitions were required before the unrewarded dogs began to disobey the command? C. Is there a decline in obedience if rewards are withheld from both dogs in the pair? D. Were dogs who received treats in one trial ever used as dogs that did not receive treats in other trials? RoBERTa <ref type="bibr" target="#b21">(Liu et al., 2019)</ref> have achieved impressive results in various NLP tasks. We challenge these neural models with ReClor to investigate how well they can perform. Details of the baseline models and implementation are shown in the Appendix A and B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EXPERIMENTS TO FIND BIASED DATA</head><p>As mentioned earlier, biases prevalently exist in human-annotated datasets <ref type="bibr" target="#b28">(Poliak et al., 2018;</ref><ref type="bibr" target="#b12">Gururangan et al., 2018;</ref><ref type="bibr" target="#b47">Zellers et al., 2019;</ref><ref type="bibr" target="#b25">Niven &amp; Kao, 2019)</ref>, which are often exploited by models to perform well without truly understanding the text. Therefore, it is necessary to find out the biased data points in ReClor in order to evaluate models in a more comprehensive manner <ref type="bibr" target="#b38">(Sugawara et al., 2018)</ref>. To this end, we feed the five strong baseline models (GPT, GPT-2, BERT BASE , XLNet BASE and RoBERTa BASE ) with ONLY THE ANSWER OPTIONS for each problem. In other words, we purposely remove the context and question in the inputs. In this way, we are able to identify those problems that can be answered correctly by merely exploiting the biases in answer options without knowing the relevant context and question. However, the setting of this task is a multiple-choice question with 4 probable options, and even a chance baseline could have 25% probability to get it right. To eliminate the effect of random guess, we set four different random seeds for each model and pick the data points that are predicted correctly in all four cases to form the EASY set. Then, the data points which are predicted correctly by the models at random could be nearly eliminated, since any data point only has a probability of (25%) 4 = 0.39% to be guessed right consecutively for four times. Then we unite the sets of data points that are consistently predicted right by each model, because intuitively different models may learn different biases of the dataset. The above process is formulated as the following expression,</p><formula xml:id="formula_0">C EASY = (C seed1 GPT ∩ C seed2 GPT ∩ C seed3 GPT ∩ C seed4 GPT ) ∪ (C seed1 GPT−2 ∩ C seed2 GPT−2 ∩ C seed3 GPT−2 ∩ C seed4 GPT−2 ) ∪ (C seed1 BERT ∩ C seed2 BERT ∩ C seed3 BERT ∩ C seed4 BERT ) ∪ (C seed1 XLNet ∩ C seed2 XLNet ∩ C seed3 XLNet ∩ C seed4 XLNet ) ∪ (C seed1 RoBERTa ∩ C seed2 RoBERTa ∩ C seed3 RoBERTa ∩ C seed4 RoBERTa ), C HARD = C TEST − C EASY ,<label>(1)</label></formula><p>where C seed1 BERT denotes the set of data points which are predicted correctly by BERT BASE with seed 1, and similarly for the rest. <ref type="table">Table 6</ref> shows the average performance for each model trained with four different random seeds and the number of data points predicted correctly by all of them. Finally, we get 440 data points from the testing set C TEST and we denote this subset as EASY set C EASY and the other as HARD set C HARD .  <ref type="table">Table 6</ref>: Average accuracy of each model using four different random seeds with only answer options as input, and the number of their common correct predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TRANSFER LEARNING THROUGH FINE-TUNING</head><p>Among multiple-choice reading comprehension or QA datasets from exams, although the size of ReClor is comparable to those of ARC  and DREAM <ref type="bibr" target="#b39">(Sun et al., 2019)</ref>, it is much smaller than <ref type="bibr">RACE Lai et al. (2017)</ref>. Recent studies <ref type="bibr" target="#b24">(Min et al., 2017;</ref><ref type="bibr" target="#b14">Howard &amp; Ruder, 2018;</ref><ref type="bibr" target="#b15">Huang et al., 2019;</ref><ref type="bibr" target="#b16">Jin et al., 2019)</ref> have shown the effectiveness of pre-training on similar tasks or datasets then fine-tuning on the target dataset for transfer learning. <ref type="bibr" target="#b16">Jin et al. (2019)</ref> find that by first training on RACE <ref type="bibr" target="#b20">(Lai et al., 2017)</ref> and then further fine-tuning on the target dataset, the performances of BERT BASE on multiple-choice dataset MC500 <ref type="bibr" target="#b33">(Richardson et al., 2013)</ref> and DREAM <ref type="bibr" target="#b39">(Sun et al., 2019)</ref> can significantly boost from 69.5% to 81.2%, and from 63.2% to 70.2%, respectively. However, they also find that the model cannot obtain significant improvement even performs worse if it is first fine-tuned on span-based dataset like SQuAD <ref type="bibr" target="#b31">(Rajpurkar et al., 2016)</ref>.</p><p>ReClor is a multiple-choice dataset, so we choose RACE for fine-tuning study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">RESULTS AND ANALYSIS</head><p>The performance of all tested models on the ReClor is presented in <ref type="table" target="#tab_7">Table 7</ref>. This dataset is built on questions designed for students who apply for admission to graduate schools, thus we randomly choose 100 samples from the testing set and divide them into ten tests, which are distributed to ten different graduate students in a university. We take the average of their scores and present it as the baseline of graduate students. The data of ReClor are carefully chosen and modified from only high-quality questions from standardized graduate entrance exams. We set the ceiling performance to 100% since ambiguous questions are not included in the dataset.</p><p>The performance of fastText is better than random guess, showing that word correlation could be used to help improve performance to some extent. It is difficult for Bi-LSTM to converge on this  dataset. Transformer-based pre-training models have relatively good performance, close to the performance of graduate students. However, we find that these models only perform well on EASY set with around 75% accuracy, showing these models have an outstanding ability to capture the biases of the dataset, but they perform poorly on HARD set with only around 30% accuracy. In contrast, humans can still keep good performance on HARD set. We notice the difference in testing accuracy performed by graduate students on EASY and HARD set, but this could be due to the small number of students participated in the experiments. Therefore, we say humans perform relatively consistent on both biased and non-biased dataset.</p><p>It is noticed that if the models are first trained on RACE and then fine-tuned on ReClor, they could obtain significant improvement, especially on HARD set. The overall performance of RoBERTa LARGE is even better than that of graduate students. This similar phenomenon can also be observed on DREAM dataset <ref type="bibr" target="#b39">(Sun et al., 2019)</ref> by <ref type="bibr" target="#b16">Jin et al. (2019)</ref>, which shows the potential of transfer learning for reasoning tasks. However, even after fine-tuning on RACE, the best performance of these strong baselines on HARD set is around 50%, still lower than that of graduate students and far away from ceiling performance.</p><p>Experiments in different input settings are also done. Compared with the input setting of answer options only (A), the setting of questions and answer options (Q, A) can not bring significant improvement. This may be because some questions e.g., Which one of the following is an assumption required by the argument?, Which one of the following, if true, most strengthens the argument? can be used in the same reasoning types of question, which could not offer much information. Further adding context causes significant boost, showing the high informativeness of the context.</p><p>We further analyze the model performance with respect to different question types of logical reasoning. Some results are shown in <ref type="figure" target="#fig_3">Figure 4</ref> and the full results are shown in <ref type="figure">Figure 5</ref>, 6 and 7 in the Appendix E. Three models of BERT LARGE , XLNet LARGE and RoBERTa LARGE perform well on most of types. On HARD set, the three models perform poorly on certain types such as STRENGTHEN, WEAKEN and ROLE which require extensive logical reasoning. However, they perform relatively better on other certain types, such as CONCLUSION/MAIN POINT and MATCH STRUCTURES that are more straight-forward. For the result of transfer learning, we analyze XLNet LARGE in detail. Though the overall performance is significantly boosted after fine-tuning on RACE first, the histograms in the bottom of <ref type="figure" target="#fig_3">Figure 4</ref> show that on EASY set, accuracy of the model with fine-tuning on RACE is similar to that without it among most question types, while on HARD set, significant improvement on some question types is observed, such as CONCLUSION/MAIN POINT and MOST STRONGLY SUPPORTED. This may be because these types require less logical reasoning to some extent compared with other types, and similar question types may also be found in RACE dataset. Thus, the pre-training on RACE helps enhance the ability of logical reasoning especially of relatively simple reasoning types, but more methods are still needed to further enhance the ability especially that of relatively complex reasoning types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Necessary Assumptions Sufficient Assumptions</head><p>Strengthen Weaken Evaluation Implication  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we introduce ReClor, a reading comprehension dataset requiring logical reasoning, with the aim to push research progress on logical reasoning in NLP forward from sentence-level to passage-level and from simple logical reasoning to multiple complicated one. We propose to identify biased data points and split the testing set into EASY and HARD group for biased and non-biased data separately. We further empirically study the different behaviors of state-of-the-art models on these two testing sets, and find recent powerful transformer-based pre-trained language models have an excellent ability to exploit the biases in the dataset but have difficulty in understanding and reasoning given the non-biased data with low performance close to or slightly better than random guess. These results show there is a long way to equip deep learning models with real logical reasoning abilities. We hope this work would inspire more research in future to adopt similar split technique and evaluation scheme when reporting their model performance. We also show by first fine-tuning on a large-scale dataset RACE then fine-tuning on ReClor, the models could obtain significant improvement, showing the potential of transfer learning to solve reasoning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A BASELINE MODELS</head><p>fastText. FastText <ref type="bibr" target="#b17">(Joulin et al., 2017)</ref> models sentences as a bag of n-grams, and tries to predict the probability of each answer being correct independently. We choose the answer with the highest score as the prediction for the multiple-choice setting.</p><p>LSTM sentence encoder. A two-layer bi-LSTM is randomly initialized as a sentence encoder with GloVe word embedding <ref type="bibr" target="#b27">(Pennington et al., 2014)</ref>. With a span of text as input, the last hidden state of the second layer is max-pooled and then fed into a fully-connected layer to compute the output score.</p><p>GPT and GPT-2. GPT <ref type="bibr" target="#b29">(Radford et al., 2018)</ref> and <ref type="bibr">GPT-2 (Radford et al., 2019)</ref> are both transformer <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref> based models which are pre-trained using unsupervised method with a standard language modeling objective. GPT is pre-trained on BooksCorpus; GPT-2 is pre-trained using a larger dataset called WebText. Here we use the smallest model proposed in <ref type="bibr" target="#b30">(Radford et al., 2019)</ref> as our GPT-2 baseline. To fine-tune on ReClor, the final hidden vector corresponding to the last input token ([ classify ]) is used as the aggregate representation followed by an extra fully connected layer to compute the score.</p><p>BERT. BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> is also a transformer <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref> based model which is trained by using BooksCorpus <ref type="bibr" target="#b49">(Zhu et al., 2015)</ref> and English Wikipedia in two unsupervised tasks, i.e., Masked LM (MLM) and Next Sentence Prediction (NSP). During fine-tuning, the final hidden vector corresponding to the first input token ([CLS]) is used as the aggregate representation followed by two extra fully connected layers to compute the score.</p><p>XLNet. XLNet <ref type="bibr" target="#b46">(Yang et al., 2019)</ref> is trained with Permutation Language Modeling and without NSP. In addition, beside BooksCorpus and English Wikipedia used in BERT, it uses Giga5 <ref type="bibr" target="#b26">(Parker et al., 2011)</ref>, ClueWeb 2012-B (extended from <ref type="bibr" target="#b5">(Callan et al., 2009</ref>)), and Common Crawl (com, 2019) for pre-training. We use the final hidden vector corresponding to the last input token &lt;cls&gt; as the aggregate representation and introduce two fully connected layers to predict the score.</p><p>RoBERTa. RoBERTa <ref type="bibr" target="#b21">(Liu et al., 2019)</ref> is an improved pre-training procedure of BERT with training the model longer, with bigger batches over more data and removing NSP objective etc.. Extra two fully connected layers are added to transform the final hidden vector of the first input token (&lt;s&gt; to the score.</p><p>The input format of different models is shown in <ref type="table" target="#tab_9">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Input Format <ref type="bibr">GPT Radford et al. (2018)</ref> start Context delimiter Question || Option classify GPT-2 <ref type="bibr" target="#b30">Radford et al. (2019)</ref> start Context delimiter Question || Option classify BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> [CLS] Context [SEP] Question || Option [SEP] [PAD]... XLNet <ref type="bibr" target="#b46">(Yang et al., 2019)</ref> &lt;pad&gt;... Context &lt;sep&gt; Question || Option &lt;sep&gt; &lt;cls&gt; RoBERTa <ref type="bibr" target="#b21">(Liu et al., 2019)</ref> &lt;s&gt; Context &lt;/s&gt; &lt;/s&gt; Question || Option &lt;/s&gt; &lt;pad&gt;... </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B IMPLEMENTATION DETAIL</head><p>Adam is used by all models. For fastText, we use its python library 6 by converting ReClor to the required form, and keep the default setting of the hyper parameters. For Bi-LSTM, we use a twolayer Bidirectional LSTM with the GloVe 300d word embedding <ref type="bibr" target="#b27">(Pennington et al., 2014)</ref> followed by max-pooling and a fully-connected layer. We train the model for 100 epochs using a batch size of 64 and learning rate of 0.1. A learning rate decay of 0.5 is also applied every 10 epochs. For pre-training models, we modify the code of Transformers of Hugging Face 7 to implement them on ReClor. We use a batch size of 24 and fine-tune for 10 epochs. The maximum input sequence length for all models is 256. The detailed hyperparameters are shown in <ref type="table" target="#tab_11">Table 9</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXAMPLES</head><p>Type: Necessary Assumptions Definition: identify the claim that must be true or is required in order for the argument to work Context: Slash-and-burn agriculture involves burning several acres of forest, leaving vegetable ash that provides ample fertilizer for three or four years of bountiful crops. On the cleared land nutrients leach out of the soil, however, and the land becomes too poor to support agriculture. New land is then cleared by burning and the process starts again. Since most farming in the tropics uses this method, forests in this region will eventually be permanently eradicated.</p><p>Question: The argument depends on the assumption that Options:</p><p>A. forests in the tropics do not regenerate well enough to restore themselves once they have been cleared by the slash-and-burn method B. some other methods of agriculture are not as destructive to the environment in tropical regions as the slash-and-burn method is C. forests in the tropics are naturally deficient in nutrients that are needed to support the growth of plants that are not native to those regions D. slash-and-burn agriculture is particularly suitable for farming in tropical areas Answer: <ref type="table" target="#tab_0">A   Table 10</ref>: The definition and an example of the logical reasoning type -Necessary Assumptions Type: Sufficient Assumptions Definition: identify a sufficient assumption, that is, an assumption that, if added to the argument, would make it logically valid Context: Geologist: A new method for forecasting earthquakes has reliably predicted several earthquakes. Unfortunately, this method can predict only that an earthquake will fall somewhere within a range of two and a half points on the Richter scale. Thus, since a difference of two and a half points can be the difference between a marginally perceptible shaking and a quake that causes considerable damage, the new method is unlikely to be useful. Question: Which one of the following, if assumed, enables the geologist's conclusion to be properly inferred?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. An earthquake-forecasting method is unlikely to be useful unless its predictions always differentiate earthquakes that are barely noticeable from ones that result in substantial destruction. B. Several well-established methods for forecasting earthquakes can predict within much narrower ranges than two and a half points on the Richter scale. C. Even if an earthquake-forecasting method makes predictions within a very narrow range on the Richter scale, this method is not likely to be useful unless its predictions are reliable.</p><p>D. An earthquake-forecasting method has not been shown to be useful until it has been used to reliably predict a large number of earthquakes. Answer: <ref type="table" target="#tab_0">A   Table 11</ref>: The definition and an example of the logical reasoning type -Sufficient Assumptions Type: Strengthen Definition: identify information that would strengthen an argument Context: Financial success does not guarantee happiness. This claim is not mere proverbial wisdom but a fact verified by statistics. In a recently concluded survey, only one-third of the respondents who claimed to have achieved financial success reported that they were happy. Question: Which one of the following, if true, most strongly supports the conclusion drawn from the survey results?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. Most of the respondents who reported they were unhappy were in fact happy. B. The respondents who reported financial success were, for the most part, financially successful. C. Many of the respondents who claimed not to have achieved financial success reported that they were happy five years ago. D. Many of the respondents who failed to report financial success were in fact financially successful. Answer: B <ref type="table" target="#tab_0">Table 12</ref>: The definition and an example of the logical reasoning type -Strengthen Type: Weaken Definition: identify information that would weaken an argument Context: "DNA fingerprinting" is a recently-introduced biochemical procedure that uses a pattern derived from a person' s genetic material to match a suspect' s genetic material against that of a specimen from a crime scene. Proponents have claimed astronomically high odds against obtaining a match by chance alone. These odds are based on an assumption that there is independence between the different characteristics represented by a single pattern. Question: Which one of the following, if true, casts the most doubt on the claim of the proponents of DNA fingerprinting?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. The skill required of laboratory technicians performing the DNA fingerprinting procedure is not extraordinary. B. There is a generally accepted theoretical basis for interpreting the patterns produced by the procedure. C. In the whole population there are various different subgroups, within each of which certain sets of genetic characteristics are shared. D. In the investigation of certain genetic diseases, the techniques used in DNA fingerprinting have traced the transmission of the diseases among the living members of very large families. Answer: <ref type="table" target="#tab_0">C   Table 13</ref>: The definition and an example of the logical reasoning type -Weaken Type: Evaluation Definition: identify information that would be useful to know to evaluate an argument Context: George: Some scientists say that global warming will occur because people are releasing large amounts of carbon dioxide into the atmosphere by burning trees and fossil fuels. We can see, though, that the predicted warming is occurring already. In the middle of last winter, we had a month of springlike weather in our area, and this fall, because of unusually mild temperatures, the leaves on our town' s trees were three weeks late in turning color. Question: Which one of the following would it be most relevant to investigate in evaluating the conclusion of George's argument?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. whether air pollution is causing some trees in the area to lose their leaves B. what proportion of global emissions of carbon dioxide is due to the burning of trees by humans C. whether unusually warm weather is occurring elsewhere on the globe more frequently than before D. when leaves on the trees in the town usually change color Answer: <ref type="table" target="#tab_0">C   Table 14</ref>: The definition and an example of the logical reasoning type -Evaluation Type: Implication Definition: identify something that follows logically from a set of premises Context:</p><p>To be horrific, a monster must be threatening. Whether or not it presents psychological, moral or social dangers, or triggers enduring infantile fears, if a monster is physically dangerous then it is threatening. In fact, even a physically benign monster is horrific if it inspires revulsion. Question: Which one of the following logically follows from the statements above?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. Any horror-story monster that is threatening is also horrific. B. If a monster triggers infantile fears but is not physically dangerous, then it is not horrific. C. All monsters that are not physically dangerous, but that are psychologically dangerous and inspire revulsion, are threatening. D. If a monster is both horrific and psychologically threatening, then it does not inspire revulsion.</p><p>Answer: C  After a nuclear power plant accident, researchers found radioactive isotopes of iodine, tellurium, and cesium-but no heavy isotopes-in the atmosphere downwind. This material came either from spent fuel rods or from the plant' s core. Spent fuel rods never contain significant quantities of tellurium isotopes. Radioactive material ejected into the atmosphere directly from the core would include heavy isotopes. After the accident, steam, which may have been in contact with the core, was released from the plant. The core contains iodine, tellurium, and cesium isotopes, which are easily dissolved by steam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>Of the following statements, which one is most strongly supported by the information above?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. The nuclear power plant's spent fuel rods were not damaged. B. Spent fuel rods do not contain heavy isotopes in significant quantities. C. The researchers found some radioactive material from spent fuel rods as well as some material that was ejected into the atmosphere directly from the plant's core. D. The radioactive material detected by the researchers was carried into the atmosphere by the steam that was released from the plant. Answer: D To reduce the mosquito population in a resort area, hundreds of trees were planted that bear fruit attractive to birds. Over the years, as the trees matured, they attracted a variety of bird species and greatly increased the summer bird population in the area. As expected, the birds ate many mosquitoes. However, the planting of the fruit trees had the very opposite of its intended effect. Question: Which one of the following, if true, most helps to explain the apparently paradoxical result?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. Most of the species of birds that were attracted by the trees that were planted did not eat mosquitoes. B. Increases and decreases in mosquito populations tend to follow a cyclical pattern. C. The species of birds that were attracted in the greatest number by the fruit of the trees that were planted did not eat mosquitoes. D. The birds attracted to the area by the trees ate many more insects that prey on mosquitoes than they did mosquitoes. Answer: <ref type="table" target="#tab_0">D   Table 18</ref>: The definition and an example of the logical reasoning type -Explain or Resolve Type: Principle Definition: identify the principle, or find a situation that conforms to a principle, or match the principles Context:</p><p>Buying elaborate screensavers -programs that put moving images on a computer monitor to prevent damage -can cost a company far more in employee time than it saves in electricity and monitor protection. Employees cannot resist spending time playing with screensavers that flash interesting graphics across their screens. Question: Which one of the following most closely conforms to the principle illustrated above?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. An electronic keyboard may be cheaper to buy than a piano but more expensive to repair.</p><p>B. An energy-efficient insulation system may cost more up front but will ultimately save money over the life of the house. C. The time that it takes to have a pizza delivered may be longer than it takes to cook a complete dinner. D. A complicated hotel security system may cost more in customer goodwill than it saves in losses by theft. Answer: D  Joanna: The only way for a company to be successful, after emerging from bankruptcy, is to produce the same goods or services that it did before going bankrupt. It is futile for such a company to try to learn a whole new business. Ruth: Wrong. The Kelton Company was a major mining operation that went into bankruptcy. On emerging from bankruptcy, Kelton turned its mines into landfills and is presently a highly successful waste-management concern. Question:</p><p>Ruth uses which one of the following argumentative techniques in countering Joanna's argument?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. She undermines a claim by showing that it rests on an ambiguity. B. She offers an alternative explanation for a phenomenon. C. She presents a counterexample to a claim. D. She establishes a conclusion by excluding the only plausible alternative to that conclusion. Answer: C The position that punishment should be proportional to how serious the offense is but that repeat offenders should receive harsher punishments than first-time offenders is unsustainable. It implies that considerations as remote as what an offender did years ago are relevant to the seriousness of an offense. If such remote considerations were relevant, almost every other consideration would be too. But this would make determining the seriousness of an offense so difficult that it would be impossible to apply the proportionality principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>The statement that considerations as remote as what an offender did years ago are relevant to the seriousness of an offense plays which one of the following roles in the argument?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. It is an allegedly untenable consequence of a view rejected in the argument's overall conclusion. B. It is a statement the argument provides grounds to accept and from which the overall conclusion is inferred. C. It is the overall conclusion in favor of which the argument offers evidence. D. It is a premise offered in support of an intermediate conclusion of the argument. Answer: A The tidal range at a particular location is the difference in height between high tide and low tide. Tidal studies have shown that one of the greatest tidal ranges in the world is found in the Bay of Fundy and reaches more than seventeen meters. Since the only forces involved in inducing the tides are the sun' s and moon' s gravity, the magnitudes of tidal ranges also must be explained entirely by gravitational forces. Question: Which one of the following most accurately describes a flaw in the reasoning above?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. It does not differentiate between the tidal effect of the sun and the tidal effect of the moon. B. It fails to consider that the size of a tidal range could be affected by the conditions in which gravitational forces act. C. It presumes, without providing warrant, that most activity within the world's oceans is a result of an interplay of gravitational forces. D. It gives only one example of a tidal range. Answer: <ref type="table" target="#tab_2">B   Table 23</ref>: The definition and an example of the logical reasoning type -Identify a Flaw Type: Match Flaws Definition: find a choice containing an argument that exhibits the same flaws as the passage's argument Context:</p><p>The museum' s night security guard maintains that the thieves who stole the portrait did not enter the museum at any point at or above ground level. Therefore, the thieves must have gained access to the museum from below ground level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>The flawed pattern of reasoning in the argument above is most similar to that in which one of the following?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. As had generally been expected, not all questionnaires were sent in by the official deadline. It follows that plans must have been made for the processing of questionnaires received late. B. The store's competitors claim that the store, in selling off the shirts at those prices, neither made any profit nor broke even. Consequently, the store's customers must have been able to buy shirts there at less than the store's cost. C. The product label establishes that this insecticide is safe for both humans and pets. Therefore, the insecticide must also be safe for such wild mammals as deer and rabbits. D. If the census is to be believed, the percentage of men who are married is higher than the percentage of women who are married. Thus, the census must show a higher number of men than of women overall. Answer: B <ref type="table" target="#tab_2">Table 24</ref>: The definition and an example of the logical reasoning type -Match Flaws Type: Match the Structure Definition: match the structure of an argument in a choice to the structure of the argument in the passage Context:</p><p>It is an absurd idea that whatever artistic endeavor the government refuses to support it does not allow, as one can see by rephrasing the statement to read: No one is allowed to create art without a government subsidy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>The pattern of reasoning in which one of the following is most similar to that in the argument above?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. The notion that every scientist who has been supported by a government grant will be successful is absurd, as one can see by rewording it:No scientist is allowed to do research without a government grant. B. The notion that every scientist who is supported by a government grant will be successful is absurd, as one can see by rewording it:No scientist lacking governmental support will be successful. C. The claim that any driver who is not arrested does not break the law is absurd, as one can see by rewording it: Every driver who gets arrested has broken the law. D. The claim that any driver who is not arrested does not break the law is absurd, as one can see by rewording it: Every driver who breaks the law gets arrested. Answer: D PhishCo runs a number of farms in the arid province of Nufa, depending largely on irrigation. Now, as part of a plan to efficiently increase the farms' total production, it plans to drill down to an aquifer containing warm, slightly salty water that will be used to raise fish in ponds. The water from the ponds will later be used to supplement piped-in irrigation water for PhishCo's vegetable fields, and the ponds and accompanying vegetation should help reduce the heat in the area of the farms. Question: Which of the following would, if true, most strongly suggest that the plan, if implemented, would increase the overall efficiency of PhishCo's farms?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. Organic waste from fish in the pond water will help to fertilize fields where it is used for irrigation. B. Fish raised on PhishCo's farms are likely to be saleable in the nearest urban areas. C. Ponds will be located on low-lying land now partially occupied by grain crops. D. The government of Nufa will help to arrange loan financing to partially cover the costs of drilling. Answer: A   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Performance comparison of state-of-the-art models and humans (graduate students) on EASY and HARD set of ReClor testing set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The distribution of the option length in ReClor with respect to right and wrong labels.4 EXPERIMENTS4.1 BASELINE MODELSMany neural network based models such as FastText<ref type="bibr" target="#b17">(Joulin et al., 2017)</ref>, Bi-LSTM, GPT<ref type="bibr" target="#b29">(Radford et al., 2018)</ref>,GPT-2 (Radford et al., 2019), BERT<ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>, XLNet<ref type="bibr" target="#b46">(Yang et al., 2019)</ref>,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Examples of some question types. The correct options are marked by . More examples are shown in the Appendix C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Performance of models on EASY (left) and HARD (right) testing sets and that of models. XLNet LARGE +Fine-Tune means the model is first fine-tuned on RACE before training on ReClor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Performance of BERT LARGE (top) and RoBERTa LARGE (bottom) on EASY (left) and HARD (right) testing sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>An example in the ReClor dataset which is modified from the Law School Admission Council (2019b).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of several multiple-choice MRC datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Top 10 tokens that correlate to right options with more than 20 occurrences.</figDesc><table><row><cell>Probability</cell><cell>0.02 0.03 0.04 0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">right wrong</cell></row><row><cell></cell><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30 Length</cell><cell>40</cell><cell>50</cell><cell>60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Accuracy (%) of models and human performance. The column Input means whether to input context (C), question (Q) and answer options (A). The RACE column represents whether to first use RACE to fine-tune before training on ReClor.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Input formats of different models. Context, Question and Option represent the token sequences of the context, question and option respectively, and || denotes concatenation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Hyperparameters for finetuning pre-training language models on ReClor</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 15 :</head><label>15</label><figDesc>The definition and an example of the logical reasoning type -Implication</figDesc><table><row><cell>Type: Conclusion/Main Point</cell></row><row><cell>Definition: identify the conclusion/main point of a line of reasoning</cell></row><row><cell>Context:</cell></row><row><cell>Whether or not one can rightfully call a person' s faithfulness a virtue depends in part on the object of that</cell></row><row><cell>person' s faithfulness. Virtues are by definition praiseworthy, which is why no one considers resentment</cell></row><row><cell>virtuous, even though it is in fact a kind of faithfulness -faithfulness to hatreds or animosities.</cell></row><row><cell>Question: Which one of the following most accurately expresses the overall conclusion drawn in the</cell></row><row><cell>argument?</cell></row><row><cell>Options:</cell></row><row><cell>A. The object of a person's faithfulness partially determines whether or not that faithfulness is virtuous.</cell></row><row><cell>B. Virtuous behavior is praiseworthy by definition.</cell></row><row><cell>C. Resentment should not be considered a virtuous emotion.</cell></row><row><cell>D. Behavior that emerges from hatred or animosity cannot be called virtuous.</cell></row><row><cell>Answer: A</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 16 :</head><label>16</label><figDesc>The definition and an example of the logical reasoning type -Conclusion/Main Point</figDesc><table><row><cell>Type: Most Strongly Supported</cell></row><row><cell>Definition: find the choice that is most strongly supported by a stimulus</cell></row><row><cell>Context:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 17 :</head><label>17</label><figDesc>The definition and an example of the logical reasoning type -Most Strongly Supported Type: Explain or Resolve Definition: identify information that would explain or resolve a situation Context:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 19 :</head><label>19</label><figDesc>The definition and an example of the logical reasoning type -Principle Forcing people to help others is morally wrong. Therefore, no government has the right to redistribute resources via taxation. Anyone who wants can help others voluntarily. Edward: Governments do have that right, insofar as they give people the freedom to leave and hence not to live under their authority.Question:Raphaela and Edward disagree about the truth of which one of the following?Options:A. Any government that forces people to help others should permit emigration. B. Any government that permits emigration has the right to redistribute resources via taxation. C. Any government that redistributes resources via taxation forces people to help others.</figDesc><table><row><cell>Type: Dispute</cell></row><row><cell>Definition: identify or infer an issue in dispute</cell></row><row><cell>Context:</cell></row><row><cell>Raphaela:</cell></row></table><note>D. Every government should allow people to help others voluntarily. Answer: B</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 20 :</head><label>20</label><figDesc>The definition and an example of the logical reasoning type -Dispute Type: Technique Definition: identify the technique used in the reasoning of an argument Context:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 21 :</head><label>21</label><figDesc>The definition and an example of the logical reasoning type -Technique</figDesc><table><row><cell>Type: Role</cell></row><row><cell>Definition: describe the individual role that a statement is playing in a larger argument</cell></row><row><cell>Context:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 22 :</head><label>22</label><figDesc>The definition and an example of the logical reasoning type -Role</figDesc><table><row><cell>Type: Identify a Flaw</cell></row><row><cell>Definition: identify a flaw in an argument's reasoning</cell></row><row><cell>Context:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 25 :</head><label>25</label><figDesc>The definition and an example of the logical reasoning type -Match the Structure Type: Others Definition: other types of questions which are not included by the above Context:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 26 :</head><label>26</label><figDesc>The definition and an example of the logical reasoning type -Others D CONSISTENCY OF DIFFERENT MODELS</figDesc><table><row><cell></cell><cell cols="5">GPT GPT-2 BERTBASE XLNetBASE RoBERTaBASE</cell></row><row><cell>GPT</cell><cell>245</cell><cell>164</cell><cell>152</cell><cell>142</cell><cell>116</cell></row><row><cell>GPT-2</cell><cell></cell><cell>238</cell><cell>151</cell><cell>144</cell><cell>123</cell></row><row><cell>BERTBASE</cell><cell></cell><cell></cell><cell>234</cell><cell>138</cell><cell>124</cell></row><row><cell>XLNetBASE</cell><cell></cell><cell></cell><cell></cell><cell>225</cell><cell>125</cell></row><row><cell>RoBERTaBASE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>200</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 27 :</head><label>27</label><figDesc>Overlap of each pair of models after intersection among 4 random seeds.Published as a conference paper at ICLR 2020 E RESULTS WITH RESPECT TO DIFFERENT QUESTION TYPES Accuracy of all baseline models on HARD set of testing set</figDesc><table><row><cell></cell><cell>fastText Bi-LSTM</cell><cell>GPT GPT-2</cell><cell>BERTBASE BERTLARGE</cell><cell>XLNetBASE XLNetLARGE</cell><cell>RoBERTaBASE RoBERTaLARGE</cell></row><row><cell>Necessary Assumptions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sufficient Assumptions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Strengthen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Weaken</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Implication</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Conclusion/Main Point</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Most Strongly Supported</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Explain or Resolve</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Principle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dispute</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Technique</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Role</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Identify a Flaw</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Match Flaws</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Match Structures</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Others</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>20</cell><cell>40</cell><cell>60 Accuracy (%)</cell><cell>80</cell><cell>100</cell></row><row><cell cols="5">Figure 5: Accuracy of all baseline models on overall testing set</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://en.wikipedia.org/wiki/Graduate Management Admission Test 3 https://en.wikipedia.org/wiki/Law School Admission Test</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://www.copyright.gov/fair-use/more-info.html 5 http://image-net.org/download-faq</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/facebookresearch/fastText 7 https://github.com/huggingface/transformers</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the anonymous reviewers for their insightful comments and suggestions; thank Rishabh Jain from Georgia Tech for helping build up the leaderboard of ReClor on EvalAI. Jiashi Feng was partially supported by NUS IDS R-263-000-C67-646, ECRA R-263-000-C87-133, MOE Tier-II R-263-000-D17-112 and AI.SG R-263-000-D97-490. Weihao Yu and Zihang Jiang would like to thank TFRC program for the support of computational resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khan</forename><surname>Academy</surname></persName>
		</author>
		<ptr target="https://www.khanacademy.org/test-prep/lsat/lsat-lessons/logical-reasoning/a/logical-reasoning--article--question-type-catalog" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognising textual entailment with logical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lsdsem 2017: Exploring data generation methods for the story cloze test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bugert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgeniy</forename><surname>Puzikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rücklé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Eckle-Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Martínez-Cámara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</title>
		<meeting>the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="56" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pay attention to the ending: Strong neural baselines for the roc story cloze task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="616" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Clueweb09 data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changkuk</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<title level="m">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Entailment, intensionality and text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cleo</forename><surname>Condoravdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dick</forename><surname>Crouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeria</forename><forename type="middle">De</forename><surname>Paiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Stolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">G</forename><surname>Bobrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLT-NAACL 2003 workshop on Text meaning</title>
		<meeting>the HLT-NAACL 2003 workshop on Text meaning</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2368" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A natural logic inference system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Fyodorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoad</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissim</forename><surname>Francez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Inference in Computational Semantics (ICoS-2)</title>
		<meeting>the 2nd Workshop on Inference in Computational Semantics (ICoS-2)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02324</idno>
		<title level="m">Annotation artifacts in natural language inference data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The argument reasoning comprehension task: Identification and reconstruction of implicit warrants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1930" to="1940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cosmos qa: Machine reading comprehension with contextual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2391" to="2401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiun-Yu</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tagyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mmm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00458</idno>
		<title level="m">Multi-stage multi-task learning for multi-choice reading comprehension</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Looking beyond the surface: A challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The narrativeqa reading comprehension challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiskỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="317" to="328" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04683</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An extended model of natural logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth international conference on computational semantics</title>
		<meeting>the eighth international conference on computational semantics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="140" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02789</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Question answering through transfer learning from large fine-grained supervision data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="510" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probing neural network comprehension of natural language arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Niven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4658" to="4664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>English gigaword fifth edition</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hypothesis only baselines in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparajita</forename><surname>Haldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Seventh Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="180" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<title level="m">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="193" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Overview of clef qa entrance exams task 2015</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselmo</forename><surname>Penas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eduard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriko</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Story cloze task: Uw nlp system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</title>
		<meeting>the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="52" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Overview of the ntcir-11 qa-lab task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideyuki</forename><surname>Shibuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotaro</forename><surname>Sakamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshinobu</forename><surname>Kano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madoka</forename><surname>Ishioroshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Itakura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriko</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kando</surname></persName>
		</author>
		<editor>Ntcir</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An analysis of prerequisite skills for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saku</forename><surname>Sugawara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods</title>
		<meeting>the Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">What makes reading comprehension questions easier?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saku</forename><surname>Sugawara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4208" to="4219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dream: A challenge data set and models for dialogue-based reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="217" to="231" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="287" to="302" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithvijit</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taranjeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shiv Baran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03570</idno>
		<title level="m">Evalai: Towards better evaluation systems for ai agents</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Hellaswag: Can a machine really finish your sentence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07830</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12885</idno>
		<title level="m">Record: Bridging the gap between human and machine commonsense reading comprehension</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
