<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting ResNets: Improved Training and Scaling Strategies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
						</author>
						<title level="a" type="main">Revisiting ResNets: Improved Training and Scaling Strategies</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Novel computer vision architectures monopolize the spotlight, but the impact of the model architecture is often conflated with simultaneous changes to training methodology and scaling strategies. Our work revisits the canonical ResNet  and studies these three aspects in an effort to disentangle them. Perhaps surprisingly, we find that training and scaling strategies may matter more than architectural changes, and further, that the resulting ResNets match recent state-of-the-art models. We show that the best performing scaling strategy depends on the training regime and offer two new scaling strategies: (1) scale model depth in regimes where overfitting can occur (width scaling is preferable otherwise); (2) increase image resolution more slowly than previously recommended (Tan &amp; Le, 2019). Using improved training and scaling strategies, we design a family of ResNet architectures, ResNet-RS, which are 1.7x -2.7x faster than EfficientNets on TPUs, while achieving similar accuracies on ImageNet. In a large-scale semi-supervised learning setup, ResNet-RS achieves 86.2% top-1 ImageNet accuracy, while being 4.7x faster than EfficientNet-NoisyStudent. The training techniques improve transfer performance on a suite of downstream tasks (rivaling state-of-the-art self-supervised algorithms) and extend to video classification on Kinetics-400. We recommend practitioners use these simple revised ResNets as baselines for future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The performance of a vision model is a product of the architecture, training methods and scaling strategy. We improve on the canonical ResNet  with modern training methods (as also used in EfficientNets ), minor architectural changes and improved scaling strategies. The resulting models, ResNet-RS, outperform EfficientNets on the speed-accuracy Pareto curve with speed-ups ranging from 1.7x -2.7x on TPUs and 2.1x -3.3x on GPUs. ResNet (•) is a ResNet-200 trained at 256×256 resolution. Training times reported on TPUs.</p><p>chitectures underlie many advances, but are often simultaneously introduced with other critical -and less publicized -changes in the details of the training methodology and hyperparameters. Additionally, new architectures enhanced by modern training methods are sometimes compared to older architectures with dated training methods (e.g. ResNet-50 with ImageNet Top-1 accuracy of 76.5% ). Our work addresses these issues and empirically studies the impact of training methods and scaling strategies on the popular ResNet architecture .</p><p>We survey the modern training and regularization techniques widely in use today and apply them to ResNets <ref type="figure" target="#fig_0">(Figure 1)</ref>. In the process, we encounter interactions between training methods and show a benefit of reducing weight decay values when used in tandem with other regularization techniques. An additive study of training methods in <ref type="table">Table  1</ref> reveals the significant impact of these decisions: a canonical ResNet with 79.0% top-1 ImageNet accuracy is improved to 82.2% (+3.2%) through improved training methods alone. This is increased further to 83.4% by two small and commonly used architectural improvements: ResNet-D  and Squeeze-and-Excitation <ref type="bibr" target="#b23">(Hu et al., 2018)</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> traces this refinement over the starting ResNet in a speed-accuracy Pareto curve.</p><p>We offer new perspectives and practical advice on scaling vision architectures. While prior works extrapolate scaling rules from small models  or from training for a small number of epochs <ref type="bibr" target="#b41">(Radosavovic et al., 2020)</ref>, we design scaling strategies by exhaustively training models across a variety of scales for the full training duration (e.g. 350 epochs instead of 10 epochs). In doing so, we uncover strong dependencies between the best performing scaling strategy and the training regime (e.g. number of epochs, model size, dataset size). These dependencies are missed in any of these smaller regimes, leading to sub-optimal scaling decisions. Our analysis leads to new scaling strategies summarized as (1) scale the model depth when overfitting can occur (scaling the width is preferable otherwise) and (2) scale the image resolution more slowly than prior works .</p><p>Using the improved training and scaling strategies, we design re-scaled ResNets, ResNet-RS, which are trained across a wide range of model sizes, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. ResNet-RS models use less memory during training and are 1.7x -2.7x faster on TPUs (2.1x -3.3x faster on GPUs) than the popular EfficientNets on the speedaccuracy Pareto curve. In a large-scale semi-supervised learning setup, ResNet-RS obtains a 4.7x training speedup on TPUs (5.5x on GPUs) over EfficientNet-B5 when co-trained on ImageNet and an additional 130M pseudolabeled images.</p><p>Finally, we conclude with a suite of experiments testing the generality of the improved training and scaling strategies. We first design a faster version of Efficient-Net using our scaling strategy, EfficientNet-RS, which improves over the original on the speed-accuracy Pareto curve. Next, we show that the improved training strategies yield representations that rival or outperform those from self-supervised algorithms (SimCLR <ref type="bibr" target="#b4">(Chen et al., 2020a)</ref> and SimCLRv2 <ref type="bibr" target="#b5">(Chen et al., 2020b)</ref>) on a suite of downstream tasks. The improved training strategies extend to video classification as well. Applying the training strategies to 3D-ResNets on the Kinetics-400 dataset yields an improvement from 73.4% to 77.4% (+4.0%).</p><p>Through combining minor architectural changes (used since 2018) and improved training and scaling strategies, we discover the ResNet architecture sets a state-of-the-art baseline for vision research. This finding highlights the importance of teasing apart each of these factors in order to understand what architectures perform better than others.</p><p>We summarize our contributions:</p><p>• An empirical study of regularization techniques and their interplay, which leads to a regularization strategy that achieves strong performance (+3% top-1 accuracy) without having to change the model architecture.</p><p>• A simple scaling strategy: (1) scale depth when overfitting can occur (scaling width can be preferable otherwise) and (2) scale the image resolution more slowly than prior works . This scaling strategy improves the speed-accuracy Pareto curve of both ResNets and EfficientNets.</p><p>• ResNet-RS: a Pareto curve of ResNet architectures that are 1.7x -2.7x faster than EfficientNets on TPUs (2.1x -3.3x on GPUs) by applying the training and scaling strategies.</p><p>• Semi-supervised training of ResNet-RS with an additional 130M pseudo-labeled images achieves 86.2% top-1 ImageNet accuracy, while being 4.7x faster on TPUs (5.5x on GPUs) than the corresponding EfficientNet-NoisyStudent .</p><p>• ResNet checkpoints that, when fine-tuned on a diverse set of computer vision tasks, rival or outperform stateof-the-art self-supervised representations from Sim-CLR <ref type="bibr" target="#b4">(Chen et al., 2020a)</ref> and SimCLRv2 <ref type="bibr" target="#b5">(Chen et al., 2020b)</ref>.</p><p>• 3D ResNet-RS by extending our training methods and architectural changes to video classification. The resulted model improves the top-1 Kinetics-400 accuracy by 4.8% over the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Characterizing Improvements on ImageNet</head><p>Since the breakthrough of AlexNet <ref type="bibr" target="#b32">(Krizhevsky et al., 2012)</ref> on ImageNet <ref type="bibr" target="#b47">(Russakovsky et al., 2015)</ref>, a wide variety of improvements have been proposed to further advance image recognition performance. These improvements broadly arise along four orthogonal axes: architecture, training/regularization methodology, scaling strategy and using additional training data.</p><p>Architecture. The works that perhaps receive the most attention are novel architectures. Notable proposals since AlexNet <ref type="bibr" target="#b32">(Krizhevsky et al., 2012)</ref> include VGG <ref type="bibr" target="#b51">(Simonyan &amp; Zisserman, 2014)</ref>, ResNet , Inception <ref type="bibr" target="#b54">(Szegedy et al., 2015;</ref>, and ResNeXt <ref type="bibr" target="#b60">(Xie et al., 2017)</ref>. Automated search strategies for designing architectures have further pushed the state-of-the-art, notably with NasNet-A , AmoebaNet-A <ref type="bibr" target="#b43">(Real et al., 2019)</ref> and EfficientNet . There have also been efforts in going beyond standard ConvNets for image classification, by adapting self-attention <ref type="bibr" target="#b58">(Vaswani et al., 2017)</ref> to the visual domain <ref type="bibr" target="#b42">Ramachandran et al., 2019;</ref><ref type="bibr" target="#b22">Hu et al., 2019;</ref><ref type="bibr" target="#b49">Shen et al., 2020;</ref><ref type="bibr" target="#b8">Dosovitskiy et al., 2020)</ref> or using alternatives such as lambda layers <ref type="bibr" target="#b0">(Bello, 2021)</ref>.</p><p>Training and Regularization Methods. ImageNet progress has been boosted by innovations in training and regularization approaches. When training models for more epochs, regularization methods such as dropout <ref type="bibr" target="#b52">(Srivastava et al., 2014)</ref>, label smoothing <ref type="bibr" target="#b55">(Szegedy et al., 2016)</ref>, stochastic depth <ref type="bibr" target="#b24">(Huang et al., 2016)</ref>, dropblock <ref type="bibr" target="#b13">(Ghiasi et al., 2018)</ref> and data augmentation <ref type="bibr" target="#b61">Yun et al., 2019;</ref><ref type="bibr" target="#b6">Cubuk et al., 2018;</ref><ref type="bibr" target="#b65">2019)</ref> have significantly improved generalization. Improved learning rate schedules <ref type="bibr" target="#b36">(Loshchilov &amp; Hutter, 2016;</ref><ref type="bibr" target="#b14">Goyal et al., 2017)</ref> have further increased final accuracy. While benchmarking architectures in a short non-regularized training setup facilitates fair comparisons with prior work, it is unclear whether architectural improvements are sustained at larger scales and improved training setups. For example, the RegNet architecture <ref type="bibr" target="#b41">(Radosavovic et al., 2020)</ref> shows strong speedups over baselines in a short non-regularized training setup, but was not tested in a state-of-the-art ImageNet setup (best top-1 is 79.9%).</p><p>Scaling Strategies. Increasing the model dimensions (e.g. width, depth and resolution) has been another successful axis to improve quality <ref type="bibr" target="#b46">(Rosenfeld et al., 2019;</ref><ref type="bibr" target="#b19">Hestness et al., 2017)</ref>. Sheer scale was exhaustively demonstrated to improve performance of neural language models  which motivated the design of ever larger models including GPT-3  and Switch Transformer <ref type="bibr" target="#b10">(Fedus et al., 2021)</ref>. Similarly, scale in computer vision has proven useful. <ref type="bibr" target="#b25">Huang et al. (2018)</ref>   <ref type="bibr" target="#b16">(He et al., 2016;</ref><ref type="bibr" target="#b64">Zhang et al., 2020;</ref><ref type="bibr" target="#b0">Bello, 2021)</ref>. Wide ResNets <ref type="bibr" target="#b62">(Zagoruyko &amp; Komodakis, 2016)</ref> and Mo-bileNets <ref type="bibr" target="#b21">(Howard et al., 2017)</ref> instead scale the width. Increasing image resolutions has also been a reliable source of progress. Thus as training budgets have grown, so have the image resolutions: EfficientNet uses 600 image resolutions  and both ResNeSt <ref type="bibr" target="#b64">(Zhang et al., 2020)</ref> and TResNet <ref type="bibr" target="#b45">(Ridnik et al., 2020)</ref> use 448 image resolutions for their largest model. In an attempt to sys-tematize these heuristics, EfficientNet proposed the compound scaling rule, which recommended balancing the network depth, width and image resolution. However, Section 7.2 shows this scaling strategy is sub-optimal for not only ResNets, but EfficientNets as well.</p><p>Additional Training Data. Another popular way to further improve accuracy is by training on additional sources of data (either labeled, weakly labeled, or unlabeled). Pretraining on large-scale datasets <ref type="bibr" target="#b53">(Sun et al., 2017;</ref><ref type="bibr" target="#b37">Mahajan et al., 2018;</ref><ref type="bibr" target="#b29">Kolesnikov et al., 2019)</ref> has significantly pushed the state-of-the-art, with ViT <ref type="bibr" target="#b8">(Dosovitskiy et al., 2020)</ref> and NFNets <ref type="bibr" target="#b2">(Brock et al., 2021)</ref> recently achieving 88.6% and 89.2% ImageNet accuracy respectively. Noisy Student, a semi-supervised learning method, obtained 88.4% ImageNet top-1 accuracy by using pseudolabels on an extra 130M unlabeled images . Meta pseudo-labels <ref type="bibr" target="#b39">(Pham et al., 2020)</ref>, an improved semi-supervised learning technique, currently holds the ImageNet state-of-the-art (90.2%). We present semisupervised learning results in <ref type="table" target="#tab_7">Table 4</ref> and discuss how our training and scaling strategies transfer to large data regimes in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work on Improving ResNets</head><p>Improved training methods combined with architectural changes to ResNets have routinely yielded competitive Im-ageNet performance <ref type="bibr" target="#b33">Lee et al., 2020;</ref><ref type="bibr" target="#b45">Ridnik et al., 2020;</ref><ref type="bibr" target="#b64">Zhang et al., 2020;</ref><ref type="bibr" target="#b0">Bello, 2021;</ref><ref type="bibr" target="#b2">Brock et al., 2021)</ref>. <ref type="bibr" target="#b17">He et al. (2018)</ref> achieved 79.2% top-1 Ima-geNet accuracy (a +3% improvement over their ResNet-50 baseline) by modifying the stem and downsampling block while also using label smoothing and mixup. <ref type="bibr" target="#b33">Lee et al. (2020)</ref> further improved the ResNet-50 model with additional architectural modifications such as Squeeze-and-Excitation <ref type="bibr" target="#b23">(Hu et al., 2018)</ref>, selective kernel <ref type="bibr" target="#b34">(Li et al., 2019)</ref>, and anti-alias downsampling <ref type="bibr" target="#b65">(Zhang, 2019)</ref>, while also using label smoothing, mixup, and dropblock to achieve 81.4% accuracy. <ref type="bibr" target="#b45">Ridnik et al. (2020)</ref> incorporated several architectural modifications to the ResNet architectures along with improved training methodologies to outperform EfficientNet-B1 to EfficientNet-B5 models on the speed-accuracy Pareto curve.</p><p>Most works, however, put little emphasis on identifying strong scaling strategies. In contrast, we only consider lightweight architectural changes routinely used since 2018 and instead focus on the training and scaling strategies to build a Pareto curve of models. Our improved training and scaling methods lead to ResNets that are 1.7x -2.7x faster than EfficientNets on TPUs. Our scaling improvements are orthogonal to the aforementioned methods and we expect them to be additive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>We describe the base ResNet architecture and the training methods used throughout this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Architecture</head><p>Our work studies the ResNet architecture, with two widely used architecture changes, the ResNet-D  modification and Squeeze-and-Excitation (SE) in all bottleneck blocks <ref type="bibr" target="#b23">(Hu et al., 2018)</ref>. These architectural changes are used in used many architectures, including TResNet, ResNeSt and EfficientNets.</p><p>ResNet-D  combines the following four adjustments to the original ResNet architecture. First, the 7×7 convolution in the stem is replaced by three smaller 3×3 convolutions, as first proposed in Inception-V3 <ref type="bibr" target="#b55">(Szegedy et al., 2016)</ref>. Second, the stride sizes are switched for the first two convolutions in the residual path of the downsampling blocks. Third, the stride-2 1×1 convolution in the skip connection path of the downsampling blocks is replaced by stride-2 2×2 average pooling and then a non-strided 1×1 convolution. Fourth, the stride-2 3×3 max pool layer is removed and the downsampling occurs in the first 3×3 convolution in the next bottleneck block. We diagram these modifications in <ref type="figure">Figure 6</ref>.</p><p>Squeeze-and-Excitation <ref type="bibr" target="#b23">(Hu et al., 2018)</ref> reweighs channels via cross-channel interactions by average pooling signals from the entire feature map. For all experiments we use a Squeeze-and-Excitation ratio of 0.25 based on preliminary experiments. In our experiments, we sometimes use the original ResNet implementation without SE (referred to as ResNet) to compare different training methods. Clear denotations are made in table captions when this is the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Methods</head><p>We study regularization and data augmentation methods that are routinely used in state-of-the art classification models and semi/self-supervised learning.</p><p>Matching the EfficientNet Setup. Our training method closely matches that of EfficientNet, where we train for 350 epochs, but with a few small differences.</p><p>(1) We use the cosine learning rate schedule <ref type="bibr" target="#b36">(Loshchilov &amp; Hutter, 2016)</ref> instead of an exponential decay for simplicity (no additional hyperparameters).</p><p>(2) We use RandAugment <ref type="bibr" target="#b7">(Cubuk et al., 2019)</ref> in all models, whereas EfficientNets were originally trained with AutoAugment <ref type="bibr" target="#b6">(Cubuk et al., 2018)</ref>. We reran EfficientNets B0-B4 with RandAugment and found it offered no performance improvement and report EfficientNet B5 and B7 with the RandAugment results from Cubuk et al.</p><p>(2019) 1 .</p><p>(3) We use the Momentum optimizer instead of RMSProp for simplicity. See <ref type="table" target="#tab_10">Table 10</ref> in the Appendix C for a comparison between our training setup and Efficient-Net.</p><p>Regularization. We apply weight decay, label smoothing, dropout and stochastic depth for regularization. Dropout <ref type="bibr" target="#b52">(Srivastava et al., 2014)</ref> is a common technique used in computer vision and we apply it to the output after the global average pooling occurs in the final layer. Stochastic depth <ref type="bibr" target="#b24">(Huang et al., 2016)</ref> drops out each layer in the network (that has residual connections around it) with a specified probability that is a function of the layer depth.</p><p>Data Augmentation. We use RandAugment <ref type="bibr" target="#b7">(Cubuk et al., 2019)</ref> data augmentation as an additional regularizer. Ran-dAugment applies a sequence of random image transformations (e.g. translate, shear, color distortions) to each image independently during training. As mentioned earlier, originally EfficientNets uses AutoAugment <ref type="bibr" target="#b6">(Cubuk et al., 2018)</ref>, which is a learned augmentation procedure that slightly underperforms RandAugment.</p><p>Hyperparameter Tuning. To select the hyperparameters for the various regularization and training methods, we use a held-out validation set comprising 2% of the ImageNet training set (20 shards out of 1024). This is referred to as the minival-set and the original ImageNet validation set (the one reported in most prior works) is referred to as validation-set. The hyperparameters of all ResNet-RS models are in <ref type="table">Table 8</ref> in the Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Improved Training Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Additive Study of Improvements</head><p>We present an additive study of training, regularization methods and architectural changes in <ref type="table">Table 1</ref>. The baseline ResNet-200 gets 79.0% top-1 accuracy. We improve its performance to 82.2% (+3.2%) through improved training methods alone without any architectural changes. When adding two common and simple architectural changes (Squeeze-and-Excitation and ResNet-D) we further boost the performance to 83.4%. Training methods alone cause 3/4 of the total improvement, which demonstrates their critical impact on ImageNet performance. Error approximately scales as a power law with FLOPs (linear fit on the log-log curve) in the lower FLOPs regime but the trend breaks for larger FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Importance of decreasing weight decay when combining regularization methods</head><p>We observe diminishing returns of scaling the image resolutions beyond 320×320, which motivates the slow image resolution scaling (Strategy #2). The scaling configurations run are width multipliers [0.25,0.5,1.0,1.5,2.0], depths <ref type="bibr">[26,</ref><ref type="bibr">50,</ref><ref type="bibr">101,</ref><ref type="bibr">200,</ref><ref type="bibr">300,</ref><ref type="bibr">350,</ref><ref type="bibr">400]</ref> and image resolutions <ref type="bibr">[128,</ref><ref type="bibr">160,</ref><ref type="bibr">224,</ref><ref type="bibr">320,</ref><ref type="bibr">448]</ref>. FLOPs is the number of floating point operations per image. All results are on the ImageNet minival-set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Improved Scaling Strategies</head><p>The prior section demonstrates the significant impact of training methodology and we now show the scaling strategy is similarly important. In order to establish scaling trends, we perform an extensive search on ImageNet over width multipliers in [0.25,0.5,1.0,1.5,2.0], depths of <ref type="bibr">[26,</ref><ref type="bibr">50,</ref><ref type="bibr">101,</ref><ref type="bibr">200,</ref><ref type="bibr">300,</ref><ref type="bibr">350,</ref><ref type="bibr">400]</ref> and resolutions of <ref type="bibr">[128,</ref><ref type="bibr">160,</ref><ref type="bibr">224,</ref><ref type="bibr">320,</ref><ref type="bibr">448]</ref>. We train these architectures for 350 epochs, mimicking the training setup of state-of-the-art ImageNet models. We increase the regularization as the model size increases to limit overfitting. See Appendix E for regularization and model hyperparameters.</p><p>FLOPs do not accurately predict performance in the bounded data regime. Prior works on scaling laws observe a power law between error and FLOPs in unbounded data regimes <ref type="bibr" target="#b18">Henighan et al., 2020)</ref>. In order to test whether this also holds in our scenario, we plot ImageNet error against FLOPs for all scaling configurations in <ref type="figure">Figure 2</ref>. For the smaller models, we observe an overall power law trend between error and FLOPs, with minor dependency on the scaling configuration (i.e. depth versus width versus image resolution). However, the trend breaks for larger model sizes. Furthermore, we observe a large variation in ImageNet performance for a fixed amount of FLOPs, especially in the higher FLOP regime. Therefore the exact scaling configuration (i.e. depth, width and image resolution) can have a big impact on performance even when controlling for the same amount of FLOPs.</p><p>The best performing scaling strategy depends on the training regime. We next look directly at latencies 2 on the hardware of interest to identify scaling strategies that improve the speed-accuracy Pareto curve. <ref type="figure">Figure 3</ref> presents accuracies and latencies of models scaled with either width or depth across four image resolutions and three different training regimes (10, 100 and 350 epochs). We observe that the best performing scaling strategy, especially whether to scale depth and/or width, highly depends on the training regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Strategy #1 -Depth Scaling in Regimes Where</head><p>Overfitting Can Occur Depth scaling outperforms width scaling for longer epoch regimes. In the 350 epochs setup <ref type="figure">(Figure 3</ref>, right panel), we observe depth scaling to significantly outperform width scaling across all image resolutions. Scaling the width is subject to overfitting and sometimes hurts performance even with increased regularization. We hypothesize that this is due to the larger increase in parameters when scaling the width. The ResNet architecture maintains constant FLOPs across all block groups and multiplies the number of parameters by 4× every block group. Scaling the depth, especially in the earlier layers, therefore introduces fewer parameters compared to scaling the width.</p><p>Width scaling outperforms depth scaling for shorter epoch regimes. In contrast, width scaling is better when only training for 10 epochs <ref type="figure">( Figure 3, left panel)</ref>. For 100 2 FLOPs is not a good indicator of latency on modern hardware. See Section 7.1 for a more detailed discussion. epochs <ref type="figure">(Figure 3, middel panel)</ref>, the best performing scaling strategy varies between depth scaling and width scaling, depending on the image resolution. The dependency of the scaling strategy on the training regime reveals a pitfall of extrapolating scaling rules. We point out that prior works also choose to scale the width when training for a small number of epochs on large-scale datasets (e.g. ∼40 epochs on 300M images), consistent with our experimental findings that scaling the width is preferable in shorter epoch regimes. In particular, <ref type="bibr" target="#b29">Kolesnikov et al. (2019)</ref> train a ResNet-152 with 4x filter multiplier while <ref type="bibr" target="#b2">Brock et al. (2021)</ref> scales the width with ∼1.5x filter multiplier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Strategy #2 -Slow Image Resolution Scaling</head><p>In <ref type="figure">Figure 2</ref>, we also observe that larger image resolutions yield diminishing returns. We therefore propose to increase the image resolution more gradually than previous works. This contrasts with the compound scaling rule proposed by EfficientNet which leads to very large images (e.g. 600 for EfficientNet-B7, 800 for EfficientNet-L2 ). Other works such as ResNeSt <ref type="bibr" target="#b64">(Zhang et al., 2020)</ref> and TResNet <ref type="bibr" target="#b45">(Ridnik et al., 2020)</ref>) scale the image resolution up to 448. Our experiments indicate that slower image scaling improves not only ResNet architectures, but also EfficientNets on a speed-accuracy basis (Section 7.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Two Common Pitfalls in Designing Scaling Strategies</head><p>Our scaling analysis surfaces two common pitfalls in prior research on scaling strategies:</p><p>(1) Extrapolating scaling strategies from small-scale regimes. Scaling strategies found in small scale regimes (e.g. on small models or with few training epochs) can fail to generalize to larger models or longer training iterations. The dependencies between the best performing scaling strategy and the training regime are missed by prior works which extrapolate scaling rules from either small models  or shorter training epochs <ref type="bibr" target="#b41">(Radosavovic et al., 2020)</ref>. We therefore do not recommend generating scaling rules exclusively in a small scale regime because these rules can break down.</p><p>(2) Extrapolating scaling strategies from a single and potentially sub-optimal initial architecture. Beginning from a sub-optimal initial architecture can skew the scaling results. For example, the compound scaling rule derived from a small grid search around EfficientNet-B0, which was obtained by architecture search using a fixed FLOPs budget and a specific image resolution. However, since this image resolution can be sub-optimal for that FLOPs budget, the resulting scaling strategy can be sub-optimal. In contrast, our work designs scaling strategies by training models across a variety of widths, depths and image resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Summary of Improved Scaling Strategies</head><p>For a new task, we recommend running a small subset of models across different scales, for the full training epochs, to gain intuition on which dimensions are the most useful across model scales. While this approach may appear more costly, we point out that the cost is offset by not searching for the architecture.</p><p>For image classification, the scaling strategies are summarized as (1) scale the depth in regimes where overfitting can occur (scaling the width is preferable otherwise) and</p><p>(2) slow image resolution scaling. Experiments indicate that applying these scaling strategies to ResNets (ResNet-RS) and EfficientNets (EfficientNet-RS) leads to significant speed-ups over EfficientNets. We note that similar scaling strategies are also employed in recent works that obtain large speed-ups over EfficientNets such as Lamb-daResNets <ref type="bibr" target="#b0">(Bello, 2021)</ref> and NFNets <ref type="bibr" target="#b2">(Brock et al., 2021</ref>  since EfficientNets significantly reduce both the parameter count and the FLOPs compared to ResNets. We next discuss why a model with fewer parameters and fewer FLOPs (EfficientNet) is slower and more memory-intensive during training.</p><p>FLOPs vs Latency. While FLOPs provide a hardwareagnostic metric for assessing computational demand, they may not be indicative of actual latency times for training and inference <ref type="bibr" target="#b21">(Howard et al., 2017;</ref><ref type="bibr" target="#b65">2019;</ref><ref type="bibr" target="#b41">Radosavovic et al., 2020)</ref>. In custom hardware architectures (e.g. TPUs and GPUs), FLOPs are an especially poor proxy because operations are often bounded by memory access costs and have different levels of optimization on modern matrix multiplication units <ref type="bibr" target="#b26">(Jouppi et al., 2017)</ref>. The inverted bottlenecks <ref type="bibr" target="#b48">(Sandler et al., 2018)</ref> used in EfficientNets employ depthwise convolutions with large activations and have a small compute to memory ratio (operational intensity) compared to the ResNet's bottleneck blocks which employ dense convolutions on smaller activations. This makes Effi-cientNets less efficient on modern accelerators compared to ResNets.  vations 3 . The large activations used in EfficientNets also cause larger memory consumption, which is exacerbated by the use of large image resolutions, compared to our rescaled ResNets. A ResNet-RS model with 3.8x more parameters than EfficientNet-B6 consumes 2.3x less memory for a similar ImageNet accuracy <ref type="table" target="#tab_4">(Table 3</ref>). We emphasize that both memory consumption and latency are tightly coupled to the software and hardware stack (TensorFlow on TPUv3) due to compiler optimizations such as operation layout assignments and memory padding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Improving the Efficiency of EfficientNets</head><p>The scaling analysis from Section 6 reveals that scaling the image resolution results in diminishing returns. This suggests that the scaling rules advocated in EfficientNets which increases model depth, width and resolution independently of model scale is sub-optimal. We apply the slow image resolution scaling strategy (Strategy #2) to Efficient-Nets and train several versions with reduced image resolutions, without changing the width or depth. The RandAugment magnitude is set to 10 for image resolution 224 or smaller, 20 for image resolution larger than 320 and 15 otherwise. All other hyperparameters are kept the same as per the original EfficientNets. <ref type="figure" target="#fig_3">Figure 5</ref> demonstrates a marked improvement of the re-scaled EfficientNets (EfficientNet-RS) on the speed-accuracy Pareto curve over the original EfficientNets.</p><p>3 Activations are typically stored during training as they are used in backpropagation. At inference, activations can be discarded and parameter count is a better proxy for actual memory consumption.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Semi-Supervised Learning with ResNet-RS</head><p>We measure how ResNet-RS performs as we scale to larger datasets in a large scale semi-supervised learning setup. We train ResNets-RS on the combination of 1.2M labeled ImageNet images and 130M pseudo-labeled images, in a similar fashion to Noisy Student . We use the same dataset of 130M images pseudo-labeled as Noisy Student, where the pseudo labels are generated from an EfficientNet-L2 model with 88.4% ImageNet accuracy. Models are jointly trained on both the labeled and pseudolabeled data and training hyperparameters are kept the same.  <ref type="table">Table 5</ref>. Representations from supervised learning with improved training strategies rival or outperform representations from state-of-the-art self-supervised learning algorithms. Comparison of supervised training methods (supervised, RS) and self-supervised methods (SimCLR, SimCLRv2) on a variety of downstream tasks. The (RS) strategy greatly outperforms the baseline supervised training, which highlights the importance of using improved supervised training techniques when comparing to self-supervised learning algorithms. The RS training method uses a subset of the training methods highlighted in this work (cosine LR decay, RandAugment label smoothing, reduced weight decay, and dropout on FC) to more closely match those used in the self-supervised algorithms. All models employ the vanilla ResNet architecture without modifications and are pre-trained on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Transfer Learning of ResNet-RS</head><p>We now investigate whether the improved supervised training strategies yield better representations for transfer learning and compare them with self-supervised learning algorithms. Recent self-supervised learning algorithms claim to surpass the transfer learning performance of supervised learning and create more universal representations <ref type="bibr" target="#b4">(Chen et al., 2020a;</ref>. Self-supervised algorithms, however, make several changes to the training methods (e.g training for more epochs, data augmentation) making comparisons to supervised learning difficult. <ref type="table">Table 5</ref> compares the transfer performance of improved supervised training strategies (denoted RS) against self-supervised SimCLR <ref type="bibr" target="#b4">(Chen et al., 2020a)</ref> and SimCLRv2 <ref type="bibr" target="#b5">(Chen et al., 2020b)</ref>. In an effort to closely match SimCLR's training setup and provide fair comparisons, we restrict the RS training strategies to a subset of its original methods. Specifically, we employ data augmentation (RandAugment), label smoothing, dropout, decreased weight decay and cosine learning rate decay for 400 epochs but do not use stochastic depth or exponential moving average (EMA) of the weights. We choose this subset to closely match the training setup of SimCLR: longer training, data augmentation and a temperature parameter for their contrastive loss 4 . We use the vanilla ResNet architecture without the ResNet-D modifications or Squeezeand-Excite, matching the SimCLR and SimCLRv2 architectures.</p><p>We evaluate the transfer performance on five downstream tasks: CIFAR-100 Classification <ref type="bibr" target="#b31">(Krizhevsky et al., 2009</ref>), Pascal Detection &amp; Segmentation <ref type="bibr" target="#b9">(Everingham et al., 2010)</ref>, ADE Segmentation  and NYU Depth <ref type="bibr" target="#b50">(Silberman et al., 2012)</ref>. We find that, even when restricted to a smaller subset, the improved training strategies improve transfer performance 5 . The improved supervised representations (RS) outperform SimCLR on 5/10 downstream tasks and SimCLRv2 on 8/10 tasks. Furthermore, the improved training strategies significantly outperform the standard supervised ResNet representations, highlighting the need for using modern training techniques when comparing to self-supervised learning. While selfsupervised learning can be used on unlabeled data, our results challenge the notion that self-supervised algorithms lead to more universal representations than supervised learning when labels are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Revised 3D ResNet for Video Classification</head><p>We conclude by applying the training strategies to the Kinetics-400 video classification task, using a 3D ResNet as the baseline architecture ) (see Appendix G for experimental details). be obtained without architectural changes. Without model scaling, 3D ResNet-RS-50 is only 2.2% less than the best number reported on Kinetics-400 at 80.4% <ref type="bibr" target="#b76">(Feichtenhofer, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Discussion</head><p>Why is it important to tease apart improvements coming from training methods vs architectures? Training methods can be more task-specific than architectures (e.g. data augmentation is more helpful on small datasets). Therefore, improvements coming from training methods do not necessarily generalize as well as architectural improvements. Packaging newly proposed architectures together with training improvements makes accurate comparisons between architectures difficult. The large improvements coming from training strategies, when not being controlled for, can overshadow architectural differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How should one compare different architectures?</head><p>Since training methods and scale typically improve performance <ref type="bibr" target="#b33">(Lee et al., 2020;</ref><ref type="bibr" target="#b27">Kaplan et al., 2020)</ref>, it is critical to control for both aspects when comparing different architectures. Controlling for scale can be achieved through different metrics. While many works report parameters and FLOPs, we argue that latencies and memory consumption are generally more relevant <ref type="bibr" target="#b41">(Radosavovic et al., 2020)</ref>. Our experimental results (Section 7.1) re-emphasize that FLOPs and parameters are not representative of latency or memory consumption <ref type="bibr" target="#b41">(Radosavovic et al., 2020;</ref><ref type="bibr">Norrie et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Do the improved training strategies transfer across tasks?</head><p>The answer depends on the domain and dataset sizes available. Many of the training and regularization methods studied here are not used in large-scale pre-training (e.g. 300M images) <ref type="bibr" target="#b29">(Kolesnikov et al., 2019;</ref><ref type="bibr" target="#b8">Dosovitskiy et al., 2020)</ref>. Data augmentation is useful for small datasets or when training for many epochs, but the specifics of the augmentation method can be taskdependent (e.g. scale jittering instead of RandAugment in <ref type="table" target="#tab_8">Table 6</ref>).</p><p>Do the scaling strategies transfer across tasks? The best performing scaling strategy depends on the training regime and whether overfitting is an issue, as discussed in Section 6. When training for 350 epochs on ImageNet, we find scaling the depth to work well, whereas scaling the width is preferable when training for few epochs (e.g. 10 epochs). This is consistent with works employing width scaling when training for few epochs on large-scale datasets <ref type="bibr" target="#b29">(Kolesnikov et al., 2019)</ref>. We are unsure how our scaling strategies apply in tasks that require larger image resolutions (e.g. detection and segmentation) and leave this to future work.</p><p>Are architectural changes useful? Yes, but training methods and scaling strategies can have even larger impacts. Simplicity often wins, especially given the nontrivial performance issues arising on custom hardware. Architecture changes that decrease speed and increase complexity may be surpassed by scaling up faster and simpler architectures that are optimized on available hardware (e.g convolutions instead of depthwise convolutions for GPUs/TPUs). We envision that future successful architectures will emerge by co-design with hardware, particularly in resource-tight regimes like mobile phones .</p><p>How should one allocate a computational budget to produce the best vision models? We recommend beginning with a simple architecture that is efficient on available hardware (e.g. ResNets on GPU/TPU) and training several models, to convergence, with different image resolutions, widths and depths to construct a Pareto curve. Note that this strategy is distinct from  which instead allocate a large portion of the compute budget for identifying an optimal initial architecture to scale. They then do a small grid search to find the compound scaling coefficients used across all model scales. RegNet <ref type="bibr" target="#b41">(Radosavovic et al., 2020)</ref> does most of their studies when training for only 10 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>By updating the de facto vision baseline with modern training methods and an improved scaling strategy, we have revealed the remarkable durability of the ResNet architecture. Simple architectures set strong baselines for stateof-the-art methods. We hope our work encourages further scrutiny in maintaining consistent methodology for both proposed innovations and baselines alike. Hyperparameters <ref type="table">Table 8</ref> presents the training and regularization hyperparameters used for training ResNet-RS models. We increase regularization as with model scale. Note that we have less hyperparameter setups compared to Efficient-Nets . We perform early stopping on the minival-set set for the two largest models from <ref type="table" target="#tab_9">Table 7</ref> (ResNet-RS-350 at resolution 320 and ResNet-RS-420 at resolution 320). For every other model, we simply report the final accuracy. We present top-1 accuracies on the ImageNet test-set for two ResNet-RS models in <ref type="table">Table 9</ref>. We observe no sign of overfitting.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ResNet-RS Training and Regularization Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. ResNet-RS Architecture Details</head><p>We provide more details of the ResNet-RS architectural changes. We reiterate that ResNet-RS is a combination of: improved scaling strategies, improved training methodologies, the ResNet-D modifications  and the Squeeze-Excitation module <ref type="bibr" target="#b23">(Hu et al., 2018)</ref>. <ref type="table" target="#tab_13">Table 11</ref> shows the block layouts for all ResNet depths used throughout our work. ResNet-50 through ResNet-200 use the standard block configurations from <ref type="bibr" target="#b15">He et al. (2015)</ref>. ResNet-270 and onward primarily scale the number of blocks in c3 and c4 and we try to keep their ratio roughly constant. We empirically found that adding blocks in the lower stages limits overfitting as blocks in the lower layers have significantly less parameters, even though all blocks have the same amount of FLOPs. <ref type="figure">Figure 6</ref> shows the ResNet-D architectural changes used in our  <ref type="bibr" target="#b15">He et al. (2015)</ref>. The different numbers represent the number of blocks in c2, c3, c4 and c5 respectively. Note that our depth scaling mainly scales the blocks in c3 and c4, which limits overfitting (due to the increase in parameters) that can occur when blocks are added to c5.</p><p>ResNet-RS models.  <ref type="figure">Figure 6</ref>. ResNet-RS Architecture Diagram. Output Size assumes a 224×224 input image resolution. In the convolutional layout column x2 refers to the the first 3×3 convolution being applied with a stride of 2. The ResNet-RS architecture is a simple combination of Squeeze-and-Excitation and ResNet-D. The × symbol refers to how many times the blocks are repeated in the ResNet-101 architecture. These values change across depths according to the blocks layouts in <ref type="table" target="#tab_13">Table 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Scaling Analysis Regularization and Model Details</head><p>Regularization for 350 epoch models. The dropout rates used for various filter multipliers (across all image resolutions and depths) are in <ref type="table">Table 12</ref>. RandAugment is used with 2 layers and its magnitude is set to 10 for filter multipliers in [0.25, 0.5] or image resolution in <ref type="bibr">[64,</ref><ref type="bibr">160]</ref>, 15 for image resolution in <ref type="bibr">[224,</ref><ref type="bibr">320]</ref> and 20 otherwise. We apply stochastic depth with a drop rate of 0.2 for image resolutions 224 and above. We do not apply stochastic depth filter multiplier 0.25 (or images smaller than 224). All models use a label smoothing of 0.1 and a weight decay of 4e-5. These values were set based on the preliminary experiments across various model scales on the ImageNet minival-set.</p><p>Filter Scaling Dropout Rate 0.25 0.0 0.5 0.1 1.0 0.25 1.5 0.6 2.0 0.75 <ref type="table">Table 12</ref>. Dropout values for filter scaling. Filter scaling refers to the filter scaling multiplier based on the number of filters in the original ResNet architecture. batch size, 0.8 learning rate with cosine decay and train for 200 epochs for the baseline. At inference, we use 256×256 crop size for the spatial domain and adopt the 30 views protocol .</p><p>Starting from the baseline, we apply the following training methods: dropout with a rate of 0.5, 0.1 label smoothing, stochastic depth with 0.2 drop rate, EMA of weights, smaller weight decay (set to 4e-5) and a 350 epoch training schedule. For data augmentation, we use scale jittering  as a replacement to RandAugment. We adjust the stochastic depth rate to 0.1 when applying scale jittering to optimize performance. To implement the ResNet-D stem for the 3D ResNet, we use the same kernel configurations for the spatial domain and use temporal kernel sizes of [5, 1, 1] for the three layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Profiling Setup</head><p>All latencies refer to training latencies. All models were run on TPUv3 <ref type="bibr" target="#b26">(Jouppi et al., 2017)</ref> with bfloat16 precision in TensorFlow 1.x. TPU latencies are measured on 8 TPUv3 cores with a batch size of 1024 (i.e. 128 per core) which is divided by 2 until it fits onto the accelerator's memory. In the cases where a smaller batch size is employed, we normalize the reported latency to the original batch size of 1024 images. For GPU profiling we use a single Tesla-V100 with float32 precision with a starting batch size of 128, also divided by multiples of 2 if necessary.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>However, research often emphasizes architectural changes. Novel ar-Improving ResNets to state-of-the-art performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Speed-Accuracy Pareto curve comparing ResNets-RS to EfficientNet. Properly scaled ResNets (ResNet-RS) are 1.7x -2.7x faster than the popular EfficientNets when closely matching their training setup. ResNet-RS are annotated with (depth -image resolution), so 152-256 means ResNet-RS-152 with image resolution 256×256. All results are on the ImageNet validation-set and training times are measured on TPUs. See Appendix B for detailed results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Speed-Accuracy Pareto curve comparing ResNets-RS and EfficientNet-RS to EfficientNet. Scaling Efficient-Nets using the slow image resolution scaling strategy (instead of the original compound scaling rule) improves the Pareto efficiency of EfficientNets. Note that ResNet-RS still outperforms EfficientNet-RS. This figure is a zoomed in version of Figure 4 with EfficientNet-RS added. Models are annotated with (model depth -image resolution), so 152-192 corresponds to ResNet-RS-152 with image resolution 192×192. Results are reported on the ImageNet validation-set and training times are measured on TPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 Table 1 Table 2 .</head><label>212</label><figDesc>highlights the importance of changing weight decay when combining regularization methods together. Decrease weight decay when using more regularization. Top-1 ImageNet accuracy for different regularization combinations. Decreasing the weight decay improves performance when combining regularization methods such as dropout (DO), stochastic depth (SD), label smoothing (LS) and RandAugment (RA). Image resolution is 224×224 for ResNet-50 and 256×256 for ResNet-200. All numbers are reported on the ImageNet minival-set from an average of two runs.When applying RandAugment and label smoothing, there is no need to change the default weight decay of 1e-4. But when we further add dropout and/or stochastic depth, the performance can decrease unless we further decrease the weight decay. The intuition is that since weight decay acts as a regularizer, its value must be decreased in order to not overly regularize the model when combining many techniques. Furthermore,<ref type="bibr" target="#b68">Zoph et al. (2020a)</ref> presents evidence that the addition of data augmentation shrinks the L2 norm of the weights, which renders some of the effects of weight decay redundant. Other works use smaller weight decay values, but do not point out the significance of the effect when using more regularization.</figDesc><table><row><cell cols="2">Improvements ResNet-200 + Cosine LR Decay + Increase training epochs + EMA of weights + Label Smoothing + Stochastic Depth + RandAugment + Dropout on FC + Decrease weight decay</cell><cell>Top-1 79.0 79.3 78.8  † 79.1 80.4 80.6 81.0 80.7  ‡ 82.2</cell><cell>∆ -+0.3 -0.5 +0.3 +1.3 +0.2 +0.4 -0.3 +1.5</cell><cell>2 × 10 1 3 × 10 1 4 × 10 1 ImageNet Error</cell><cell cols="2">Scaling Properties of ResNets</cell><cell>Image Size 128 160 224 320 448</cell></row><row><cell cols="2">+ Squeeze-and-Excitation</cell><cell>82.9</cell><cell>+0.7</cell><cell></cell><cell></cell></row><row><cell cols="2">+ ResNet-D</cell><cell>83.4</cell><cell>+0.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>10 8</cell><cell>10 9</cell><cell>10 10 FLOPs</cell><cell>10 11</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Figure 2. Scaling properties of ResNets across varying model</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>scales.</cell><cell></cell></row><row><cell>Model</cell><cell>Regularization</cell><cell cols="2">Weight Decay</cell><cell>∆</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1e-4</cell><cell>4e-5</cell><cell></cell><cell></cell></row><row><cell>ResNet-50</cell><cell>None</cell><cell>79.7</cell><cell>78.7</cell><cell>-1.0</cell><cell></cell></row><row><cell>ResNet-50</cell><cell>RA-LS</cell><cell>82.4</cell><cell>82.3</cell><cell>-0.1</cell><cell></cell></row><row><cell>ResNet-50</cell><cell>RA-LS-DO</cell><cell>82.2</cell><cell>82.7</cell><cell>+0.5</cell><cell></cell></row><row><cell cols="2">ResNet-200 None</cell><cell>82.5</cell><cell>81.7</cell><cell>-0.8</cell><cell></cell></row><row><cell cols="2">ResNet-200 RA-LS</cell><cell>85.2</cell><cell>84.9</cell><cell>-0.3</cell><cell></cell></row><row><cell cols="3">ResNet-200 RA-LS-SD-DO 85.3</cell><cell>85.5</cell><cell>+0.2</cell><cell></cell></row></table><note>. Additive study of the ResNet-RS training recipe. The colors refer to Training Methods , Regularization Methods and Architecture Improvements . The baseline ResNet-200 was trained for the standard 90 epochs using a stepwise learn- ing rate decay schedule. The image resolution is 256×256. All numbers are reported on the ImageNet validation-set and averaged over 2 runs.† Increasing training duration to 350 epochs only becomes useful once the regularization methods are used, otherwise the accuracy drops due to over-fitting.‡ dropout hurts as we have not yet decreased the weight decay (See Table 2 for more details).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Scaling of ResNets across depth, width, image resolution and training epochs. We compare depth scaling and width scaling across four different image resolutions[128,160,224,320]  when training models for 10, 100 or 350 epochs. We find that the best performing scaling strategy depends on the training regime, which reveals the pitfall of extrapolating scaling rules from small scale regimes. (Left) 10 Epoch Regime: width scaling is the best strategy for the speed-accuracy Pareto curve. (Middle) 100 Epoch Regime: depth scaling is sometimes outperformed by width scaling. (Right) 350 Epoch Regime: depth scaling consistently outperforms width scaling by a large margin. Overfitting remains an issue even when using regularization methods. Model Details: All models start from a depth of 101 and are increased through[101,200,300,400]. All model widths start with a multiplier of 1.0x and are increased through [1.0,1.5,2.0]. For all models, we tune regularization in an effort to limit overfitting (see Appendix E). Accuracies are reported on the ImageNet minival-set and training times are measured on TPUs.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epochs: 10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epochs: 100</cell><cell></cell><cell></cell><cell>86</cell><cell></cell><cell>Epochs: 350</cell></row><row><cell>68 70 72 74 Top-1 ImageNet Accuracy</cell><cell cols="3">Res: 128 Res: 224 Res: 160</cell><cell>Res: 320</cell><cell cols="2">Depth Scaling Width Scaling</cell><cell>77 78 79 80 81 82 Top-1 ImageNet Accuracy</cell><cell></cell><cell cols="2">Res: 128 Res: 160 Res: 224</cell><cell cols="2">Res: 320 Depth Scaling Width Scaling</cell><cell>80 81 82 83 84 85 Top-1 ImageNet Accuracy</cell><cell></cell><cell>Res: 128</cell><cell>Res: 160 Res: 224</cell><cell>Res: 320 Depth Scaling Width Scaling</cell></row><row><cell></cell><cell>0.05</cell><cell>0.10</cell><cell cols="3">0.15 Time Per Training Step (Sec) 0.20 0.25 0.30</cell><cell>0.35</cell><cell>0.40</cell><cell>0.05</cell><cell>0.10</cell><cell cols="2">0.15 Time Per Training Step (Sec) 0.20 0.25 0.30</cell><cell>0.35</cell><cell>0.40</cell><cell>0.05</cell><cell>0.10</cell><cell>0.15 Time Per Training Step (Sec) 0.20 0.25 0.30</cell><cell>0.35</cell><cell>0.40</cell></row><row><cell cols="2">Figure 3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>illustrates this point: a ResNet-RS model with 1.8x more FLOPs than EfficientNet-B6 is 2.7x faster on a TPUv3 hardware accelerator.Parameters vs Memory. Parameter count does not necessarily dictate memory consumption during training because memory is often dominated by the size of the acti-</figDesc><table><row><cell>Model</cell><cell cols="4">RS-350 ENet-B6 RS-420 ENet-B7</cell></row><row><cell>Resolution</cell><cell>256</cell><cell>528</cell><cell>320</cell><cell>600</cell></row><row><cell>Top-1 Acc.</cell><cell>84.0</cell><cell>84.0</cell><cell>84.4</cell><cell>84.7</cell></row><row><cell>Params (M)</cell><cell>164</cell><cell>43 (3.8x)</cell><cell>192</cell><cell>66 (2.9x)</cell></row><row><cell>FLOPs (B)</cell><cell>69</cell><cell>38 (1.8x)</cell><cell>128</cell><cell>74 (1.7x)</cell></row><row><cell>TPU-v3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Latency (s)</cell><cell>1.1</cell><cell>3.0 (2.7x)</cell><cell>2.1</cell><cell>6.0 (2.9x)</cell></row><row><cell>Memory (GB)</cell><cell>7.3</cell><cell>16.6 (2.3x)</cell><cell>15.5</cell><cell>28.3 (1.8x)</cell></row><row><cell>V100</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Latency (s)</cell><cell>4.7</cell><cell>15.7 (3.3x)</cell><cell>10.2</cell><cell>29.9 (2.8x)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Performance</figDesc><table><row><cell>comparison of ResNet-RS and Efficient-</cell></row><row><cell>Net (abbreviated ENet). Although ResNet-RS has more parame-</cell></row><row><cell>ters and FLOPs, the model employs less memory and runs faster</cell></row><row><cell>on TPUs and GPUs. TPU latency is reported as the time per train-</cell></row><row><cell>ing step for 1024 images on 8 TPUv3 cores. Memory is reported</cell></row><row><cell>on 32 images per core, using bfloat16 precision without fusion</cell></row><row><cell>or rematerialization. See Appendix H for more profiling details.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">reveals that ResNet-RS models are very</cell></row><row><cell cols="4">strong in the semi-supervised learning setup as well. We</cell></row><row><cell cols="4">obtain a top-1 ImageNet accuracy of 86.2%, while being</cell></row><row><cell cols="4">4.7x faster on TPU (5.5x on GPU) than the corresponding</cell></row><row><cell cols="3">Noisy Student EfficientNet-B5 model.</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">V100 (s) TPUv3 (ms) Top-1</cell></row><row><cell>EfficientNet-B5</cell><cell>8.16</cell><cell>1510</cell><cell>86.1</cell></row><row><cell>ResNet-RS-152</cell><cell cols="3">1.48 (5.5x) 320 (4.7x) 86.2</cell></row><row><cell cols="4">Table 4. ResNet-RS are efficient semi-supervised learners.</cell></row><row><cell cols="4">ResNet-RS-152 with image resolution 224 is 4.7x faster on TPU</cell></row><row><cell cols="4">(5.5x on GPU) than EfficientNet-B5 Noisy Student (Xie et al.,</cell></row><row><cell cols="4">2020) for a similar ImageNet accuracy. Both models train on the</cell></row><row><cell cols="4">same additional 130M pseudo-labeled images. See Appendix H</cell></row><row><cell cols="2">for details on latency measurements.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>presents an</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>A. Author Contributions IB, BZ: led the research, designed and ran the scaling experiments, designed and experimented with the training strategies. JS, TL, EC, AS, WF, XD: advised the research, proposed experiments and helped with the writing. AS, IB, BZ: ran preliminary experiments using label smoothing, longer training and RandAugment. IB: demonstrated ResNets outperforming EfficientNets across all scales, designed the scaling strategies and the Pareto curve of models, designed/ran (semi-)supervised learning experiments and significantly contributed to the writing. BZ: ran the regularization studies. WF, BZ: did a majority of the writing. BZ, EC: analyzed scaling experiments and generated the scaling plots. XD: proposed, designed and ran the 3D video classification experiments, lead the open-sourcing. AS: proposed lowering the weight decay for better performance and ran preliminary experiments comparing SimCLR to supervised learning. TL: designed and ran the transfer learning experiments comparing to self-supervised learning.B. Details of all ResNet-RS models in the Pareto curveThis section details all the models in the ResNet-RS Pareto curve. InTable 7, we observe that our ResNet-RS models get speedups ranging from 1.7x -2.7x across the EfficientNet Pareto curve on TPUs. Details of ResNet-RS models in Pareto curve. All models are trained for 350 epochs using the improvements mentioned in Section 5. The exact hyperparameters for all ResNet-RS models are inTable 8. Latencies on Tesla V100 GPUs are measured with full precision (float32). Latencies on TPUv3 are measured using bfloat16 precision. All latencies are measured with an initial training batch size of 128 images, which is divided by 2 until it fits onto the accelerator.</figDesc><table><row><cell>Model</cell><cell cols="6">Image Resolution Params (M) FLOPs (B) V100 Latency (s) TPUv3 Latency (ms) Top-1</cell></row><row><cell>EfficientNet-B0</cell><cell>224</cell><cell>5.3</cell><cell>0.8</cell><cell>0.47</cell><cell>90</cell><cell>77.1</cell></row><row><cell>EfficientNet-B1</cell><cell>240</cell><cell>7.8</cell><cell>1.4</cell><cell>0.82</cell><cell>150</cell><cell>79.1</cell></row><row><cell>ResNet-RS-50</cell><cell>160</cell><cell>36</cell><cell>4.6</cell><cell>0.31</cell><cell>70</cell><cell>78.8</cell></row><row><cell>EfficientNet-B2</cell><cell>260</cell><cell>9.2</cell><cell>2.0</cell><cell>1.03</cell><cell>210</cell><cell>80.1</cell></row><row><cell>ResNet-RS-101</cell><cell>160</cell><cell>64</cell><cell>8.4</cell><cell>0.48 (2.1×)</cell><cell>120 (1.8×)</cell><cell>80.3</cell></row><row><cell>EfficientNet-B3</cell><cell>300</cell><cell>12</cell><cell>3.6</cell><cell>1.76</cell><cell>340</cell><cell>81.6</cell></row><row><cell>ResNet-RS-101</cell><cell>192</cell><cell>64</cell><cell>12</cell><cell>0.70</cell><cell>170</cell><cell>81.2</cell></row><row><cell>ResNet-RS-152</cell><cell>192</cell><cell>87</cell><cell>18</cell><cell>0.99</cell><cell>240</cell><cell>82.0</cell></row><row><cell>EfficientNet-B4</cell><cell>380</cell><cell>19</cell><cell>8.4</cell><cell>4.0</cell><cell>710</cell><cell>82.9</cell></row><row><cell>ResNet-RS-152</cell><cell>224</cell><cell>87</cell><cell>24</cell><cell>1.48 (2.7×)</cell><cell>320 (2.2×)</cell><cell>82.8</cell></row><row><cell>ResNet-RS-152</cell><cell>256</cell><cell>87</cell><cell>31</cell><cell>1.76 (2.3×)</cell><cell>410 (1.7×)</cell><cell>83.0</cell></row><row><cell>EfficientNet-B5</cell><cell>456</cell><cell>30</cell><cell>20</cell><cell>8.16</cell><cell>1510</cell><cell>83.7</cell></row><row><cell>ResNet-RS-200</cell><cell>256</cell><cell>93</cell><cell>40</cell><cell>2.86</cell><cell>570</cell><cell>83.4</cell></row><row><cell>ResNet-RS-270</cell><cell>256</cell><cell>130</cell><cell>54</cell><cell>3.76 (2.2×)</cell><cell>780 (1.9×)</cell><cell>83.8</cell></row><row><cell>EfficientNet-B6</cell><cell>528</cell><cell>43</cell><cell>38</cell><cell>15.7</cell><cell>3010</cell><cell>84.0</cell></row><row><cell>ResNet-RS-350</cell><cell>256</cell><cell>164</cell><cell>69</cell><cell>4.72 (3.3×)</cell><cell>1100 (2.7×)</cell><cell>84.0</cell></row><row><cell>EfficientNet-B7</cell><cell>600</cell><cell>66</cell><cell>74</cell><cell>29.9</cell><cell>6020</cell><cell>84.7</cell></row><row><cell>ResNet-RS-350</cell><cell>320</cell><cell>164</cell><cell>107</cell><cell>8.48</cell><cell>1630</cell><cell>84.2</cell></row><row><cell>ResNet-RS-420</cell><cell>320</cell><cell>192</cell><cell>128</cell><cell>10.16</cell><cell>2090</cell><cell>84.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10</head><label>10</label><figDesc></figDesc><table /><note>shows the differences in training and regularization methods between ResNets, ResNet-RS and EfficientNets. Overall we closely match EfficientNet's training setup, while making a few minor simplications: cosine learning rate</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Hyperparameters for all ResNet-RS models. All models train for 350 epochs, use a weight decay of 4e-5, an EMA value of 0.9999 (for both weights and Batch Norm moving averages), 2 layers of RandAugment (with different magnitudes as shown above) and a label smoothing rate of 0.1. The learning rate is warmed up to a maximum value of 0.1/B, with B the batch size, and decayed to 0 using a cosine schedule<ref type="bibr" target="#b36">(Loshchilov &amp; Hutter, 2016)</ref>. Dropout rate means each activation after the global average pooling layers gets dropped out with probability dropout rate. ImageNet accuracies on the validation and test splits.isntead of exponential decay and Momentum instead of RMSProp. Both simplifications reduce the total number of hyperparameters as (1) cosine decay has no hyperparameters associated with it and (2) Momentum has one less than RMSProp.</figDesc><table><row><cell>Model</cell><cell cols="4">Image Resolution top-1 Val top-1 Test</cell></row><row><cell cols="2">ResNet-RS-152</cell><cell>224</cell><cell>82.8</cell><cell>82.7</cell></row><row><cell cols="2">ResNet-RS-270</cell><cell>256</cell><cell>83.8</cell><cell>83.7</cell></row><row><cell></cell><cell cols="4">ResNet (2015) ResNet-RS (2021) EfficientNets (2019)</cell></row><row><cell>Epochs Trained</cell><cell>90</cell><cell></cell><cell>350</cell><cell>350</cell></row><row><cell>LR Decay Schedule</cell><cell>Stepwise</cell><cell></cell><cell>Cosine</cell><cell>Exponential Decay</cell></row><row><cell>Optimizer</cell><cell cols="2">Momentum</cell><cell>Momentum</cell><cell>RMSProp</cell></row><row><cell>EMA of Weights</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Label Smoothing</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stochastic Depth</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RandAugment</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dropout on FC</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Smaller Weight Decay</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Squeeze-Excitation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stem Modifications</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>Comparing training method between ResNet, ResNet-RS and EfficientNet. ResNet (2015) refers to the ResNet originally trained in<ref type="bibr" target="#b15">He et al. (2015)</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 .</head><label>11</label><figDesc>Block configurations for all ResNet depths used in the ResNet-RS Pareto Curve. ResNets of depths 50, 101, 152 and 200 use the standard block allocations from</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Google Brain 2 UC Berkeley. Correspondence to: Irwan Bello and Barret Zoph &lt;{ibello,barretzoph}@google.com&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This makes our comparison to EfficientNet-B6 more nuanced as the B6 performance most likely could be improved by 0.1-0.3% top-1 if ran with RandAugment (based on improvements obtained from B5 and B7).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that SimCLR and SimCLRv2 might benefit further when combining with RandAugment, but the same may also hold true when combining SimCLR's augmentation with RandAugment for supervised learning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5"><ref type="bibr" target="#b30">Kornblith et al. (2019)</ref> similarly observed that better Ima-geNet top-1 accuracy (either through better architectures or training strategies) strongly correlates with improved transfer learning performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambdanetworks</surname></persName>
		</author>
		<title level="m">Modeling long-range interactions without attention. International Conference in Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Highperformance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semisupervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>X3d</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="200" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6201" to="6210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10727" to="10737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Accurate</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14701</idno>
		<title level="m">Scaling laws for autoregressive generative modeling</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep learning scaling is predictable, empirically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kianinejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M A</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00409</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobilenets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11491</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06965</idno>
		<title level="m">Efficient training of giant neural networks using pipeline parallelism</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th annual international symposium on computer architecture</title>
		<meeting>the 44th annual international symposium on computer architecture</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amodei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
	</analytic>
	<monogr>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Big</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Transfer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<title level="m">General visual representation learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2661" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Compounding the performance improvements of assembled techniques in a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06268</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The design process for google&apos;s training chips: Tpuv2 and tpuv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page">2021</biblScope>
			<pubPlace>Micro</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10580</idno>
		<title level="m">Meta pseudo labels</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<title level="m">Spatiotemporal contrastive video representation learning. arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13678</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
		<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tresnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13630</idno>
		<title level="m">High performance gpu-dedicated architecture</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A constructive prediction of the generalization error across scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12673</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Global self-attention networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03019</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02968</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resnest</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11486</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning data augmentation strategies for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="566" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Rethinking pre-training and selftraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">We did not use RandAugment, Dropout, Stochastic Depth or Label Smoothing</title>
		<imprint/>
	</monogr>
	<note>Regularization for 10 and 100 epochs</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">For ResNet 101 and ResNet-200 we use the block allocations decribed in Table 11. For ResNet-300, our block allocation is</title>
		<idno>4-36-54-4] and ResNet-400 is [6-48-72-6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Tuning Protocols For fine-tuning we initialize the parameters in the ResNet backbone with a pre-trained model and randomly initialize the rest of the layers. We perform end-to-end fine-tuning with an extensive grid search of the combinations of learning rate and training steps to ensure each pre-trained model achieves its best fine-tuning performance. We experiment with different weight decays but do not find it making a big difference and set it to 1e-4. All models are trained with cosine learning rate for simplicity. Below we describe the dataset, evaluation metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fine</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and training parameters for each task</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">We resize the image resolution to 256×256. We replace the classification head in the pre-trained model with a randomly initialized linear layer that predicts 101 classes, including background</title>
	</analytic>
	<monogr>
		<title level="m">CIFAR-100: We use standard CIFAR-100 train and test sets and report the top-1 accuracy</title>
		<imprint>
			<date type="published" when="20000" />
		</imprint>
	</monogr>
	<note>We use a batch size of 512 and search the combination of training steps from 5000 to. and learning rates from 0.005 to 0.32. We find the best learning rate for SimCLR (0.16) is much higher than SimCLRv2 (0.01) and the supervised model (0.005). This trend holds for the following tasks</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">2017) layers. We follow the practice in (Zoph et al., 2020b) to combine P 3 to P 7 and upsample it to P 2 . The segmentation head consists of 3 convolution layers after P 2 layer and a linear layer to predict 21 categories including background at each pixel location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Pascal Segmentation</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">We use PASCAL VOC 2012 train and validation sets and report the mIoU metric. The training images are resampled into 512 × 512 with scale jittering</title>
		<imprint/>
	</monogr>
	<note>randomly resample image between 256 × 256 to 1024 × 1024 and crop it to 512 × 512). We use a batch size of 64 and search the combination of training steps from 5000 to 20000 and learning</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">2015) consisting a region proposal head and a 4conv1fc Fast R-CNN head. We use a batch size of 32 and search the combination of training steps from 5000 to 20000 and learning rates from 0.005 to 0.32. NYU Depth: We use NYU depth v2 dataset with 47584 train and 654 validation images. We report the percentage of predicted depth values within 1.25 relative ratio compared to the ground truth. The training images are resampled into 640 with scale jittering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Pascal Detection</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">We use PASCAL VOC 2007+2012 trainval set and VOC 2007 test set and report the AP 50 with 11 recall points to compute average precision. The training images are resampled into 896 with scale jittering</title>
		<imprint/>
	</monogr>
	<note>The model architecture is identical to segmentation model. except the last linear layer predicts a single depth value per pixel. We use a batch size of 64 and search the combination of training steps from 10000</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">We train with a random 224×224 crop or its horizontal flip on the spatial domain and sample a 32-frame clip with temporal stride 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Video ; Qian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1024</biblScope>
		</imprint>
	</monogr>
	<note>Classification Experimental Details We follow the training and inference protocols in</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
