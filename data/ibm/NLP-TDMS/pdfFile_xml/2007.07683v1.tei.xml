<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UniTrans : Unifying Model Transfer and Data Transfer for Cross-Lingual Named Entity Recognition with Unlabeled Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianhui</forename><surname>Wu</surname></persName>
							<email>wuqianhui@tsinghua.org.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist) Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijia</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Börje</forename><forename type="middle">F</forename><surname>Karlsson</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist) Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
							<email>jlou@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UniTrans : Unifying Model Transfer and Data Transfer for Cross-Lingual Named Entity Recognition with Unlabeled Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prior works in cross-lingual named entity recognition (NER) with no/little labeled data fall into two primary categories: model transfer based and data transfer based methods. In this paper we find that both method types can complement each other, in the sense that, the former can exploit context information via language-independent features but sees no task-specific information in the target language; while the latter generally generates pseudo target-language training data via translation but its exploitation of context information is weakened by inaccurate translations. Moreover, prior works rarely leverage unlabeled data in the target language, which can be effortlessly collected and potentially contains valuable information for improved results. To handle both problems, we propose a novel approach termed UniTrans to Unify both model and data Transfer for cross-lingual NER, and furthermore, to leverage the available information from unlabeled target-language data via enhanced knowledge distillation. We evaluate our proposed UniTrans over 4 target languages on benchmark datasets. Our experimental results show that it substantially outperforms the existing stateof-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER) is a fundamental task in natural language processing, which seeks to locate and classify named entities, like locations, organizations, etc., in unstructured texts. NER has been extensively studied, especially monolingual NER, as it is widely incorporated in various downstream tasks, e.g., question answering <ref type="bibr" target="#b1">[Mollá et al., 2006]</ref>. Recently, deep neural networks have become the dominant approach due to their superior performance. However, the key to their success is the availability of adequate labeled training data; and building large labeled training sets for a new language of interest can be time consuming and labor costly. This motivates researches on cross-lingual transfer, which leverages labeled data from a source language (e.g., English) to overcome the data scarcity issue in the target language. In this paper, following <ref type="bibr">[Wu and Dredze, 2019]</ref> and <ref type="bibr" target="#b1">[Wu et al., 2020]</ref>, we focus on zero-resource cross-lingual transfer, where NO labeled data is available in the target language. In this way, our work can act as a basis for further researches where some labeled target-language data exists.</p><p>The state-of-the-art methods for zero-resource crosslingual NER mainly fall into two categories: i) model transfer based methods <ref type="bibr">[Wu and Dredze, 2019;</ref><ref type="bibr" target="#b1">Wu et al., 2020]</ref>, which use labeled source-language data to train an NER model with language-independent features (e.g., crosslingual word representations <ref type="bibr">[Devlin et al., 2019]</ref>), and then directly apply it to the target language; ii) data transfer based methods, which generally construct pseudo labeled data in the target language via translating the source-language training data into parallel target-language data and mapping the entity labels, and then use such pseudo labeled data to train a target-language NER model. For example, <ref type="bibr" target="#b1">[Mayhew et al., 2017]</ref> and <ref type="bibr" target="#b1">[Xie et al., 2018]</ref> employed word-to-word and phrase-to-phrase translation, respectively, to generate targetlanguage training data, so that entity labels in the sourcelanguage training data can be directly copied to the generated target-language data.</p><p>In this paper, we hold the idea that the aforementioned model transfer based methods and data transfer based methods are complementary to each other. Specifically, for the former, models trained with labeled source-language data only learn the knowledge w.r.t. the task in the source language, but cannot see any task-specific information in the target language. And thus they probably rely more on context information mined by language independent features to make predictions, as word-label relations in the target language are unavailable for them. As for the latter, though models have access to word-label pairs in the target language to exploit their relations, inaccurate translations caused by sense ambiguity and word order differences will probably weaken the models' capability to predict through context information. Moreover, both methods generally do not leverage the language information contained in the unlabeled target-language data to benefit the cross-lingual transfer.</p><p>Therefore, here we propose a novel approach termed UniTrans to unify both model transfer and data transfer for cross-lingual NER, and furthermore, to leverage the beneficial information from unlabeled target-language data via enhanced knowledge distillation. Specifically, following <ref type="bibr" target="#b1">[Lample et al., 2018]</ref>, we first construct a pseudo training set for the target language by performing word-to-word translation and copying entity labels. Then, we use a pre-trained cross-lingual language model, i.e., multilingual BERT <ref type="bibr">[Devlin et al., 2019]</ref>, to generate language-independent features to train two separate NER models. One is trained on the labeled source-language data, and the other is trained on the pseudo target-language training data. We then integrate the knowledge contained in the source-language training data, the pseudo target-language training data, and the unlabeled target-language data as follows: i) we fine-tune the model trained on source-language data with the pseudo target-language training data, and take the fine-tuned model as a teacher NER model to predict probability distributions of entity labels (i.e., soft labels) for each word in the unlabeled target-language data; ii) we propose a voting scheme that associates the three aforementioned NER models to predict high-confidence one-hot label vectors (i.e., hard labels) for part of words in the unlabeled target-language data; iii) we train a student NER model on the unlabeled target-language data with supervision from both the aforementioned soft labels and hard labels, as how knowledge distillation works, and we use it as the final target-language NER model.</p><p>Extensive experiments conducted on benchmark datasets for 4 target languages well demonstrate that the proposed UniTrans substantially outperforms the existing state-of-theart methods. We also extend UniTrans by ensembling multiple teacher models with different random seeds, and show further performance improvement.</p><p>Our major contributions are summarized as follows:</p><p>• We propose a novel approach termed UniTrans to unify model transfer and data transfer based on their complementarity for cross-lingual NER, with the help of beneficial information from unlabeled target-language texts.</p><p>• We propose a voting scheme to generate pseudo hard labels on unlabeled target-language data, so as to enhance knowledge distillation in UniTrans with supervision from both soft labels and pseudo hard labels.</p><p>• We conduct extensive experiments on benchmark datasets and show that UniTrans yields new state-of-theart cross-lingual NER performance, which can even be promoted via teacher ensembling. Data-transfer based methods generally train a monolingual NER model for the target language with pseudo training data constructed from the labeled source-language data. <ref type="bibr" target="#b1">[Ni et al., 2017]</ref> proposed to use bilingual parallel texts and their word alignment information to project labels from the source language to the target language. Considering it is expensive to build parallel corpora manually, most recent methods propose to translate the source-language texts into the target language in a word-by-word <ref type="bibr" target="#b1">[Xie et al., 2018]</ref> or phraseby-phrase <ref type="bibr" target="#b1">[Mayhew et al., 2017]</ref> manner, and then copy the label of each word/phrase to its corresponding translated word/phrase. <ref type="bibr" target="#b0">[Jain et al., 2019]</ref> further proposed to utilize Google Translate twice to translate sentences as well as entities, and align entity labels based on distributional statistics derived from the dataset.</p><p>As mentioned above, both model transfer and data transfer can be complementary to each other. And thus in this paper, we propose UniTrans to unify both, so as to retain the capability of predicting through context information and meanwhile exploiting word-label relations in the target language. Additionally, few prior works on cross-lingual NER leveraged unlabeled data in the target language. <ref type="bibr" target="#b0">[Bari et al., 2019]</ref> proposed to fine-tune the model trained on source-language data with unlabeled target-language data in a manner similar to self-training <ref type="bibr" target="#b1">[Scudder, 1965]</ref>. As shown by <ref type="bibr">[He and Sun, 2017]</ref> and <ref type="bibr" target="#b0">[Bari et al., 2019]</ref> that unlabeled data is beneficial, our UniTrans also exploits the unlabeled target-language data via an enhanced knowledge distillation process, as mentioned before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Distillation</head><p>Knowledge distillation was originally proposed for model compression <ref type="bibr" target="#b0">[Bucilu et al., 2006]</ref>, i.e., to learn a compact student model that retains most performance of a larger teacher model or ensemble of models that require more space to deploy or more computation to make predictions <ref type="bibr" target="#b1">[Rusu et al., 2015;</ref><ref type="bibr" target="#b0">Hinton et al., 2015;</ref><ref type="bibr" target="#b1">Sanh et al., 2019]</ref>. Besides model compression, researchers also applied knowledge distillation to various tasks, like deep reinforcement learning <ref type="bibr" target="#b1">[Rusu et al., 2015]</ref>, image classification <ref type="bibr" target="#b0">[Hinton et al., 2015]</ref>, and language modeling <ref type="bibr" target="#b1">[Sanh et al., 2019]</ref>.</p><p>In this paper, we adapt knowledge distillation to crosslingual NER. And different from typical application scenarios of knowledge distillation that do not consider unlabeled data, in our proposed UniTrans, the student model is completely trained on the unlabeled target-language data. In addition, we propose a voting scheme to generate pseudo hard labels for the unlabeled target-language data, thus enhancing knowledge distillation with supervision from both soft labels and pseudo hard labels. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the framework of the proposed UniTrans. Specifically, we first generate pseudo training data for the target language by performing word-to-word translation and copying entity labels. Then, we train an NER model using the labeled source-language data and derive a teacher model by fine-tuning it with the pseudo target-language training data. Finally, we adapt knowledge distillation to train a student NER model for the target language on unlabelled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Transfer via Word-to-Word Translation</head><p>Following [Xie et al., 2018], we apply <ref type="bibr" target="#b1">[Lample et al., 2018]</ref> to translate the source-language training data word-by-word into the target language, and then directly copy the entity label of each source-language word to its corresponding translated word. The pipeline of data transfer is briefly introduced below. One can refer to <ref type="bibr" target="#b1">[Lample et al., 2018]</ref> for more details. We first leverage identical "character strings" <ref type="bibr" target="#b1">[Smith et al., 2017]</ref> in both languages, i.e., shared words in most cases, to create a seed dictionary</p><formula xml:id="formula_0">{s i , t i } D i=1 , where s i , t i ∈ R d ,</formula><p>respectively, denote the source-language word embedding and the target-language word embedding of an identical character string, and D is the size of the dictionary. Then, we learn a linear mapping P ∈ R d×d between the source-language embedding matrix S = [s 1 , s 2 , ..., s D ] ∈ R d×D and the targetlanguage embedding matrix T = [t 1 , t 2 , ..., t D ] ∈ R d×D , with its objective function formulated as follows.</p><formula xml:id="formula_1">P = arg min P P S − T F s.t., P T P = I<label>(1)</label></formula><p>where · F denotes the Frobenius norm. The closed-form solution for Eq. 1 can be derived via singular value decomposition (SVD):</p><formula xml:id="formula_2">P = U V T s.t., U ΣV T = SVD(T S T )</formula><p>(2) To produce word-to-word translations, for a sourcelanguage word, we use the learned P to map its embedding vector into the target-language embedding space, and take its nearest neighbor in the target language as its corresponding translation result. Specifically, we use the cross-domain similarity local scaling (CSLS) <ref type="bibr" target="#b1">[Lample et al., 2018]</ref> to measure the distance between the mapped embedding vector of a source-language word (denoted as P s i ) and the embedding vector of a target-language word (denoted as t j ):</p><formula xml:id="formula_3">CSLS(P s i , t j ) = 2 cos(P s i , t j ) − r T (P s i ) − r S (t j ) (3) r T (P s i ) = 1 K t k ∈N T (P si) cos(P s i , t k ) (4) r S (t j ) = 1 K P s k ∈N S (tj ) cos(P s k , t j )<label>(5)</label></formula><p>where N T (P s i ) denotes the K target-language nearest neighbors of P s i , N S (t j ) denotes the K mapped source-language nearest neighbors of t j , and cos(·) denotes cosine similarity. By word-by-word translation and copying word labels, we can build a pseudo target-language training set D trans .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Base Model for NER</head><p>The base model for NER in this paper consists of a feature encoder and a linear classification layer. Given an input text sequence</p><formula xml:id="formula_4">x = {x i } N i=1 with N words, we first feed it into the feature encoder f θ to obtain feature vectors h = {h i } N i=1 for all words: h = f θ (x)<label>(6)</label></formula><p>where f θ can be any feature encoder that produces languageindependent features, and h i is the feature vector corresponding to the i-th word x i . Following [Wu and Dredze, 2019], here we utilize multilingual <ref type="bibr">BERT [Devlin et al., 2019]</ref> as the language-independent feature encoder.</p><p>Then for each word x i , its corresponding feature vector h i is fed into the linear classification layer with the softmax function to predict the probability distribution of entity labels for it, which is formulated as follows.</p><p>p</p><formula xml:id="formula_5">(x i , Θ) = softmax(W h i + b)<label>(7)</label></formula><p>where p(x i , Θ) ∈ R |C| with C being the entity label set, and Θ = {f θ , W, b} denotes all the to-be-learned parameters.</p><p>Suppose a labeled training set is denoted as</p><formula xml:id="formula_6">D = {(x, y)}, where y = {y i } N i=1</formula><p>is the corresponding one-hot entity label vectors for words in the corresponding x. Then the loss function L for the NER model is defined as the cross entropy between the predicted probability distribution of entity labels and the ground-truth one for each word, as formulated below:</p><formula xml:id="formula_7">L(Θ) = 1 |D| (x,y)∈D 1 N N i=1 CrossEntropy (y i , p(x i , Θ))<label>(8)</label></formula><p>where y i is the one-hot label vector corresponding to x i .</p><p>Note that in this paper, we use the base model here to train all mentioned NER models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unifying Model Transfer and Data Transfer</head><p>With the base model and loss function defined above, we can train an NER model on the labeled source-language data like previous model transfer based methods <ref type="bibr">[Wu and Dredze, 2019]</ref>, denoted as Θ src . Then for unifying model transfer and data transfer based on their complementarity, the proposed UniTrans further trains a teacher model Θ teach by finetuning Θ src on the pseudo target-language training data, with the same loss function as Eq. 8. And thus Θ teach is expected to combine the advantages of both model transfer and data transfer. Moreover, our experiments also show that, Θ teach obtains superior performance than simply combining the labeled source-language data and the pseudo target-language training data to train an NER model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Knowledge Distillation on Unlabeled Data</head><p>Using Soft Labels For knowledge distillation, we first apply the teacher model Θ teach to the unlabeled target-language data D T = {x}, wherex = {x i } N i denotes an unlabeled target-language text with N words. Considering that predicted soft labels (i.e., probability distribution of entity labels) can contain richer information than predicted hard labels <ref type="bibr" target="#b0">[Hinton et al., 2015]</ref>, we use the soft labels output by Θ teach as the supervision to train a student model Θ stu , by minimizing the mean squared error (MSE) between the prediction of Θ stu and that of Θ teach for each word. The loss function w.r.t. an unlabelled targetlanguage textx is formulated as:</p><formula xml:id="formula_8">Lx sof t = 1 N N i=1 MSE (p(x i , Θ teach ), p(x i , Θ stu ))<label>(9)</label></formula><p>where p(x i , Θ stu ) denotes the probability distribution of entity labels predicted by the student model for the i-th wordx i , and p(x i , Θ teach ) denotes that of the teacher model. In the way above, we can not only transfer the knowledge learned by the teacher model Θ teach to the student model Θ stu , but also enable the student model Θ stu to capture language specific information in unlabeled target-language data D T . And thus, with an identical base model, the student model Θ stu is expected to gain performance improvement over the teacher model Θ teach . <ref type="bibr" target="#b0">[Hinton et al., 2015]</ref> pointed out that when correct hard labels are available, the student model trained with only soft labels can be further improved by also training it with supervision from the correct hard labels. However, in the case of zero-resource cross-lingual NER, there are no ground-truth entity labels available in D T . To this end, we propose a voting scheme to (partially) generate pseudo hard labels for the unlabeled target-language data. And we leverage them to help the training of the student model, via adding an extra loss to Eq. 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporating Pseudo Hard Labels</head><p>Specifically, we train another NER model in a supervised manner on the pseudo target-language training data generated by data transfer (section 3.1), denoted as Θ trans . Then, we predict the probability distributions of entity labels for each wordx i in any unlabeled textx, using all learned NER models, i.e., Θ src , Θ teach and Θ trans , respectively. And we take the entity label c ∈ C with the highest probability as the predicted labelŷ</p><formula xml:id="formula_9">( * ) i forx i : y ( * ) i = arg max c p(x i , Θ * ) c<label>(10)</label></formula><p>where Θ * stands for Θ src , Θ teach and Θ trans , with the correspondingŷ</p><formula xml:id="formula_10">( * ) i beingŷ (src) i ,ŷ (teach) i andŷ (trans) i</formula><p>. And we only generate a pseudo hard labelŷ i =ŷ (teach) i for the word</p><formula xml:id="formula_11">x i whenŷ (teach) i =ŷ (src) i =ŷ (trans) i</formula><p>, asŷ i would be of highconfidence then. Hereŷ i can be seen as the voting result of Θ src , Θ teach and Θ trans . We denote the set of such words with a pseudo hard label as X :</p><formula xml:id="formula_12">X = {x i |ŷ (teach) i =ŷ (src) i =ŷ (trans) i , ∀x i ∈ D T } (11)</formula><p>With the pseudo hard labels derived, we further define a hard label based loss Lx hard as follows for the unlabeled target-language textx.</p><formula xml:id="formula_13">Lx hard = 1 N N i=1 I(x i ) · CrossEntropy(ŷ i , p(x i , Θ stu )) (12)</formula><p>where I(x i ) is an indicator function that returns 1 ifx i ∈ X and otherwise 0.</p><p>Then the loss function combining both the supervision from pseudo hard labels and that from the soft labels output by Θ teach is formulated as follows to better guide the training of Θ stu :</p><formula xml:id="formula_14">L(Θ stu ) = 1 |D T | x∈D T ηLx hard + Lx sof t<label>(13)</label></formula><p>where η &gt; 0 is a trade-off parameter, and we simply set η = 1 in this paper. With L(Θ stu ), the student model Θ stu is learned in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Inference in Target Language</head><p>For inference in the target language, we only use the learned student model Θ stu to predict the probability distribution of entity labels for each word in a given test case, as Eq. 7. Note that here we do not straightly take the entity label with the highest probability as the prediction result for each word (Eq. 10). Instead, we apply Viterbi decoding  by just adding constraints to ensure that predicted entity labels for all words in the test case would not violate the NER tagging scheme and meanwhile would obtain the highest probability as a label sequence. And thus we don't need to train a transition matrix here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the proposed UniTrans for zero-resource crosslingual NER through experiments over benchmark datasets on 4 target languages (i.e., Spanish, Dutch, German, and Norwegian 1 ) and comparisons with state-of-the-art methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Settings Datasets</head><p>We use the following widely-used benchmark datasets for experiments: <ref type="bibr">CoNLL-2002</ref><ref type="bibr">[Tjong Kim Sang, 2002</ref> for Spanish [es] and Dutch <ref type="bibr">[nl] NER, CoNLL-2003 [Tjong Kim Sang and</ref><ref type="bibr">De Meulder, 2003]</ref> for English <ref type="bibr">[en]</ref> and German <ref type="bibr">[de] NER, and</ref><ref type="bibr">NoDaLiDa-2019 [Johansen, 2019]</ref> for Norwegian [no] NER. All datasets are annotated with 4 entity types: LOC, MISC, ORG, and PER. Each dataset is split into training, dev, and test sets. <ref type="table" target="#tab_2">Table 1</ref> reports the statistics of each.</p><p>We leverage WordPiece <ref type="bibr" target="#b1">[Wu et al., 2016]</ref> to tokenize each sentence into a sequence of subwords and, following <ref type="bibr">[Wu and Dredze, 2019;</ref><ref type="bibr" target="#b1">Wu et al., 2020]</ref>, we use the BIO entity labeling scheme. Moreover, as previous works <ref type="bibr" target="#b1">[Wu et al., 2020]</ref>, all experiments use English as the source language and the others as the target language.</p><p>Note that for each target language, we delete all entity labels in its training set, and use it only as unlabeled targetlanguage data. Moreover, to imitate the zero-resource crosslingual NER case, we ignore all target-language dev sets, and directly evaluate the learned models on their test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We implement our UniTrans with PyTorch 2 . For word-toword translation, we use fastText 3 monolingual word embeddings, and use MUSE 4 [Lample et al., 2018] to perform translation. For the feature encoder of the base model, i.e., f θ in Eq. 6, we employ the pretrained multilingual BERT model (case-sensitive version) <ref type="bibr">[Devlin et al., 2019]</ref> in Hugging-Face's Transformers 5 , which has 12 Transformer blocks, 12 attention heads, and 768 hidden units.</p><p>We empirically set UniTrans hyper-parameters by following previous works (as cited below), and utilize them in all experiments. Specifically, we adopt a dropout rate of 0.  <ref type="table">Table 2</ref>: Results of the proposed UniTrans and prior state-of-the-art methods for zero-resource cross-lingual NER. † denotes the reported results w.r.t. freezing the bottom 3 layers of BERT as in this paper. We also report the standard deviation for reimplemented baselines and the proposed UniTrans (i.e., numbers in parentheses).</p><p>of 5e−5 <ref type="bibr" target="#b1">[Wolf et al., 2019]</ref> for teacher models and 1e−4 for the student model <ref type="bibr" target="#b1">[Yang et al., 2019]</ref>. Note that if a word is split into several subwords after tokenization, only the first subword is considered in the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Metric</head><p>Following <ref type="bibr">[Tjong Kim Sang, 2002]</ref>, we use entity level F1score as the performance metric. Moreover, we conduct each experiment 5 times and report the mean F1-score. <ref type="table">Table 2</ref> reports the zero-resource cross-lingual NER results of the proposed UniTrans on the 4 target languages, alongside those reported by prior state-of-the-art methods and those of two re-implemented baseline methods, i.e., Model Transfer (Θ src in 3.3) and Data Transfer (Θ trans in 3.4). <ref type="table">Table 2</ref> shows that our proposed UniTrans significantly outperforms the prior state-of-the-art methods and the reimplemented baselines on all target languages. Particularly, compared with the best prior method <ref type="bibr" target="#b1">[Wu et al., 2020]</ref>, the proposed UniTrans achieves an improvement of F1-score ranging from 1.66 for German [de] to 2.56 for Spanish <ref type="bibr">[es]</ref>. Moreover, UniTrans achieves an average improvement of 1.63 F1-score over Data Transfer, and 2.60 F1-score over Model Transfer. All these results well demonstrate the effectiveness of the proposed UniTrans, which is mainly attributed to unifying model transfer with data transfer and leveraging unlabeled target-language data in UniTrans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To validate the contributions of different components in the proposed UniTrans, we introduce the following variants of UniTrans and baselines to perform ablation study: 1) UniTrans w/o Lx sof t , which removes Lx sof t from Eq.13 to train the target-language NER model Θ stu with only the pseudo hard labels; 2) UniTrans w/o Lx hard , which eliminates Lx hard from Eq.13 (i.e., η = 0) to train Θ stu with only soft labels output by the teacher model Θ teach ; 3) UniTrans  w/ Θ src , which uses the model Θ src learned on the sourcelanguage training data with language-independent features as the teacher to train Θ stu on unlabeled target-language data; 4) UniTrans w/ Θ trans , which uses the model Θ trans learned on translated target-language training data as the teacher to train Θ stu on unlabeled target-language data; 5) UniTrans w/o D T (i.e., Θ teach ), which cuts out the access to the unlabeled target-language data D T and directly applies the teacher model Θ teach to the target language; 6) Model Transfer (i.e., Θ src ), which directly applies the model Θ src to the target language; 7) Data Transfer (i.e., Θ trans ), which directly applies the model Θ trans to the target language; 8) Data Combination, which directly combines the labeled source-language data and the translated target-language data to train an NER model for the target language with languageindependent features. Note that for variants 3) UniTrans w/ Θ trans and 4) UniTrans w/ Θ src , the hard label loss Lx hard is also removed from Eq. 13, as we cannot use one model to generate pseudo hard labels. <ref type="table" target="#tab_5">Table 3</ref> highlights the performance contributions of each component in our proposed UniTrans, and removing any of them will generally lead to a performance drop. Moreover, we can draw more in-depth observations as follows.</p><p>1) The proposed UniTrans outperforms UniTrans w/o Lx sof t and UniTrans w/o Lx hard in most cases, indicating that combining both for training the target-language model Θ stu is reasonable. That also validates the effectiveness of the proposed voting scheme to generate pseudo hard labels.</p><p>2) UniTrans w/o Lx hard outperforms UniTrans w/ Θ trans and UniTrans w/ Θ src . Meanwhile, UniTrans w/o D T (i.e., Θ teach ) outperforms Model Transfer (i.e., Θ src ) and Data Transfer (i.e., Θ trans ). Such results well demonstrate that the proposed teacher model Θ teach , which unifies model transfer and data transfer, is superior to just using one or the other, no matter whether unlabelled target-language data is used or not. That also well verifies the complementarity between model transfer and data transfer.</p><p>3) Comparing UniTrans with UniTrans w/o D T (i.e., Θ teach ), UniTrans w/ Θ src with Model Transfer (i.e., Θ src ), and UniTrans w/ Θ trans with Data Transfer (i.e., Θ trans ), respectively, we can see that eliminating the usage of unlabeled target-language data will lead to a consistent performance drop in all experiments. That further demonstrates the importance of leveraging information contained in unlabeled target-language data for cross-lingual NER. 4) UniTrans w/o D T (i.e., Θ teach ) outperforms Data Com-  <ref type="table">Table 4</ref>: Results of teacher ensembling (M = 5) for UniTrans bination, meaning that the proposed Θ teach utilizes a superior way to unify model transfer and data transfer than simply combining available labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion: Extend with Teacher Ensembling</head><p>Considering the randomness brought by dropout layers in the feature encoder (i.e., multilingual BERT) of the proposed UniTrans, we can extend UniTrans with teacher ensembling via taking advantage of the randomness. Specifically, we use an ensemble of teacher models learned via M runs, each denoted as Θ </p><p>where Θ * ∈ {Θ src , Θ teach , Θ trans }. Note that Eq. 14 is not only adopted to predict soft labels for unlabeled targetlanguage data, but also employed in the generation of pseudo hard labels. <ref type="table">Table 4</ref> reports the results of teacher ensembling with M = 5. It is evident that ensembling further brings consistent performance improvements on nearly all target languages for UniTrans, with an average gain of 0.24 F1-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel approach for cross-lingual NER termed UniTrans, which unifies both model transfer and data transfer based on their complementarity via enhanced knowledge distillation on unlabeled target-language data. We also propose a voting scheme to generate pseudo hard labels for part of words in the unlabeled target-language data, so as to enhance knowledge distillation with supervision from both hard and soft labels. We evaluate the proposed UniTrans on benchmark datasets for four target languages. Experimental results show that UniTrans achieves new state-of-the-art performance for all target languages. We also extend UniTrans with teacher ensembling, which leads to further performance gains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Framework of the proposed UniTrans. (a) Unifying model transfer and data transfer. (b) Knowledge distillation on unlabeled data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>m = 1, 2, ..., M . And we simply average the predictions of Θ (m) * to train the student target-language model Θ stu :p(x i , Θ * )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the benchmark datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1 [Wu et al., 2020] and freeze the parameters of the embedding layer and the bottom three layers of the multilingual BERT [Wu and Dredze, 2019]. Following [Wu et al., 2020], we train all models for 3 epochs using a batch size of 32, maximum sequence length of 128, and AdamW [Loshchilov and Hutter, 2017] as the optimizer. For AdamW, we use a learning rate</figDesc><table><row><cell></cell><cell>es</cell><cell>nl</cell><cell>de</cell><cell>no</cell><cell>Average</cell></row><row><cell cols="2">Täckström et al. [2012] 59.30</cell><cell>58.40</cell><cell>40.40</cell><cell>-</cell><cell>-</cell></row><row><cell>Tsai et al. [2016]</cell><cell>60.55</cell><cell>61.56</cell><cell>48.12</cell><cell>-</cell><cell>-</cell></row><row><cell>Ni et al. [2017]</cell><cell>65.10</cell><cell>65.40</cell><cell>58.50</cell><cell>-</cell><cell>-</cell></row><row><cell>Mayhew et al. [2017]</cell><cell>64.10</cell><cell>63.37</cell><cell>57.23</cell><cell>-</cell><cell>-</cell></row><row><cell>Xie et al. [2018]</cell><cell>72.37</cell><cell>71.25</cell><cell>57.76</cell><cell>-</cell><cell>-</cell></row><row><cell>Jain et al. [2019]</cell><cell>73.5</cell><cell>69.9</cell><cell>61.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Bari et al. [2019]</cell><cell>75.93</cell><cell>74.61</cell><cell>65.24</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Wu and Dredze [2019]  † 74.50</cell><cell>79.50</cell><cell>71.10</cell><cell>-</cell><cell>-</cell></row><row><cell>Wu et al. [2020]</cell><cell>76.75</cell><cell>80.44</cell><cell>73.16</cell><cell>-</cell><cell>-</cell></row><row><cell>Model Transfer (reimp.)</cell><cell cols="4">76.34 (± 0.96) (± 0.46) (± 1.05) (± 0.36) 80.61 72.39 78.47</cell><cell>76.95</cell></row><row><cell>Data Transfer (reimp.)</cell><cell cols="4">78.14 (± 0.97) (± 0.72) (± 0.36) (± 0.50) 80.98 73.65 78.91</cell><cell>77.92</cell></row><row><cell>UniTrans</cell><cell cols="4">79.31 (± 0.39) (± 0.43) (± 0.60) (± 0.63) 82.90 74.82 81.17</cell><cell>79.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>D T (i.e., Θ teach ) 78.24 (-1.07) 81.73 (-1.17) 73.97 (-0.85) 79.07 (-2.10) 78.25(-1.30)    </figDesc><table><row><cell></cell><cell>es</cell><cell>nl</cell><cell>de</cell><cell>no</cell><cell>Average</cell></row><row><cell>UniTrans</cell><cell>79.31</cell><cell>82.90</cell><cell>74.82</cell><cell>81.17</cell><cell>79.55</cell></row><row><cell>1) UniTrans w/o Lx sof t</cell><cell cols="5">78.93 (-0.38) 82.48 (-0.42) 75.23 (0.41) 80.91 (-0.26) 79.39 (-0.16)</cell></row><row><cell>2) UniTrans w/o Lx hard</cell><cell cols="5">79.54 (0.23) 82.78 (-0.12) 74.43 (-0.39) 81.06 (-0.11) 79.45 (-0.10)</cell></row><row><cell>3) UniTrans w/ Θ src</cell><cell cols="5">77.30 (-2.01) 81.20 (-1.70) 73.61 (-1.21) 80.42 (-0.75) 78.13 (-1.42)</cell></row><row><cell>4) UniTrans w/ Θ trans</cell><cell cols="5">79.24 (-0.07) 82.13 (-0.77) 74.91 (0.09) 80.06 (-1.11) 79.09 (-0.46)</cell></row><row><cell>5) UniTrans w/o 6) Model Transfer (i.e., Θ src )</cell><cell cols="5">76.34 (-2.97) 80.61 (-2.29) 72.39 (-2.43) 78.47 (-2.70) 76.95 (-2.60)</cell></row><row><cell>7) Data Transfer (i.e., Θ trans )</cell><cell cols="5">78.14 (-1.17) 80.98 (-1.92) 73.65 (-1.17) 78.91 (-2.26) 77.92 (-1.63)</cell></row><row><cell>8) Data Combination</cell><cell cols="5">77.31 (-2.00) 80.75 (-2.15) 73.66 (-1.16) 76.59 (-4.58) 77.08 (-2.47)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation study for the proposed UniTrans, where numbers in parenthesis denote performance changes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Teacher Ensembling 79.26 83.07 75.55 81.30 79.79</figDesc><table><row><cell></cell><cell>es</cell><cell>nl</cell><cell>de</cell><cell>no Average</cell></row><row><cell>UniTrans</cell><cell cols="4">79.31 82.90 74.82 81.17 79.55</cell></row><row><cell>+</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use Bokmål rather than Nynorsk here, considering that it is used by 85-90% of the population of Norway.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://pytorch.org/ 3 https://fasttext.cc/docs/en/pretrained-vectors.html 4 https://github.com/facebookresearch/MUSE 5 https://github.com/huggingface/transformers</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A unified model for cross-domain and semi-supervised named entity recognition in chinese social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09812</idno>
		<idno>arXiv:1503.02531</idno>
	</analytic>
	<monogr>
		<title level="m">Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network</title>
		<editor>Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova</editor>
		<imprint>
			<publisher>Geoffrey Hinton</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1083" to="1092" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tjong Kim Sang and De Meulder, 2003] Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johansen ; Bjarte Johansen ; Lample</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<idno>arXiv:1910.08381</idno>
	</analytic>
	<monogr>
		<title level="m">Tjong Kim Sang. Introduction to the CoNLL-2002 shared task: Languageindependent named entity recognition</title>
		<editor>Tsai et al., 2016] Chen-Tse Tsai, Stephen Mayhew, and Dan Roth</editor>
		<meeting><address><addrLine>Julien Chaumond, Clement Delangue, Anthony Moi</addrLine></address></meeting>
		<imprint>
			<publisher>Victor Sanh</publisher>
			<date type="published" when="1965" />
			<biblScope unit="page" from="390" to="396" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ACL-IJCNLP</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
