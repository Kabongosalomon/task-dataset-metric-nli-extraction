<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-based Context Aggregation Network for Monocular Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuru</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">East China University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">East China University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengwei</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">East China University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attention-based Context Aggregation Network for Monocular Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth estimation is a traditional computer vision task, which plays a crucial role in understanding 3D scene geometry. Recently, deep-convolutional-neural-networks based methods have achieved promising results in the monocular depth estimation field. Specifically, the framework that combines the multi-scale features extracted by the dilated convolution based block (atrous spatial pyramid pooling, ASPP) has gained the significant improvement in the dense labeling task. However, the discretized and predefined dilation rates cannot capture the continuous context information that differs in diverse scenes and easily introduce the grid artifacts in depth estimation. In this paper, we propose an attention-based context aggregation network (ACAN) to tackle these difficulties. Based on the self-attention model, ACAN adaptively learns the task-specific similarities between pixels to model the context information. First, we recast the monocular depth estimation as a dense labeling multi-class classification problem. Then we propose a soft ordinal inference to transform the predicted probabilities to continuous depth values, which can reduce the discretization error (about 1% decrease in RMSE). Second, the proposed ACAN aggregates both the image-level and pixel-level context information for depth estimation, where the former expresses the statistical characteristic of the whole image and the latter extracts the long-range spatial dependencies for each pixel. Third, for further reducing the inconsistency between the RGB image and depth map, we construct an attention loss to minimize their information entropy. We evaluate on public monocular depth-estimation benchmark datasets (including NYU Depth V2, KITTI). The experiments demonstrate the superiority of our proposed ACAN and achieve the competitive results with the state of the arts. The source code of ACAN can be found in</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Depth information has a significant impact on understanding 3D scenes and can benefit the tasks such as 3D reconstruction <ref type="bibr" target="#b36">[36]</ref> , 3D object detection <ref type="bibr" target="#b37">[37]</ref>, visual simultaneous localization and mapping (SLAM) <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b17">[18]</ref>, and autonomous driving <ref type="bibr" target="#b8">[9]</ref>. Estimating the pixel-wise depth of scenes from RGB images has triggered wide research recently in the computer vision community. The goal of depth estimation is to assign each pixel in an image the distance between the observer and the scene point represented by this pixel. Estimating the depth from a single monocular image is illposed without any geometric cues or priors. Therefore the previous works mainly focus on the stereo vision <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b28">[29]</ref>, in which the binocular images or multi-view images are adopted to obtain the disparity map, and the depth information can be further reconstructed from the disparity map by utilizing the camera parameters. However, the drawbacks of stereo matching lie in the blind areas of the prediction due to the existence of occlusion, and the predicted results might be distorted by inaccurate camera parameters.</p><p>Recently, deep-neural-network-based methods have been widely used in computer vision tasks and achieved great performances. Convolutional neural networks (CNNs) have been proved effective for image classification. Simultaneously, people have applied CNN to dense labeling tasks, such as monocular depth estimation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b4">[5]</ref>, semantic segmentation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b49">[49]</ref> and edge detection <ref type="bibr" target="#b43">[43]</ref> by modifying the network structure of CNN.</p><p>Despite the above success, there still has existed some key challenges in monocular depth estimation tasks. In common deep-CNN-based image-processing, the spatial scales of feature maps continue to shrink as the network goes deeper due to the successive pooling and stride operations, which allows the deep CNN to learn the increasingly abstract representations and fuse the global features to obtain the image-level prediction. However, this translation invariance property may hinder the dense prediction tasks, such as semantic segmentation and depth estimation, where detailed spatial information and image structure are crucial. To overcome this problem, some previous works utilize the skip connection <ref type="bibr" target="#b29">[30]</ref> to combine the feature maps produced by shallow layers and deep layers of the same spatial scales. Moreover, the intermediate supervision <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b42">[42]</ref> is applied to the multi-scale cues to progressively refine the prediction. In other works <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b46">[46]</ref>, the application of dilated convolution maintains the resolution while extending the receptive fields and without introducing extra parameters.</p><p>Another challenge comes from the depth distribution of objects in the scene. Huang et al. <ref type="bibr" target="#b14">[15]</ref> studied the statistics of range images of natural scenes (called depth maps in the depth estimation field), which showed that the range images can be decomposed into piecewise smooth regions that show little dependencies with each other and the sharp discontinuities typically exist in the object boundaries. Therefore, the concept of "objects" in the scene can be better defined in terms of changes in depth rather than some low-level features, such as color, intensity, texture, lighting etc. From this perspective, depth estimation as a classification task can be regarded as a generalized semantic segmentation task while the labels between pixels are not independent. Accordingly, the key point in depth estimation is how to capture the long-range context information of intra-object and inter-object. Yu et al. <ref type="bibr" target="#b46">[46]</ref> used serialized layers with increasing dilation rates to extend the receptive fields of convolutional kernels, while the research works <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> implement an "atrous spatial pyramid pooling (ASPP)" framework to capture multi-scale objects and context information by placing multiple dilated convolution layers in parallel. However, the discretized and limited dilation rates cannot cover the complex and continuous scene depth and easily introduce the grid artifacts <ref type="bibr" target="#b40">[40]</ref>, which can be found in <ref type="figure" target="#fig_6">Fig. 9</ref>.</p><p>In view of the challenges, this paper proposes a novel depth estimation algorithm, called the attention-based context aggregation network (ACAN), to tackle the monocular depth estimation problem. The deep residual architecture <ref type="bibr" target="#b10">[11]</ref> is adopted by ACAN, where dilated convolutions are used to maintain the spatial scale. To extract the continuous pixel-level context information, the self-attention module <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b24">[25]</ref> is plugged into our model to approximate depth distribution of scenes by learning an attention map that carries the normalized similarities between all the pixels. According to the learned attention map, we can obtain the context information of each pixel. Different from prefixed or prestructured local kernels, our proposed attention model can obtain adaptive similarities, which reflect the relationships between each pixel and any other pixels in the whole feature map. Instead of using predefined regions and extracting sparse context information in ASPP, the proposed ACAN can learn the attention weights associated with meaningful contextual areas, resulting in predicting the piecewise smooth depth. The comparison between ASPP and our proposed ACAN can be seen in <ref type="figure">Fig. 2</ref>. To reduce the inconsistency between RGB image and depth map, KL divergence is adopted to model the divergence between the distribution produced by the self-attention model and the distribution constructed by the corresponding ground truth depth. To further incorporate the image-level information for depth estimation, the image-pooling <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b24">[25]</ref> is utilized in this paper. Finally, our proposed soft ordinal inference translates the predicted probabilities into the continuous depth values and produce more realistic transitional regions.</p><p>The main contributions of this paper can be summarized as follows:</p><p>• We propose a pixel-level attention model for the monocular depth estimation that can capture the context information associated with each pixel. In addition, the aggregation of pixel-level context and image-level context is effective to promote the estimation performance. Our experimental results demonstrate that the proposed pixellevel attention model outperform the ASPP based model since the generated pixel-level context information of ACAN is flexible and continuous, and therefore avoid the grid effect. • To eliminate the large semantic gap from 2D image texture and depth map, we introduce KL divergence as our attention loss to minimize the divergence between the distribution of the attention map and the distribution of the similarity map constructed by the ground truth depth. The effectiveness of the attention loss is confirmed by our ablation experiments. • An easy-implemented soft inference strategy is proposed in this paper, which can reduce the discretization error and produce more realistic depth map compared with the naïve hard inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Estimating the depth of a scene is a traditional task in computer vision and has been studied for a long period. As a pioneering work, Saxena et al. <ref type="bibr" target="#b34">[34]</ref> infer the depth from monocular cues based on Markov Random Field (MRF), and further develop their method in <ref type="bibr" target="#b33">[33]</ref>, where the smoothness assumption is imposed to the superpixels to enforce the neighboring constraint. Their work later extended for the 3D model generation <ref type="bibr" target="#b35">[35]</ref>. In <ref type="bibr" target="#b21">[22]</ref>, semantic labels are incorporated into the MRF framework to guide the depth estimation. Ladicky et al. <ref type="bibr" target="#b16">[17]</ref> showed that the property of perspective geometry could be used to learn a much simpler classifier to predict the likelihood of a pixel instead of a pixel-wise depth classifier. All these works provide novel thoughts, while most of them rely on strong geometric constraints and hand-crafted features thus limit their models to generalize to diverse scenarios.</p><p>Recently, a large body of works adopts the deep neural network for monocular depth estimation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b45">[45]</ref>. The seminal work of Eigen et al. <ref type="bibr" target="#b5">[6]</ref> first proposed a multi-scale coarse-to-fine model, where the fine network refines the global prediction from coarse network to produce a more detailed result, and the innovative scale-invariant loss is proved an effective loss function both for training and evaluation. They then extended their model to a three-scale architecture for three dense labeling tasks, i.e. predicting normal, label and depth <ref type="bibr" target="#b4">[5]</ref>. In order to solve the heavytailed effect of depth values reported in <ref type="bibr" target="#b31">[31]</ref>, Laina et at. <ref type="bibr" target="#b17">[18]</ref> presented that the reverse Huber loss <ref type="bibr" target="#b53">[53]</ref> is more appropriate than standard L2 regression loss for depth estimation since Huber loss is more sensitive to small errors. While the deep CNN-based methods are excellent at extracting image features, they are weak in reconstructing high-resolution images due to the down-sampling operation and lack of structural constraints, therefore, often obtain the depth estimation with distorted boundaries and counterfeit regions. To tackle this problem, Hu et al. <ref type="bibr" target="#b13">[14]</ref> proposed the notable loss function whose three items are complementary with each other and the loss function is edge-aware. Garg et al <ref type="bibr" target="#b7">[8]</ref> proposed an unsupervised framework for single view depth estimation with a photometric reconstruction loss between stereo pairs. Under this setting, Godard et al. <ref type="bibr" target="#b9">[10]</ref> further proposed a combination of an L1 loss and the structural similarity index (SSIM) term <ref type="bibr" target="#b52">[52]</ref> as the reconstruction loss and explicitly imposed a spatial smoothness constraint <ref type="bibr" target="#b11">[12]</ref> for the synthesized image. Chen et al. <ref type="bibr" target="#b3">[4]</ref> regarded the depth estimation as an image-to-image translation task, additionally utilized an adversarial loss with the discriminator as a structural penalty.</p><p>Besides the above methods using the task-specific loss or geometric prior to supervise the network learning, there exists another research route that fuses multi-scale information in CNNs for pixel-level prediction <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Most of them applied an encoder-decoder architecture, where a reliable encoder adaptively learns the hierarchical features of input RGB images. In the decoder, the specially designed building blocks are employed to recover the spatial resolution or leverage the multi-scale context to restore the finer details. Laina <ref type="bibr" target="#b17">[18]</ref> introduced an up-sampling block to <ref type="figure">Figure 1</ref>: Network architecture. The ResNet is adopted by the ACAN as the encoder, where the cascaded 2-dilated and 4-dilated convolutions are used to avoid the over-downsampling. In the decoder, the CAM is proposed to extract and aggregate both the pixel-level and image-level context. Finally, our proposed soft ordinal inference will translate the predicted probabilities into continuous depth values.</p><p>improve the output resolution. In the research works <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b45">[45]</ref>, the conditional random field (CRF) based models have been utilized for the multi-scale features to estimate the fine-grained depth maps. Kim et al. <ref type="bibr" target="#b15">[16]</ref> proposed a deep variational model that integrates the predictions from the global and local networks. In the research works <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b18">[19]</ref>, skip connections were added to concatenate the detail-abundant features from the encoder with the decoder features of the corresponding scales. Although these works give the impressively sharp inferences, they also introduce the inevitable artifacts in some highly textured regions <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b25">[26]</ref>. To address this problem, Fu et al. <ref type="bibr" target="#b6">[7]</ref> employed the dilated convolutions to capture context information in multiple scales, a typical example is ASPP <ref type="bibr" target="#b2">[3]</ref>, which has been well studied in semantic segmentation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b2">[3]</ref>. While the dilated-convolution-based methods have achieved the state-ofthe-art, the dilated kernels introduce a sparse sub-sampling of activations, which results in an inherent problem identified as "gridding" <ref type="bibr" target="#b2">[3]</ref>.</p><p>To deal with the gridding problem, different from using prefixed structures of the dilated kernels, we design an attention model to extract the continuous multi-scale context by adaptively learning the pixel-level similarity map. The output features of the decoder can be computed by a weighted sum of contextual regions, which is essential for the fine-grained depth estimation. Moreover, due to our designed attention loss, the ambiguity caused by the large semantic gap could be partly eliminated and the produced attention map could be task-specific. CRF is widely adopted to obtain the pixellevel pairwise similarities as the context information <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>. However, the similarities are patch-wise and only able to compute between the pixel and its local neighborhoods. Armed with the pixel-level attention, which could be regarded as a global structural extractor, our proposed ACAN can capture the long-range dependencies of intra-object by directly computing interactions between widely scattered pixels which share similar depth values.</p><p>Although estimating a depth range is more robust than estimating a depth value for each pixel and the classification strategy can put different weights on different depth ranges according to the tasks of depth estimation <ref type="bibr" target="#b0">[1]</ref>, the naïve hard-threshold-based depth inference ignores the predicted confidence of the depth distribution and usually introduces the stepped artifacts <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b0">[1]</ref>. In this paper, taking the full advantage of the output confidence of the proposed network, we propose a soft inference strategy to reduce the discretization error and eliminate the stepped artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head><p>This section introduces the architecture of our proposed ACAN and associated loss functions for the monocular depth estimation, which maps an RGB image to its corresponding depth map in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>The network architecture is illustrated in <ref type="figure">Fig. 1</ref>, which also uses the encoder-decoder framework. We consider the ResNet as the encoder to extract the dense features of RGB image. ResNet shows great gradient-propagating capability in deeper networks by adding identity branches to plain network, which is essential for depth estimation due to its large receptive field <ref type="bibr" target="#b17">[18]</ref>. However, the over-downsampling of original ResNet may hinder the reconstruction of the fine-grained depth map. Instead, we replace the block3 and block4 in ResNet with 2-dilated and 4-dilated residual blocks, which favor for the initialization of pre-trained parameters and maintain the scale of the subsequent feature maps <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b46">[46]</ref>.</p><p>In the decoder, we propose a novel building block that called the context aggregation module (CAM) to enable the network to capture the discriminative image-level and pixellevel context information. We finally jointly train our model using the combination of attention loss and ordinal loss. We then describe CAM and the training losses in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Context Aggregation Module</head><p>As illustrated in <ref type="figure">Fig. 1</ref>, the CAM includes two branches. The top branch is a pixel-level attention model, i.e. selfattention, the bottom branch is the image pooling operation. In the end, the resulting output features from the two branches are concatenated and passed to the subsequent classifier. Specifically, as demonstrated in <ref type="figure">Fig. 2a</ref>, the feature map x ∈ R N ×Cin inputted to the self-attention module is first encoded into two embedded features, i.e. key feature K ∈ R N ×C K and query feature Q ∈ R N ×C Q , where K = φ(x), Q = ϕ(x), φ and ϕ are the transformation functions, N is the number of spatial positions, i.e. N = H × W , and C K = C Q &lt; C in . The normalized attention weight w i,j can be computed by a pairwise function F as follows,</p><formula xml:id="formula_0">w i,j = 1 Ω i (x) F(x i , x j ) i, j = 1, . . . , N<label>(1)</label></formula><p>Where</p><formula xml:id="formula_1">Ω i (x) is the normalized factor, defined as Ω i (x) = Σ N j=1 F(x i , x j ) .</formula><p>To be more specific, we consider the embedded Gaussian as the pairwise function.</p><formula xml:id="formula_2">F (x i , x j ) = e φ(x i ) T ϕ(x j ) √ C K<label>(2)</label></formula><p>where √ C K is the scaling factor to prevent values produced by outer-product operation growing large in magnitude, thus pushing the attention weights into saturation. Therefore, the pixel-level attention map W ∈ R N ×N can be denoted equivalently as</p><formula xml:id="formula_3">W = softmax Q T K (3)</formula><p>By virtue of the learned attention weights, the output of self-attention module at position i can be defined as</p><formula xml:id="formula_4">c i = N j=1 w i,j ψ (x j )<label>(4)</label></formula><p>where ψ transforms x into value feature V ∈ R N ×C V . By this way, the process of feature extraction is enhanced via explicitly aggregating the context representation of the i th pixel according to the learned attention weights. In this paper, we choose the 1 × 1 convolutions followed by the batch normalization layer and ReLU activation function as φ and ϕ and they share the same parameters.</p><p>Image Pooling: The image pooling has been widely used to produce a class-specific activation map <ref type="bibr" target="#b51">[51]</ref>. We first apply global average pooling (GAP) over the whole image to reduce the 3D input feature maps to a 1D context vector, i.e., output one response for every input feature map. Then by replicating feature vector to the size of the input feature map, we can achieve the image-level context map, which carries the mixture of information belonging to different categories (channels) and helps to clarify local confusions <ref type="bibr" target="#b24">[25]</ref>. Essentially, we discover that the GAP is similar to the channel-wise attention mechanism, the difference is that the latter applies a softmax to the context vector produced by the GAP to get an output probability. The effectiveness of image pooling lies in its classawareness. Given the input image of a scene, the GAP can obtain its statistic prior to the features of the whole image. Our experiment at Section IV-E3 confirms our assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Loss</head><p>Our overall training loss L includes two items</p><formula xml:id="formula_5">L =α att L att + α ord L ord<label>(5)</label></formula><p>Where L att is the attention loss and L ord is the ordinal loss, α att and α ord are the coefficients.</p><p>Attention Loss: As illustrated in <ref type="figure">Fig. 2a</ref>, to bridge the semantic gap between the RGB image and depth, we consider the KL divergence as the attention loss for training the attention model, which measures the distance between the attention weights produced by the self-attention with respect to its ground truth,</p><formula xml:id="formula_6">L att = 1 N N i=1 N j=1 w * i,j ln w * i,j w i,j<label>(6)</label></formula><p>Where w i,j is the attention weights produced by Eq. 1 and w * i,j can be computed by the ground truth depth values of i th pixel and j th pixel as follows,</p><formula xml:id="formula_7">w * i,j = exp ln d max − | ln d * i − ln d * j | * j=1 exp ln d max − | ln d * i − ln d * j |<label>(7)</label></formula><p>where d max denotes the preset value that is slightly larger than the maximum depth value in the dataset. It is noted that w i,· (the i th row of w) is the normalized attention distribution of the i th pixel. Without using L att , our attention model can also produce certain plausible attention map according to the extracted features of the image. However, this is problematic on the highly textured surface as the assumption of appearance-depth correlation is violated in these regions.</p><p>Ordinal Loss: The depth estimation is regarded as a pixellevel classification problem. Due to the severe imbalance of depth data, the samples are distributed more frequently in the small depth value intervals <ref type="bibr" target="#b18">[19]</ref>. However, since the magnitude of the error of the large depth sample is larger than that of the small depth sample, the network may over-fit the former. Hence, we discretize the ground truth depth value d * i in logarithmic space into K sub-intervals equally,</p><formula xml:id="formula_8">l * i = ln d * i − ln d min ln d max − ln d min × K<label>(8)</label></formula><p>Where l * i ∈ {0, 1, · · · , K − 1} is the quantified label of i th pixel, d * i is the continuous depth value of i th pixel. The ordered discretization thresholds t k ∈ {0, 1, · · · , t K−1 } can be obtained as follows,</p><formula xml:id="formula_9">t k = e ln dmin+ ln dmax−ln d min K−1 * k (9)</formula><p>The ordinal loss <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b6">[7]</ref> is adopted in the proposed ACAN to learn our network parameters rather than the straightforward cross entropy loss, which transfers the multi-class classification problem into a series of simpler binary classification problems, each of which only decides whether the sample is larger than t k . The ordinal loss imposes large loss on predictions that are not consistent with the sequential property of the depth labels.</p><p>Formally, assuming Y ∈ R N ×2K denotes the output (confidence map) of the network. We can compute the ordinal loss at spatial position i,</p><formula xml:id="formula_10">θ(y i ) = − l * i −1 k=0 ln P k i − K−1 k=l * i 1 − ln P k i ,<label>(10)</label></formula><formula xml:id="formula_11">P k i = P (l i &gt; k) = e y i,2k+1 e y i,2k + e y i,2k+1</formula><p>Where l i is the estimated label and P k i is the ordinal probability that l i is larger than k at position i. The imagewise ordinal loss is defined as the average of θ(y i ) over all spatial positions,</p><formula xml:id="formula_12">L ord = 1 N N i=1 θ(y i )<label>(11)</label></formula><p>Soft Ordinal Inference: Classification instead of regression for depth estimation has been well studied in previous works, which can naturally obtain the confidence of the depth distribution <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b0">[1]</ref>. The element in the confidence map of each class only pays attention to the specific depth interval, which simplifies the network learning. However, it introduces the discretization error, which is sensitive to the number of depth intervals. In addition, the hard-threshold-based inference strategies <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b0">[1]</ref> ignored the obtained probability distribution which can be an important cue during evaluating and may result in the step effect in the depth map, reported in our experiment IV-E3. Instead, we generalize the naïve hard inference to a soft version, called the soft ordinal inference to solve the above problems. The soft ordinal inference takes full advantage of the confidence of predictions and shows a strong ability to classify the transitional regions of inter-object.</p><p>After obtaining the probabilities of K binary classification for each pixel, the predicted depth d i of hard inference can be computed as,</p><formula xml:id="formula_13">d i = t li + t li+1 2<label>(12)</label></formula><formula xml:id="formula_14">l i = K−1 k=0 η(P k i ≥ 0.5)</formula><p>where η(· ) is an indicator function such that η(true) = 1 and η(f alse) = 0. The rounding operation of hard inference ignores the probability (or confidence) predicted by the network, which may distort the predictions of transitional regions that difficult to distinguish.</p><p>However, our soft ordinal inference can transfer the predicted probabilities to continuous depth values as follows,</p><formula xml:id="formula_15">d i = t li + t li+1 2 * (1 − D i ) + t li+1 + t li+2 2 * D i (13) l i = ∫ i , D i = ∫ i − l i ∫ i = K−1 k=0 P k i</formula><p>where · means the floor operation. D i is between 0 and 1, which represents the extent to which the predicted category is close to l i+1 . Actually, ∫ i is the area under the probability distribution curve, which will be discussed in Section IV-E3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we investigate the performance of the proposed ACAN model on two publicly available monocular depth datasets, NYU v2 Depth <ref type="bibr" target="#b36">[36]</ref>, KITTI <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. NYU v2 Depth</head><p>The original NYU v2 Depth dataset <ref type="bibr" target="#b36">[36]</ref>consists of around 240k RGB-D images of 464 indoor scenes, captured by a Microsoft Kinect camera as video sequences. Following the research works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b17">[18]</ref>, we use the official train\test split, where 249 scenes for training and 215 for testing. For training, we sample approximately 12k unique images with a fixed sampling frequency from each training sequence and then fill in the invalid pixels of the depth map using the colorization method, which is available in the toolbox of NYU v2 dataset. The original image resolution is 480 × 640, we first downsample it to 288 × 384 using bilinear interpolation and then randomly crop to 256 × 352 pixels, as inputs to the network. It is noted that the output of ACAN is 1/8 of ground truth depth in scale, we upsample the output to the desired spatial dimension bilinearly. Following <ref type="bibr" target="#b5">[6]</ref>, we use the same online data augmentation strategies to increase the diversity of samples, which include random scaling, random rotation, color, flips, and contrast. For testing, we use the official 654 images and report our scores on a predefined center cropping by Eigen <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. KITTI</head><p>KITTI dataset <ref type="bibr" target="#b8">[9]</ref> is composed of several outdoor scenes captured by LIDAR sensor and car-mounted cameras while driving. Following <ref type="bibr" target="#b5">[6]</ref>, we use the part of raw data selected from the "city", "residual" and "road" categories for training, which including around 22k images from 28 scenes, and we evaluate on 697 images selected from the other 28 scenes. The original resolution is 375×1242, and are resized to 160×512 to form the inputs. As the target depth maps projected by the point cloud are sparse, we mask them out and evaluate the loss only on valid points in both the training and testing phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>We implement our proposed model using the public deep learning framework Pytorch on a single Nvidia GTX1080Ti GPU. In the proposed ACAN, both ResNet-50 and ResNet-101 are the candidates for the encoder, whose parameters are pretrained on the ImageNet classification task <ref type="bibr" target="#b32">[32]</ref>. The depth intervals are set to 80 in all of our experiment. The learning rate strategy applies a polynomial decay, which starts with the learning rate of 2e-4 and is decayed with the power of 0.9 in the encoder. Since the shallow convolution kernels are optimized well to extract the general low-level features, we set the learning rate of the newly added decoder layers to 10 times to that of the encoder layers. SGD Optimization Algorithm is used to update the parameters, where momentum and weight decay are set to 0.9 and 5e-4 respectively. We set the weights of the different loss items to α att and α ord =0.1. The number of epoches is set to 50 both for KITTI and NYU v2, and batch size is set to 8. We find that further increasing the iteration number can hardly improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation Metrics</head><p>Following previous works <ref type="bibr" target="#b5">[6]</ref>, we evaluate our depth predictions using the following quantitative metrics:</p><formula xml:id="formula_16">Threshold: % of d i s.t. max di d * i , d * i di</formula><p>= δ &lt; thr, thr = 1.25, 1.25 2 , 1.25 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RMSE(linear):</head><formula xml:id="formula_17">1 N i ||d i − d * i || 2 RMSE(log): 1 N i || ln d i − ln d * i || 2 Abs Relative Difference: 1 N i di−d * i d * i Squared Relative Difference: 1 N |di−d * i | 2 d * i</formula><p>Please note that N denotes the number of valid pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discussion on Our Work</head><p>In this subsection, we dig into the proposed context aggregation module and the training loss for the proposed ACAN. The second column presents the ground truth contextual region computed by equation <ref type="formula" target="#formula_7">(7)</ref>, the third column and fourth column present the attention map produced by our ACAN trained without and with L_att respectively. The first and second rows demonstrate the different attention maps located at "+" of the same image.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Effect of the attention model and attention loss:</head><p>We first study the effectiveness of the proposed pixel-level attention model. For qualitative analysis, we visualize the attention maps produced by the attention model with and without attention loss respectively. <ref type="figure" target="#fig_1">Fig. 3</ref> shows that (1) the pixel-level attention model does predict the meaningful contextual regions which capture the long-range dependencies, and is well adjusted to different scenarios adaptively. (2) The visual comparison of the produced attention maps reveals that the model trained with our L att can give the more detailed and global contextual regions, which also acts as a structural extractor. For example, in the first row of <ref type="figure" target="#fig_1">Fig. 3a</ref>, the attention map without L att can only capture a local contextual region at this location, while the attention map with L att highlights the area of the standing man. Moreover, it also captures the context of the stair that is away from the man but similar in depth, which proves that the attention model can extract the contextual region according to the task-specific semantical correlation rather than the similarity of local and low-level features, i.e. the intensity or texture.</p><p>Quantitative results can be seen in <ref type="table" target="#tab_1">Table I</ref>, where "w/o" denotes the model trained without the attention loss and "w" denotes the model trained with the attention loss. We can find that our ACAN with attention loss can obtain a better performance in all of the metrics.</p><p>2) Effect of the image-level feature: We conduct the ablation experiment to reveal the effectiveness of incorporating   the image-level context to the proposed module. Results are shown in <ref type="table" target="#tab_1">Table II</ref>. All of these models are built on ResNet-101. In the <ref type="table" target="#tab_1">Table III</ref>, "w" represents an ACAN model with image-pooling block, "w/o" represents an ACAN model that sets the responses from GAP to a zero vector for a comparison. We further explore the effect of the image-pooling module by visualizing the L2 norm matrix where each entry is calculated from the context vectors produced by the GAP between the two images. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, the images are selected from KITTI dataset, each row in <ref type="figure" target="#fig_2">Fig.4</ref> is from the same scenario and is visually similar. We observe that the image-pooling module has the remarkable distinguishability, as it shows significant differences between different scenes and little but not completely undifferentiated difference from the same scene. It reveals that the image-pooling module can extract the discriminative pattern of scenes.</p><p>The above experiment reveals that the image-level context information does act as a variant of channel-wise attention mechanism, which considers the class-specific statistics prior that expresses the visual characteristic of a scene. Therefore, our proposed ACAN is robust to the varied depth samples from the dataset.</p><p>3) Effect of the ordinal loss and soft ordinal inference: To demonstrate the effectiveness of the ordinal loss, we compared the depth estimation obtained by ordinal inference and that obtained using cross entropy. The experiment is evaluated on KITTI dataset. Normalized confusion matrices are plotted in <ref type="figure" target="#fig_3">Fig. 5</ref>. On the plots of confusion matrices, the columns show the predicted depth label, and the rows correspond to the true class. The diagonal elements of the plots show what percentage of the pixels the trained network correctly estimates their true classes. That is, it shows what percentage of the true and predicted labels match. The off-diagonal elements show where the depth estimation has made mistakes. From <ref type="figure" target="#fig_3">Fig. 5</ref>, it can be found that the ordinal-inference-based depth estimation achieves higher estimation accuracy.  To illustrate the effectiveness of the proposed soft ordinal inference, we give the output probabilities of the ACAN in <ref type="figure" target="#fig_4">Fig. 6</ref>, which is defined by Eq. 10. In the inference phase, the predicted depth map can be inferred from the output probability distribution. Different position has different distributions of possible depth classes, some of which are easy to estimate while others are not. For example, in <ref type="figure" target="#fig_4">Fig.  6</ref>, the depth classes of the green curves in the plot can be easily determined, while those of the red curves are hard to distinguish clearly. One plausible explanation is that the model for depth estimation has uncertainty to distinguish the depth classes from its nearby intervals at these locations. Interestingly, the probability distribution curves are roughly symmetric and centralized around the right labels. Therefore, the area under the curve of probability distribution is nearest to the ground truth depth label. Considering this statistical analysis, we propose the soft ordinal inference to estimate the depth values from the output probability effectively, which can not only infer the correct depth class but also make up the decimal error of quantization.</p><p>Furthermore, we then present both the qualitative and quantitative experimental results. All these experiments are implemented with ResNet-50. As illustrated in <ref type="table" target="#tab_1">Table III</ref>, 'CE(hard)' represents the model trained by cross entropy and applies hard- max inference, while 'CE(soft)' applies soft-weighted-sum inference proposed in <ref type="bibr" target="#b18">[19]</ref>. 'OR(hard)' represents the model trained by ordinal loss and applies hard-threshold inference defined in Eq. 12, while 'OR(soft)' applies the proposed soft ordinal inference defined in Eq. 13. For fair comparison, the results of existing methods are also constructed with ResNet-50.</p><p>It can be found that <ref type="bibr" target="#b0">(1)</ref> No matter using the cross entropy loss or ordinal loss, soft inference is always better than their hard counterparts. (2) The RMSEs reduce clearly (NYU v2: 1.11% for CE, 1.14% for OR; KITTI: 0.56% for CE, 1.32% for OR) while applying the soft inference. (3) The proposed soft ordinal inference achieves the best result.</p><p>The qualitative comparison of the hard inference and our soft ordinal inference are illustrated in <ref type="figure" target="#fig_5">Fig.7</ref>. As observed in <ref type="figure" target="#fig_5">Fig. 7</ref>, the results of OR(hard) produce distorted predictions on the transition area of depth while the results of OR(soft) are smooth, continuous and similar to the ground truth depth map, which well indicates that our soft ordinal inference can give the more realistic depth map without introducing the stepped artifact.</p><p>4) Comparisons with the-state-of-the-arts: 1) NYU v2 depth: We compare our proposed ACAN with state-of-thearts on NYU v2 depth dataset. The results are shown in <ref type="table" target="#tab_1">Table  IV</ref>, and the values in <ref type="table">Table are</ref> copied from their respective papers directly. In <ref type="table" target="#tab_1">Table IV</ref>, the 'RX' in the brackets means the model is backboned on ResNet-X.</p><p>As we observe, our model obtains the best performance among all of the ResNet-50 based methods in all metrics and is even better than some methods built on a more stronger backbone; our ResNet-101 based model obtains competitive performance campared with some satae-of-the-arts. Specifically, in terms of RMSE, our best model outperforms the previous works in a large margin, as our quantization strategy and soft ordinal inference greatly reduce the discretization error.</p><p>Qualitative results are illustrated in <ref type="figure" target="#fig_6">Fig. 8 and Fig. 9</ref>. As we can observe in <ref type="figure">Fig. 8</ref>, the results of <ref type="bibr" target="#b17">[18]</ref> give the semantical predictions, as their method imposed the over-downsampling to the feature maps and lack of the detail-reconstruction mechanism, resulting in their predictions corrupt into the combination of simple geometries. The results of <ref type="bibr" target="#b4">[5]</ref> contain more details but introduce certain distortion. For example, in the third row of <ref type="figure">Fig. 8</ref>, the depth of the person is estimated inaccurate in the result of <ref type="bibr" target="#b4">[5]</ref>. The result of <ref type="bibr" target="#b44">[44]</ref> is blurry. In contrast, our results are detail-abundant and match the ground truth well as a whole, as our attention model can extract the global context features and is structure-aware. <ref type="figure" target="#fig_6">Fig. 9</ref> shows that the results of the ASPP-based method will introduce severe grid artifacts. The reason is that the kernels of the ASPP-based method are predefined elaborately, which cannot adapt to different objects in the image. However, the proposed ACAN method can produce the piecewise smoothness depth map with more details visually.</p><p>2) KITTI: <ref type="table" target="#tab_7">Table V</ref> shows the experimental results of the proposed ACAN and the several state-of-the-art methods on KITTI dataset.</p><p>As we observe, our proposed ACAN (no matter using ResNet-50 or ResNet-101 as the encoder) achieves the excellent performance in all of the settings. Moreover, ACAN (ResNet-50) outperforms the other methods even some of their models are built on a stronger encoder. This can demonstrate that our ACAN with soft ordinal inference is a more efficient method for depth estimation.</p><p>Qualitative results are illustrated in <ref type="figure">Fig. 10</ref>. As observed in <ref type="figure">Fig. 10</ref>, the result of <ref type="bibr" target="#b5">[6]</ref> only give the coarse and blurry predictions. The result of <ref type="bibr" target="#b9">[10]</ref> are visually plausible, however, the depth maps of which are reconstructed indirectly via learning the disparity of the given view under the stereo constraint, which may introduce the noise. For example, the predictions of the car and the tree are confused with the background in the result of <ref type="bibr" target="#b9">[10]</ref>. In contrast, the predictions of our method are visually satisfactory, where objects of different scales can be recognized and our model can predict the sharp boundaries as our attention model can capture the variable pixel-level context adaptively.</p><p>V. CONCLUSION In this paper, we propose a deep-CNN-based method, called the attention-based context aggregation network (ACAN), for monocular depth estimation. By utilizing the self-attention model, the proposed ACAN is able to capture the long-range contextual information by learning the pixel-level attention map adaptively, which is essential for the fine-grained depth estimation. The image pooling module is also incorporated in the ACAN, which can obtain the discriminative image-level context. The aggregation of the pixel-level and image-level context is effective to promote the performance of depth estimation. Soft ordinal inference is also proposed in this paper, which takes full advantage of the output ordinal probabilities to reduce the discretization error. The experiments on NYU v2 dataset and KITTI dataset well demonstrate the superiority of our model. In the future, we plan to investigate the more effective variant of ACAN and extend our method to other dense labeling tasks, such as semantic semantation and surface normal prediction. Moreover, incorporating these tasks into the depth estimation is also our interesting work.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) self-attention module (b) ASPPFigure 2: (a) Our pixel-level attention model can capture the global and dense context for each location, while ASPP only parallelizes a limited amount of convolution kernels thus resulting in the sparse sampling. In addition, the attention loss is proposed to reduce the semantic gap between RGB image and depth map.Self-Attention:The self-attention module maps a query and a set of key-value pairs to an output, where query, key and value represent three feature vectors extracted by the input via three transformation functions respectively. The output is computed as a weighted sum of the values in the feature space, where the weight assigned to each value is computed by a pairwise function of the query with the corresponding key. The details of the self-attention model are illustrated inFig. 2a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(a) example on NYU v2; (b) example on KITTI. The first column shows the RGB images from the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>(a) input RGB images from KITTI, the images of the first row are from 'City' category, and the images of the second row are from 'Residential' category; (b) L2 norm of image-level context vector of the four images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>(a) The plot of confusion matrix (ordinal inference) (b) The plot of confusion matrix (cross entropy) The plots of the normalized confusion matrices, where the predicted labels are produced by (a) ordinal inference and (b) cross entropy. Here we only show the depth labels between 20 and 60, as the samples in this range is dominant and representative in KITTI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The typical probability distribution of the output of ordinal inference. Each curve on the plot represents the predicted ordinal probability set {P 0 i , P 1 i , . . . , P K i } at position i.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>From left to right: input RGB image; ground truth; results of OR(hard); results of OR(soft). The images in bottom rows show the details in the red frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>From left to right: input RGB images; ground truth; results of ours; results of the ASPP-based method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I :</head><label>I</label><figDesc>Comparisons of ACAN trained with and without attention loss</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table II :</head><label>II</label><figDesc>Comparisons with and without image-level features on NYU and KITTI</figDesc><table><row><cell>(a) Input RGB images from KITTI</cell><cell>(b) Matrix of L2</cell></row><row><cell></cell><cell>norm</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table III :</head><label>III</label><figDesc>Comparisons of different training losses and inference strategies On NYU and KITTI with ResNet-50</figDesc><table><row><cell></cell><cell>δ 1</cell><cell>δ 2</cell><cell>δ 3</cell><cell cols="2">RMSE RMSE(log) 0.569 /</cell><cell>0.133</cell><cell>/</cell></row><row><cell>Our (R50)</cell><cell>81.5%</cell><cell>96.0%</cell><cell>98.9%</cell><cell>0.518</cell><cell>0.180</cell><cell cols="2">0.144 0.110</cell></row><row><cell>Our (R101)</cell><cell>82.6%</cell><cell>96.4%</cell><cell>99.0%</cell><cell>0.496</cell><cell>0.174</cell><cell cols="2">0.138 0.101</cell></row><row><cell></cell><cell cols="3">higher is better</cell><cell></cell><cell cols="2">lower is better</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table IV :</head><label>IV</label><figDesc>Comparisons between Our Proposed Method and The Different Previous State-of-The-Arts on NYU v2 Depth Dataset</figDesc><table><row><cell></cell><cell>δ 1</cell><cell>δ 2</cell><cell>δ 3</cell><cell cols="2">RMSE RMSE(log)</cell><cell>ARE</cell><cell>SRE</cell></row><row><cell>Liu [23]</cell><cell>64.7%</cell><cell>88.2%</cell><cell>96.1%</cell><cell>6.986</cell><cell>0.289</cell><cell>0.217</cell><cell>/</cell></row><row><cell>Eigen [6]</cell><cell>69.2%</cell><cell>89.9%</cell><cell>96.7%</cell><cell>7.156</cell><cell>0.270</cell><cell cols="2">0.190 1.515</cell></row><row><cell>Garg [8]</cell><cell>74.0%</cell><cell>90.4%</cell><cell>96.2%</cell><cell>5.104</cell><cell>0.273</cell><cell cols="2">0.169 1.080</cell></row><row><cell>Godard (R50) [10]</cell><cell>86.1%</cell><cell>94.9%</cell><cell>97.6%</cell><cell>4.935</cell><cell>0.206</cell><cell cols="2">0.114 0.898</cell></row><row><cell>Zhang (R50) [48]</cell><cell>86.4%</cell><cell>96.6%</cell><cell>98.9%</cell><cell>4.082</cell><cell>0.164</cell><cell>0.136</cell><cell>/</cell></row><row><cell>Li (R50) [19]</cell><cell>83.3%</cell><cell>95.6%</cell><cell>98.5%</cell><cell>5.325</cell><cell>/</cell><cell>0.128</cell><cell>/</cell></row><row><cell>Li (R101) [19]</cell><cell>85.7%</cell><cell>96.5%</cell><cell>98.9%</cell><cell>4.528</cell><cell>/</cell><cell>0.106</cell><cell>/</cell></row><row><cell>Li (R152) [19]</cell><cell>86.8%</cell><cell>96.7%</cell><cell>99.0%</cell><cell>4.513</cell><cell>0.164</cell><cell cols="2">0.104 0.697</cell></row><row><cell>Cao (R152) [1]</cell><cell>88.7%</cell><cell>96.3%</cell><cell>98.2%</cell><cell>4.712</cell><cell>0.198</cell><cell>0.115</cell><cell>/</cell></row><row><cell>Our (R50)</cell><cell>91.5%</cell><cell>98.3%</cell><cell>99.5%</cell><cell>3.637</cell><cell>0.130</cell><cell cols="2">0.085 0.445</cell></row><row><cell>Our (R101)</cell><cell>91.9%</cell><cell>98.2%</cell><cell>99.5%</cell><cell>3.599</cell><cell>0.127</cell><cell cols="2">0.083 0.437</cell></row><row><cell></cell><cell cols="3">higher is better</cell><cell></cell><cell cols="2">lower is better</cell><cell></cell></row></table><note>Figure 8: From left to right: input RGB images; ground truth; results of ours; results of [18]; results of [5]; results of [44].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table V :</head><label>V</label><figDesc>Comparisons between Our Proposed Method and Different State-of-The-Art Method On KITTI Dataset</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b25">[26]</ref> <p>83.0% 96.6% 99.3%</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Estimating depth from monocular images as classification using deep fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhouhan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits &amp; Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Liang Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking monocular depth estimation with adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas J</forename><surname>Durr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07528</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6602" to="6611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pmhuber: Patchmatch with huber regularization for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Heise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Klose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alois</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate and efficient stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschmller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Revisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mete</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistics of range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinggang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="324" to="331" />
		</imprint>
	</monogr>
	<note>Proceedings. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep monocular depth estimation via integration of global and local predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with hierarchical fusion of dilated cnns and soft-weighted-sum inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anton Van Den Hengel, and Mingyi He. Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Single image depth estimation from predicted semantic labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1253" to="1260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5162" to="5170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep multi-scale architectures for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Moukari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvaine</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Jurie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03051</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ordinal regression with multiple output cnn for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structure from motion for scenes with large duplicate structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steedly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3137" to="3144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Figure 10: From up to bottom: input RGB images</title>
		<imprint/>
	</monogr>
	<note>ground truth; ours; results of [6]; results of [10</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<title level="m">Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5506" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning 3-d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1161" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3-d depth reconstruction from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">7576</biblScope>
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Complex-yolo: Real-time 3d object detection on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Amende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><forename type="middle">Michael</forename><surname>Gross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Cnnslam: Real-time dense monocular slam with learned depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6565" to="6574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Abhinav Gupta, and Kaiming He. Nonlocal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Shih En Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multi-scale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="161" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with guidance of surface normal map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">280</biblScope>
			<biblScope unit="page" from="86" to="100" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="636" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Progressive hard-mining network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheikh</forename><surname>Conrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Hamid Rahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The berhu penalty and the grouped effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Zwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Lambertlacroix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
