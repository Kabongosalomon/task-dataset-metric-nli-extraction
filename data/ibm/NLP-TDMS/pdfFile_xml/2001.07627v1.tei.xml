<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">batchboost: REGULARIZATION FOR STABILIZING TRAINING WITH RESISTANCE TO UNDERFITTING &amp; OVERFITTING DRAFT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-01-22">January 22, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><forename type="middle">A</forename><surname>Czyzewski</surname></persName>
							<email>maciejanthonyczyzewski@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Science</orgName>
								<orgName type="institution">Poznan University of Technology</orgName>
								<address>
									<addrLine>Piotrowo 2</addrLine>
									<postCode>60-965</postCode>
									<settlement>Poznan</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">batchboost: REGULARIZATION FOR STABILIZING TRAINING WITH RESISTANCE TO UNDERFITTING &amp; OVERFITTING DRAFT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-01-22">January 22, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>regularization · underfitting · overfitting · generalization · mixup</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Overfitting &amp; underfitting and stable training are an important challenges in machine learning. Current approaches for these issues are mixup[1], SamplePairing[2] and BC learning <ref type="bibr" target="#b2">[3]</ref>. In our work, we state the hypothesis that mixing many images together can be more effective than just two. batchboost pipeline has three stages: (a) pairing: method of selecting two samples. (b) mixing: how to create a new one from two samples. (c) feeding: combining mixed samples with new ones from dataset into batch (with ratio γ). Note that sample that appears in our batch propagates with subsequent iterations with less and less importance until the end of training. Pairing stage calculates the error per sample, sorts the samples and pairs with strategy: hardest with easiest one, than mixing stage merges two samples using mixup, x 1 + (1 − λ)x 2 . Finally, feeding stage combines new samples with mixed by ratio 1:1. batchboost has 0.5-3% better accuracy than the current state-of-the-art mixup regularization on CIFAR-10[4] &amp; Fashion-MNIST[5]. Our method is slightly better than SamplePairing technique on small datasets (up to 5%). batchboost provides stable training on not tuned parameters (like weight decay), thus its a good method to test performance of different architectures. Source code is at: https://github.com/maciejczyzewski/batchboost</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In order to improve test errors, regularization methods which are processes to introduce additional information to DNN have been proposed <ref type="bibr" target="#b5">[6]</ref>. Widely used regularization methods include data augmentation, stochastic gradient descent (SGD) <ref type="bibr" target="#b6">[7]</ref>, weight decay <ref type="bibr" target="#b7">[8]</ref>, batch normalization (BN) <ref type="bibr" target="#b8">[9]</ref>, label smoothing <ref type="bibr" target="#b9">[10]</ref> and mixup <ref type="bibr" target="#b0">[1]</ref>. Our idea comes from mixup flaws. In a nutshell, mixup constructs virtual training example from two samples. In term of batch construction, it simply gets some random samples from dataset and randomly mix together. The overlapping example of many samples (more than two) has not been considered in previous work. Probably because the imposition of 3 examples significantly affects the model leading to underfitting. It turned out that in many tasks, linear mixing (like BC learning or mixup) leads to underfitting (figure 3). Therefore, these methods are not applicable as universal tools.</p><p>Contribution Our work shows that the imposition of many examples in subsequent iterations (which are slowly suppressed by new overlays) can improve efficiency, but most importantly it ensures stability of training and resistance to attacks. However, it must be done wisely: that's why we implemented two basic mechanisms:</p><p>• (a) new information is provided gradually, thus half-batch adds new examples without mixing • (b) mixing is carried out according to some criterion, in our case it is the best-the-worst strategy to mediate the error The whole procedure is made in three steps to make it more understandable: Note that sample that appears in our batch propagates with subsequent iterations with less and less importance until the end of training. Source code with sample implementation and experiments to verify the results we present here:</p><p>https://github.com/maciejczyzewski/batchboost</p><p>To understand the effects of bootstrap, we conduct a thorough set of study experiments (Section 3). Parameter γ means the ratio of the number of samples in half-batch to feed-batch, in our work we have not considered other values than 1. However, we believe that this is an interesting topic for further research and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pairing Method</head><p>Combining many overlapping samples may have a negative impact on our optimizer used in training. In our implementation, it calculates the error for each sample in batch. Then it sorts this vector, and pairs samples by connecting the easiest (smallest error) with the most difficult sample. The goal of this procedure is to create new artificial samples that are between classes, as described in BC learning.</p><p>However, in this case they are not random pairs, but those that 'require' additional work. In this way, the learning process is more stable because there are no cases when it mix only difficult with difficult or easy with easy (likely is at the beginning or end of the learning process). In our case, the error was calculated using L2 metric between one-hot labels and the predictions (thus we analyzed batchboost only on classification problems like CIFAR-10 <ref type="bibr" target="#b3">[4]</ref> or Fashion-MNIST <ref type="bibr" target="#b4">[5]</ref>). For other problems, there is probably a need to change the metric/method of error calculation. We were also thinking about using RL to pair samples. However, it turns out to be a more complicated problem thus we leave it here for further discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mixing Method</head><p>Selected two samples should be combined into one. There are three methods for linearly mixing samples: SamplePairing, Mixup, BC Learning. Due to the simplicity of implementation and the highest scores, we used a mixup, which looks like this:x</p><formula xml:id="formula_0">= λx i + (1 − λ)x j , where x i , x j are raw input vectors y = λy i + (1 − λ)y j ,</formula><p>where y i , y j are one-hot label encodings (x i , y i ) and (x j , y j ) are two examples drawn at random from our training data, and λ ∈ [0, 1]. Label for many samples was averaged over the last 2 labels (due to small differences in results, and large tradeof in memory).</p><p>Why it works? The good explanation is provided in BC learning research, that images and sound can be represented as waves. Mixing is an interpolation that human don't understand but machine could interpret. However, also a good explanation of this process is: that by training on artificial samples, we supplement the training data by artificial examples between-classes (visually, it fills space between clusters in UMAP/t-SNE visualization). Thus, it generalizes problem more by aggressive cluster separation during training (the clusters are moving away from each other, because model learns artificial clusters made up by mixing). Mixing multiple classes allows for more accurate separation (higher dimensions), however model starts to depart from original problem (new distribution) losing accuracy on test dataset.</p><p>The question is whether linear interpolation is good for all problems. Probably the best solution would be to use a GAN for this purpose (two inputs + noise to control). We tried to use the technique described in SinGAN <ref type="bibr" target="#b10">[11]</ref> but it failed in batchboost. It was unsuccessful due to the high cost of maintaining such a structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Continuous Feeding</head><p>The final stage is for 'feeding' new artificial samples on the model's input. In the previous researches, considered were only cases with mixing two samples along batch. batchboost do this by adding new samples with γ ratio to mixed ones. An interesting observation is that once we mix samples, they are in learning process till end (at each batch continuously). When applying a mixing it has only three options: We found that for problems by nature not linear, for which the mixup did poorly, it was caused by the fact that model learned at the time when very low/high λ was assigned (i.e. model learned on a single example, without mixing). In batchboost it doesn't look much better. However, half-batch contains new information, and feed-batch has examples mixed not randomly but by pairing method. With this clues, optimizer can slightly improve the direction of optimization by better interpreting loss landscape.  <ref type="figure">Figure 3</ref>: Evaluation on CIFAR-10, for EfficientNet-b0 and SGD(weight-decay=10e-4, lr=0.1) (as recommended in the mixup research), same parameters for each model. As a result, the models behave differently, although they differ only in the method of constructing the batch.</p><p>Another problem that mixup often encounters is very unstable loss landscape. Therefore, without a well-chosen weight decay, it cannot stabilize in minimums. To solve this problem, we tune the optimizer parameters for mixup, after that it could achieve a similar result to batchboost (figure 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overfitting (comparison to mixup)</head><p>The most important observation of this section is that batchboost retains the properties of the mixup (similarly to SamplePairing or BC learning). It protects against overfitting, having slightly better results.  <ref type="figure">Figure 4</ref>: batchboost is a new state-of-the-art because it is a slightly better than mixup (here mixup has been tuned for best parameters, batchboost uses configuration from <ref type="figure">figure 3</ref>).</p><p>The only difference is that the α coefficient from the original mixup is weakened.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Accelerating Training &amp; Adversarial Attacks</head><p>In the early stages, it learns faster than a classic mixup. The difference becomes significant when working on very small datasets, e.g. medical challenges on Kaggle. In this work, we have limited Fashion-MNIST to 64 examples we compared to the classic model and SamplePairing. The results were better by 5$. When the model perform well at small datasets, it means that training generalizes problem. On (figure 5) we present samples generated during this process. We tried to modify batchboost to generate samples similar to those of adversarial attacks (by uniformly mixing all samples backward with some Gaussian noise) without any reasonable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Our method is easy to implement and can be used for any model as an additional BlackBox at input. It provides stability and slightly better results. Using batchboost is certainly more important in problems with small data sets. Thanks to the property of avoiding underfitting for misconfigured parameters, this is a good regularization method for people who want to compare two architectures without parameter tuning. Retains all properties of mixup, SamplePairing and BC learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>batchboost presented in three phases: (a) pairing by sorting error (b) mixing with mixup (c) feeding: a mixed feed-batch and new samples in half-batch by 1:1 ratio.Batch as input for training is a combination of two different mini-batches:• (a) half-batch: new samples from dataset, classical augmentation is possible here • (b) feed-batch (mixup): samples mixed together (in-order presented infigure 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) new sample with new sample (b) new sample with previously mixed sample (c) previously mixed sample with previously mixed sample. Pairing method cannot choose only one option for all samples because of non-zero γ ratio. To maintain compatibility with the mixup algorithm, it chooses new λ when constructing the batch. That is why past samples have less and less significance in training process, until they disappear completely (figure 2).batchboost: regularization for stabilizing training with resistance to underfitting &amp; overfitting Orange squares indicates how information is propagated between batches in the batchboost method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>More than two samples have been mixed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>pairing: a method for selecting two samples • (b) mixing: how to create a new one from two samples • (c) feeding: to the mixed samples it supplements the batch with new examples from datasets</figDesc><table /><note>arXiv:2001.07627v1 [cs.LG] 21 Jan 2020batchboost: regularization for stabilizing training with resistance to underfitting &amp; overfitting • (a)</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We focused on the current state-of-the-art mixup. The architecture we used was EfficientNet-b0 <ref type="bibr" target="#b11">[12]</ref> and ResNet100k <ref type="bibr" target="#b12">[13]</ref> (having only 100k parameters from DAWNBench <ref type="bibr" target="#b13">[14]</ref>). The problems we've evolved are CIFAR-10 and Fashion-MNIST. We intend to update this work with more detailed comparisons and experiments, test on different architectures and parameters. The most interesting issue which requires additional research is artificial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Underfitting &amp; Stabilizing Training</head><p>We described this problem in the (section 2.3). The main factors that stabilize training are: (a) the appropriate pairing of samples for mixing, i.e. by error per sample (b) propagation of new information in half-batch.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Data augmentation by pairing samples for images classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Inoue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02929</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Between-class learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5486" to="5494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="950" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Singan: Learning a generative model from a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamar</forename><forename type="middle">Rott</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4570" to="4580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dawnbench: An end-to-end deep learning benchmark and competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Nardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunle</forename><surname>Olukotun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Training</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">101</biblScope>
			<biblScope unit="page">102</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
