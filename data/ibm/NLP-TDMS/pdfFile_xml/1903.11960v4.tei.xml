<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Discrete Structures for Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>He</surname></persName>
						</author>
						<title level="a" type="main">Learning Discrete Structures for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) are a popular class of machine learning models whose major advantage is their ability to incorporate a sparse and discrete dependency structure between data points. Unfortunately, GNNs can only be used when such a graph-structure is available. In practice, however, real-world graphs are often noisy and incomplete or might not be available at all. With this work, we propose to jointly learn the graph structure and the parameters of graph convolutional networks (GCNs) by approximately solving a bilevel program that learns a discrete probability distribution on the edges of the graph. This allows one to apply GCNs not only in scenarios where the given graph is incomplete or corrupted but also in those where a graph is not available. We conduct a series of experiments that analyze the behavior of the proposed method and demonstrate that it outperforms related methods by a significant margin. Code is available at https: //github.com/lucfra/LDS-GNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Relational learning is concerned with methods that cannot only leverage the attributes of data points but also their relationships. Diagnosing a patient, for example, not only depends on the patient's vitals and demographic information but also on the same information about their relatives, the information about the hospitals they have visited, and so on. Relational learning, therefore, does not make the assumption of independence between data points but models their dependency explicitly. Graphs are a natural way to represent relational information and there is a large number of learning algorithms leveraging graph structure. Graph neural networks (GNNs) <ref type="bibr" target="#b41">(Scarselli et al., 2009)</ref>  class of algorithms that are able to incorporate sparse and discrete dependency structures between data points.</p><p>While a graph structure is available in some domains, in others it has to be inferred or constructed. A possible approach is to first create a k-nearest neighbor (kNN) graph based on some measure of similarity between data points. This is a common strategy used by several learning methods such as LLE <ref type="bibr" target="#b40">(Roweis &amp; Saul, 2000)</ref> and Isomap <ref type="bibr" target="#b46">(Tenenbaum et al., 2000)</ref>. A major shortcoming of this approach, however, is that the efficacy of the resulting models hinges on the choice of k and, more importantly, on the choice of a suitable similarity measure over the input features. In any case, the graph creation and parameter learning steps are independent and require heuristics and trial and error. Alternatively, one could simply use a kernel matrix to model the similarity of examples implicitly at the cost of introducing a dense dependency structure.</p><p>With this paper, we follow a different route with the aim of learning discrete and sparse dependencies between data points while simultaneously training the parameters of graph convolutional networks (GCN), a class of GNNs. Intuitively, GCNs learn node representations by passing and aggregating messages between neighboring nodes <ref type="bibr" target="#b26">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b36">Monti et al., 2017;</ref><ref type="bibr" target="#b17">Gilmer et al., 2017;</ref><ref type="bibr" target="#b21">Hamilton et al., 2017;</ref><ref type="bibr" target="#b9">Duran &amp; Niepert, 2017;</ref><ref type="bibr" target="#b48">Velickovic et al., 2018)</ref>. We propose to learn a generative probabilistic model for graphs, samples from which are used both during training and at prediction time. Edges are modelled with random variables whose parameters are treated as hyperparameters in a bilevel learning framework <ref type="bibr" target="#b14">(Franceschi et al., 2018)</ref>. We iteratively sample the structure while minimizing an inner objective (a training error) and optimize the edge distribution parameters by minimizing an outer objective (a validation error).</p><p>To the best of our knowledge, this is the first method that simultaneously learns the graph and the parameters of a GNN for semi-supervised classification. Moreover, and this might be of independent interest, we adapt gradientbased hyperparameter optimization to work for a class of discrete hyperparameters (edges, in this work). We conduct a series of experiments and show that the proposed method is competitive with and often outperforms existing approaches. We also verify that the resulting graph generative models have meaningful edge probabilities. <ref type="bibr">arXiv:1903.11960v4 [cs.</ref>LG] 19 Jun 2020</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>We first provide some background on graph theory, graph neural networks, and bilevel programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Graph Theory Basics</head><p>A graph G is a pair (V, E) with V = {v 1 , ..., v N } the set of vertices and E ⊆ V × V the set of edges. Let N and M be the number of vertices and edges, respectively. Each graph can be represented by an adjacency matrix A of size N × N :</p><formula xml:id="formula_0">A i,j = 1 if there is an edge from vertex v i to vertex v j , and A i,j = 0 otherwise. The graph Laplacian is defined by L = D − A where D i,i = j A i,j and D i,j = 0 if i = j.</formula><p>We denote the set of all N × N adjacency matrices by H N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graph Neural Networks</head><p>Graph neural networks are a popular class of machine learning models for graph-structured data. We focus specifically on graph convolutional networks (GCNs) and their application to semi-supervised learning. All GNNs have the same two inputs. First, a feature matrix X ∈ X N ⊂ R N ×n where n is the number of different node features, second, a graph G = (V, E) with adjacency matrix A ∈ H N . Given a set of class labels Y and a labeling function y : V → Y that maps (a subset of) the nodes to their true class label, the objective is, given a set of training nodes V Train , to learn a function</p><formula xml:id="formula_1">f w : X N × H N → Y N by minimizing some regularized empirical loss L(w, A) = v∈VTrain (f w (X, A) v , y v ) + Ω(w), (1) where w ∈ R d are the parameters of f w , f w (X, A) v is the output of f w for node v, : Y ×Y → R + is a point-wise loss function,</formula><p>and Ω is a regularizer. An example of the function f w proposed by <ref type="bibr" target="#b26">Kipf &amp; Welling (2017)</ref> is the following two hidden layer GCN that computes the class probabilities as</p><formula xml:id="formula_2">f w (X, A) = Softmax(Â ReLu(Â X W 1 ) W 2 ),<label>(2)</label></formula><p>where w = (W 1 , W 2 ) are the parameters of the GCN andÂ is the normalized adjacency matrix, given byÂ = D −1/2 (A + I)D −1/2 , with diagonal,D ii = 1 + j A ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Bilevel Programming in Machine Learning</head><p>Bilevel programs are optimization problems where a set of variables occurring in the objective function are constrained to be an optimal solution of another optimization problem (see <ref type="bibr" target="#b5">Colson et al., 2007</ref>, for an overwiew). Formally given two objective functions F and L, the outer and inner objectives, and two sets of variables, θ ∈ R m and w ∈ R d , the outer and inner variables, a bilevel program is given by</p><formula xml:id="formula_3">min θ,w θ F (w θ , θ) such that w θ ∈ arg min w L(w, θ).<label>(3)</label></formula><p>Bilevel programs arise in numerous situations such as hyperparmeter optimization, adversarial, multi-task, and metalearning <ref type="bibr" target="#b3">(Bennett et al., 2006;</ref><ref type="bibr" target="#b12">Flamary et al., 2014;</ref><ref type="bibr" target="#b37">Muñoz-González et al., 2017;</ref><ref type="bibr" target="#b14">Franceschi et al., 2018)</ref>. <ref type="formula" target="#formula_3">(3)</ref> is challenging since the solution sets of the inner problem are usually not available in closedform. A standard approach involves replacing the minimization of L with the repeated application of an iterative optimization dynamics Φ such as (stochastic) gradient descent <ref type="bibr" target="#b8">(Domke, 2012;</ref><ref type="bibr" target="#b33">Maclaurin et al., 2015;</ref><ref type="bibr" target="#b13">Franceschi et al., 2017)</ref>. Let w θ,T denote the inner variables after T iterations of the dynamics Φ, that is,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solving Problem</head><formula xml:id="formula_4">w θ,T = Φ(w θ,T −1 , θ) = Φ(Φ(w θ,T −2 , θ), θ)</formula><p>, and so on. Now, if θ and w are realvalued and the objectives and dynamics smooth, we can compute the gradient of the function F (w θ,T , θ) w.r.t. θ, denoted throughout as the hypergradient ∇ θ F (w θ,T , θ), as</p><formula xml:id="formula_5">∂ w F (w θ,T , θ)∇ θ w θ,T + ∂ θ F (w θ,T , θ),<label>(4)</label></formula><p>where the symbol ∂ denotes the partial derivative (the Jacobian) and ∇ either the gradient (for scalar functions) or the total derivative. The first term can be computed efficiently in time O(T (d + m)) with reverse-mode algorithmic differentiation <ref type="bibr" target="#b19">(Griewank &amp; Walther, 2008)</ref> by unrolling the optimization dynamics, repeatedly substituting w Φ,t = Φ(w θ,t−1 , θ) and applying the chain rule. This technique allows to optimize a number of hyperparameters several orders of magnitude greater than classic methods for hyperparameter optimization <ref type="bibr" target="#b11">(Feurer &amp; Hutter, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Discrete Graph Structures</head><p>With this paper we address the challenging scenarios where a graph structure is either completely missing, incomplete, or noisy. To this end, we learn a discrete and sparse dependency structure between data points while simultaneously training the parameters of a GCN. We frame this as a bilevel programming problem whose outer variables are the parameters of a generative probabilistic model for graphs. The proposed approach, therefore, optimizes both the parameters of a GCN and the parameters of a graph generator so as to minimize the classification error on a given dataset. We developed a practical algorithm based on truncated reversemode algorithmic differentiation <ref type="bibr" target="#b53">(Williams &amp; Peng, 1990)</ref> and hypergradient estimation to approximately solve the bilevel problem. A schematic illustration of the resulting method is presented in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Jointly Learning the Structure and Parameters</head><p>Let us suppose that information about the true adjacency matrix A is missing or incomplete. Since, ultimately, we are interested in finding a model that minimizes the generalization error, we assume the existence of a second subset of instances with known target, V Val (the validation set), from</p><formula xml:id="formula_6">A τ~Pθ θ ... w t+1 = Φ(w t ,A 1 ) = w t -γ∇L t (w t ,A 1 ) w t+τ = w t+τ-1 -γ∇L t+τ-1 (w t+τ-1 ,A τ )</formula><p>.  <ref type="figure">Figure 1</ref>. Schematic representation of our approach for learning discrete graph structures for GNNs. which we can estimate the generalization error. Hence, we propose to find A ∈ H N that minimizes the function</p><formula xml:id="formula_7">F (w A , A) = v∈VVal (f w A (X, A) v , y v ),<label>(5)</label></formula><p>where w A is the minimizer, assumed unique, of L (see Eq.</p><p>(1) and Sec. 2.3) for a fixed adjacency matrix A. We can then consider Equations <ref type="formula">(1)</ref> and <ref type="formula" target="#formula_7">(5)</ref> as the inner and outer objective of a mixed-integer bilevel programming problem where the outer objective aims to find an optimal discrete graph structure and the inner objective the optimal parameters of a GCN given a graph.</p><p>The resulting bilevel problem is intractable to solve exactly even for small graphs. Moreover, it contains both continuous and discrete-valued variables, which prevents us from directly applying Eq. (4). A possible solution is to construct a continuous relaxation (see e.g. <ref type="bibr" target="#b15">Frecon et al., 2018)</ref>, another is to work with parameters of a probability distribution over graphs. The latter is the approach we follow in this paper. We maintain a generative model for the graph structure and reformulate the bilevel program in terms of the (continuous) parameters of the resulting distribution over discrete graphs. Specifically, we propose to model each edge with a Bernoulli random variable. Let H N = Conv(H N ) be the convex hull of the set of all adjacency matrices for N nodes. By modeling all possible edges as a set of mutually independent Bernoulli random variables with parameter matrix θ ∈ H N we can sample graphs as H N A ∼ Ber(θ). Eqs.</p><p>(1) and <ref type="formula" target="#formula_7">(5)</ref> can then be replaced, by using the expectation over graph structures. The resulting bilevel problem can be written as</p><formula xml:id="formula_8">min θ∈H N E A∼Ber(θ) [F (w θ , A)]<label>(6)</label></formula><formula xml:id="formula_9">such that w θ = arg min w E A∼Ber(θ) [L(w, A)] . (7)</formula><p>By taking the expectation, both the inner and the outer objectives become continuous (and possibly smooth) functions of the Bernoulli parameters. The bilevel problem given by Eqs. (6)- <ref type="formula">(7)</ref> is still challenging to solve efficiently. This is because the solution of the inner problem is not available in closed form for GCNs (the objective is non-convex); and the expectations are intractable to compute exactly 1 .</p><p>1 This is different than e.g. (model free) reinforcement learning, An efficient algorithm, therefore, will only be able to find approximate stochastic solutions, that is, θ i,j ∈ (0, 1).</p><p>Before describing a method to solve the optimization problem given by Eqs. (6)- <ref type="formula">(7)</ref> approximately with hypergradient descent, we first turn to the question of obtaining a final GCN model that we can use for prediction. For a given distribution P θ over graphs with N nodes and with parameters θ, the expected output of a GCN is</p><formula xml:id="formula_10">f exp w (X) = E A [f w (X, A)] = A∈H N P θ (A)f w (X, A). (8)</formula><p>Unfortunately, computing this expectation is intractable even for small graphs; we can, however, compute an empirical estimate of the output aŝ</p><formula xml:id="formula_11">f w (X) = 1 S S i=1 f w (X, A i ),<label>(9)</label></formula><p>where S &gt; 0 is the number of samples we wish to draw. Note thatf is an unbiased estimator of f exp w . Hence, to use a GCN f w learned with the bilevel formulation for prediction, we sample S graphs from the distribution P θ and compute the prediction as the empirical mean of the values of f w .</p><p>Given the parametrization of the graph generator with Bernoulli variables (P θ = Ber(θ)), one can sample a new graph in O(N 2 ). Sampling from a large number of Bernoulli variables, however, is highly efficient, trivially parallelizable, and possible at a rate of millions per second. Other sampling strategies such as MCMC sampling are possible in constant time. Given a set of sampled graphs, it is more efficient to evaluate a sparse GCN S times than to use the Bernoulli parameters as weights of the GCN's adjacency matrix 2 . Indeed, for GCN models, computingf w has a cost of O(SCd), rather than O(N 2 d) for a fully connected graph, where C = ij θ ij is the expected number of edges, and d is the dimension of the weights. Another advantage of using a graph-generative model is that we can interpret it probabilistically which is not the case when learning a dense adjacency matrix.</p><p>where the objective function is usually unknown, depending in an unknown way from the action and the environment.</p><p>2 Note also that Efw(X, A) = fw(X, EA) = fw(X, θ), as the model fw is, in general, nonlinear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Structure Learning via Hypergradient Descent</head><p>The bilevel programming formalism is a natural fit for the problem of learning both a graph generative model and the parameters of a GNN for a specific downstream task. Here, the outer variables θ are the parameters of the graph generative model and the inner variables w are the parameters of the GCN.</p><p>We now discuss a practical algorithm to approach the bilevel problem defined by Eqs. <ref type="formula" target="#formula_8">(6)</ref> and <ref type="formula">(7)</ref>. Regarding the inner problem, we note that the expectation</p><formula xml:id="formula_12">E A∼Ber(θ) [L(w, A)] = A∈H N P θ (A)L(w, A)<label>(10)</label></formula><p>is composed of a sum of 2 N 2 terms, which is intractable even for relatively small graphs. We can, however, choose a tractable approximate learning dynamics Φ such as stochastic gradient descent (SGD),</p><formula xml:id="formula_13">w θ,t+1 = Φ(w θ,t , A t ) = w θ,t − γ t ∇L(w θ,t , A t ), (11)</formula><p>where γ t is a learning rate and A t ∼ Ber(θ) is drawn at each iteration. Under appropriate assumptions and for t → ∞, SGD converges to a weight vector w θ that depends on the edges' probability distribution <ref type="bibr" target="#b4">(Bottou, 2010)</ref>.</p><p>Let w θ,T be an approximate minimizer of E [L] where T may depend on θ. We now need an estimator for the hypergradient ∇ θ E A∼Ber(θ) [F (w θ,T , A)]. Let us first consider the more general case of estimating ∇ θ E z∼P θ [h(z)] for some distribution z ∼ P θ with parameters θ. If there exists a differentiable and reversible sampling path sp(θ, ε) for P θ , with z = sp(θ, ε) for ε ∼ P ε , then one can use the general form of the pathwise gradient estimator (see <ref type="bibr" target="#b35">Mohamed et al. (2019)</ref>, Sec. 5):</p><formula xml:id="formula_14">∇ θ E z∼P θ [h(z)] = E ε∼Pε [∇ θ h(sp(θ, ε))] = (12) E z∼P θ [∇ z h(z)∇ θ z] .<label>(13)</label></formula><p>Since we are concerned with discrete random variables, any sampling path would have discontinuities, making (12)-(13) not directly applicable. Nevertheless, by using an inexact but smooth reparameterization for P θ , we may employ an approximate version of (12)-(13) that allows us to derive a biased estimator of the gradient ∇E[h]. For z = sp(θ, ε) = θ the resulting gradient estimator is an instance of the class of straight-through estimators (STE) <ref type="bibr" target="#b2">(Bengio et al., 2013)</ref>. Now, in our setting, we simply use the identity mapping A = sp(θ, ε) = θ and approximate</p><formula xml:id="formula_15">∇ θ E A∼Ber(θ) [F (w θ,T , A)] ≈ E A∼Ber(θ) [∇ A F (w θ,T , A)] .<label>(14)</label></formula><p>The second line instantiates (13) since ∇ θ A = ∇ θ θ = I with our choice of reparameterization for P θ . This allows us</p><formula xml:id="formula_16">Algorithm 1 LDS 1: Input data: X, Y , Y [, A] 2: Input parameters: η, τ [, k] 3: [A ← kNN(X, k)] {Init. A to kNN graph if A = 0} 4: θ ← A</formula><p>{Initialize P θ as a deterministic distribution} 5: while Stopping condition is not met do 6:</p><p>t ← 0 7:</p><p>while Inner objective decreases do 8:</p><formula xml:id="formula_17">At ∼ Ber(θ) {Sample structure} 9: w θ,t+1 ← Φt(w θ,t , At) {Optimize inner objective} 10: t ← t + 1 11: if t = 0 (mod τ ) or τ = 0 then 12: G ← computeHG(F , Y , θ, (w θ,i ) t i=t−τ ) 13: θ ← Proj H N [θ − ηG] {Optimize outer objective} 14:</formula><p>end if 15:</p><p>end while 16: end while 17: return w, P θ {Best found weights and prob. distribution} to both take discrete samples in the forward pass and to use an efficient (low variance) pathwise gradient estimator in the reverse pass. The cost of this operation is the introduction of a bias, as setting A = sp(θ, ε) = θ is not the same as sampling A from Ber(θ). Recalling equation <ref type="formula" target="#formula_5">(4)</ref>, we can</p><formula xml:id="formula_18">further write E A∼Ber(θ) [∇ A F (w θ,T , A)] as E A [∂ w F (w θ,T , A)∇ A w θ,T + ∂ A F (w θ,T , A)]<label>(15)</label></formula><p>noting that w θ,T depends on the distribution of A through the optimization dynamics (11). We then take the single sample Monte Carlo estimator of (15) to update the parameters θ, projecting on the unit hypercube. We refer to this last quantity as the STE hypergradient, or simply hypergradient.</p><p>We provide additional details about the computation of the term ∇ A w θ,T and the STE in the appendix.</p><p>Computing the STE hypergradient by fully unrolling the dynamics may be too expensive both in time and memory 3 . We propose to truncate the computation and estimate the hypergradient every τ iterations, where τ is a parameter of the algorithm. This is essentially an adaptation of truncated back-propagation through time <ref type="bibr" target="#b50">(Werbos, 1990;</ref><ref type="bibr" target="#b53">Williams &amp; Peng, 1990</ref>) and can be seen as a short-horizon optimization procedure with warm restart on w. A sketch of the method is presented in Algorithm 1, while a more complete version that includes details on the STE hypergradient computation can be found in Appendix A. Inputs and operations in squared brackets are optional.</p><p>The algorithm contains stopping conditions at the outer and at the inner level. While it is natural to implement the latter with a decrease condition on the inner objective 4 , we find it useful to implement the first with a simple early stopping criterion. A fraction of the examples in the validation set is held-out to compute, in each outer iteration, the accuracy using the predictions of the empirically expected model <ref type="formula" target="#formula_11">(9)</ref>. The optimization procedure terminates if there is no improvement for some consecutive outer loops. This helps avoiding overfitting the outer objective <ref type="formula" target="#formula_8">(6)</ref>, which may be a concern in this context given the quantity of (hyper)parameters being optimized and the relative small size of the validation sets.</p><p>The STE hypergradients estimated with Algorithm 1 at each outer iteration are biased. The bias stems from both the straight-trough estimator and from the truncation procedure introduced in lines 11-13 <ref type="bibr" target="#b45">(Tallec &amp; Ollivier, 2017)</ref>. Nevertheless, we find empirically that the algorithm is able to make reasonable progress, finding configurations in the distribution space that are beneficial for the tasks at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conducted a series of experiments with three main objectives. First, we evaluated LDS on node classification problems where a graph structure is available but where a certain fraction of edges is missing. Here, we compared LDS with graph-based learning algorithms including vanilla GCNs. Second, we wanted to validate our hypothesis that LDS can achieve competitive results on semi-supervised classification problems for which a graph is not available.</p><p>To this end, we compared LDS with a number of existing semi-supervised classification approaches. We also compared LDS with algorithms that first create k-NN affinity graphs on the data set. Third, we analyzed the learned graph generative model to understand to what extent LDS is able to learn meaningful edge probability distributions even when a large fraction of edges is missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Cora and Citeseer are two benchmark datasets that are commonly used to evaluate relational learners in general and GCNs in particular <ref type="bibr" target="#b43">(Sen et al., 2008)</ref>. The input features are bag of words and the task is node classification. We use the same dataset split and experimental setup of previous work <ref type="bibr" target="#b54">(Yang et al., 2016;</ref><ref type="bibr" target="#b26">Kipf &amp; Welling, 2017)</ref>. To evaluate the robustness of LDS on incomplete graphs, we construct graphs with missing edges by randomly sampling 25%, 50%, and 75% of the edges. In addition to Cora and Citeseer where we removed all edges, we evaluate LDS on benchmark datasets that are available in scikit-learn <ref type="bibr" target="#b39">(Pedregosa et al., 2011)</ref> such as Wine, Breast Cancer (Cancer), Digits, and 20 Newsgroup (20news). We take 10 classes from 20 Newsgroup and use words (TFIDF) with a frequency of more than 5% as features. We also use FMA, a dataset where 140 audio features are extracted from 7,994 music tracks and where the problem is genre classification <ref type="bibr" target="#b7">(Defferrard et al., 2017)</ref>. The statistics of the datasets are reported in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Setup and Baselines</head><p>For the experiments on graphs with missing edges, we compare LDS to vanilla GCNs. In addition, we also conceived a method (GCN-RND) where we add randomly sampled edges at each optimization step of a vanilla GCN. With this method we intend to show that simply adding random edges to the standard training procedure of a GCN model (perhaps acting as a regularization technique) is not enough to improve the generalization.</p><p>When a graph is completely missing, GCNs boil down to feed-forward neural networks. Therefore, we evaluate different strategies to induce a graph on both labeled and unlabeled samples by creating (1) a sparse Erdős-Rényi random graph <ref type="bibr" target="#b10">(Erdos &amp; Rényi, 1960</ref>) (Sparse-GCN); (2) a dense graph with equal edge probabilities (Dense-GCN); (3) a dense RBF kernel on the input features (RBF-GCN); and (4) a sparse k-nearest neighbor graph on the input features (kNN-GCN). For LDS we initialize the edge probabilities using the k-NN graph (kNN-LDS). We further include a dense version of LDS where we learn a dense similarity matrix (kNN-LDS (dense)). In this setting, we compare LDS to popular semi-supervised learning methods such as label propagation (LP) <ref type="bibr" target="#b58">(Zhu et al., 2003)</ref>, manifold regularization (ManiReg) <ref type="bibr" target="#b1">(Belkin et al., 2006)</ref>, and semi-supervised embedding (SemiEmb) <ref type="bibr" target="#b51">(Weston et al., 2012)</ref>. ManiReg and SemiEmb are given a k-NN graph as input for the Laplacian regularization. We also compare LDS to baselines that do not leverage a graph-structure such as logistic regression (LogReg), support vector machines (Linear and RBF SVM), random forests (RF), and feed-forward neural networks (FFNN). For comparison methods that need a kNN graph, k ∈ {2, 3, . . . , 20} and the metric (Euclidean or Cosine) are tuned using validation accuracy. For kNN-LDS, k is tuned from 10 or 20.</p><p>We use the two layers GCN given by Eq.</p><p>(2) with 16 hidden neurons and ReLu activation. Given a set of labelled training instances V Train (nodes or examples) we use the regularized cross-entropy loss</p><formula xml:id="formula_19">L(w, A) = − v∈VTrain y v • log [f w (X, A) v ] + ρ||w 1 || 2 ,<label>(16)</label></formula><p>where y v is the one-hot encoded target vector for the v-th instance, • denotes the element-wise multiplication and ρ is a non-negative coefficient. As additional regularization technique we apply dropout <ref type="bibr" target="#b44">(Srivastava et al., 2014)</ref> with β = 0.5 as in previous work. We use Adam <ref type="bibr" target="#b24">(Kingma &amp; Ba, 2015)</ref> for optimizing L, tuning the learning rate γ from {0.005, 0.01, 0.02}. The same number of hidden neurons and the same activation is used for SemiEmb and FFNN.  For LDS, we set the initial edge parameters θ i,j to 0 except for the known edges (or those found by kNN) which we set to 1. We then let all the parameters (including those initially set to 1) to be optimized by the algorithm. We further split the validation set evenly to form the validation (A) and early stopping (B) sets. As outer objective we use the unregularized cross-entropy loss on (A) and optimize it with stochastic gradient descent. with exponentially decreasing learning rate. Initial experiments showed that accelerated optimization methods such as Adam or SGD with momentum underperform in this setting. We tune the step size η of the outer optimization loop and the number of updates τ used to compute the truncated hypergradient. Finally, we draw S = 16 samples to compute the output predictions (see Eq. <ref type="formula" target="#formula_11">(9)</ref>). For LDS and GCN, we apply early stopping with a window size of 20 steps.</p><p>LDS was implemented in TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref> and is available at https://github.com/lucfra/ LDS. The implementations of the supervised baselines and LP are those from the scikit-learn python package <ref type="bibr" target="#b39">(Pedregosa et al., 2011)</ref>. GCN, ManiReg, and SemiEmb are implemented in Tensorflow. The hyperparameters for all the methods are selected through the validation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>The results on the incomplete graphs are shown in <ref type="figure" target="#fig_0">Figure  2</ref> for Cora (left) and Citeseer (center). For each percentage of retained edges the accuracy on validation (used for early stopping) and test sets are plotted. LDS achieves competitive results in all scenarios and accuracy gains of up to 7 percentage points. Notably, LDS improves the generalization accuracy of GCN models also when the given graph is that of the respective dataset (100% of edges retained), by learning additional helpful edges. The accuracy of 84.1%  and 75.0% for Cora and Citeseer, respectively, exceed all previous state-of-the-art results. Conversely, adding random edges does not help decreasing the generalization error. GCN and GCN-RND perform similarly which indicates that adding random edges to the graph is not helpful. <ref type="figure" target="#fig_0">Figure 2</ref> (right) depicts the impact of the number of iterations τ to compute the STE hypergradients. Taking multiple steps strongly outperforms alternating optimization 5 (i.e. τ = 0) in all settings. Increasing τ further to the value of 20, however, does not yield significant benefits, while increasing the computational cost.</p><p>In <ref type="table" target="#tab_3">Table 2</ref> we computed the expected number of edges in a sampled graph for Cora and Citeseer, to analyze the properties of the graphs sampled from the learned graph generator. The expected number of edges for LDS is higher than the original number which is to be expected since LDS has better accuracy results than the vanilla GCN in <ref type="figure" target="#fig_0">Figure  2</ref>. Nevertheless, the learned graphs are still very sparse (e.g. for Cora, on average, less than 0.2% edges are present). This facilitates efficient learning of the GCN in the inner learning loop of LDS. <ref type="table" target="#tab_2">Table 1</ref> lists the results for semi-supervised classification problems. The supervised learning baselines work well on some datasets such as Wine and Cancer but fail to pro-5 For τ = 0, one step of optimization of L w.r.t. w, fixing θ is interleaved with one step of minimization of F w.r.t. θ, fixing w. Even if computationally lighter, this approach disregards the nested structure of (6)- <ref type="formula">(7)</ref>, not computing the first term of Eq. (4). vide competitive results on others such as Digits, Citeseer, Cora, and 20News. The semi-supervised learning baselines LP, ManiReg and SemiEmb can only improve the supervised learning baselines on 1, 3 and 4 datasets, respectively. The results for the GCN with different input graphs show that kNN-GCN works well and provides competitive results compared to the supervised baselines on all datasets. kNN-LDS significantly outperforms kNN-GCN on 4 out of the 7 datasets. In addition, kNN-LDS is among the most competitive methods on all datasets and yields the highest gains on datasets that have an underlying graph. Moreover, kNN-LDS performs slightly better than its dense counterpart where we learn a dense adjacency matrix. The added benefit of the sparse graph representation lies in the potential to scale to larger datasets.</p><p>In <ref type="figure" target="#fig_1">Figure 3</ref>, we show the evolution of mean edge probabilities during optimization on three types of nodes (train, validation, test) on the Cora dataset. LDS is able to learn a graph generative model that is, on average, attributing 10 to 100 times more probability to edges between samples sharing the same class label. LDS often attributes a higher probability to edges that are present in the true held-out adjacency matrix (green lines in the plots). In <ref type="figure" target="#fig_2">Figure 4</ref> we report the normalized histograms of the optimized edges probabilities for the same nodes of <ref type="figure" target="#fig_1">Figure 3</ref>, sorted into six bins in log 10 -scale. Edges are divided in two groups: edges between nodes of the same class (blue) and between nodes of unknown or different classes (orange). LDS is able to learn highly non-uniform edge probabilities that reflect the class membership of the nodes. <ref type="figure" target="#fig_3">Figure 5</ref> shows similar qualitative results as <ref type="figure" target="#fig_2">Figure 4</ref>, this time for three Citeseer test nodes, missclassified by kNN-GCN and correctly classified by kNN-LDS. Again, the learned edge probabilities linking to nodes of the same classes is significantly different to those from different classes; but in this case the densities are more skewed toward the first bin. On the datasets we considered, what seems to matter is to capture a useful distribution (i.e. higher probability for links between same class) rather than pick exact links; of course for other datasets this may vary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related work</head><p>Semi-supervised learning. Early works on graph-based semi-supervised learning use graph Laplacian regularization and include label propagation (LP) <ref type="bibr" target="#b58">(Zhu et al., 2003)</ref>, manifold regularization (ManiReg) <ref type="bibr" target="#b1">(Belkin et al., 2006)</ref>, and semi-supervised embedding (SemiEmb) <ref type="bibr" target="#b51">(Weston et al., 2012)</ref>. These methods assume a given graph whose edges represent some similarity between nodes. Later, <ref type="bibr" target="#b54">(Yang et al., 2016)</ref> proposed a method that uses graphs not for regularization but rather for embedding learning by jointly classification and graph context prediction. <ref type="bibr" target="#b26">Kipf &amp; Welling (2017)</ref> presented the first GCN for semi-supervised learning.</p><p>There are now numerous GCN variants all of which assume a given graph structure. Contrary to all existing graph-based semi-supervised learning approaches, LDS is able to work even when the graph is incomplete or missing.</p><p>Graph synthesis and generation. LDS learns a probabilistic generative model for graphs. The earliest probabilistic generative model for graphs was the Erdős-Rényi random graph model <ref type="bibr" target="#b10">(Erdos &amp; Rényi, 1960)</ref>, where edge probabilities are modelled as identically distributed and mutually independent Bernoullis. Several network models have been proposed to model well particular graph properties such as degree distribution <ref type="bibr" target="#b27">(Leskovec et al., 2005)</ref> or network diameter <ref type="bibr" target="#b49">(Watts &amp; Strogatz, 1998)</ref>. <ref type="bibr" target="#b28">Leskovec et al. (2010)</ref> proposed a generative model based on the Kronecker product that takes a real graph as input and generates graphs that have similar properties. Recently, deep learning based approaches have been proposed for graph generation <ref type="bibr" target="#b55">(You et al., 2018;</ref><ref type="bibr" target="#b29">Li et al., 2018;</ref><ref type="bibr" target="#b20">Grover et al., 2018;</ref><ref type="bibr" target="#b6">De Cao &amp; Kipf, 2018)</ref>. The goal of these methods, however, is to learn a sophisticated generative model that reflects the properties of the training graphs. LDS, on the other hand, learns graph generative models as a means to perform well on classification problems and its input is not a collection of graphs. More recent work proposed an unsupervised model that learns to infer interactions between entities while simultaneously learning the dynamics of physical systems such as spring systems . Contrary to LDS, the method is specific to dynamical interacting systems, is unsupervised, and uses a variational encoder-decoder. Finally, we note that <ref type="bibr" target="#b23">Johnson (2017)</ref> proposed a fully differentiable neural model able to process and produce graph structures at both input, representation and output levels; training the model requires, however, supervision in terms of ground truth graphs.</p><p>Link prediction. Link prediction is a decades-old problem <ref type="bibr" target="#b30">(Liben-Nowell &amp; Kleinberg, 2007)</ref>. Several survey papers cover the large body of work ranging from link prediction in social networks to knowledge base completion <ref type="bibr" target="#b31">(Lü &amp; Zhou, 2011;</ref><ref type="bibr" target="#b38">Nickel et al., 2016)</ref>. While a majority of the methods are based on some similarity measure between node pairs, there has been a number of neural network based methods <ref type="bibr" target="#b56">(Zhang &amp; Chen, 2017;</ref>. The problem we study in this paper is related to link prediction as we also want to learn or extend a graph. However, existing link prediction methods do not simultaneously learn a GNN node classifier. Statistical relational learning (SRL) <ref type="bibr" target="#b16">(Getoor &amp; Taskar, 2007</ref>) models often perform both link prediction and node classification through the existence of binary and unary predicates. However, SRL models are inherently intractable and the structure and parameter learning steps are independent.</p><p>Gradient estimation for discrete random variables. Due to the intractable nature of the two bilevel objectives, LDS needs to estimate the hypergradients through a stochastic computational graph <ref type="bibr" target="#b42">(Schulman et al., 2015)</ref>. Using the score function estimator, also known as REINFORCE <ref type="bibr" target="#b52">(Williams, 1992)</ref>, would treat the outer objective as a blackbox function and would not exploit F being differentiable w.r.t. the sampled adjacency matrices and inner optimization dynamics. Conversely, the path-wise estimator is not readily applicable, since the random variables are discrete. LDS borrows from a solution proposed before <ref type="bibr" target="#b2">(Bengio et al., 2013)</ref>, at the cost of having biased estimates. Recently, <ref type="bibr" target="#b22">Jang et al. (2017)</ref>; <ref type="bibr" target="#b34">Maddison et al. (2017)</ref> presented an approach based on continuous relaxations to reduce variance, which <ref type="bibr" target="#b47">Tucker et al. (2017)</ref> combined with REINFORCE to obtain an unbiased estimator. <ref type="bibr" target="#b18">Grathwohl et al. (2018)</ref> further introduced surrogate models to construct control variates for black-box functions. Unfortunately, these latter methods require to compute the function in the interior of the hypercube, possibly in multiple points <ref type="bibr" target="#b47">(Tucker et al., 2017)</ref>. This would introduce additional computational overhead 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose LDS, a framework that simultaneously learns the graph structure and the parameters of a GNN. While we have used a specific GCN variant <ref type="bibr" target="#b26">(Kipf &amp; Welling, 2017)</ref> in the experiments, the method is more generally applicable to other GNNs. The strengths of LDS are its high accuracy gains on typical semi-supervised classification datasets at a reasonable computational cost. Moreover, due to the graph generative model LDS learns, the edge parameters have a probabilistic interpretation.</p><p>The method has its limitations. While relatively efficient, it cannot currently scale to large datasets: this would require an implementation that works with mini-batches of nodes. We evaluated LDS only in the transductive setting, when all data points (nodes) are available during training. Adding additional nodes after training (the inductive setting) would currently require retraining the entire model from scratch. When sampling graphs, we do not currently enforce the graphs to be connected. This is something we anticipate to improve the results, but this would require a more sophisticated sampling strategy. All of these shortcomings motivate future work. In addition, we hope that suitable variants of LDS algorithm will also be applied to other problems such as neural architecture search or to tune other discrete hyperparameters.</p><p>When z is Bernoulli distributed, such reparameterization can be simply chosen 7 as z = sp(θ, ε) = θ in which case the STE boils down tô</p><formula xml:id="formula_20">g(z) = ∂h(z) ∂z , z ∼ P θ .<label>(17)</label></formula><p>If h is a smooth function of z, Eq. 17 is well defined and yields, in general, non-zero quantities. This operation may be viewed under different angles: e.g. as "setting" ∂z ∂θ to the identity, or, as "ignoring" the hard thresholds in the backward pass.ĝ is a random variable that depends, again, from θ. The true gradient ∇ (θ) can be estimated by drawing one or more samples fromĝ. whereas the corresponding straight-through estimator, which is a random variable, is given bŷ g(z) = ∂h(z) ∂z = (az − b)a, z ∼ Ber(θ).</p><p>One has, however, that E z∼Ber(θ) [ĝ(z)] = θ(a − b)a + (1 − θ)(−ab) = θa 2 − ab, resulting inĝ to be biased for θ = 1 2 . <ref type="table" target="#tab_4">Table 3 contains</ref> the list of datasets we used in the experimental section, with relevant statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Tables</head><p>We report in <ref type="table">Table 4</ref> the numerical results relative to the experiments with various percentages of edges retained on Citeseer and Cora datasets. We refer to Section 4 for a complete description.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualization of Embeddings</head><p>We further visualize the embeddings learned by GCN and LDS using T-SNE <ref type="bibr" target="#b32">(Maaten &amp; Hinton, 2008)</ref>. <ref type="figure" target="#fig_5">Figure 6</ref> depicts the T-SNE visualizations of the embeddings learned on Citeseer with Dense-GCN (left), kNN-GCN (center), and kNN-LDS (right). As can be seen, the embeddings learned by kNN-LDS provides the best separation among different classes. <ref type="bibr">7</ref> An exact, but discontinuous reparameterization for z ∼ Ber(θ) is, for instance, z = sp(θ, ε) = H(θ −ε) for ε ∼ U (0, 1), where H is the Heaviside function and U is the (continuous) uniform distribution. <ref type="table">Table 4</ref>. Test accuracy (± standard deviation) in percentage for the experiments on Citeseer and Cora, with various percentages of edges retained (see <ref type="figure" target="#fig_0">Figure 2</ref>). The data split is the same used in <ref type="bibr" target="#b26">(Kipf &amp; Welling, 2017)</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Mean accuracy ± standard deviation on validation (early stopping; dashed lines) and test (solid lines) sets for edge deletion scenarios on Cora (left) and Citeseer (center). (Right) Validation of the number of steps τ used to compute the STE hypergradient (Citeseer); τ = 0 corresponds to alternating minimization. All results are obtained from five runs with different random seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Mean edge probabilities to nodes aggregated w.r.t. four groups during LDS optimization, in log10 scale for three example nodes. For each example node, all other nodes are grouped by the following criteria: (a) adjacent in the ground truth graph; (b) same class membership; (c) different class membership; and (d) unknown class membership. Probabilities are computed with LDS (τ = 5) on Cora with 25% retained edges. From left to right, the example nodes belong to the training, validation, and test set, respectively. The vertical gray lines indicate when the inner optimization dynamics restarts, that is, when the weights of the GCN are reinitialized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Normalized histograms of edges' probabilities for the same nodes of Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Histograms for three Citeseer test nodes, missclassified by kNN-GCN and rightly classified by kNN-LDS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>As an illustrative example, consider the very simple casewhere h(z) = (az − b) 2 /2 for scalars a and b, with z ∼ Ber(θ), θ ∈ [0, 1]. The gradient (derivative) of E [h] w.r.t.θ can be easily computed as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>T-SNE visualization of the output activations (before the classification layer) on the Citeseer dataset. Left: Dense-GCN, Center: kNN-GCN, Right kNN-LDS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>are one such 1 CSML, Istituto Italiano di Tecnologia, Genoa, Italy. 2 University College London, London, UK. 3 NEC Labs Europe, Heidelberg, Germany. Work done in part as Luca Franceschi was visiting researcher at NEC. Correspondence to: Luca Franceschi &lt;luca.franceschi@iit.it&gt;, Xiao He &lt;xiao.he@neclab.eu&gt;. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Test accuracy (± standard deviation) in percentage on various classification datasets. The best results and the statistical competitive ones (by paired t-test with α = 0.05) are in bold. All experiments have been repeated with 5 different random seeds. We compare kNN-LDS to several supervised baselines and semi-supervised learning methods. No graph is provided as input. kNN-LDS achieves high accuracy results on most of the datasets and yields the highest gains on datasets with underlying graphs (Citeseer, Cora).</figDesc><table><row><cell></cell><cell>Wine</cell><cell>Cancer</cell><cell>Digits</cell><cell>Citeseer</cell><cell>Cora</cell><cell>20news</cell><cell>FMA</cell></row><row><cell>LogReg</cell><cell cols="2">92.1 (1.3) 93.3 (0.5)</cell><cell>85.5 (1.5)</cell><cell cols="4">62.2 (0.0) 60.8 (0.0) 42.7 (1.7) 37.3 (0.7)</cell></row><row><cell cols="3">Linear SVM 93.9 (1.6) 90.6 (4.5)</cell><cell>87.1 (1.8)</cell><cell cols="4">58.3 (0.0) 58.9 (0.0) 40.3 (1.4) 35.7 (1.5)</cell></row><row><cell>RBF SVM</cell><cell cols="2">94.1 (2.9) 91.7 (3.1)</cell><cell>86.9 (3.2)</cell><cell cols="4">60.2 (0.0) 59.7 (0.0) 41.0 (1.1) 38.3 (1.0)</cell></row><row><cell>RF</cell><cell cols="2">93.7 (1.6) 92.1 (1.7)</cell><cell>83.1 (2.6)</cell><cell cols="4">60.7 (0.7) 58.7 (0.4) 40.0 (1.1) 37.9 (0.6)</cell></row><row><cell>FFNN</cell><cell cols="7">89.7 (1.9) 92.9 (1.2) 36.3 (10.3) 56.7 (1.7) 56.1 (1.6) 38.6 (1.4) 33.2 (1.3)</cell></row><row><cell>LP</cell><cell cols="2">89.8 (3.7) 76.6 (0.5)</cell><cell>91.9 (3.1)</cell><cell cols="4">23.2 (6.7) 37.8 (0.2) 35.3 (0.9) 14.1 (2.1)</cell></row><row><cell>ManiReg</cell><cell cols="2">90.5 (0.1) 81.8 (0.1)</cell><cell>83.9 (0.1)</cell><cell cols="4">67.7 (1.6) 62.3 (0.9) 46.6 (1.5) 34.2 (1.1)</cell></row><row><cell>SemiEmb</cell><cell cols="2">91.9 (0.1) 89.7 (0.1)</cell><cell>90.9 (0.1)</cell><cell cols="4">68.1 (0.1) 63.1 (0.1) 46.9 (0.1) 34.1 (1.9)</cell></row><row><cell cols="3">Sparse-GCN 63.5 (6.6) 72.5 (2.9)</cell><cell>13.4 (1.5)</cell><cell cols="4">33.1 (0.9) 30.6 (2.1) 24.7 (1.2) 23.4 (1.4)</cell></row><row><cell cols="8">Dense-GCN 90.6 (2.8) 90.5 (2.7) 35.6 (21.8) 58.4 (1.1) 59.1 (0.6) 40.1 (1.5) 34.5 (0.9)</cell></row><row><cell>RBF-GCN</cell><cell cols="2">90.6 (2.3) 92.6 (2.2)</cell><cell>70.8 (5.5)</cell><cell cols="4">58.1 (1.2) 57.1 (1.9) 39.3 (1.4) 33.7 (1.4)</cell></row><row><cell>kNN-GCN</cell><cell cols="2">93.2 (3.1) 93.8 (1.4)</cell><cell>91.3 (0.5)</cell><cell cols="4">68.3 (1.3) 66.5 (0.4) 41.3 (0.6) 37.8 (0.9)</cell></row><row><cell>kNN-LDS</cell><cell cols="2">97.3 (0.4) 94.4 (1.9)</cell><cell>92.5 (0.7)</cell><cell cols="4">71.5 (1.1) 71.5 (0.8) 46.4 (1.6) 39.7 (1.4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Initial number of edges and expected number of sampled edges of learned graph by LDS.</figDesc><table><row><cell>% Edges</cell><cell>25%</cell><cell>50%</cell><cell>75%</cell><cell>100%</cell></row><row><cell>Cora Initial</cell><cell>1357</cell><cell>2714</cell><cell>4071</cell><cell>5429</cell></row><row><cell>Cora Learned</cell><cell cols="4">3635.6 4513.9 5476.9 6276.4</cell></row><row><cell>Citeseer Initial</cell><cell>1183</cell><cell>2366</cell><cell>3549</cell><cell>4732</cell></row><row><cell cols="5">Citeseer Learned 3457.4 4474.2 7842.5 6745.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Summary statistics of the datasets.</figDesc><table><row><cell>Name</cell><cell cols="3">Samples Features |Y|</cell><cell>Train/Valid/Test</cell></row><row><cell>Wine</cell><cell>178</cell><cell>13</cell><cell>3</cell><cell>10 / 20 / 158</cell></row><row><cell>Cancer</cell><cell>569</cell><cell>30</cell><cell>2</cell><cell>10 / 20 / 539</cell></row><row><cell>Digits</cell><cell>1,797</cell><cell>64</cell><cell>10</cell><cell>50 / 100 / 1,647</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>3,703</cell><cell>6</cell><cell>120 / 500 / 1,000</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>1,433</cell><cell>7</cell><cell>140 / 500 / 1,000</cell></row><row><cell>20news</cell><cell>9,607</cell><cell>236</cell><cell>10</cell><cell>100 / 200 / 9,307</cell></row><row><cell>FMA</cell><cell>7,994</cell><cell>140</cell><cell>8</cell><cell>160 / 320 / 7,514</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Moreover, since we rely on biased estimations of the gradients, we do not expect to gain too much from a full computation.4  We continue optimizing L until L(wt−1, A)(1 + ε) ≥ L(w θ,t , A), for ε &gt; 0 (ε = 10 −3 in the experiments). Since L is non-convex, we also use a patience window of p steps.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Recall that F can be computed only after (approximately) solving the inner optimization problem.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>t ← 0 7:</p><p>while Inner objective decreases do 8:</p><p>At ∼ Ber(θ) {Sample structure} 9:</p><p>w θ,t+1 ← Φt(w θ,t , At) {Optimize inner objective} 10:</p><p>t ← t + 1 11:</p><p>if t = 0 (mod τ ) or τ = 0 then 12:</p><p>At ∼ Ber(θ) 13:</p><p>As ∼ Ber(θ) 17:</p><p>p ← pDs(w θ,s , As) 18:</p><p>end while 23: end while 24: return w, P θ {Best found weights and prob. distribution}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Extended algorithm</head><p>In this section we provide an extended version of the Algorithm 1 that includes the explicit computation of the STE hypergradient by truncated reverse mode algorithmic differentiation. Recall that the inner objective is replaced by an iterative dynamics Φ such as stochastic gradient descent. Hence, starting from an initial point w 0 , the iterates are computed as w θ,t+1 = Φ(w θ,t , A t ).</p><p>Let D t and E t denote the Jacobians of the dynamics:</p><p>and recall that, because of our reparameterization choice (for the backward pass), we have ∇ θ A = ∇ θ θ = I. We report the pseudocode in Algorithm 2, where the letter p is used to indicate the adjoint variables (Lagrangian multipliers). Note that for τ = 0 the algorithm does not enter in the loop at line 15. Finally, note also that at line 16, we re-sample the adjacency matrices instead of reusing those computed in the forward pass (lines 8-10).</p><p>Algorithm 2 was implemented in TensorFlow as an extension of the software package Far-HO, freely available at https://github.com/lucfra/FAR-HO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. On the Straight-through Estimator</head><p>LDS borrows from an heuristic solution proposed before <ref type="bibr" target="#b2">(Bengio et al., 2013)</ref>, at the cost of having biased (hyper)gradient estimates. Given a function h(z), where z ∼ P θ is a discrete random variable whose distribution depends on parameters θ, the STE is a technique that consists in computing a biased estimator of the gradient of (θ) = E z∼P θ h(z) by using an inexact, but smooth, reparameterization for z, together with the application of an approximate version of (12)-(13).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/.Softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model selection via bilevel optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kunapuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks, 2006. IJCNN&apos;06. International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1922" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An overview of bilevel optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Colson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marcotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Savard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of operations research</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Molgan: An implicit generative model for small molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A dataset for music analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Benzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fma</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1612.01840" />
	</analytic>
	<monogr>
		<title level="m">18th International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generic methods for optimization-based modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="318" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning graph representations with embedding propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5119" to="5130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the evolution of random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Erdos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publ. Math. Inst. Hung. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="60" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="http://automl.org/book" />
	</analytic>
	<monogr>
		<title level="m">Automatic Machine Learning: Methods, Systems, Challenges</title>
		<editor>Hutter, F., Kotthoff, L., and Vanschoren, J.</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="38" />
		</imprint>
	</monogr>
	<note>press</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning constrained task similarities in graph-regularized multitask learning. Regularization, Optimization, Kernels, and Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gasso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">103</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Forward and reverse gradient-based hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bilevel programming for hyperparameter optimization and meta-learning. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bilevel learning of the group lasso structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frecon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="8311" to="8321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Backpropagation through the void: Optimizing control variates for black-box gradient estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Evaluating derivatives: principles and techniques of algorithmic differentiation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Griewank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walther</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">105</biblScope>
			<pubPlace>Siam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphite</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10459</idno>
		<title level="m">Iterative generative modeling of graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning graphical state transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04687</idno>
		<title level="m">Neural relational inference for interacting systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graphs over time: densification laws, shrinking diameters and possible explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</title>
		<meeting>the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="177" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kronecker graphs: An approach to modeling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="985" to="1042" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning deep generative models of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03324</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liben-Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Link prediction in complex networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lü</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: statistical mechanics and its applications</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1150" to="1170" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gradientbased hyperparameter optimization through reversible learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2113" to="2122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Monte carlo gradient estimation in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards poisoning of deep learning algorithms with back-gradient optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Muñoz-González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paudice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wongrassamee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="27" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gradient estimation using stochastic computation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3528" to="3536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Unbiasing truncated backpropagation through time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08209</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2627" to="2636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Collective dynamics of small-worldnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="issue">6684</biblScope>
			<biblScope unit="page">440</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Backpropagation through time: What it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An efficient gradient-based algorithm for on-line training of recurrent network trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="490" to="501" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphrnn</surname></persName>
		</author>
		<title level="m">A deep generative model for graphs. ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman neural machine for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="575" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09691</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semisupervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
		<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
