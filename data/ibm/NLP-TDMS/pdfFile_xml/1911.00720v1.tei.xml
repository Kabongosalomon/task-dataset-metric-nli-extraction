<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Diao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Bai</surname></persName>
							<email>jbai@ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
							<email>songyan@chuangxin.com</email>
							<affiliation key="aff1">
								<orgName type="department">Sinovation Ventures</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
							<email>tongzhang@ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggang</forename><surname>Wang</surname></persName>
							<email>wangyonggang@chuangxin.com</email>
							<affiliation key="aff1">
								<orgName type="department">Sinovation Ventures</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename></persName>
						</author>
						<title level="a" type="main">ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The pre-training of text encoders normally processes text as a sequence of tokens corresponding to small text units, such as word pieces in English and characters in Chinese. It omits information carried by larger text granularity, and thus the encoders cannot easily adapt to certain combinations of characters. This leads to a loss of important semantic information, which is especially problematic for Chinese because the language does not have explicit word boundaries. In this paper, we propose ZEN, a BERT-based Chinese (Z) text encoder Enhanced by N-gram representations, where different combinations of characters are considered during training. As a result, potential word or phrase boundaries are explicitly pre-trained and fine-tuned with the character encoder (BERT). Therefore ZEN incorporates the comprehensive information of both the character sequence and words or phrases it contains. Experimental results illustrated the effectiveness of ZEN on a series of Chinese NLP tasks. We show that ZEN, using less resource than other published encoders, can achieve state-of-the-art performance on most tasks. Moreover, it is shown that reasonable performance can be obtained when ZEN is trained on a small corpus, which is important for applying pre-training techniques to scenarios with limited data. The code and pre-trained models of ZEN are available at https:// github.com/sinovation/ZEN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained text encoders <ref type="bibr" target="#b21">(Peters et al., 2018b;</ref><ref type="bibr" target="#b6">Devlin et al., 2018;</ref><ref type="bibr" target="#b22">Radford et al., 2018</ref><ref type="bibr" target="#b23">Radford et al., , 2019</ref> have drawn much attention in natural language processing (NLP), because state-of-theart performance can be obtained for many NLP tasks using such encoders. In general, these encoders are implemented by training a deep neural * Work done during the internship at Sinovation Ventures. model on large unlabeled corpora. Although the use of big data brings success to these pre-trained encoders, it is still unclear whether existing encoders have effectively leveraged all useful information in the corpus. Normally, the pre-training procedures are designed to learn on tokens corresponding to small units of texts (e.g., word pieces for English, characters for Chinese) for efficiency and simplicity. However, some important information carried by larger text units may be lost for certain languages when we use a standard encoder, such as BERT. For example, in Chinese, text semantics are greatly affected by recognizing valid n-grams 1 . This means a pre-trained encoder can potentially be improved by incorporating such boundary information of important n-grams.</p><p>Recently, there are studies adapting BERT for Chinese with word information, yet they are limited in maintaining the original BERT structure, augmented with learning from weakly supervised word information or requiring external knowledge. As an example, a representative study in <ref type="bibr" target="#b5">Cui et al. (2019)</ref> proposed to use the whole-word masking strategy to mitigate the limitation of word information. They used an existing segmenter to produce possible words in the input sentences, and then train a standard BERT on the segmented texts by masking whole words. <ref type="bibr" target="#b30">Sun et al. (2019a)</ref> proposed to perform both entity-level and phraselevel masking to learn knowledge and information from the pre-training corpus. However, their approaches are limited in the following senses. First, both methods rely on the word masking strategy so that the encoder can only be trained with existing word and phrase information. Second, similar to the original BERT, the masking strategy results in the mis-match of pretraining and fine-tuning, i.e., no word/phrase information is retained when the encoders are applied to downstream prediction tasks. Third, incorrect word segmentation or entity recognition results cause errors propagated to the pre-training process and thus may negatively affected the generalization capability of the encoder.</p><p>In this paper, we propose ZEN, a Chinese (Z) text encoder Enhanced by representing N-grams, which provides an alternative way to improve character based encoders (e.g., BERT) by using larger text granularity. To train our model, one uses an n-gram lexicon from any possible sources such as pre-defined dictionaries and n-gram lists extracted via unsupervised approaches. Such lexicon is then mapped to training texts, and is used to highlight possible combinations of characters that indicate likely salient contents during the training process. Our model then integrate the representations of these n-gram contexts with the character encoder. Similarly, the fine-tune process on any task-specific dataset further enhances ZEN with such n-gram representations. An important feature of our method is that while the model explicitly takes advantage of n-gram information, the model only outputs character-level encodings that is consistent with BERT. Therefore downstream tasks are not affected. ZEN extends the original BERT model and incorporate learning from large granular text explicitly into the model, which is different (and complementary) from previous methods that relied on weak supervision such as whole-word masking. 2 Our experiments follow the standard procedure, i.e., training ZEN on the Chinese Wikipedia dump and fine-tune it on several Chinese downstream NLP tasks. Experiment results demonstrate its validity and effectiveness where state-of-the-art performance is achieved on many tasks using the n-grams automatically learned from the training data other than external or prior knowledge. In particular, our method outperforms some existing encoders trained on much larger corpora on these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ZEN</head><p>The overall architecture of ZEN is shown in <ref type="figure" target="#fig_0">Figure  1</ref>, where the backbone model (character encoder) is BERT 3 <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>, enhanced by ngram information represented by a multi-layer en-coder. Since the basis of BERT is well explained in previous studies <ref type="bibr" target="#b6">(Devlin et al., 2018;</ref><ref type="bibr" target="#b37">Yu and Jiang, 2019)</ref>, in this paper, we focus on the details of ZEN, by explaining how n-grams are processed and incorporated into the character encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">N-gram Extraction</head><p>Pre-training ZEN requires n-gram extraction in the first place before training starts, where two different steps are performed. The first one is to prepare an n-gram lexicon, L, from which one can use any unsupervised method to extract n-grams for later processing. The second step of n-gram extraction is performed during pre-training, where some ngrams in L are selected according to each training instance c = (c 1 , c 2 , ..., c i , ..., c kc ) with k c characters. Once these n-grams are extracted, we use an n-gram matching matrix, M, to record the positions of the extracted n-grams in each training instance. M is thus an k c × k n matrix, where each element is represented by</p><formula xml:id="formula_0">m ij = 1 c i ∈ n j 0 c i ∈ n j ,</formula><p>where k n is the number of extracted n-grams from c, and n j denotes the j-th extracted n-gram. A sample M with respect to an input text is shown in the bottom part of <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Encoding N-grams</head><p>As shown in the right part of <ref type="figure" target="#fig_0">Figure 1</ref> (dashed box marked as 'B'), ZEN requires a multi-layer encoder to represent all n-grams, whose information are thus encoded in different levels matching the correspondent layers in BERT. We adopt Transformer <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref> as the encoder, which is a multi-layer encoder that can model the interactions among all n-grams through their representations in each layer. This modeling power is of high importance for ZEN because for certain context, salient n-grams are more useful than random others, and such salient n-grams are expected to be emphasized in pre-training. This effect can be achieved by multi-head self-attention (MhA) mechanism in Transformer <ref type="bibr" target="#b2">(Clark et al., 2019)</ref>. In detail, the transformer for n-grams is the same as its original version for sequence modeling, except that it does not encode n-gram positions because all n-grams are treated equally without a sequential order. Formally, denote the j-th refer to two BERT objectives: next sentence prediction and masked language model, respectively.</p><p>[MSK] is the masked token. The incorporation of n-grams into the character encoder is illustrated by the addition operation presented in blue color. The bottom part presents n-gram extraction and preparation for the given input instance.</p><p>n-gram in layer l by µ (l) j , the n-gram encoder represents each of them by MhA via</p><formula xml:id="formula_1">µ (l+1) j = M hA(Q = µ (l) j , K = V = U (l) ) (1) where µ (l)</formula><p>j is used as the query (Q) vector to calculate the attentions over all other input n-grams from the same layer, and U (l) refers to the matrix that stacks all n-gram representations in the layer l that servers as the key (K) and value (V ) in MhA. This encoding process is repeated layer-by-layer along with the character encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Representing N-grams in Pre-training</head><p>With the n-gram encoder, ZEN combine the representations of each character and its associated ngrams to train the backbone model, as shown in the left upper part of <ref type="figure" target="#fig_0">Figure 1</ref> (dashed box marked as 'A'). In detail, let υ (l) i and µ (l) i,k represent embeddings for the i-th character and the k-th n-gram associated to this character at layer l, the enhanced representation for this character is computed by</p><formula xml:id="formula_2">υ (l) * i = υ (l) i + k µ (l) i,k<label>(2)</label></formula><p>where υ (l) * i is the resulting embedding sent to the next layer. Herein + and refer to the elementwise addition operation. Therefore, υ (l) * i = υ (l) i when no n-gram covers this character. For the entire layer l, this enhancement can be formulated by</p><formula xml:id="formula_3">V (l) * = V (l) + M × U (l)<label>(3)</label></formula><p>where V (l) is the embedding matrix for all characters, and its combination with U (l) can be directly <ref type="table" target="#tab_0">TASK   CWS  POS  NER  DC  SA  SPM  NLI   DATASET  MSR  CTB5  MSRA  NEWS  CSC  LCQMC  XNLI  S#  C#  S#  C#  S#  C#  D#  C#  D#  C#  SP#  C#  SP#  C#   TRAIN  87K  4M 18K 720K 45K  2M 50K 41M 10K 927K 239K 5M 393K 23M  DEV  --350 10K  --5K  4M  1K 115K 9K 209K 3K 136K  TEST  4K 173K 348 13K  3K 153K 10K  9M  1K 114K 13K 233K 3K</ref>   done through M. This process is repeated for each layer in the backbone BERT excecept for the last one. The final output of all character embeddings from the last layer is sent to optimize BERT objectives, i.e., mask recovery and next sentence prediction. Note that, since there is masking in BERT training, when a character is masked, n-grams that cover this character are not considered.</p><p>3 Experiment Settings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tasks and Datasets</head><p>For pre-training, following previous studies <ref type="bibr" target="#b6">(Devlin et al., 2018;</ref><ref type="bibr" target="#b5">Cui et al., 2019)</ref>, we use Chinese Wikipedia dump 4 as the base corpus to learn different encoders including ZEN. To clean the base corpus, we remove useless symbols and translate all traditional characters into simplified ones, and lowercase all English letters. The resulted corpus contains 474M tokens and 23K unique characters. For fine-tuning, we choose seven NLP tasks and their corresponding benchmark datasets in our experiments, many of them have been used in previous studies <ref type="bibr" target="#b5">(Cui et al., 2019;</ref><ref type="bibr">Sun et al., 2019a,b)</ref>. These tasks and datasets are described as follows.</p><p>• Chinese word segmentation (CWS): MSR dataset from SIGHAN2005 Chinese word segmentation Bakeoff <ref type="bibr" target="#b8">(Emerson, 2005</ref>  <ref type="bibr" target="#b15">Liu et al. (2018)</ref>, where each instance is a pair of two sentences with a label indicating whether their intent is matched.</p><p>• Natural language inference (NLI): The Chinese part of the XNLI <ref type="bibr" target="#b4">(Conneau et al., 2018)</ref>.</p><p>The statistics of these datasets with respect to their splits are reported in <ref type="table" target="#tab_0">Table 1</ref>. For CWS, POS, we fine-tune and test according to their standard split of training and test sets. For the other tasks, we follow the settings of <ref type="bibr" target="#b5">Cui et al. (2019)</ref> to process those datasets in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation</head><p>N-grams to build the lexicon L are extracted from the same training corpus, i.e., Chinese Wikipedia dump, and prepared by sorting them (except for unigrams) according to their frequencies. We try the cut-off threshold between 5 and 40 where all those n-grams with frequency lower than the threshold are not included in L. As a result, the sizes of L with respect to different threshold range from 179K to 64K n-grams in them. 7 The embeddings of the n-grams are randomly initialized. For the backbone BERT in ZEN, we use the same structure as that in previous work <ref type="bibr" target="#b6">(Devlin et al., 2018;</ref><ref type="bibr" target="#b30">Sun et al., 2019a;</ref><ref type="bibr" target="#b5">Cui et al., 2019)</ref>, i.e., 12 layers with 12 self-attention heads, 768 dimensions for hidden states and 512 for max input length, etc. The pre-training tasks also employ the same masking strategy and next sentence prediction as in <ref type="bibr" target="#b6">Devlin et al. (2018)</ref>, so that ZEN can be compared with BERT on a fair basis. We use the same parameter setting for the n-gram encoder as in BERT, except that we only use 6 layers and set 128 as the max length of n-grams <ref type="bibr">8</ref>   the original BERT base model. We adopt mixed precision training <ref type="bibr" target="#b17">(Micikevicius et al., 2017)</ref> by the Apex library 9 to speed up the training process. Each ZEN model is trained simultaneously on 4 NVIDIA Tesla V100 GPUs with 16GB memory. Our task-specific fine-tuning uses similar hyperparameters reported in <ref type="bibr" target="#b5">Cui et al. (2019)</ref>, with slightly different settings on max input sequence length and batch size for better utilization of computational resources. Specifically, we set max length to 256 for CWS and POS, and 96 for their batch size. For NER, SPM and NLI, we set both the max length and batch size to 128. For the other two tasks, DC and SA, we set the max length and batch size to 512 and 32, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall Performance</head><p>The first experiment is to compare ZEN and BERT with respect to their performance on the aforementioned NLP tasks. In this experiment, ZEN and BERT use two settings, i.e., training from (R): randomly initialized parameters and (P): pretrained model, which is the Google released Chinese BERT base model. The results are reported in <ref type="table" target="#tab_3">Table 2</ref>, with the evaluation metrics for each task denoted in the second row. Overall, in both R and P settings, ZEN outperforms BERT in all seven tasks, which clearly indicates the advantage of introducing n-grams into the encoding of character sequences. This observation is similar to that from Dos Santos and Gatti <ref type="formula" target="#formula_2">(2014)</ref>  <ref type="bibr" target="#b13">Liu et al. (2019a)</ref>. In detail, when compare R and P settings, the performance gap between ZEN (P) and BERT (P) is larger than that in their R setting, which illustrates that learning an encoder with reliable initialization is more important and integrating n-gram information contributes a better enhancement on well-learned encoders. For two types of tasks, it is noticed that token-level tasks, i.e., CWS, POS and NER, demonstrate a bigger improvement of ZEN over BERT than that of sentence-level tasks. where the potential boundary information presented by n-grams are essential to provide a better guidance to label each character. Particularly for CWS and NER, these boundary information are directly related to the outputs. Similarly, sequence-level tasks show a roughly same trend on the improvement of ZEN over BERT, which also shows the capability of combining both character and n-gram information in a text encoder. The reason behind this improvement is that in token-level tasks, where highfrequent n-grams 10 in many cases are valid chunks in a sentence that carry key semantic information.</p><p>We also compare ZEN (P) and existing pretrained encoders on the aforementioned NLP tasks, with their results listed in the middle part of <ref type="table" target="#tab_3">Table 2</ref>. 11 Such encoders include BERT-wwm <ref type="bibr" target="#b5">(Cui et al., 2019)</ref>, ERNIE 1.0 <ref type="bibr" target="#b30">(Sun et al., 2019a)</ref>, ERNIE 2.0 (B) <ref type="bibr" target="#b31">(Sun et al., 2019b)</ref>, ERNIE 2.0 10 Such as fixed expressions and common phrases, which may have less varied meanings than other ordinary combinations of characters and random character sequences. <ref type="bibr">11</ref> We only report the performance on their conducted tasks.  (L) <ref type="bibr" target="#b31">(Sun et al., 2019b)</ref>, NEZHA (B) and (L) <ref type="bibr" target="#b33">(Wei et al., 2019)</ref> where B and L denote the base and large model of BERT, respectively. Note that although there are other pre-trained encoders with exploiting entity knowledge or multi-model signals, they are not compared in this paper because external information are required in their work. In fact, even though without using such external information, ZEN still achieves the state-of-the-art performance on many of the tasks experimented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CWS</head><note type="other">POS NER DC SA SPM NLI TEST</note><p>In general, the results clearly indicate the effectiveness of ZEN. In detail, for the comparison between ZEN and BERT-wwm, it shows that, when starting from pre-trained BERT, ZEN outperforms BERT-wwm on all tasks that BERTwwm has results reported. This observation suggests that explicitly representing n-grams and integrating them into BERT has its advantage over using masking strategy, and using n-grams rather than word may have better tolerance on error propagation since word segmentation is unreliable in many cases. The comparison between ZEN and ERNIE encoders also illustrates the superiority of enhancing BERT with n-grams. For example, ZEN shows a consistent improvement over ERNIE 1.0 even though significantly larger non-public datasets were utilized in their pre-training. Compared to ERNIE 2.0, which used many more pretraining tasks and significantly more non-public training data, ZEN is still competitive on SA, SPM and NLI tasks. Particularly, ZEN outperforms ERNIE 2.0 (B) on SA (TEST) and SPM (TEST), which indicates that n-gram enhanced character-based encoders of ZEN can achieve performance comparable to approaches using significantly more resources. Since the two approaches are complementary to each other, one might be able to combine them to achieve higher performance. Moreover, ZEN and ERNIE 2.0 (L) have comparable performance on some certain tasks (e.g., SA and SPM), which further confirms the power of ZEN even though the model of ERNIE 2.0 is significantly larger. Similar observation is also drawn from the comparison between ZEN and NEZHA, where ZEN illustrates its effectiveness again when compared to a model that learning with larger model and more data, as well as more tricks applied in pre-training. However, for NLI task, ZEN's performance is not as good as ERNIE 2.0 and NEZHA (B &amp; L), which further indicates that their model are good at inference task owing to their larger model setting and large-scale corpora have much more prior knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-training with Small Corpus</head><p>Pre-trained models usually require a large corpus to perform its training. However, in many applications in specialized domains, a large corpus may not be available. For such applications with limited training data, ZEN, with n-gram enhancement, is expected to encode text much more effectively. Therefore, to further illustrate the advantage of ZEN, we conduct an experiment that uses a small corpus to pre-train BERT and ZEN. In detail, we prepare a corpus with 1/10 size of the entire Chinese Wikipedia by randomly selecting sentences from it. Then all encoders are pre-trained on it with random initialization and tested on the same NLP tasks in the previous experiment. The results are reported in <ref type="table" target="#tab_5">Table 3</ref>. In general, same trend is shown in this experiment when compared with that in the previous one, where ZEN constantly outperform BERT in all task. This observation confirms that representing n-grams provides stable enhancement when our model is trained on corpora with different sizes. In detail, these results also reveals that n-gram information helps more on some tasks, e.g., CWS, NER, NLI, over the others. The reason is not surprising since that boundary information carried by n-grams can play a pivotal role in these tasks. Overall, this experiment simulates the situation of pretraining a text encoder with limited data, which could be a decisive barrier in pre-training a text encoder in the coldstart scenario, and thus demonstrates that ZEN has its potential to perform well in this situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analyses</head><p>We analyze ZEN with several factors affecting its performance. Details are illustrated in this section.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effects of Pre-training Epochs</head><p>The number of pretraining epochs is another factor affecting the performance of pre-trained encoders. In this analysis, we use CWS and SA as two probing tasks to test the performance of different encoders (BERT and ZEN) against the number of pretraining epochs. The pretrained models at certain epochs are fine-tuned on these tasks, and the results are illustrated in <ref type="figure" target="#fig_3">Figure 2</ref> and 3. We have the following observations. First, for both P and R models, ZEN shows better curves than those of BERT in both tasks, which indicates that ZEN achieves higher performance at comparable pretraining stages. Second, for R settings, ZEN shows a noticeable faster convergence than BERT, especially during the first few epochs of pretraining. This demonstrates that n-gram information improves the encoder's performance when pretraining starts from random initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effects of N-gram Extraction Threshold</head><p>To explore how n-gram extraction cutoff threshold affects the performance of ZEN, we test it with different thresholds for n-gram lexicon extraction. Similar to the previous experiment, we also use  CWS and SA as the probe tasks in this analysis.</p><p>The first analysis on threshold-performance relations is demonstrated in <ref type="figure" target="#fig_5">Figure 4</ref>, where we set the threshold ranging from 0 to 40 and use the max number of 128 n-grams in pre-training. In doing so, we observe that the best performed ZEN on both tasks is obtained when the threshold is set to 15, where increasing the threshold value under 15 causes improved performance of ZEN and vice versa when it gets over 15. This observation confirms that either too many (lower threshold) or too few (higher threshold) n-grams in the lexicon are less helpful in enhancing ZEN's performance, since there exists a balance between introducing enough knowledge and noise.</p><p>For the second analysis, when an optimal threshold is given (i.e., 15), one wants to know the performance of ZEN with different maximum number of n-grams in pre-training for each input sequence. In this analysis we test such number ranging from 0 (no n-grams encoded in ZEN) to 128, with the results shown in <ref type="figure" target="#fig_6">Figure 5</ref> (X-axis is in log view with base 2). It shows that the number 32 (2 5 ) gives a good tradeoff between performance and computation, although there is a small gain by using more n-grams. This analysis illustrates that ZEN only requires a small numbers of n-grams to achieve good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Visualization of N-gram Representations</head><p>In addition to quantitative analysis, we also conduct case studies on some certain instances to further illustrate the effectiveness of n-gram representations in pre-training ZEN. <ref type="figure" target="#fig_7">Figure 6</ref> and 7 visualize the weights of extracted n-grams from two input instances when they are encoded by ZEN across different layers. In general, 'valid' n-grams are more favored than others, e.g., 提高 (improve), 波士顿 (Boston) have higher weights than 会提高 (will improve) and 士顿 (Ston), especially those ones that have cross ambiguities in the context, e.g., 高速 (high speed) should not be considered in the first instance so that 速度 (speed) has a higher weight than it. This observation illustrates that ZEN is able to not only distinguish those phrasal n-grams to others but also select appropriate ones according to the context. Interestingly, for different layers, long (and valid) n-grams, e.g., 提高速 度 (speed up) and 波士顿咨询 (Boston consulting group), tend to receive more intensive weights at higher layers, which implicitly indicates that such n-grams contain more semantic rather than morphological information. We note that information encoded in BERT follows a similar layerwise order as what is suggested in <ref type="bibr" target="#b10">Jawahar et al. (2019)</ref>. The observations from this case study, as well as the overall performance in previous experiments, suggest that integration of n-grams in ZEN not only enhances the representation power of character-based encoders, but also provides a potential solution to some text analyzing tasks, e.g., chunking and keyphrase extraction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Representation learning of text attracts much attention in recent years, with the rise of deep learning in NLP <ref type="bibr" target="#b3">(Collobert et al., 2011;</ref><ref type="bibr" target="#b18">Mikolov et al., 2013;</ref><ref type="bibr" target="#b19">Pennington et al., 2014)</ref>. There are considerable interests in representing text with contextualized information <ref type="bibr" target="#b12">(Ling et al., 2015;</ref><ref type="bibr" target="#b16">Melamud et al., 2016;</ref><ref type="bibr" target="#b0">Bojanowski et al., 2017;</ref><ref type="bibr" target="#b25">Song et al., 2017</ref><ref type="bibr" target="#b20">Peters et al., 2018a;</ref>. Following this paradigm, pre-trained models have been proposed and are proven useful in many NLP tasks <ref type="bibr" target="#b6">(Devlin et al., 2018;</ref><ref type="bibr" target="#b22">Radford et al., 2018</ref><ref type="bibr" target="#b23">Radford et al., , 2019</ref>. In detail, such models can be categorized into two types: autoregressive and autoencoding encoders. The former models behave like normal language models that predict the probability distributions of text units following observed texts. These models, such as GPT <ref type="bibr" target="#b22">(Radford et al., 2018)</ref> and GPT2 <ref type="bibr" target="#b23">(Radford et al., 2019)</ref>, are trained to encode a unidirectional context. Differently, the autoencoding models, such as BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref> and XLNet , leverage bidirectional context, and encode text by reconstructing the masked tokens in each text instance according to their context from both sides. Because words carry important linguistic information in Chinese, many enhanced pre-train models are proposed specifically for Chinese that can utilize word-level information in one way or another. For example, ERNIE 1.0 <ref type="bibr" target="#b30">(Sun et al., 2019a)</ref> adopted a multi-level masking strategy performed on different level of texts; its improved version, ERNIE 2.0 <ref type="bibr" target="#b31">(Sun et al., 2019b)</ref> used continual pretraining strategy which is benefited from multitask learning with more parameters in the model. Recently, BERT-wwm <ref type="bibr" target="#b5">(Cui et al., 2019)</ref> enhanced Chinese BERT with a simple masking of wholewords. In addition, there are other recent studies that enhanced BERT for Chinese language processing, such as optimizing training via special optimization techniques <ref type="bibr" target="#b33">(Wei et al., 2019)</ref> or from prior knowledge <ref type="bibr">(Liu et al., 2019b)</ref>. All the studies revealed that processing on larger granularity of text is helpful in Chinese, which is consistent with previous findings in many Chinese NLP tasks <ref type="bibr" target="#b24">(Song et al., 2009;</ref><ref type="bibr" target="#b28">Song and Xia, 2012;</ref><ref type="bibr" target="#b34">Wu et al., 2015;</ref><ref type="bibr" target="#b9">Higashiyama et al., 2019)</ref>. However, previous approach are limited to the use of weak supervision, i.e., masking, to incorporate word/phrase information. ZEN thus provides an alternative solution that explicitly encodes n-grams into character-based encoding, which is effective for downstream NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed ZEN, a pre-trained Chinese text encoder enhanced by n-gram representations, where different combinations of characters are extracted, encoded and integrated in training a backbone model, i.e., BERT. In ZEN, given a sequence of Chinese characters, n-grams are extracted and their information are effectively incorporated into the character encoder. Different from previous work, ZEN provides an alternative way of learning larger granular text for pre-trained models, where the structure of BERT is extended by another Transformer-style encoder to represent the extracted n-grams for each input text instance.</p><p>Experiments on several NLP tasks demonstrated the validity and effectiveness of ZEN. Particularly, state-of-the-art results were obtained while ZEN only uses BERT base model requiring less training data and no knowledge from external sources compared to other existing Chinese text encoders. Further analyses of ZEN are conducted, showing that ZEN is efficient and able to learn with limited data. We note that ZEN employs a different method to incorporate word information that is complementary to some other previous approaches. Therefore it is potentially beneficial to combine it with previous approaches suggested by other researchers, as well as to other languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The overall architecture of ZEN, where the area marked by dashed box 'A' presents the character encoder (BERT, in Transformer structure); and the area marked by dashed box 'B' is the n-gram encoder.[NSP] and[MLM]   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>273K</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>; Lam-9 https://github.com/NVIDIA/apex ple et al. (2016); Bojanowski et al. (2017);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>CWS performance against training epochs of BERT and ZEN with different parameter initialization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>SA performance against training epochs of BERT and ZEN with different parameter initialization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>CWS and SA performance of ZEN against frequency threshold of constructing n-gram lexicons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>CWS and SA performance of ZEN against maximum n-gram numbers for training each instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>think drunk driving will speed up?) The heatmap of n-grams encoded by ZEN across different layers for an example sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>The heatmap of n-grams encoded by ZEN across different layers for an example sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The statistics of task datasets used in our experiments. S#, C#, D# and SP# refer to numbers of sentences, characters, documents and sentence pairs, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. The resulting ZEN requires only 20% additional inference time (averaged by testing on the seven tasks) over TEST TEST DEV TEST DEV TEST DEV TEST DEV TEST BERT (R) 97.20 95.72 95.43 93.12 96.90 96.71 94.00 94.10 87.22 85.13 75.67 75.01 BERT (P) 97.95 96.30 96.10 94.78 97.60 97.50 94.53 94.67 88.50 86.59 77.40 77.52 BERT-WWM ---95.10 97.60 97.60 94.50 95.00 89.20 86.80 78.40 78.00 ERNIE 1.0 ---95.10 97.30 97.30 95.20 95.40 89.70 87.40 79.90 78.40 95.70 95.50 90.90 87.90 81.20 79.70 .12 95.82 93.24 97.20 96.87 94.87 94.42 88.10 85.27 77.11 77.03 ZEN (P) 98.35 97.43 96.64 95.25 97.66 97.64 95.66 96.08 90.20 87.95 80.48 79.20</figDesc><table><row><cell></cell><cell>CWS</cell><cell>POS</cell><cell></cell><cell>NER</cell><cell>DC</cell><cell>SA</cell><cell>SPM</cell><cell>NLI</cell></row><row><cell cols="8">TEST DEV ERNIE 2.0 (B) ---NEZHA (B) ---------94.74 95.17 89.98 87.41 81.37 79.32</cell></row><row><cell>NEZHA-WWM (B)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-94.75 95.84 89.85 87.10 81.25 79.11</cell></row><row><cell>ERNIE 2.0 (L)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-96.10 95.80 90.90 87.90 82.60 81.00</cell></row><row><cell>NEZHA (L)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-95.92 95.83 90.18 87.20 81.53 80.44</cell></row><row><cell>NEZHA-wwm (L)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-95.75 96.00 90.87 87.94 82.21 81.17</cell></row><row><cell>ZEN (R)</cell><cell>97.89 96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The overall performance of ZEN and the comparison against existing models on seven NLP tasks, where R denotes that pre-training starts from random initialization and P is that model parameters are initialized from Google's released Chinese BERT base model. B and L refer to each backbone model uses BERT base or large model, respectively. Since ZEN uses BERT base model, encoders using BERT large model and their performance are listed as references in italic fonts. The bold numbers are the best results from all base models in each column.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>93.64 93.23 87.11 96.02 95.77 93.41 92.33 85.62 85.53 72.12 71.44 ZEN (R) 96.05 93.79 93.37 88.39 96.11 96.05 93.92 93.51 86.12 85.78 72.66 72.31</figDesc><table><row><cell></cell><cell>DEV TEST TEST</cell><cell>DEV TEST</cell><cell>DEV TEST</cell><cell>DEV TEST</cell><cell>DEV TEST</cell></row><row><cell>BERT (R)</cell><cell>95.14</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The performance of BERT and ZEN on seven NLP tasks when they are trained on a small corpus.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Herein 'valid' regards to that an n-gram is a proper chunk or phrase that is frequently used in the running text.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Although the character encoder may still use masking as a learning objective, the encoded n-grams are explicitly leveraged in our model.3  The two terms, 'BERT' and 'character encoder' are used interchangeably in this paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://dumps.wikimedia.org/zhwiki/ 5 http://sighan.cs.uchicago.edu/bakeoff2006/ 6 https://github.com/pengming617/bert classification</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Our main experiments are conducted on cut-off=15, resulting in 104K n-grams in the lexicon.8  That is, we extract up to 128 n-grams per instance.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh-Teng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chia</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><forename type="middle">Yungjen</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09968</idno>
		<title level="m">A Hybrid Word-Character Model for Abstractive Summarization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04341</idno>
		<title level="m">What Does BERT Look At? An Analysis of BERT&apos;s Attention</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural Language Processing (Almost) from Scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05053</idno>
		<title level="m">XNLI: Evaluating Cross-lingual Sentence Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08101</idno>
		<title level="m">Pre-Training with Whole Word Masking for Chinese BERT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santos</forename><surname>Cicero Dos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maira</forename><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Second International Chinese Word Segmentation Bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">Emerson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth SIGHAN workshop on Chinese language Processing</title>
		<meeting>the fourth SIGHAN workshop on Chinese language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating Word Attention into Character-Based Word Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Higashiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Ideuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiaki</forename><surname>Oida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohei</forename><surname>Sakamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Okada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2699" to="2709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What Does BERT Learn about the Structure of Language?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Unicomb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerardo</forename><surname>Iñiguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Márton</forename><surname>Karsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Léo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Márton</forename><surname>Karsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Sarraute</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Fleury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural Architectures for Named Entity Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two/Too Simple Adaptations of Word2Vec for Syntax Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1299" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An Encoding Strategy Based Word-Character LSTM for Chinese NER</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongge</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueran</forename><surname>Zu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2379" to="2389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07606</idno>
		<title level="m">Qi Ju, Haotang Deng, and Ping Wang. 2019b. K-BERT: Enabling Language Representation with Knowledge Graph</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LCQMC: A Large-scale Chinese Question Matching Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongfang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1952" to="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">context2vec: Learning Generic Context Embedding with Bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<title level="m">Mixed precision training</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient Estimation of Word Representations in Vector Space</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>New Orleans</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep Contextualized Word Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improving Language Understanding by Generative Pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ope-nAI Blog</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transliteration of Name Entity via Improved Statistical Translation on Character Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Kit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration</title>
		<meeting>the 2009 Named Entities Workshop: Shared Task on Transliteration<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
	<note>Suntec</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Word Representations with Regularization from Prior Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Jung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Complementary Learning of Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4368" to="4374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Directional Skip-Gram: Explicitly Distinguishing Left and Right Context for Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="175" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using a Goodness Measurement for Domain Adaptation: A Case Study on Chinese Word Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3853" to="3860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">THUCTC: An Efficient Chinese Text Classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<ptr target="https://github.com/thunlp/THUCTC" />
	</analytic>
	<monogr>
		<title level="j">GitHub Repository</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">ERNIE: Enhanced Representation through Knowledge Integration</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">ERNIE 2.0: A Continual Pre-training Framework for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12412</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqiu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00204</idno>
		<title level="m">NEZHA: Neural Contextualized Representation for Chinese Language Understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Named Entity Recognition in Chinese Clinical Text Using Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in health technology and informatics</title>
		<imprint>
			<biblScope unit="volume">216</biblScope>
			<biblScope unit="page">624</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The Penn Chinese TreeBank: Phrase Structure Annotation of a Large Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adapting bert for target-oriented multimodal sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5408" to="5414" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
