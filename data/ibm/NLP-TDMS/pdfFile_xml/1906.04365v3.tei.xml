<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Representation Learning-Assisted Click-Through Rate Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Marketing Platform</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuwu</forename><surname>Zhang</surname></persName>
							<email>xiuwu.zxw@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Marketing Platform</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shukui</forename><surname>Ren</surname></persName>
							<email>shukui.rsk@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Marketing Platform</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Marketing Platform</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojie</forename><surname>Liu</surname></persName>
							<email>zhaojie.lzj@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Marketing Platform</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanlong</forename><surname>Du</surname></persName>
							<email>yanlong.dyl@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Marketing Platform</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Representation Learning-Assisted Click-Through Rate Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Click-through rate (CTR) prediction is a critical task in online advertising systems. Most existing methods mainly model the feature-CTR relationship and suffer from the data sparsity issue. In this paper, we propose DeepMCP, which models other types of relationships in order to learn more informative and statistically reliable feature representations, and in consequence to improve the performance of CTR prediction. In particular, DeepMCP contains three parts: a matching subnet, a correlation subnet and a prediction subnet. These subnets model the user-ad, ad-ad and feature-CTR relationship respectively. When these subnets are jointly optimized under the supervision of the target labels, the learned feature representations have both good prediction powers and good representation abilities. Experiments on two large-scale datasets demonstrate that DeepMCP outperforms several state-of-the-art models for CTR prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Click-through rate (CTR) prediction is to predict the probability that a user will click on an item. It plays an important role in online advertising systems. For example, the ad ranking strategy generally depends on CTR × bid, where bid is the benefit the system receives if an ad is clicked by a user. Moreover, according to the common cost-per-click charging model, advertisers are only charged once their ads are clicked by users. Therefore, in order to maximize the revenue and to maintain a desirable user experience, it is crucial to estimate the CTR of ads accurately.</p><p>CTR prediction has attracted lots of attention from both academia and industry <ref type="bibr" target="#b1">[He et al., 2014;</ref><ref type="bibr" target="#b8">Shan et al., 2016;</ref><ref type="bibr" target="#b1">Guo et al., 2017]</ref>. For example, the Logistic Regression (LR) model <ref type="bibr" target="#b7">[Richardson et al., 2007]</ref> considers linear feature importance and models the predicted CTR asŷ = σ(w 0 + i w i x i ), where σ(·) is the sigmoid function, x i is the ith feature and w 0 , w i are model weights. The Factorization Machine (FM) <ref type="bibr" target="#b6">[Rendle, 2010]</ref> is proposed to further model pairwise feature interactions. It models the predicted CTR asŷ = σ(w 0 + i w i</p><formula xml:id="formula_0">x i + i j v T i v j x i x j ),</formula><p>where v i is the latent embedding vector of the ith feature. In recent years, Deep Neural Networks (DNNs) <ref type="bibr" target="#b3">[LeCun et al., 2015]</ref> are exploited for CTR prediction and item recommendation in order to automatically learn feature representations and high-order feature interactions [Van den <ref type="bibr" target="#b8">Oord et al., 2013;</ref><ref type="bibr">Zhang et al., 2016b;</ref><ref type="bibr" target="#b5">Qu et al., 2016;</ref><ref type="bibr" target="#b0">Covington et al., 2016]</ref>.</p><p>To take advantage of both shallow and deep models, hybrid models are also proposed. For example, <ref type="bibr">Wide&amp;Deep [Cheng et al., 2016]</ref> combines LR and DNN, in order to improve both the memorization and generalization abilities of the model. DeepFM <ref type="bibr" target="#b1">[Guo et al., 2017]</ref> combines FM and DNN, which further improves the model ability of learning feature interactions. Neural Factorization Machine <ref type="bibr">[He and Chua, 2017]</ref> combines the linearity of FM and the non-linearity of DNN. Nevertheless, these models only consider the feature-CTR relationship. In contrast, the DeepMCP model proposed in this paper additionally considers feature-feature relationships, such as the user-ad relationship and the ad-ad relationship. We illustrate their difference in <ref type="figure">Figure 1</ref>. Note that the feature interaction in FM still models the feature-CTR relationship. It can be considered as two features-CTR, because it models how the feature interaction v T i v j x i x j relates to the CTRŷ, but does not model whether the two feature representations v i and v j should be similar to each other.</p><p>In particular, our proposed DeepMCP model contains three parts: a matching subnet, a correlation subnet and a prediction subnet. They share the same embedding matrix. The matching subnet models the user-ad relationship (i.e., whether an ad matches a user's interest) and aims to learn useful user and ad representations. The correlation subnet models the ad-ad relationship (i.e., which ads are within a time window in a user's click sequence) and aims to learn useful ad representations. The prediction subnet models the feature-CTR relationship and aims to predict the CTR given all the fea-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label User ID User Age</head><p>Ad <ref type="table" target="#tab_1">Title  1  2135147  24  Beijing flower delivery  0  3467291  31  Nike shoes, sporting shoes  0  1739086  45  Female clothing and jeans   Table 1</ref>: Each row is an instance for CTR prediction. The first column is the label (1 -clicked, 0 -unclicked). Each of the other columns is a field. The instantiation of a field is a feature. tures. When these subnets are jointly optimized under the supervision of the target labels, the feature representations are learned in such a way that they have both good prediction powers and good representation abilities. Moreover, as the same feature appears in different subnets in different ways, the learned representations are more statistically reliable. In summary, the main contributions of this paper are • We propose a new model DeepMCP for CTR prediction. Unlike classical CTR prediction models that mainly consider the feature-CTR relationship, DeepMCP further considers user-ad and ad-ad relationships. • We conduct extensive experiments on two large-scale datasets to compare the performance of DeepMCP with several state-of-the-art models. We make the implementation code of DeepMCP publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep Matching, Correlation and Prediction (DeepMCP) Model</head><p>In this section, we present the DeepMCP model in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Overview</head><p>The task of CTR prediction in online advertising is to estimate the probability of a user clicking on a specific ad. <ref type="table">Table  1</ref> shows some example instances. Each instance can be described by multiple fields such as user information (user ID, city, etc.) and ad information (creative ID, title, etc.). The instantiation of a field is a feature. Unlike most existing CTR prediction models that mainly consider the feature-CTR relationship, our proposed DeepMCP model additionally considers the user-ad and adad relationships. DeepMCP contains three parts: a matching subnet, a correlation subnet and a prediction subnet (cf. <ref type="figure" target="#fig_1">Figure 2(a)</ref>). When these subnets are jointly optimized under the supervision of the target labels, the learned feature representations have both good prediction powers and good representation abilities. Another property of DeepMCP is that although all the subnets are active during training, only the prediction subnet is active during testing (cf. <ref type="figure" target="#fig_1">Figure 2(b)</ref>). This makes the testing phase rather simple and efficient.</p><p>We segregate the features into four groups: user (e.g., user ID, age), query (e.g., query, query category), ad (e.g., creative ID, ad title) and other features (e.g., hour of day, day of week). Each subnet uses a different set of features. In particular, the prediction subnet uses all the four groups of features, the matching subnet uses the user, query and ad features, and the correlation subnet uses only the ad features. All the subnets share the same embedding matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivating Example</head><p>Before we present the details of DeepMCP, we first illustrate the rationale of DeepMCP through a motivating example in <ref type="figure" target="#fig_2">Figure 3</ref>. For simplicity, we only show user features u, ad features a and other features o.</p><p>Because the feature embeddings (i.e., representations) are randomly initialized, when we consider the prediction task only, it is likely that the learned representation of user u 1 and that of user u 2 are largely different. This is because the prediction task does not model the relationship between features. As a consequence, it is hard to accurately estimate the pCTR of user u 2 on ad a 3 . If we further consider the matching task which models the user-ad relationship and the correlation task which models the ad-ad relationship, the learned representation of user u 2 should be similar to that of user u 1 and the representation of ad a 3 should be similar to that of ad a 1 . The pCTR of user u 2 on ad a 3 would be similar to the pCTR of u 1 on a 3 (as well as the pCTR of u 1 on a 1 ). As a consequence, the target pCTR is more likely to be accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prediction Subnet</head><p>The prediction subnet presented here is a typical DNN model. It models the feature-CTR relationship (where explicit or implicit feature interactions are modeled). It aims to predict the CTR given all the features, supervised by the target labels. Nevertheless, the DeepMCP model is flexible that the First, a feature x i ∈ R (e.g., a user ID) goes through an embedding layer and is mapped to its embedding vector e i ∈ R K , where K is the vector dimension and e i is to be learned. The collection of all the feature embeddings is an embedding matrix E ∈ R N ×K , where N is the number of unique features. For multivalent categorical features such as the bi-grams in the ad title, we first map each bi-gram to an embedding vector and then perform sum pooling to generate the aggregated embedding vector of the ad title.</p><p>We then concatenate the embedding vectors from all the features as a long vector m. The vector m then goes through several fully connected (FC) layers with the ReLU activation function (ReLU(x) = max(0, x)), in order to exploit highorder nonlinear feature interactions <ref type="bibr">[He and Chua, 2017]</ref>. Nair and Hinton <ref type="bibr">[2010]</ref> show that ReLU has significant benefits over sigmoid and tanh activation functions in terms of the convergence rate and the quality of obtained results.</p><p>Finally, the output z of the last FC layer goes through a sigmoid function to generate the predicted CTR aŝ</p><formula xml:id="formula_1">y = 1 1 + exp[−(w T z + b)] ,</formula><p>where w and b are model parameters to be learned. To avoid model overfitting, we apply dropout <ref type="bibr" target="#b8">[Srivastava et al., 2014]</ref> after each FC layer. Dropout prevents feature co-adaptation by setting to zero a portion of hidden units during parameter learning.</p><p>All the model parameters are learned by minimizing the average logistic loss on a training set as</p><formula xml:id="formula_2">loss p = − 1 |Y| y∈Y [y logŷ + (1 − y) log(1 −ŷ)],<label>(1)</label></formula><p>where y ∈ {0, 1} is the true label of the target ad corresponding toŷ and Y is the collection of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Matching Subnet</head><p>The matching subnet models the user-ad relationship (i.e., whether an ad matches a user's interest) and aims to learn useful user and ad representations. It is inspired by semantic matching models for web search <ref type="bibr" target="#b2">[Huang et al., 2013]</ref>.</p><p>In classical matrix factorization for recommendation <ref type="bibr" target="#b3">[Koren et al., 2009]</ref>, the rating score is approximated as the inner product of the latent vectors of the user ID and the item ID. In our problem, instead of directly matching the user ID and the ad ID, we perform matching at a higher level, incorporating all the features related to the user and the ad. When a user clicks an ad, we assume that the clicked ad is relevant, at least partially, to the user's need (given the query submitted by the user, if any). In consequence, we would like the representation of the user features (and the query features) and the representation of the ad features to match well.</p><p>In particular, the matching subnet contains two parts: "user part" and "ad part". The input to the "user part" is the user features (e.g., user ID, age) and query features (e.g., query, query category). As in the prediction subnet, a feature x i ∈ R first goes through an embedding layer and is mapped to its embedding vector e i ∈ R K . We then concatenate all the feature embeddings as a long vector m u ∈ R Nu (N u is the vector dimension). The vector m u then goes through several FC layers in order to learn more abstractive, high-level representations. We use tanh (rather than ReLU) as the activation function of the last FC layer, which is defined as</p><formula xml:id="formula_3">tanh(x) = 1−exp(−2x)</formula><p>1+exp(−2x) . We will explain the reason later. The output of the "user part" is a high-level user representation vector v u ∈ R M (M is the vector dimension).</p><p>The input to the "ad part" is the ad features (e.g., creative ID, ad title). Similarly, we first map each ad feature to its embedding vector and then concatenate them as a long embedding vector m a ∈ R Na (N a is the vector dimension). The vector m a then goes through several FC layers and results in a high-level ad representation vector v a ∈ R M . Note that, the inputs to the "user" and "ad" parts usually have different sizes, i.e., N u = N a (because the number of user features and the number of ad features may not necessarily be the same). However, after the matching subnet, v u and v a have the same size M . In other words, we project two different sets of features into a common low-dimensional space.</p><p>We then compute the matching score s as</p><formula xml:id="formula_4">s(v u , v a ) = 1 1 + exp(−v T u v a )</formula><p>.</p><p>We do not use ReLU as the activation function of the last FC layer because the output after ReLU will contain lots of zeros, which makes v T u v a → 0. There are at least two choices to model the matching score: point-wise and pairwise <ref type="bibr">[Liu and others, 2009]</ref>. In a point-wise model, we could model s(v u , v a ) → 1 if user u clicks ad a and model</p><formula xml:id="formula_5">s(v u , v a ) → 0 otherwise. In a pair-wise model, we could model s(v u , v ai ) &gt; s(v u , v aj ) + δ where δ &gt; 0 is a margin, if user u clicks ad a i but not ad a j .</formula><p>We choose the point-wise model because it can directly reuse the training dataset for the prediction subnet. Formally, we minimize the following loss for the matching subnet</p><formula xml:id="formula_6">loss m = − 1 |Y| y∈Y y(u, a) log s(v u , v a ) + (1 − y(u, a)) log(1 − s(v u , v a )) ,<label>(2)</label></formula><p>where y(u, a) = 1 if user u clicks ad a and it is 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Correlation Subnet</head><p>The correlation subnet models the ad-ad relationship (i.e., which ads are within a time window in a user's click sequence) and aims to learn useful ad representations. The skipgram model is proposed in <ref type="bibr" target="#b4">[Mikolov et al., 2013]</ref> to learn useful representations of words in a sequence, where words within a context window have certain correlation. It has been widely applied in many tasks to learn useful low-dimensional representations .</p><p>In our problem, we apply the skip-gram model to learn useful ad representations, because the clicked ads of a user also form a sequence with certain correlation over time. Formally, given a sequence of ads {a 1 , a 2 , · · · , a L } clicked by a user, we would like to maximize the average log likelihood as</p><formula xml:id="formula_7">ll = 1 L L i=1 1≤i+j≤L,j =0 −C≤j≤C log p(a i+j |a i ),</formula><p>where L is the number of ads in the sequence and C is a context window size. The probability p(a i+j |a i ) can be defined in different ways such as softmax, hierarchical softmax and negative sampling <ref type="bibr" target="#b4">[Mikolov et al., 2013]</ref>. We choose the negative sampling technique due to its efficiency. p(a i+j |a i ) is then defined as</p><formula xml:id="formula_8">p(a i+j |a i ) = σ(h T ai+j h ai ) Q q=1 σ(−h T aq h ai ),</formula><p>where Q is the number of sampled negative ads and σ(h T ai+j h ai ) = 1 1+exp(−h T a i+j ha i ) . h ai is a high-level representation vector that involves all the features related to ad a i and that goes through several FC layers (cf. <ref type="figure" target="#fig_3">Figure 4)</ref>.  The loss function of the correlation subnet is then given by minimizing the negative average log likelihood as</p><formula xml:id="formula_9">loss c = 1 L L i=1 1≤i+j≤L,j =0 −C≤j≤C − log σ(h T ai+j h ai ) − Q q=1 log σ(−h T aq h ai ) .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Offline Training Procedure</head><p>The final joint loss function of DeepMCP is given by loss = loss p + αloss m + βloss c , (4) where loss p is the prediction loss in Eq. (1), loss m is the matching loss in Eq. <ref type="formula" target="#formula_6">(2)</ref>, loss c is the correlation loss in Eq.</p><p>(3), and α and β are tunable hyperparameters for balancing the importance of different subnets.</p><p>The DeepMCP model is trained by minimizing the joint loss function on a training dataset. Since our aim is to maximize the CTR prediction performance, we evaluate the model on a separate validation dataset and record the validation AUC (an evaluation metric, which will be explained in §3.4) during the training procedure. The optimal model parameters are obtained at the highest validation AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Online Procedure</head><p>As we have illustrated in <ref type="figure" target="#fig_1">Figure 2(b)</ref>, in the online testing phase, the DeepMCP model only needs to compute the predicted CTR (pCTR). Therefore, only the features from the target ad are needed and only the prediction subnet is active. This makes the online phase of DeepMCP rather simple and efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we conduct experiments on two large-scale datasets to evaluate the performance of DeepMCP and several state-of-the-art methods for CTR prediction. <ref type="table" target="#tab_1">Table 2</ref> lists the statistics of two large-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>1) Avito advertising dataset 2 . This dataset contains a random sample of ad logs from avito.ru, the largest general classified website in Russia. We use the ad logs from 2015-04-28 to 2015-05-18 for training, those on 2015-05-19 for validation, and those on 2015-05-20 for testing. In CTR prediction, testing is usually the next-day prediction. The test set contains 2.3 × 10 6 instances. The features used include 1) user features such as user ID, IP ID, user agent and user device, 2) query features such as search query, search category and search parameters, 3) ad features such as ad ID, ad title and ad category, and 4) other features such as hour of day and day of week.</p><p>2) Company advertising dataset. This dataset contains a random sample of ad impression and click logs from a commercial advertising system in Alibaba. We use ad logs of 30 consecutive days during Aug.-Sep. 2018 for training, logs of the next day for validation, and logs of the day after the next day for testing. The test set contains 1.9 × 10 6 instances. The features used also include user, query, ad and other features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Methods Compared</head><p>We compare the following methods for CTR prediction.</p><p>1. LR. Logistic Regression <ref type="bibr" target="#b7">[Richardson et al., 2007]</ref>. It models linear feature importance. 2. FM. Factorization Machine <ref type="bibr" target="#b6">[Rendle, 2010]</ref>. It models both first-order feature importance and second-order feature interactions. 3. DNN. Deep Neural Network. It contains an embedding layer, several fully connected layers and an output layer. 4. PNN. The Product-based Neural Network in <ref type="bibr" target="#b5">[Qu et al., 2016]</ref>. It introduces a production layer between the embedding layer and fully connected layers of DNN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parameter Settings</head><p>We set the embedding dimension of each feature as K = 10, because the number of distinct features is huge. We set the number of fully connected layers in neural network-based models as 2, with dimensions 512 and 256. We set the batch size as 128, the context window size as C = 2 and the number of negative ads as Q = 4. The dropout ratio is set to 0.5. All the methods are implemented in Tensorflow and optimized by the Adagrad algorithm <ref type="bibr" target="#b0">[Duchi et al., 2011]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Metrics</head><p>We use the following evaluation metrics.   We now examine our proposed models. DeepCP contains the correlation subnet and the prediction subnet. DeepMP contains the matching subnet and the prediction subnet. It is observed that both DeepCP and DeepMP outperform the best-performing baseline on the two datasets. As the baseline methods only consider the prediction task, these observations show that additionally consider representation learning tasks can aid the performance of CTR prediction. It is also observed that DeepMP performs much better than DeepCP. It indicates that the matching subnet brings more benefits than the correlation subnet. This makes sense because the matching subnet considers both the users and the ads, while the correlation subnet considers only the ads. It is observed that DeepMCP that contains the matching, correlation and prediction subnets performs best on both datasets. These observations demonstrate the effectiveness of DeepMCP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Effectiveness</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Effect of the Balancing Parameters</head><p>In this section, we examine the effect of tuning balancing hyperparameters of DeepMCP. <ref type="figure" target="#fig_5">Figure 5</ref> and <ref type="figure">Figure 6</ref> examine α (matching subnet) and β (correlation subnet) respectively. It is observed that the AUCs increase when one hyperparameter enlarges at the beginning, but then decrease when it further enlarges. On the Company dataset, large β can lead to very bad performance that is even worse than the prediction subnet only. Overall, the matching subnet leads to larger AUC im- provement than the correlation subnet. The Company dataset is more sensitive to the β parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Effect of the Hidden Layer Size</head><p>In this section, we examine the effect of the hidden layer sizes of neural network-based methods. In order not to make the figure cluttered, we only show the results of DNN, Wide&amp;Deep and DeepMCP. <ref type="figure">Figure 7</ref> plots the AUCs vs. the hidden layer sizes when the number of hidden layers is 2. We use a shrinking structure, where the second layer dimension is only half of the first. It is observed that when the first layer dimension increases from 128 to 512, AUCs generally increase. But when the dimension further enlarges, the performance may degrade. This is possibly because it is more difficult to train a more complex model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Effect of the Number of Hidden Layers</head><p>In this section, we examine the effect of the number of hidden layers.  <ref type="bibr">[2048,</ref><ref type="bibr">1024,</ref><ref type="bibr">512,</ref><ref type="bibr">256]</ref>. It is observed in <ref type="figure">Figure 8</ref> that when the number of hidden layers increases from 1 to 2, the performance generally increases. This is because more hidden layers have better expressive abilities <ref type="bibr">[He and Chua, 2017]</ref>. But when the number of hidden layers further increases, the performance then decreases. This is because it is more difficult to train deeper neural networks.  <ref type="bibr" target="#b6">[Rendle, 2010;</ref><ref type="bibr" target="#b7">Rendle, 2012]</ref> are proposed to model pairwise feature interactions in terms of the latent vectors corresponding to the involved features. Fieldaware FM <ref type="bibr" target="#b3">[Juan et al., 2016]</ref> and Field-weighted FM <ref type="bibr" target="#b4">[Pan et al., 2018]</ref> further consider the impact of the field that a feature belongs to in order to improve the performance of FM. In recent years, Deep Neural Networks (DNNs) are exploited for CTR prediction and item recommendation in order to automatically learn feature representations and high-order feature interactions [Van den <ref type="bibr" target="#b8">Oord et al., 2013;</ref><ref type="bibr" target="#b0">Covington et al., 2016;</ref><ref type="bibr" target="#b8">Wang et al., 2017;</ref><ref type="bibr">He and Chua, 2017]</ref>. <ref type="bibr">Zhang et al. [2016b]</ref> propose Factorization-machine supported Neural Network (FNN), which pre-trains an FM before applying a DNN. <ref type="bibr" target="#b5">Qu et al. [2016]</ref> propose the Product-based Neural Network (PNN) where a product layer is introduced between the embedding layer and the fully connected layer.  propose Wide&amp;Deep, which combines LR and DNN in order to improve both the memorization and generalization abilities of the model. <ref type="bibr" target="#b1">Guo et al. [2017]</ref> propose DeepFM, which models low-order feature interactions like FM and models high-order feature interactions like <ref type="bibr">DNN. He et al. [2017]</ref> propose the Neural Factorization Machine which combines the linearity of FM and the non-linearity of neural networks. Nevertheless, these methods mainly model the feature-CTR relationship. Our proposed DeepMCP model further considers user-ad and ad-ad relationships.</p><p>Multi-modal / Multi-task Learning Our work is also closely related to multi-modal / multi-task learning, where multiple kinds of information or auxiliary tasks are introduced to help improve the performance of the main task. For example, <ref type="bibr" target="#b8">Zhang et al. [2016a]</ref> leverage heterogeneous information (i.e., structural content, textual content and visual content) in a knowledge base to improve the quality of recommender systems.  utilize textual content and social tag information, in addition to classical item structure information, for improved recommendation. <ref type="bibr" target="#b2">Huang et al. [2018]</ref> introduce context-aware ranking as an auxiliary task in order to better model the semantics of queries in entity recommendation. <ref type="bibr">Gong et al. [2019]</ref> propose a multi-task model which additionally learns segment tagging and named entity tagging for slot filling in online shopping assistant. In our work, we address a different problem and we introduce two auxiliary but related tasks (i.e., matching and correlation with shared embeddings) to improve the performance of CTR prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose DeepMCP, which contains a matching subnet, a correlation subnet and a prediction subnet for CTR prediction. These subnets model the user-ad, ad-ad and feature-CTR relationship respectively. Compared with classical CTR prediction models that mainly consider the feature-CTR relationship, DeepMCP has better prediction power and representation ability. Experimental results on two largescale datasets demonstrate the effectiveness of DeepMCP in CTR prediction. It is observed that the matching subnet leads to higher performance improvement than the correlation subnet. This is possibly because the former considers both users and ads, while the latter considers ads only.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Classical CTR prediction methods model the feature-CTR relationship. (b) DeepMCP further models feature-feature relationships, such as the user-ad relationship (dashed curve) and the ad-ad relationship (dotted curve).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Sketch view of the DeepMCP model: (a) Training, (b) Testing. All the subnets are active during training, but only the prediction subnet is active during testing. pCTR is predicted CTR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>1 https://github.com/oywtece/deepmcp Motivating example (u -user features, a -ad features, oother features). Please refer to §2.1 for more detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Detailed view of the DeepMCP model. The prediction, matching and correlation subnets share the same embedding matrix. prediction subnet can be replaced by any other CTR prediction model, such as Wide&amp;Deep [Cheng et al., 2016] and DeepFM [Guo et al., 2017].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>5. Wide&amp;Deep. The Wide&amp;Deep model in [Cheng et al., 2016]. It combines LR (wide part) and DNN (deep part). 6. DeepFM. The DeepFM model in [Guo et al., 2017]. It combines FM (wide part) and DNN (deep part). 7. DeepCP. A variant of the DeepMCP model, which contains only the correlation and the prediction subnets. It is equivalent to setting α = 0 in Eq. (4). 8. DeepMP. A variant of the DeepMCP model, which contains only the matching and the prediction subnets. It is equivalent to setting β = 0 in Eq. (4). 9. DeepMCP. The DeepMCP model ( §2) which contains the matching, correlation and prediction subnets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>AUC vs. α in DeepMCP (β = 0): effect of the matching subnet, in addition to the prediction subnet. DNN = Pred, DeepMP = Pred+Match. (a) Avito (b) Company Figure 6: AUC vs. β in DeepMCP (α = 0): effect of the correlation subnet, in addition to the prediction subnet. DNN = Pred, DeepCP = Pred+Corr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of experimental large-scale datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1. AUC: the Area Under the ROC Curve over the test set. Logloss: the value of Eq. (1) over the test set. The smaller the better.</figDesc><table><row><cell>The larger the better. It reflects the probability that a model ranks a randomly chosen positive instance higher than a randomly chosen negative instance. A small im-provement in AUC is likely to lead to a significant in-crease in online CTR [Cheng et al., 2016]. 2. Avito Algorithm AUC Logloss AUC Logloss Company LR 0.7556 0.05918 0.7404 0.2404 FM 0.7802 0.06094 0.7557 0.2365 DNN 0.7816 0.05655 0.7579 0.2360 PNN 0.7817 0.05634 0.7593 0.2357 Wide&amp;Deep 0.7817 0.05595 0.7594 0.2355 DeepFM 0.7819 0.05611 0.7592 0.2358 DeepCP 0.7844 0.05546 0.7610 0.2354 DeepMP 0.7917 0.05526 0.7663 0.2345 DeepMCP 0.7927 0.05518 0.7674 0.2341</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Test AUC and Logloss on two large-scale datasets. DNN =</cell></row><row><cell>Pred, DeepCP = Pred+Corr, DeepMP = Pred+Match, DeepMCP =</cell></row><row><cell>Pred+Match+Corr.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>lists the AUC and Logloss values of different meth-</cell></row><row><cell>ods. It is observed that FM performs much better than LR, be-</cell></row><row><cell>cause FM models second-order feature interactions while LR</cell></row><row><cell>models linear feature importance. DNN further outperforms</cell></row><row><cell>FM, because it can learn high-order nonlinear feature interac-</cell></row><row><cell>tions [He and Chua, 2017]. PNN outperforms DNN because</cell></row><row><cell>it further introduces a production layer. Wide&amp;Deep further</cell></row><row><cell>outperforms PNN, because it combines LR and DNN, which</cell></row><row><cell>improves both the memorization and generalization abilities</cell></row><row><cell>of the model. DeepFM combines FM and DNN. It performs</cell></row><row><cell>slightly better than Wide&amp;Deep on the Avito dataset, but</cell></row><row><cell>slightly worse on the Company dataset.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.kaggle.com/c/avito-context-ad-clicks/data</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References [chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011-07" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
		</imprint>
	</monogr>
	<note>In IJCAI. Gong et al., 2019] Yu Gong</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Xiangnan He and Tat-Seng Chua. Neural factorization machines for sparse predictive analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep cascade multi-task learning for slot filling in online shopping assistant</title>
		<editor>Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich</editor>
		<meeting><address><addrLine>Stuart Bowers</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>SIGIR. He et al., 2014] Xinran He. et al. Practical lessons from predicting clicks on ads at facebook. In ADKDD</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<editor>Jizhou Huang, Wei Zhang, Yaming Sun, Haifeng Wang, and Ting Liu</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="4107" to="4114" />
		</imprint>
	</monogr>
	<note>CIKM</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
	</analytic>
	<monogr>
		<title level="m">Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature</title>
		<editor>Koren, Robert Bell, and Chris Volinsky</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
		</imprint>
	</monogr>
	<note>RecSys</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Field-weighted factorization machines for click-through rate prediction in display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to rank for information retrieval. Foundations and Trends R in Information Retrieval</title>
		<imprint>
			<publisher>Nair and Hinton</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1349" to="1357" />
		</imprint>
	</monogr>
	<note>WWW. International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Product-based neural networks for user response prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1149" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Steffen Rendle. Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting clicks: estimating the click-through rate for new ads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Rendle ; Steffen Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<editor>Matthew Richardson, Ewa Dominowska, and Robert Ragno</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
	<note>Factorization machines with libfm</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep crossing: Webscale modeling without manually crafted combinatorial features</title>
	</analytic>
	<monogr>
		<title level="m">ADKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
	<note>KDD</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
