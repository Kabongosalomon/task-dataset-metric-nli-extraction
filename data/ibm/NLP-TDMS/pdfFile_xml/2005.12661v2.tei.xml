<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Monti</surname></persName>
							<email>alessio.monti@unimore.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AImageLab</orgName>
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
								<address>
									<settlement>Modena</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessia</forename><surname>Bertugli</surname></persName>
							<email>alessia.bertugli@unimore.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AImageLab</orgName>
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
								<address>
									<settlement>Modena</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
							<email>simone.calderara@unimore.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AImageLab</orgName>
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
								<address>
									<settlement>Modena</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
							<email>rita.cucchiara@unimore.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AImageLab</orgName>
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
								<address>
									<settlement>Modena</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address the aforementioned aspects by proposing a new recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and to integrate it with data about agents' possible future objectives. Our proposal is general enough to be applied to different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Trajectory prediction has become an essential component for several applications: self-driving cars and social robots may leverage useful insights about human motion to preventively forecast pedestrian actions and avoid collisions <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, while surveillance systems can benefit from knowing how crowds will move to better monitor huddled environments <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. There is also a high interest outside the smart cities context, like for example in team sports, where such predictions could give important insights for tactical analysis. However, designing a model to help to predict agents' trajectories is as desirable as difficult: a series of demanding challenges has to be faced.</p><p>First of all, the task results particularly tough because human motion is inherently multi-modal: when moving, people may follow several plausible trajectories, as there is a rich distribution of potential human behaviours. This means that, given a particular set of past observations, there is no unique correct future since several behaviours could be equally appropriate <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. In an urban environment, a pedestrian could choose to reach his/her destination following several plausible paths: cut straight to the objective crossing the road, or maybe take a longer walk following physical clues like sidewalks and pedestrian crossings. Similarly, in a basketball match, when an attacking player is running towards an opponent, several modes of behaviour develop: the player may choose to avoid the defender by passing the ball to a teammate, or perhaps *Equal contribution. achieve the same result by dribbling the adversary directly. Furthermore, each of these possible modes may exhibit a high variance in return: agents can modify step after step some of their features, like speed. Another challenge, especially when agents move in crowded scenarios, is represented by social interactions. Interactions heavily impact on future trajectories <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>: since people plan their paths reading each other future possible behaviours, each person's motion is influenced by the subjects around them. Interactions do not have to be necessarily as explicit as walking in groups or talking to each other: a moving pedestrian can interact with other people just by implicitly considering the positions of the surrounding agents to avoid future collisions. In team sports, interactions take on an even more important role: an attacker could put in place a certain set of movements just because the rest of his team has a specific disposition, as well as the whole defending team could in turn react and put in place a predefined tactic only because some opponents are arranged in a certain way. The varied nature of interactions and their heavy impact on agents' behaviour leads to develop sophisticated methods to accurately interpret and integrate such information into the prediction method.</p><p>Even the future knowledge about interacting agents positions can be a relevant feature that affects the development of each path. Taking into account this aspect during the prediction can improve the accuracy of the model.</p><p>To address these challenges we propose DAG-Net, a double attentive graph neural network for trajectory forecasting. The network backbone is a recurrent version of the Variational Autoencoder (VAE) <ref type="bibr" target="#b9">[10]</ref>: time-step after time-step, the autoencoder is used to generate the future position in terms of the displacement from the current location. The modules of our recurrent autoencoder are conditioned on subjects' objectives so that the model can accordingly produce likely future positions. The backbone is integrated with a double Graph Neural Network (GNN)-based mechanism: the first GNN defines the future objectives of each agent in a structured way, distilling each goal with proximity knowledge; the second GNN models agents' interactions, filtering the hidden states of the recurrent network through neighbourhood information. Both the GNNs use a self-attention mechanism to assign different weights to each edge of the graph. The entire model is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Literature extensively demonstrated that trajectory prediction cannot leave aside modelling the interactions between different agents. Early works in trajectory prediction took advantage of hand-crafted features and energy potential parameters <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. In particular, Helbing and Molnar <ref type="bibr" target="#b10">[11]</ref> modelled an interaction between two agents as a social force: the idea is to use attractive forces to guide agents toward their destination while employing repulsive forces to encourage collision avoidance. Other noteworthy examples are the Discrete Choice framework by Antonini et al. <ref type="bibr" target="#b11">[12]</ref>, Continuum Dynamics by Treuille et al. <ref type="bibr" target="#b12">[13]</ref>, and Gaussian Processes by Wang et al. <ref type="bibr" target="#b13">[14]</ref>. Although these methods exhibit robust performance, they share a common weakness: the drawback is represented by the very same hand-crafted features, which fail to generalise properly and struggle in complex scenarios, limiting the results in terms of prediction accuracy. More modern approaches <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> rely on recurrent neural networks (RNNs) or one of their more efficient variants (LSTMs or GRUs) to learn these features directly from data: such architectures are specifically designed to exploit the temporal dependencies that characterise time-series; hence they are particularly suited for predicting trajectories, where every position is strictly correlated with the previous. To model all the agents inside a particular scene, the naïve use of such models foresees the employing of a single RNN with shared parameters. However, without further solutions, such a network would predict single agents trajectories independently. To share information across different subjects, several mechanisms have been proposed.</p><p>In this sense, one of the most important contributions is Social Pooling by Alahi et al. <ref type="bibr" target="#b8">[9]</ref>: the mechanism merges agents' recurrent hidden states inside a social tensor. To build the social tensor the model employs a grid-based pooling: given an agent i and its neighbourhood N i , all the hidden states of agent i's neighbours are pooled together inside the social tensor, which is then fed to the recurrent cell as one of the inputs. The main limitation of this approach is the neighbourhood itself: such a local solution fails to capture the global context, as it does not allow the model to consider the interactions between all the possible agents inside the scene in a computationally efficient manner.</p><p>Social GAN by Gupta et al. <ref type="bibr" target="#b6">[7]</ref> introduces, inside the Generator, a new Pooling Module that instead combines information coming from all the possible agents present in the scene. For every agent i, all the hidden states h j t of the other agents are processed by an MLP and max-pooled together element-wise into the tensor P i . Social Ways by Amirian et al. <ref type="bibr" target="#b16">[17]</ref> takes up and customise this solution: instead of a simple max-pooling, the influence of the other agents on the generic agent i is evaluated by applying an attention weighting procedure. The model builds a so-called interaction feature vector: the i th vector is created by combining new social features that come from predefined geometric properties. Given two agents i and j, the vector is built by stacking information like the Euclidean distance between the agents or the bearing angle from agent j to agent i. A simple scalar product between the i th interaction vector and the hidden h j brings to the attention coefficient a ij .</p><p>Another relevant work is STGAT by Huang et al. <ref type="bibr" target="#b17">[18]</ref>. Instead of employing custom pooling modules, the authors exploited the recent progress in Graph Neural Networks (GNNs). To share information across different pedestrians, STGAT treats each agent as a node of a graph. In this manner, the model can employ a GNN <ref type="bibr" target="#b18">[19]</ref> as its sharing mechanism, allowing to aggregate information from neighbours by performing self-attention <ref type="bibr" target="#b19">[20]</ref> on graph nodes.</p><p>A different approach comes instead from Zhan et al. <ref type="bibr" target="#b20">[21]</ref>: rather than sharing the hidden states, the authors tried to induce coordination by working on agents' future intentions. The method is described as weakly supervised, since it requires a preliminary extraction from ground-truth trajectories of some low-dimensional features (called macro-intents) that will hopefully provide a tractable way to capture the coordination between agents. By conditioning the generation of the future intent on the hidden state of a recurrent cell that watches the whole set of agents, the produced macro-intents will somehow be also influenced by what other agents are willing to do. This allows the model to keep in consideration the coordination between the different subjects.</p><p>DAG-Net is inspired by <ref type="bibr" target="#b20">[21]</ref> since it employs a similar idea of intents. However, our objectives are generated and used in a different way. We treat goals as structured components by exploiting them as graph nodes and we accordingly condition the generation process on the resulting interrelated future objectives. Furthermore, we use an attentive module to associate importance weights to different interactive nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD A. Problem definition</head><p>The position of a generic agent i at time t is represented by</p><formula xml:id="formula_0">x i t = (x i , y i ) t ,</formula><p>where (x i , y i ) t are the coordinates that localise the agent in the scene at the given time-step; agent i's trajectory can be therefore defined as a series X i = {x i 1 , ..., x i T } of consecutive positions. Every trajectory X i is split into past and future: given a certain number T obs of past positions, the goal is to predict in most accurate way the next T pred future positions. All the coordinates are taken with respect to a realworld reference system, thus are expressed in meters or feet and do not refer to single pixels.</p><p>Before feeding the data to the model, every trajectory is transformed into a series of relative positions: every absolute position x i t is transformed in a couple of displacements (∆x i t , ∆y i t ) that express the movement along the two axes with respect to the previous absolute position x i t−1 . Coming back to the original coordinates is always possible: given the initial absolute position x i 1 , the other absolute coordinates can be obtained with a cumulative sum along the remaining timesteps. To simplify the notation, we use x t instead of ∆x t to denote a displacement at time-step t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Recurrent VAE</head><p>The network backbone is realised with a recurrent version of the Variational Autoencoder <ref type="bibr" target="#b21">[22]</ref>. For non-sequential data, Variational Autoencoders have already been shown to be effective in recovering and modelling complex multi-modal distributions over the data space: for this purpose, a VAE introduces a set of latent random variables z within the latent space Z, specifically designed to capture the characterising variations in the observed input variables x.</p><p>In order to model and generate sequences that can be both highly variable and highly structured (e.g. trajectories), Chung et al. <ref type="bibr" target="#b22">[23]</ref> extended this approach by proposing a recurrent version of the VAE, namely the Variational Recurrent Neural Network (VRNN). The network retains the necessary flexibility to model highly non-linear dynamics, but also explicitly models the dependencies between latent variables across subsequent time-steps. The VRNN can be described as a combination of the VAE and the RNN architectures: more specifically, the VRNN contains a variational autoencoder for every time-step of the input sequence, whose prior distribution over z is conditioned on the hidden state h t of a common RNN. The combination with a recurrent cell helps the variational autoencoder architecture to keep into consideration the temporal structure of the input data. Generally, the model is employed in a completely generative setting: after training, the model is used to generate brand new sequences that however resemble the original dataset examples. Since we want to accurately reproduce agents' future path, we instead employ the model in a predictive setting: after a burn-in period of T obs observation time-steps, the model is used to generate T pred new future positions that have to be as close as possible to the ground-truth positions we want to forecast.</p><p>The model can be decomposed in four sub-components: the encoder, the decoder, the prior, and the RNN, that are implemented like neural networks as ϕ enc , ϕ dec , ϕ prior and ϕ rnn .</p><p>The first stage is represented by the encoder: the encoder receives raw data (in our case, single x t displacements at a given time-step), embeds them in a fixed-length feature vector, incorporates the vector with the last hidden state h t−1 of the recurrent cell, and obtains the representation of their combination in the latent space Z. Thus, the approximate posterior will not only be a function of x t as in the VAE but also a function of h t−1 :</p><formula xml:id="formula_1">µ µ µ z,t , σ σ σ z,t = ϕ enc (ϕ x (x t ), h t−1 ) ,<label>(1)</label></formula><formula xml:id="formula_2">q φ (z t |x ≤t , z &lt;t ) = N z t |µ z,t , (σ z,t ) 2 ,<label>(2)</label></formula><p>where ϕ x is a neural network that extracts the features from the input and φ are the approximation function parameters. The decoder network takes the latent variable z t , embeds it in a fixed-size vector, incorporates the embedding with the last hidden state h t−1 of the recurrent cell, and provides a reconstructionx t of x t :</p><formula xml:id="formula_3">µ µ µx ,t σ σ σx ,t = ϕ dec (ϕ z (z t ), h t−1 ) ,<label>(3)</label></formula><formula xml:id="formula_4">p θ (x t |x &lt;t , z ≤t ) = N x t |µx ,t , (σx ,t ) 2 ,<label>(4)</label></formula><p>where ϕ z is a feature extracting neural network and θ are the approximation function parameters. In the end, the objective of the decoder is to output a samplex t that resembles as much as possible the original input x t . The prior network is instead able to reach the latent space Z starting only from the last hidden state h t−1 of the recurrent cell. It is computed as follow:</p><formula xml:id="formula_5">µ µ µ 0,t , σ σ σ 0,t = ϕ prior (h t−1 ) ,<label>(5)</label></formula><formula xml:id="formula_6">p θ (z t |x &lt;t , z &lt;t ) = N z t |µ 0,t , (σ 0,t ) 2 .<label>(6)</label></formula><p>The prior is essential to generate new data: to take a step forward and not just reconstruct the input (e.g. when we want to predict the next future displacement), the encoding network is detached. Here the prior network ensures that we are still able to reach the latent space Z, even without the encoder: furthermore, since the produced latent variables resemble the ones returned by the encoder, passing them to the decoder allows to obtain likely (yet new) examples, as if they were actual inputs coming from the dataset.</p><p>Finally, the RNN updates its hidden state by taking into account both the input x t and the latent variable z t : this encourages the explicit modelling of the temporal dependencies across subsequent time-steps.</p><formula xml:id="formula_7">h t = ϕ rnn (x t , z t , h t−1 )<label>(7)</label></formula><p>The entire model is trained by maximising the sequential evidence lower-bound (ELBO):</p><formula xml:id="formula_8">E q φ (z ≤T |x ≤T ) T log p θ (x t | z ≤t , x &lt;t ) −D KL (q φ (z t | x ≤t , z &lt;t ) || p θ (z t | x &lt;t , z &lt;t )) .<label>(8)</label></formula><p>The loss can be interpreted as the variational autoencoder ELBO summed over each time-step t of the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Conditioning VAE to agents' goals</head><p>Inspired by <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, we provide an additional input to our backbone in order to condition the displacements generation process to agents' future objectives. We choose to describe agents' future goals in terms of spatial information <ref type="figure">(Fig. 2a)</ref>. To make our model as invariant as possible with respect to the different characteristics of the environment, we divided the top-down view of the scene in a grid of macro-areas: each cell can potentially represent the future objective of a single agent. Agent i's goal at time t, g i t , is then represented by a one-hot encoding of the grid, where the cell in which the agent will land in the future is filled with a 1.</p><p>To obtain ground-truth objectives, a sliding window approach has been used: a window of size w slides through the original absolute trajectory and captures a goal every w time-steps. This information is used to condition the prior, the encoder and the decoder networks, as shown in Eq. (9), <ref type="bibr" target="#b9">(10)</ref> and <ref type="formula" target="#formula_1">(11)</ref> where we drop the superscripts to refer to the behaviour of a general agent.</p><formula xml:id="formula_9">µ µ µ 0,t , σ σ σ 0,t = ϕ prior (h t−1 , g t ) , (9) µ µ µ z,t , σ σ σ z,t = ϕ enc (ϕ x (x t ), h t−1 , g t ) ,<label>(10)</label></formula><formula xml:id="formula_10">µ µ µx ,t σ σ σx ,t = ϕ dec (ϕ z (z t ), h t−1 , g t ) .<label>(11)</label></formula><p>To produce likely goals during the inference phase, we employ a further network. This network is again conditioned on the hidden state h t−1 of the recurrent cell, and takes as additional inputs the last predicted objective for the agent and the concatenation d t−1 of the absolute positions of the other agents in the scene (i.e. their disposition).</p><formula xml:id="formula_11">g t = ϕ goal (g t−1 , d t−1 , h t−1 )<label>(12)</label></formula><formula xml:id="formula_12">E q φ (z ≤T |x ≤T ) T t=1 K k=1 log p θ (x t | z ≤t , x &lt;t ) − g k t log(g k t ) −D KL (q φ (z t | x ≤t , z &lt;t ) || p θ (z t | x &lt;t , z &lt;t ))<label>(13)</label></formula><p>The additional loss term is computed as a Cross-Entropy between the ground-truth goal g t and the predicted one g t , where K is total the number of cells inside their one-hot encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Double Attentive Graph Neural Networks</head><p>DAG-Net leverages two graph attentive networks to model two different kinds of interactions: the interactions between agents and the relationships between future goals. In structured motion environments, where agents' behaviours are moved not only by single intentions but also by social rules and/or (a) (b) <ref type="figure">Fig. 2</ref>. In (a) we can observe how goals deeply influence past and future trajectories, guiding agents to specific portions of the court. In (b) we can observe the similarities between the green player and his teammates: these values will directly influence the recombination of both goals and hidden states at the green node.</p><p>common goals, it is important to condition the prediction to both mutual interactions and neighbours objectives. DAG-Net jointly employs past data and future intentions to improve forecasting in such contexts. 1) Goals relationships: As seen in Eq. <ref type="formula">(9)</ref>, <ref type="formula" target="#formula_1">(10)</ref> and <ref type="formula" target="#formula_1">(11)</ref>, agents' future objectives are used to condition the backbone: nevertheless, without further solutions, a single predicted goal g t focuses only on the corresponding agent objective. To effectively capture the coordination between the different subjects in the scene, DAG-Net shares goals information among agents relying on group interactions. To model the structure of future interactions, an attentive GNN <ref type="bibr" target="#b18">[19]</ref> is employed. At every time-step, the network takes as an input node the one-hot encoding of each agent's predicted goal g t , and produces a new distilled goalg t built on proximity notions. After the concatenation of the distilled goal with the original one, the final refined goal is obtained through a linear projection:</p><formula xml:id="formula_13">g t = W (g t g t )<label>(14)</label></formula><p>where the parameter matrix W ∈ R dxd (with d the number of goals grid cells) is learnt in an end-to-end fashion during training. The new produced goalĝ t will then take the place of g t inside the ELBO loss presented in Eq. (13).</p><p>2) Agents' interactions: To model the interactions between the different agents in the scene, our model uses again a graphbased approach. Each agent is connected to the others as a node of a graph where edges weights are defined by a selfattention mechanism. A distance-based adjacency matrix is used in addition to the attentive module to consider proximity information <ref type="figure">(Fig. 2b)</ref>. At every time-step, the hidden state h t of each agent is fed to the GNN: the network outputs a new distilled hidden stateh t that takes into account the history of the neighbours. After the concatenation of the distilled hidden state with the original one, a linear layer is used to achieve the final output:ĥ where the parameter matrix H is learnt during the training phase. The new refined hidden stateĥ t will then be used in the next time-step as the current hidden state of the agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Training protocol</head><p>During training, we let the network see the entire T = T obs + T pred time-steps from ground-truth sequences. The solution gives the model the opportunity to collect important features also from the latest time-steps of the sequence: this is useful in urban contexts and results particularly effective in sports, where we have long trajectories that usually start as linear but seldom continue in the same way, often bending and turning back upon themselves.</p><p>During validation and testing, we instead divide the trajectories into an observation and a prediction split: specifically, the network burns in for T obs time-steps observing the first portion of the ground-truth trajectory, then it's let predicting the remaining T pred time-steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>The VRNN recurrent cell is a GRU with 1 recurrent layer and a hidden state dimension of 64; the dimension of the latent variable is set to 32. For each graph, we then employ two attentive GNN layers: the first layer reduces the input to lowerdimensional hidden space, the second layer returns instead to the original input space. Each GNN layer uses 4 attention heads. The entire model has been optimised with Adam optimiser. To cope with the differences between urban and sport settings, we employ different sets of hyper-parameters.</p><p>For the urban setting, we use a learning rate of 10 −4 and a batch size of 16; the Cross-Entropy contribution is weighted with a factor of 10 −2 . The hidden state dimension between the two graph layers is set to 4. The model has been trained for 500 epochs.</p><p>For the sports setting, we use a learning rate of 10 −3 and a batch-size of 64; the Cross-Entropy contribution is weighted with a factor of 10 −2 . The hidden state dimension between the two graph layers is set to 8. The model has been trained for 300 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Metrics</head><p>Similar to prior works in literature, the model is evaluated with respect to two error metrics on prediction results, Average Displacement Error (ADE) and Final Displacement Error (FDE).</p><p>The Average Displacement Error represents the average Euclidean distance, over the entire predicted sequence, between the ground-truth positions and the predicted ones:</p><formula xml:id="formula_14">ADE = i∈P T pred t=0 ((x i t ,ŷ i t ) − (x i t , y i t )) 2 | P | · T pred<label>(16)</label></formula><p>where P is the set of pedestrians considered, | P | is its cardinality, (x i t ,ŷ i t ) are the predicted absolute coordinates at time t, and (x i t , y i t ) are the ground-truth absolute coordinates at time t.</p><p>The Final Displacement Error follows the same logic but focuses only on the last time-step, evaluating the Euclidean distance between the final ground-truth position and the predicted one.</p><formula xml:id="formula_15">F DE = i∈P ((x i T pred ,ŷ i T pred ) − (x i T pred , y i T pred )) 2 | P |<label>(17)</label></formula><p>C. Datasets 1) Stanford Drone Dataset: The Stanford Drone Dataset <ref type="bibr" target="#b29">[29]</ref> is composed of a series of top-down videos recorded by a hovering drone in 8 different college campus scenes. This large scale dataset collects complex and crowded scenarios with various types of interacting targets: apart from classic pedestrians, we can also find bikes, skateboarders, cars, buses, and other vehicles, therefore the navigation inside such environments results particularly tough. We use the TrajNet benchmark version of the dataset <ref type="bibr" target="#b30">[30]</ref>: trajectories are composed of a series of consecutive positions expressed as (x, y) world coordinates and recorded at 2.5FPS. Due to the lack of annotation in the test set, we split the training set into three sub-sets for the training, test and validation phases.</p><p>2) STATS SportVU NBA Dataset: The dataset comes from the player tracking data provided by STATS SportVU <ref type="bibr" target="#b31">[31]</ref>. The dataset contains tracking positions from the 2016 NBA regular season on a span of over 1200 different games: the data are recorded with a series of cameras that surround the court and give back a bird-eye view of players' positions. The games are split into offensive plays, hence every sequence starts when the ball crosses the middle of the court. A play ends when one the following conditions is met: a shot is made (missed or scored), the ball exceeds the court bounds, the ball is intercepted by the defending team, or the shot clock runs out. Each play composes of 50 time-steps sampled at 5Hz, where each time-step contains the positions (expressed as (x, y, z) world coordinates) for all the 10 players on the court (5 attackers, 5 defenders) plus the ball. All the data have been subsequently normalised and shifted to have zerocentred sequences to the middle of court and plays that always develop towards the right basket.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Quantitative Results</head><p>For the basketball setting, we evaluated separately offence and defence: since agents are placed in an explicit competitive setting, their nature is intrinsically different, both from the goals and the trajectories points of view. The attackers call the shots trying to score, while the defenders usually react to their moves: training the network simultaneously on both teams would distract the final results.</p><p>The values reported in <ref type="table" target="#tab_0">Table I</ref> support our choice, with defence trajectories being clearly easier than attacking ones. All the metrics are expressed in feet and refer to a prediction horizon of 40 time-steps, with 10 initial steps of observation. The model achieves impressive results compared to stateof-the-art methods. STGAT shows quite strong performances, being able to weight all the possible contributions from the agents in the scene, but lacks additional information about what could happen in the long-term. With its attention-based pooling, Social-Ways returns very similar results: the combination of its geometric social features gives useful clues on such complicated trajectories and interactions as the basketball ones. Even though its different approach, Weak Supervision gain promising results too: the exploitation of future intentions allow agents to plan correctly their future path. However, by jointly considering agents' interactions and future goals, our model is more able to capture the nature of real paths and to reach smaller errors with respect to all these competitive methods.</p><p>To evaluate whether our model could show appreciable performance on different prediction horizons, we produced some long-term evaluations: since basketball trajectories offered a high number of time-steps with which we could produce various splits, we focused again on sports. For producing such evaluations, we concentrated on different observationprediction sequences: given 10 time-steps of observation, we evaluated all the methods on increasing prediction splits, from 10 time-steps to 40 time-steps, with steps of 10. As <ref type="figure" target="#fig_1">Fig. 3</ref> shows, our method globally outperforms the competitors in all the different evaluations and in both the metrics. As for the numbers in <ref type="table" target="#tab_0">Table I</ref>, the difference is more pronounced for the attack than for the defence.</p><p>We have also run some long-term evaluations considering a longer observation period <ref type="table" target="#tab_0">(Table II)</ref>, mainly to observe how the prediction accuracy changes when the model is allowed to adjust to a greater initial period: we let the model burnin for 20 initial time-steps and then predict the remaining ones, again with increasing steps of 10. In this setting we are able to compare our model to a further autoencoder architecture, by Felsen et al. <ref type="bibr" target="#b26">[27]</ref>, that briefly employs a C-VAE <ref type="bibr" target="#b24">[25]</ref> conditioned on players' role. Our model, even without additional information about players' identities, shows better metrics in terms of the average distance from groundtruth positions.</p><p>In urban settings such as the ones in SDD <ref type="table" target="#tab_0">(Table III)</ref>, the aforementioned distinction between different categories of agents is no longer necessary, because all the pedestrians share the same nature and actively cooperate to not interfere with each other. The trajectories are split into segments of 8s: we observe 3.2s of history and predict over a 4.8s future horizon. Operating at 0.4s per time-step results in 8 time-steps of observation and a future prediction span of 12 time-steps; all the metrics are in meters. We can not report results of Weak-Supervision for SDD: since the model adopts a separate VRNN for each agent, its architecture is not suitable for less constrained scenes that exhibit a variable number of agents. For this reason, Weak-Supervision could not be tested outside the basket environment.</p><p>The results are in line with the ones presented for basketball. The attentive sharing of agents' goals and the graph distillation step for the hidden states outperforms the competitors in both ADE and FDE: individual goals force agents to pass  through specific future areas coherent to their past trajectory, while the two attentive mechanisms help them to keep in consideration both other agents' immediate will and their longterm objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Results</head><p>In competitive settings such as sports, the opposing teams are intrinsically different. The attacking team drives the game trying to score, while the defenders often limit to counter react to its moves. These behaviours deeply affect the resulting trajectories, as can be clearly seen in the roll-outs presented in <ref type="figure" target="#fig_2">Fig. 4</ref>. Attackers trajectories tend to be particularly varied and intricate, often bending and intersecting; on the contrary, defenders tend to move linearly and occasionally deflect to follow an opponent or to close a gap. Despite some sudden changes of direction in the real trajectories, especially for the attackers, our model is able to correctly predict the overall future movement of the players and rather trace the groundtruth. Because of the complexity of such trajectories, the predictions do not always precisely resemble the expected output: nevertheless, even when the predictions fail to follow the real ground-truth trajectories, the model still predicts a likely behaviour coherent with the play development, proving its strength in capturing the multi-modal nature of players' movements.</p><p>On the other hand, urban trajectories are more straightforward, because pedestrian obviously tend to move linearly, doing only some occasional deviations to avoid collisions or to turn. Nevertheless, the adoption of agents' goals still gives the model the possibility to produce more likely trajectories. Since agents are constrained to pass through specific portions of the scene coherent with their motion behaviour, predictions can closely resemble real future movements: in both the plot reported in <ref type="figure" target="#fig_3">Fig. 5</ref>, DAG-Net is able to keep closer to the ground-truth, while both the competitors tend to predict more linear trajectories and consequently deviate from the expected output. For the very same reasons, final predictions can also be more precise: having important insights about the regions the agent will occupy in the future can help the model to appropriately predict the overall portion of the scene where the agent will land at the end of his trajectory. DAG-Net predicted final location resembles the agent's real destination, while both the competitors fail to approximately forecast such information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ABLATION EXPERIMENTS</head><p>In this section we present ablation experiments to show the improvements introduced by each component of our model <ref type="table" target="#tab_0">(Table IV and Table V)</ref>. Results present two baselines: the Vanilla VRNN and the Attentive-VRNN (A-VRNN), i.e. a version of our network that presents only the attentive graph for the hidden states refinement. DAG-Net outperforms both baselines on STATS SportVU and SDD. The Vanilla VRNN experiments show that using a stand-alone network without considering interactions between agents does not allow the model to capture the nature of real paths. For this reason, A-VRNN achieves better performance than Vanilla VRNN; still, this version is not able to capture future structured dependencies between agents. The results obtained with DAG-Net highlight the importance of inserting future information into the prediction and combining humans objectives in a structured way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We propose a novel architecture called DAG-Net, a double graph-based network that deals with both past interactions and future goals through attentive mechanisms. By facing trajectory prediction as a structured problem, our model overcomes state-of-the-art performances on both STATS SportVU NBA Dataset and Stanford Drone Dataset, proving its strength on team sports and urban contexts. It shows impressive results also on long-term predictions. Our future work will focus on the application of our model to more general settings, involving time-series forecasting such as finance and health care.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Scheme of DAG-Net architecture. It is composed of a Goal-Net that learns to predict agents' future goals; a VAE to generate displacements at every time-step; a RNN to consider the temporal nature of the sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Long-term evaluations: the method is evaluated both in ADE and FDE for increasing prediction lengths, from 10 to 40 time-steps. Attack on the top, defence on the bottom. All the metrics are in feet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Basketball roll-outs. After an initial observation stage (black), model predictions (red) are evaluated against the ground-truth (blue), The top rollouts refer to three different attack plays, while the bottom one represent three different defensive actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative samples that compare DAG-Net and state-of-the-art methods on Stanford Drone Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">BASKETBALL SPORTVU RESULTS</cell></row><row><cell>Team</cell><cell></cell><cell>Model</cell><cell></cell><cell>ADE</cell><cell>FDE</cell></row><row><cell></cell><cell cols="2">STGAT [18]</cell><cell></cell><cell>9.94</cell><cell>15.80</cell></row><row><cell>ATK</cell><cell cols="3">Social-Ways [17] Weak-Supervision [21]</cell><cell>9.91 9.47</cell><cell>15.19 16.98</cell></row><row><cell></cell><cell cols="2">DAG-Net (Our)</cell><cell></cell><cell>8.98</cell><cell>14.08</cell></row><row><cell></cell><cell cols="2">STGAT [18]</cell><cell></cell><cell>7.26</cell><cell>11.28</cell></row><row><cell>DEF</cell><cell cols="3">Social-Ways [17] Weak-Supervision [21]</cell><cell>7.31 7.05</cell><cell>10.21 10.56</cell></row><row><cell></cell><cell cols="2">DAG-Net (Our)</cell><cell></cell><cell>6.87</cell><cell>9.76</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE II</cell><cell></cell></row><row><cell></cell><cell cols="4">LONG-TERM EVALUATIONS</cell></row><row><cell>Model</cell><cell>Team</cell><cell cols="4">20-10 Split 20-20 Split 20-30 Split ADE ADE ADE</cell></row><row><cell>C-VAE [27]</cell><cell>ATK</cell><cell>3.95</cell><cell></cell><cell>5.80</cell><cell>7.08</cell></row><row><cell>DAG-Net (Our)</cell><cell>ATK</cell><cell>2.09</cell><cell></cell><cell>4.58</cell><cell>6.66</cell></row><row><cell>C-VAE [27]</cell><cell>DEF</cell><cell>3.01</cell><cell></cell><cell>4.10</cell><cell>4.98</cell></row><row><cell>DAG-Net (Our)</cell><cell>DEF</cell><cell>2.05</cell><cell></cell><cell>4.07</cell><cell>5.01</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE III</cell><cell></cell></row><row><cell cols="6">STANFORD DRONE DATASET RESULTS</cell></row><row><cell></cell><cell cols="2">Model</cell><cell cols="2">ADE FDE</cell></row><row><cell></cell><cell cols="2">STGAT [18]</cell><cell>0.58</cell><cell>1.11</cell></row><row><cell></cell><cell cols="2">Social-Ways [17]</cell><cell>0.62</cell><cell>1.16</cell></row><row><cell></cell><cell cols="2">DAG-Net (Our)</cell><cell>0.53</cell><cell>1.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">STATS SPORTVU</cell><cell></cell><cell></cell></row><row><cell>Team</cell><cell>Model</cell><cell cols="2">Agents' interact.</cell><cell cols="2">Future object.</cell><cell>ADE</cell><cell>FDE</cell></row><row><cell></cell><cell cols="2">Vanilla VRNN [23]</cell><cell></cell><cell></cell><cell></cell><cell>9.41</cell><cell>15.56</cell></row><row><cell>ATK</cell><cell>A-VRNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9.48</cell><cell>15.52</cell></row><row><cell></cell><cell>DAG-Net (Our)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8.98</cell><cell>14.08</cell></row><row><cell></cell><cell cols="2">Vanilla VRNN [23]</cell><cell></cell><cell></cell><cell></cell><cell>7.16</cell><cell>10.50</cell></row><row><cell>DEF</cell><cell>A-VRNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7.05</cell><cell>10.34</cell></row><row><cell></cell><cell>DAG-Net (Our)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.87</cell><cell>9.76</cell></row><row><cell></cell><cell></cell><cell>TABLE V</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">STANFORD DRONE DATASET</cell><cell></cell></row><row><cell></cell><cell>Model</cell><cell>Agents' interact.</cell><cell cols="2">Future object.</cell><cell cols="2">ADE FDE</cell></row><row><cell cols="2">Vanilla VRNN [23]</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.58</cell><cell>1.17</cell></row><row><cell></cell><cell>A-VRNN</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.56</cell><cell>1.14</cell></row><row><cell></cell><cell>DAG-Net (Our)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.53</cell><cell>1.04</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = H h t h t(15)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Forecasting interactive dynamics of pedestrians with fictitious play</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="774" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-agent tensor fusion for contextual trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Glmprealtime pedestrian path prediction using global and local movement patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Randhavane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pratapa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5528" to="5535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Goal set inverse optimal control and iterative replanning for predicting human reaching motions in shared workspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mainprice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="897" to="908" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A large-scale benchmark dataset for event recognition in surveillance video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoogs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cuntoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3153" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Social-bigat: Multimodal trajectory forecasting using bicycle-gan and graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Social LSTM: Human Trajectory Prediction in Crowded Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1312.6114</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4282</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discrete Choice Models for Pedestrian Walking Behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Antonini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bierlaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="667" to="687" />
			<date type="published" when="2006-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Continuum crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Treuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Popovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1160" to="1168" />
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gaussian Process Dynamical Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. C. Platt</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1441" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Soft+ hardwired attention: An lstm framework for human trajectory prediction and abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="466" to="478" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Desire: Distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2165" to="2174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Social ways: Learning multimodal distributions of pedestrian trajectories with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Javad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jean-Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">STGAT: Modeling Spatial-Temporal Interactions for Human Trajectory Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yingfan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhaoxin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tianlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhaoqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generating multi-agent trajectories using programmatic weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Recurrent Latent Variable Model for Sequential Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>ser. NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semisupervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
	<note>ser. NIPS&apos;14</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
	<note>ser. NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Conditional flow variational autoencoders for structured sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanselmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><surname>Straehle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Where will they go? predicting fine-grained adversarial multi-agent motion using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Conditional generative neural system for probabilistic trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiachen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hengbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Masayoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Forecasting Social Navigation in Crowded Complex Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Anenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno>abs/1601.00998</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Trajnet: Towards a benchmark for human trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">SportVU -STATS Perform</title>
		<ptr target="https://www.statsperform.com/team-performance/basketball/optical-tracking/" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
