<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SentenceMIM: A Latent Variable Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Livne</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
						</author>
						<title level="a" type="main">SentenceMIM: A Latent Variable Language Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>SentenceMIM is a probabilistic auto-encoder for language data, trained with Mutual Information Machine (MIM) learning to provide a fixed length representation of variable length language observations (i.e., similar to VAE). Previous attempts to learn VAEs for language data faced challenges due to posterior collapse. MIM learning encourages high mutual information between observations and latent variables, and is robust against posterior collapse. As such, it learns informative representations whose dimension can be an order of magnitude higher than existing language VAEs. Importantly, the SentenceMIM loss has no hyper-parameters, simplifying optimization. We compare sentenceMIM with VAE, and AE on multiple datasets. SentenceMIM yields excellent reconstruction, comparable to AEs, with a rich structured latent space, comparable to VAEs. The structured latent representation is demonstrated with interpolation between sentences of different lengths. We demonstrate the versatility of sentenceMIM by utilizing a trained model for question-answering and transfer learning, without fine-tuning, outperforming VAE and AE with similar architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative modelling of text has become one of the predominant approaches to natural language processing (NLP), particularly in the machine learning community. It is favoured because it supports probabilistic reasoning and it provides a principled framework for unsupervised learning in the form of maximum likelihood. Unlike computer vision, where various generative approaches have proliferated <ref type="bibr" target="#b11">(Dinh et al., 2017;</ref><ref type="bibr" target="#b15">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b22">Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b36">Oord et al., 2016;</ref><ref type="bibr" target="#b40">Rezende et al., 2014;</ref><ref type="bibr" target="#b48">Vahdat &amp; Kautz, 2020)</ref>, current methods for text mainly rely on autoregressive models (e.g., <ref type="bibr">Brown et al. (2020)</ref>).</p><p>Generative latent variable models (LVMs), such as the variational auto-encoder (VAE) <ref type="bibr" target="#b22">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b40">Rezende et al., 2014)</ref>, are versatile and have been successfully applied to myriad domains. Such models consist of an encoder, which maps observations to distributions over latent codes, and a decoder that maps latent codes to distributions over observations. LVMs are widely used and studied because they can learn a latent representation that carries many useful properties. Observations are encoded as fixedlength vectors that capture salient information, allowing for semantic comparison, interpolation, and search. They are often useful in support of downstream tasks, such as transfer or k-shot learning. They are also often interpretable, capturing distinct factors of variation in different latent dimensions. These properties have made LVMs especially compelling in the vision community.</p><p>Despite their desirable qualities, generative LVMs have not enjoyed the same level of success with text data. There have been recent proposals to adapt VAEs to text <ref type="bibr" target="#b7">(Bowman et al., 2015;</ref><ref type="bibr" target="#b16">Guu et al., 2017;</ref><ref type="bibr" target="#b26">Kruengkrai, 2019;</ref><ref type="bibr" target="#b29">Li et al., 2019b;</ref><ref type="bibr" target="#b51">Yang et al., 2017;</ref><ref type="bibr" target="#b6">Bosc &amp; Vincent, 2020)</ref>, but despite encouraging progress, they have not reached the same level of performance on natural language benchmarks as auto-regressive models (e.g., <ref type="bibr" target="#b33">(Merity et al., 2017;</ref><ref type="bibr" target="#b39">Rae et al., 2018;</ref><ref type="bibr" target="#b50">Wang et al., 2019)</ref>). This is often attributed to the phenomenon of posterior collapse <ref type="bibr" target="#b13">(Fang et al., 2019;</ref><ref type="bibr" target="#b28">Li et al., 2019a)</ref>, in which the decoder captures all of the modelling power and the encoder conveys little to no information. For text, where the decoder is naturally auto-regressive, this has proven challenging to mitigate. A notable exception by <ref type="bibr" target="#b27">Li et al. (2020)</ref> utilizes pre-trained BERT encoder <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> and GPT-2 decoder <ref type="bibr" target="#b38">(Radford et al., 2019)</ref> in order to train a powerful VAE model. While showing strong PPL results, the training requires carefully designed heuristics to reduce posterior collapse. This paper introduces sentenceMIM (sMIM), a new LVM for text. We use the Mutual Information Machine (MIM) framework by <ref type="bibr" target="#b30">Livne et al. (2019)</ref> for learning, and base our architecture on <ref type="bibr" target="#b7">Bowman et al. (2015)</ref>. MIM is a recently introduced LVM framework that shares the same underlying architecture as VAEs, but uses a different learning objective arXiv:2003.02645v5 [cs.CL] 21 Apr 2021 that is robust against posterior collapse. MIM learns a highly informative and compressed latent representation, and often strictly benefits from more powerful architectures.</p><p>We argue that an ideal LVM should be able to capture all aspects of variation of variable-size text observations within a fixed-size latent representation. As such, high-dimensional latent codes are required, which is challenging with VAEs. An ideal model should provide excellent reconstruction, with fixed-size codes for variable-length sentences, and be useful for various downstream tasks.</p><p>Auto-encoders (AEs) <ref type="bibr" target="#b18">(Hinton &amp; Zemel, 1994)</ref> provide excellent reconstruction, but lack useful semantic structure in the learned representations. AEs also allow one to learn high dimensional latent codes, only limited in practice by over-fitting. VAEs, on the other hand, encourage semantic representations by regularizing the distribution over latent codes to match a given prior. Such regularization, however, also contributes to posterior collapse, limiting the dimension of latent codes and reconstruction quality. Here we propose MIM learning to enable high dimensional representations, while encouraging low latent entropy (under certain conditions) to promote clustering of semantically similar observations. By encouraging the latent codes to match a known distribution we preserve the ability generate samples. The resulting model offers a learned representation with high mutual information (i.e., to capture aspects of variation in the data), with low marginal entropy (i.e., introducing semantic structure to the learned representation), while aligning the latent distribution with a known prior. sMIM also requires no hyper-parameter tuning for the loss, similar to AEs, which simplifies training. This paper explores and contrasts properties of VAE learning, MIM learning, and AE learning on four well-known text datasets, all with similar architectures. We show that sMIM provides better reconstruction than VAE models, matching the reconstruction accuracy of AEs, but with semantically meaningful representations, comparable to VAEs. We further demonstrate the quality of the sMIM representation by generating diverse samples around a given sentence and interpolating between sentences. Finally, we show the versatility of the learned representation by applying a pre-trained sMIM model to a question answering task with state-of-art performance as compared to single task, supervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Formulation</head><p>Let x ∈ X = {x i } X i=1 be a discrete variable representing a sequence of T tokens from a finite vocabulary V. A sequence might be a sentence or a paragraph, for example. The set X comprises all sequences we aim to model. The size of X , i.e., X, is typically unknown and large. Let P(x) be the unknown probability of sentence x ∈ X .</p><formula xml:id="formula_0">z h 1 d p seq θ (x 1 |h 1 d ) the h 2 d p seq θ (x 2 |h 2 d ) cat h 3 d p seq θ (x 3 |h 3 d ) is h 4 d p seq θ (x 4 |h 4 d ) sitting h 5 d p seq θ (x 5 |h 5 d )</formula><p>&lt;EOT&gt; &lt;BOT&gt; the cat is sitting GRU GRU GRU GRU <ref type="figure">Figure 1</ref>. The decoder, implemented with GRU, is auto-regressive and conditioned on latent code z. Words are represented by parametric embeddings. At each step the inputs are the latent code and previous output token. The GRU output provides a categorical distribution over tokens x k , from which the next token is sampled.</p><p>We model the distribution over a sequence of length T as an auto-regressive distribution over T + 1 tokens, where the additional end-of-text special token 1 , &lt;EOT&gt;, effectively captures the probability that the sequence length is T . More explicitly, we model the following distribution</p><formula xml:id="formula_1">P(x) = T k=0 p(x k |x &lt;k )<label>(1)</label></formula><p>where x &lt;k denotes the tokens preceding x k , p(x k |x &lt;k ) is a categorical distribution over V, and p(x T = &lt;EOT&gt;|x &lt;T ) is the probability that T is the sentence length.</p><p>We learn a latent variable model given N fair samples from P(x), where N X, with discrete observations x ∈ X , and a continuous latent space z ∈ R d . The encoder, q θ (z|x), maps sequences to a distribution over continuous latent codes, and a corresponding decoder, p θ (x|z), maps a latent code to a distribution over sequences. Let θ be the joint parameters of the encoder and decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Encoder-Decoder Specification</head><p>In what follows we adapt the architecture proposed by <ref type="bibr" target="#b7">Bowman et al. (2015)</ref>, the main difference being the use of GRUs <ref type="bibr" target="#b9">(Cho et al., 2014)</ref> instead of LSTMs <ref type="bibr" target="#b19">(Hochreiter &amp; Schmidhuber, 1997)</ref>. We opt for a simple architecture instead of more recent variants, such as Transformers <ref type="bibr" target="#b49">(Vaswani et al., 2017)</ref> or AWD-LSTMs <ref type="bibr" target="#b34">(Merity et al., 2018)</ref>, to focus on the effect of the learning framework on a given architecture (i.e., MIM, VAE, AE), rather than the architecture itself.</p><p>Beginning with the generative process, let p θ (x|z) be a conditional auto-regressive distribution over a sequence of T tokens, x = (x 0 , . . . , x T −1 , x T = &lt;EOT&gt;),  <ref type="figure">Figure 2</ref>. The encoder is implemented with GRU. Each word is represented by a parametric embedding. Given the input sequence, the encoder maps the last hidden state to the mean and variance of a Gaussian posterior over latent codes.</p><formula xml:id="formula_2">log p θ (x|z) = T k=0 log p θ (x k | x &lt;k , z)<label>(2)</label></formula><p>where p θ (x k |·) is a categorical distribution over |V| possible tokens for the k th element in x. According to the model <ref type="figure">(Fig. 1)</ref>, generating a sentence x with latent code z entails sampling each token from a distribution conditioned on the latent code and previously sampled tokens. Tokens are modelled with a parametric embedding. Conditioning the distribution over z entails concatenating z to the input embeddings per token <ref type="bibr" target="#b7">(Bowman et al., 2015)</ref>.</p><p>The encoder q θ (z|x) is the posterior distribution over the latent variable z, conditioned on a sequence x. We take this to be Gaussian whose mean and diagonal covariance are specified by mappings µ θ and σ θ :</p><formula xml:id="formula_3">q θ (z|x) = N (z; µ θ (x), σ θ (x))<label>(3)</label></formula><p>Linear mappings µ θ and σ θ are computed from the last hidden state of a GRU (see <ref type="figure">Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">MIM Learning Objective</head><p>The Mutual Information Machine (MIM) <ref type="bibr" target="#b30">(Livne et al., 2019)</ref> is a versatile generative LVM which can be used for representation learning, and sample generation. MIM learns a model with high mutual information between observations and latent codes, and is robust against posterior collapse.</p><p>The MIM framework begins with two anchor distributions, P(x) and P(z), for observations and the latent space, from which one can draw samples. They are fixed and not learned. MIM also has a parameterized encoder-decoder pair, q θ (z|x) and p θ (x|z), and parametric marginal distributions q θ (x) and p θ (z). These parametric elements define joint encoding and decoding model distributions:</p><formula xml:id="formula_4">q θ (x, z) = q θ (z|x) q θ (x) , (4) p θ (x, z) = p θ (x|z) p θ (z) .<label>(5)</label></formula><p>MIM learning entails the minimization of the cross-entropy between a sample distribution and the model encoding and decoding distributions <ref type="bibr" target="#b30">(Livne et al., 2019)</ref>. This simple loss constitutes a variational upper bound on a regularized Jensen-Shannon divergence, resembling VAE in which a model distribution matches samples from a sample distribution via KL divergence minimization <ref type="bibr" target="#b52">(Zhao et al., 2018)</ref>. Fundamentally, MIM learning differs from VAE learning, with the former being an upper bound on the joint cross-entropy, while the latter being an upper bound on the marginal cross-entropy of the observations. MIM requires sampling from the decoder during training, which can be slow for sequential computational models. For language modeling we therefore use A-MIM learning, a MIM variant that minimizes a loss defined on the encoding and decoding distributions, with samples drawn from an</p><formula xml:id="formula_5">encoding sample distribution, denoted M q S (x, z); i.e., M q S (x, z) = q θ (z|x) P(x) .<label>(6)</label></formula><p>The A-MIM loss is defined as follows,</p><formula xml:id="formula_6">L A-MIM (θ) = 1 2 CE ( M q S (x, z) , q θ (x, z) ) (7) + CE ( M q S (x, z) , p θ (x, z) ) ≥ H M q S (x) + H M q S (z) − I M q S (x; z) , where CE ( · , · ) is cross-entropy, H M q S (·)</formula><p>is information entropy over distribution M q S , and I(·; ·) is mutual information. Minimizing L A-MIM (θ) learns a model with a consistent encoder-decoder, high mutual information, and low marginal entropy <ref type="bibr" target="#b30">(Livne et al., 2019)</ref>. The A-MIM loss is in fact a variational upper bound on the joint entropy of the encoding sample distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Implicit and Explicit Model Marginals</head><p>To complete the model specification, we define the model marginals q θ (x) and p θ (z). We call the marginal explicit when we can evaluate the probability of a sample under the corresponding distribution, and implicit otherwise.</p><p>Examples of explicit marginals are a Gaussian for p θ (z), and an auto-regressive distribution for q θ (x). They enable evaluation of the probability of a sample straightforwardly. However, the inductive bias in such distributions, or the architecture, can lead to a challenging optimization problem.</p><p>An implicit marginal distribution can be defined via marginalization of a joint distribution. To help encourage consistency, and avoid introducing more model parameters, one can define model marginals in terms of the sample distributions, like M q S (x, z) above <ref type="bibr" target="#b5">(Bornschein et al., 2015;</ref><ref type="bibr" target="#b30">Livne et al., 2019;</ref><ref type="bibr" target="#b47">Tomczak &amp; Welling, 2017)</ref>. They allow one to share parameters between a marginal and the corresponding conditional distribution, and to reduce the inductive bias in the architecture. Unfortunately, evaluating the probability of a sample under an implicit distribution is intractable in general.</p><p>We define q θ (x) as a marginal over the decoder <ref type="bibr" target="#b5">(Bornschein et al., 2015)</ref>; i.e.,</p><formula xml:id="formula_7">q θ (x) = E P(z) [p θ (x|z)] ,<label>(8)</label></formula><p>where the latent anchor is defined to be a standard normal, P(z) = N (z; 0, 1). Similarly, the model density over latent codes is defined as</p><formula xml:id="formula_8">p θ (z) = E P(x) [q θ (z|x)] .<label>(9)</label></formula><p>i.e., the latent marginal is defined as the aggregated posterior, in the spirit of the VampPrior <ref type="bibr" target="#b47">(Tomczak &amp; Welling, 2017)</ref> and Exemplar VAE <ref type="bibr" target="#b35">(Norouzi et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Tractable Bounds to Loss</head><p>Given</p><formula xml:id="formula_9">training data D = {x i } N i=1 , the empirical loss iŝ L A-MIM (θ) = − 1 2N xi E q θ (z|xi) [log q θ (z|x i ) q θ (x i )] − 1 2N xi E q θ (z|xi) [log p θ (x i |z) p θ (z)] (10)</formula><p>where xi denotes a sum over N fair samples drawn from P(x), as a MC approximation to expectation over P(x).</p><p>Unfortunately, the empirical loss in Eqn. (10) is intractable since we cannot evaluate the log-probability of the marginals p θ (z) and q θ (x). In what follows we obtain a tractable empirical bound on the loss in Eqn. (10) for which, with one joint sample, we obtain an unbiased and low-variance estimate of the gradient using reparameterization <ref type="bibr" target="#b22">(Kingma &amp; Welling, 2013)</ref>.</p><p>We first derive a tractable lower bound to log q θ (x i ) :</p><formula xml:id="formula_10">log q θ (x i ) = (Eqn. 8) log E P(z) [p θ (x i |z)]<label>(11)</label></formula><formula xml:id="formula_11">= (IS) log E q θ (z|xi) p θ (x i |z) P(z) q θ (z|x i ) ≥ (JI) E q θ (z|xi) log p θ (x i |z) P(z) q θ (z|x i )</formula><p>where the second and third lines are obtained using importance sampling and Jensen's inequality. We remind the reader that q θ (x i ) is a variational marginal that can depend on x i . Indeed, Eqn. (11) is the usual ELBO.</p><p>To derive a lower bound to log p θ (z), we begin with the following inequality,</p><formula xml:id="formula_12">log E P(x) [h(x; ·)] = log i P(x i ) h(x i ; ·) ≥ log P(x ) h(x ; ·) ,<label>(12)</label></formula><p>for any sample x , any discrete distribution P(x), and any non-negative function h(x; ·) ≥ 0. The inequality in Eqn. </p><formula xml:id="formula_14">D enc ← {x j , z j ∼ q θ (z|x)P(x)} N j=1 3:L MIM (θ; D) = − 1 N N i=1 log p θ (x i |z i ) + 1 2 (log q θ (z i |x i ) + log P(z i )) 4:</formula><p>∆θ ∝ −∇ θLMIM (θ; D) {Gradient computed through sampling using reparameterization} 5: end while for any sample x . During training, given a joint sample</p><formula xml:id="formula_15">x i , z i ∼ q θ (z|x) P(x), we choose x = x i .</formula><p>Substituting Eqns. (11) and (13) into Eqn. (10) gives the final form of an upper bound on the empirical loss; i.e.,</p><formula xml:id="formula_16">L A-MIM ≤ − 1 N i E q θ (z|xi) [log p θ (x i |z)] − 1 2N i E q θ (z|xi) log q θ (z|x i )P(z) + 1 2 H P ( x ) .<label>(14)</label></formula><p>We find an unbiased, low variance estimate of the gradient of Eqn. (14) with a single joint sample z i , x i ∼ q θ (z|x) P(x) and reparameterization. The last term, H P ( x ), is a constant, independent of model parameters and can therefore be ignored during optimization. The resulting learning process is described in Algorithm 1.</p><p>To better understand the proposed bounds, we note that A-MIM achieves good reconstruction by learning posteriors with relatively small variances (i.e., relative to the distance between latent means). Our choice of x = x i exploits this, allowing good gradient estimation, and facilitating fast convergence. We further provide empirical evidence for these properties below in <ref type="figure">Fig. 3</ref> We show experimental results on four word level datasets described in <ref type="table">Table 1</ref>, namely, Penn Tree Bank <ref type="bibr" target="#b31">(Marcus et al., 1993)</ref>, Yahoo Answers and Yelp15 (following <ref type="bibr" target="#b51">Yang et al. (2017)</ref>), and WikiText-103 <ref type="bibr" target="#b32">(Merity et al., 2016)</ref>. We use the Yahoo and Yelp15 datasets of <ref type="bibr" target="#b51">Yang et al. (2017)</ref>, which draw 100k samples for training, and 10k for validation and testing. For WT103 we draw 200k samples for training, 10k for validation, and retain the original test data. Empty lines and headers were filtered from the WT103 data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture and Optimization</head><p>Our auto-encoder architecture (Figs. 1 and 2) followed that proposed by <ref type="bibr" target="#b7">Bowman et al. (2015)</ref>. As is common, we concatenated z with the input to the decoder (i.e., a "context", similar to <ref type="bibr" target="#b17">He et al. (2019)</ref>; <ref type="bibr" target="#b51">Yang et al. (2017)</ref>; <ref type="bibr" target="#b7">Bowman et al. (2015)</ref>). We use the same architecture, parameterization, and latent dimensions for both sMIM and a VAE variant called sVAE, for comparison. We also trained deterministic auto-encoders with the same architecture, called sAE, by replacing the sampled latent code with the deterministic mean of the posterior (i.e., z i = E z [q θ (z |x i )]). Effectively, the only difference between these variants is the choice of loss function. Training times for all models are similar.</p><p>For PTB we trained models with 1 layer GRU, latent space dimensions of 16D, 128D, and 512D, a 512D hidden state, 300D word embeddings, and 50% embedding dropout. We trained all models with Adam <ref type="bibr" target="#b21">(Kingma &amp; Lei Ba, 2014)</ref> with initial learning rate lr = 10 −3 . Training took less than 30 minutes on a single TITAN Xp 12G GPU. For Yahoo Answers, Yelp15, and WT103 we trained models with 1 layer GRU, latent space dimensions of 32D, 512D, 1024D, a 1024D hidden state, 512D word embeddings, and 50% embedding dropout. We trained these models with SGD <ref type="bibr" target="#b41">(Sutskever et al., 2013)</ref>, with initial lr = 5.0, and 0.25 L 2 gradient clipping. All model and optimization hyperparameters were taken from publicly available implementation of the method proposed by <ref type="bibr" target="#b7">Bowman et al. (2015)</ref>.</p><p>In all cases we use a learning rate scheduler that scaled the learning rate by 0.25 following two/one epochs (PTB/other datasets, respectively) with no improvement in the validation loss. We used a mini-batch size of 20 in all cases. Following <ref type="bibr" target="#b42">(Sutskever et al., 2014)</ref> we feed the input in reverse to the encoder, such that the last hidden state in the encoder depends on the first word of the sentence.</p><p>We trained sVAEs with the regular ELBO, and with KL divergence annealing (denoted "+ kl"), where a scalar weight on the KL divergence term is increased from 0 to 1 over 10k mini-batches to lower the risk of posterior collapse <ref type="bibr" target="#b7">(Bowman et al., 2015)</ref>. We use no loss manipulation heuristics in the optimization of sMIM or sAE.  <ref type="table">Table 3</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Information Content in the Latent Code</head><p>Directly estimating the mutual information (MI) between the high-dimensional categorical observations x and the corresponding latent codes z is computationally expensive <ref type="bibr" target="#b4">(Belghazi et al., 2018)</ref>. Instead, we focus here on reconstruction, which is related to MI <ref type="bibr" target="#b37">(Poole et al., 2019)</ref>. We choose the reconstruction entropy (Enc. Recon.), which is the negative expected log-probability of the decoder, given a sample from the corresponding posterior. In addition, we show the reconstruction entropy when the latent code is sampled from a Gaussian prior (Rand. Recon.). The Gaussian has 0 mean and standard deviation fitted to the latent codes (see <ref type="table">Table  11</ref> in supplementary material). When the latent code conveys little information to the decoder, we expect Enc. Recon. and Rand. Recon. to be similar. When the decoder utilizes highly informative latent codes, we expect Rand. Recon. to be significantly larger (i.e., worse). Finally we also show the 1-BLEU score, the fraction of words recovered in the sampled reconstruction.</p><p>Tables <ref type="formula">(</ref> learning; due to the small size of PTB, annealing overfit. Model sMIM (1024) † is trained on all datasets (i.e., PTB, Yahoo Answers, Yelp15 and WT103). The BLEU score is computed between test sentences and their reconstructed samples (higher is better), and |θ| indicates the number of parameters in each model.</p><p>Tables <ref type="bibr">(2)</ref><ref type="bibr">(3)</ref><ref type="bibr">(4)</ref> show that sMIM outperforms sVAE in reconstruction and BLEU score, and is comparable to sAE. In addition, the reconstruction of sMIM and sAE improves with more latent dimensions, showing that more information is captured by the latent codes, whereas sVAE shows the opposite trend due to posterior collapse (i.e., encoder and random reconstructions are similar). Notice that sAE is more susceptible to over-fitting, as is evident in <ref type="table" target="#tab_4">Table 4</ref>. WT103 results in <ref type="table">Table 5</ref> show that the superior reconstruction of sMIM also holds with longer sentences.</p><p>In summary, sMIM shows improved performance with more expressive architecture (higher latent dimension here), similar to sAE, while sVAE deteriorates due to posterior collapse. This suggests that sMIM could benefit from more powerful architectures like Transformers <ref type="bibr" target="#b49">(Vaswani et al., 2017)</ref>, without the need for posterior collapse-mitigating heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Posterior Collapse in VAE</head><p>The performance gap between sMIM and sVAE with identical architectures is due in part to posterior collapse in VAEs (i.e., optimization is likely to have a role too), where the encoder has high posterior variance over latent codes, and hence low mutual information (cf. <ref type="bibr" target="#b52">(Zhao et al., 2018;</ref><ref type="bibr" target="#b1">Alemi et al., 2017)</ref>); it coincides with the KL divergence term in the usual ELBO approaching zero in some or all 535 0 535 0 (a) log p θ (xi|zj) histograms.</p><formula xml:id="formula_17">10 5 0 10 3 MIM [i=j] MIM [i j] 10 5 0 10 3 VAE [i=j] VAE [i j] (b) log q θ (zi|xj) histograms. Figure 3.</formula><p>Histograms of log probabilities of test data for sMIM and sVAE trained on PTB: Overlap between curves indicates potential for poor reconstruction of input sentences. (a) Histograms of log p θ (xi|zj) for zj ∼ q θ (z|xj) when i = j (same input), and when i = j (when xi is evaluated with the decoder distribution from a latent code associated with a different input sentence). (b) Histograms of log q θ (zi|xj) for zi ∼ q θ (z|xi), when conditioned on the same input i = j, or a different input i = j.   dimensions. In such cases, different sentences are mapped to similar regions of the latent space (see <ref type="figure">Fig. 3, bottom)</ref>.</p><formula xml:id="formula_18">y O C Y T W Z P j Y 6 E Z V I e q T 0 M = " &gt; A A A M o X i c j V Z b b 9 s 2 F F a 7 W + d 6 a 7 o + 7 o W Y E S z F C s N O g W 3 Y y 5 r F G d I l H r w 0 S Y t Y r k F J l M O Y u k y i X c U M f 8 t e t 5 + 0 n 7 K 3 H V J y L I k K E A G J K Z 7 v c n h 4 E Z 2 Y 0 Z T 3 e v 8 + e P j R x 5 9 8 + t m j z 1 u P 2 1 9 8 + W T r 6 V f n a b R I X H L m R i x K 3 j k 4 J Y y G 5 I x T z s i 7 O C E 4 c B h 5 6 8 z 3 V f z t k i Q p j c J T f h 2 T S Y B n I f W p i z l 0 T b e e 2 R m y a Y j s A P N L x x E n 8 v 3 u d K v T 6 / b 0 g 8 x G v 2 h 0 f v 7 P 1 8 9 o + v T x 0 v Y i d x G Q k L s M p + m 4 3 4 v 5 R O C E U 5 c R 2 b I X K Y m x O 8 c z M o Z m i A O S T o T O X q J t 6 P G Q H y X w F 3 K k e 8 s M g Y M 0 v Q 4 c Q K o k 0 3 p M d T b F x g v u / z g R N I w X n I R u b u Q v G O I R U q V A H k 2 I y 9 k 1 N L C b U M g V u Z c 4 w S 6 H g r V a d k g + u F E Q 4 N A T t p N K + B c x T 2 U S M d l q b a M l T i i G Q o N r Q s r g u R R C F z Q J x F x K W Z W a U 5 C a v x f U C F z l g S s j 4 O B E 2 e N E a Y N 3 z Y 8 r P y c V 3 P D i y o s 3 e P G r P G B 4 8 Z I X 1 1 7 l a F Y 4 Z d q p m k W m v L I G r + w q D x h e W c k r M 7 x W u R d a S V 3 t G G Y m I H p q K j D d n x Z Y m 1 8 S j h U B G B 5 s n 4 Q 6 C 7 X S a 6 S h z C f I x U w M 6 2 k N g 8 g j D B D D 6 a 1 8 H Z L i I C 4 Q a 5 0 3 0 h h C f B X R k J e 8 R n W h 2 M O Q 7 x o 5 v V 0 2 u t v U 8 4 g r R V z X U K t b / F k H D z V Y c + 4 e h 6 Z q h T K m t X 2 P B y p 8 R J Y 0 / D Z F h V 6 t y C N Q h U q q K V Y + O / Y K 3 S A 7 e 4 6 K 1 + x 5 L Z s R p L o m Q B M Q i r B S B P 2 6 M g i F w 7 r M a 9 Y L 1 I A t x O + F D S i s d J s R n 6 O d e i 7 5 V K l s 0 H e o O r L W b V C N M q G z S 4 6 a c j Y r s u E 1 p G 3 W Y 5 N D f U I B V x 4 h 0 o M A p h p m k Z L B g V R u O d C + D 8 f L V n K z W K W d l d o r e b / 1 o 7 c o f D P U x y l F I I v C R e A Y G / w U Q F K c S r N X H T j q t + H M u d C k C 8 i k E E W R j + Z h 9 I E R b w Z H f x R g W j 8 U j g A p x X 6 V o z Y i i l V 1 a u i B R v + u j h o g z O A o T D C r 5 X h x B O f S 6 e o I G f v u N Z x A A v 4 b k f O 9 A x a l a e n M O J Z T m 5 O M C w g 1 o A l z o h L 6 B N D F V D T g w f E u d Z V S D b 1 / Z y r 7 h v I e m D U A 9 8 w c B k f H c r w 7 2 Y A H a z A E 8 q X X 6 d s v 7 B v 7 B e r s N i + / 3 9 4 M c g 3 N U 2 9 V 4 l 2 8 g 4 0 z X H Y O 5 F R 0 + j l 1 v K Z M 6 u k e n O Y k h b q x P c K R 7 c M l Q R + X n G K G l I Y s v + 5 K m W v d m F q D X 7 W a 2 s j 3 l A J o T e Z 4 k 1 M 0 Q 0 p V 8 T S 4 C j z R 6 2 y 9 H m B p 3 L b 1 b N f R h x X s u n m Y f 3 8 V j B i 7 Q F 3 l v C I k x y + L G X H 8 / F f f 5 Y r c f o L R v J S G q Y e X 1 J P j / q S q J R L i S T H Q Q V 2 U K i u g c F N r Y D l s Q a Q Y 6 m g D b a 6 + V w 2 0 W U J I K I X + n B U 8 N W T s O A l 0 4 Y Y L B J m p G w c J E O n O u n U b S o o g 7 R I j u E y L 4 D I 1 Y n B 3 Y W t d j j A z A K 5 f h F 1 f x e C</formula><formula xml:id="formula_19">4 = " &gt; A A A M o X i c j V Z b b 9 s 2 F F a 7 W + d 6 a 7 o + 7 o W Y E S z F C s N O g W 3 Y y 5 r F G d I l H r w 0 S Y t Y r k F J l M O Y u k y i X c U M f 8 t e t 5 + 0 n 7 K 3 H V J y L I k K E A G J K Z 7 v c n h 4 E Z 2 Y 0 Z T 3 e v 8 + e P j R x 5 9 8 + t m j z 1 u P 2 1 9 8 + W T r 6 V f n a b R I X H L m R i x K 3 j k 4 J Y y G 5 I x T z s i 7 O C E 4 c B h 5 6 8 z 3 V f z t k i Q p j c J T f h 2 T S Y B n I f W p i z l 0 T b e e 2 S t k 0 x D Z A e a X j i N O 5 P v + d K v T 6 / b 0 g 8 x G v 2 h 0 f v 7 P 1 8 9 o + v T x 0 v Y i d x G Q k L s M p + m 4 3 4 v 5 R O C E U 5 c R 2 b I X K Y m x O 8 c z M o Z m i A O S T o T O X q J t 6 P G Q H y X w F 3 K k e 8 s M g Y M 0 v Q 4 c Q K o k 0 3 p M d T b F x g v u / z g R N I w X n I R u b u Q v G O I R U q V A H k 2 I y 9 k 1 N L C b U M g V u Z c 4 w S 6 H g r V a d k g + u F E Q 4 N A T t p N K + B c x T 2 U S M d l q b a M l T i i G Q o N r Q s r g u R R C F z Q J x F x K W Z W a U 5 C a v x f U C F z l g S s j 4 O B E 2 e N E a Y N</formula><p>In contrast, given the high mutual information and reconstruction quality of sMIM, we only expect high decoding probability of a sentence when a latent code is sampled from the corresponding posterior. In other words, for sMIM, the posterior variances for different input sequences are relatively small compared to the distance between the posterior means <ref type="figure">(Fig. 3, top)</ref>, allowing for accurate reconstruction.</p><p>The issue of posterior collapse becomes harder to mitigate as the dimension of the latent space increases, because the decoder becomes more expressive. As a consequence, language VAEs are typically limited to 32 dimensions or fewer (e.g., <ref type="bibr" target="#b17">He et al. (2019)</ref>), with only a few exceptions, such as <ref type="bibr" target="#b16">Guu et al. (2017)</ref> which opted for 128 dimensions in a very particular problem settings. On the other hand, sMIM can easily scale up the latent dimension without issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Structure in the Latent Space</head><p>Here, we explore the structure in the learned representation.  <ref type="table" target="#tab_5">Table 6</ref>. Empirical entropy of the latent codes, estimated with a NN entropy estimator. For comparison, column N shows the entropy of a standard Normal in R d of corresponding latent dimension. In brackets is the ratio of the NN entropy to the entropy of an isotropic Gaussian fit to the latent codes. Ratios below 1 indicate that the latent codes are more clustered than a Gaussian, suggesting the low entropy for sMIM is not a simple consequence of down-scaling the latent codes.</p><p>entropy estimator <ref type="bibr" target="#b23">Kraskov et al. (2004)</ref>, of the latent codes for sMIM, sVAE, and sAE. Notice how the representation learned by sMIM has a significantly lower entropy. We note that sVAE is regularized to match a Gaussian, which is known to introduce smoother structure into the learned representation (see discussion by <ref type="bibr" target="#b6">Bosc &amp; Vincent (2020)</ref>).</p><p>With sMIM, we propose the use of information entropy minimization as an alternative regularization, which introduces meaningful structure without suffering from posterior collapse (see schematic plot in <ref type="figure" target="#fig_2">Fig. 4)</ref>. Interestingly, neuroscientists have proposed that entropy minimization is an organizing principle in neural representations and information processing in the brain. Entropy minimization is viewed as allowing the agent to learn better to predict likely events <ref type="bibr" target="#b14">(Friston, 2010;</ref><ref type="bibr" target="#b3">Barlow et al., 1972)</ref>, compression/redundancy elimination <ref type="bibr" target="#b2">(Barlow, 1961)</ref>, and in terms of efficiency in energy consumption <ref type="bibr" target="#b43">(Takagi, 2020)</ref>.</p><p>By scaling the latent codes to reduce the variance of the aggregate posterior one can trivially reduce entropy with no benefit in terms of latent structure. To test whether this might be the case, we also fit an isotropic Gaussian to the latent codes (see <ref type="table">Table 11</ref> in supplementary materials), and show the ratio between the NN entropy and the fitted entropy in brackets. A ratio smaller than 1 suggests that the empirical entropy is more clustered than a Gaussian. <ref type="table" target="#tab_5">Table  6</ref> clearly shows that the lower empirical entropy cannot be explained by the scaling alone. We attribute the gap between the fitted and empirical entropies to clustering in the latent space, which MIM learning empirically demonstrates <ref type="bibr" target="#b30">(Livne et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Reconstruction, Interpolation, and Perturbation</head><p>We further probe the structure in the learned representation, demonstrating that sMIM learns a dense, meaningful latent space. We present latent interpolation results in <ref type="table">Table 7</ref> for samples (i.e., reviews) with the different ratings from Yelp5. Interpolation entails sampling x ∼ p θ (x|z α ) where z α is 5 stars → 1 star &lt;BOT&gt; awesome food , just awesome ! top notch beer selection . great staff . beer garden is great setting .</p><p>• awesome food , just top notch ! great beer selection . staff has great craft beer . top notch is that . &lt;EOT&gt; • awesome food ! just kidding , beer selection is great . staff has trained knowledge on top . &lt;EOT&gt; • cleanliness is awesome ! not only on their game , food . server was polite his hand sanitizer outside . &lt;EOT&gt; • cleanliness is not on their patio . server was outside , kept running his hand sanitizer his hand . &lt;EOT&gt; &lt;BOT&gt; cleanliness is not on their radar . outside patio was filthy , server kept running his hand thru his hair . <ref type="table">Table 7</ref>. Interpolation results between latent codes of input sentences (with gray) from Yelp15 for sMIM (1024).</p><p>(D) &lt;BOT&gt; the company did n't break out its fourth-quarter results (M) the company did n't break out its results &lt;EOT&gt; (R) the company did n't break out its fourth-quarter results &lt;EOT&gt; (P) the company did n't accurately out its results &lt;EOT&gt; interpolated at equispaced points between two randomly sampled latent codes, z i ∼ q θ (z|x i ), and z j ∼ q θ (z|x j ).</p><p>Next we show reconstruction, and perturbation results for for sMIM (512) trained on PTB. <ref type="table" target="#tab_6">Table 8</ref> shows four sentences: (D) the input sentence; (M) the mean reconstruction given the posterior mean z; (R) a reconstruction given a random sample z from the posterior; and (P) a perturbed reconstruction, given a sample z from a Gaussian distribution with 10 times the posterior standard deviation. The high mutual information learned by sMIM leads to good reconstruction, as clear in (M) and (R). sMIM also exhibits good clustering in the latent space, shown here by the similarity of (R) and (P).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Question-Answering</head><p>So far we have discussed abstract aspects of representations learned by sMIM, such as high mutual information, and low marginal entropy. To demonstrate the benefits of representations learned by sMIM, we consider a downstream task in which sMIM is pre-trained on Yahoo Answers, then used for question-answering on YahooCQA <ref type="formula">(</ref>  Q unk i is simply Q i concatenated with "?" and a sequence of &lt;unk&gt; tokens to represent the |A k i | unknown words of the answer. We then rank question-answer pairs according to the score</p><formula xml:id="formula_20">S k i = ||z unk i − z k i ||/σ k,unk i where σ k,unk i</formula><p>is the standard deviation of q θ (z|Q unk i ). In other words, we rank each question-answer pair according to the normalized distance between the latent code of the question with, and without, the answer. This score is similar to log q θ (z k i |Q unk i ), but without taking the log standard deviation into account.</p><p>As is common in the literature, <ref type="table">Table 9</ref> quantifies test performance using average precision (P @1 = 1 N i 1(rank(A 1 i ) = 1)), and Mean Reciprocal Ranking (M RR = 1 N i 1 rank(A 1 i ) ). As baselines, we consider best performing single-task models trained directly on Ya-hooCQA (dos <ref type="bibr" target="#b12">Santos et al., 2016;</ref><ref type="bibr">Tay et al., 2017a)</ref>. Interestingly, sMIM (512), pre-trained on Yahoo Answers, exhibits state-of-the-art performance compared to these baselines. For an even larger sMIM model, pre-trained on all of PTB, Yahoo Answers, Yelp15 and WT103, the questionanswering performance of sMIM is even better (last row of <ref type="table">Table 9</ref>).</p><p>The results for sVAE are based on the mean of the posterior rather than a random sample. This is a common heuristic in the NLP literature which has been proven useful for downstream tasks, but is problematic when considering the generative process which relies on the sample rather than the mean <ref type="bibr" target="#b6">(Bosc &amp; Vincent, 2020)</ref>. Finally, as another point of comparison, we repeated the experiment with a deterministic sAE model (with σ k,unk i = 1). In this case performance drops, especially average precision, indicating that the latent representations are not as meaningfully structured.</p><p>Importantly, sMIM can generate novel answers rather than simply ranking a given set of alternatives. To this end, we sample z unk i ∼ q θ (z k i |Q unk i ), as described above, followed by modified reconstruction Q i ∼ p θ (x|z unk i ). We modify the sampling procedure to be greedy (i.e., top 1 token), and prevent the model from sampling the "&lt;UNK&gt;" token. We consider all words past the first "?" as the answer. (We also removed HTML tags (e.g., "&lt;br&gt;").) <ref type="table" target="#tab_8">Table 10</ref> gives several selected answers. The examples were chosen to be short, and with appropriate (non-offensive) content. We note that we did not use any common techniques to manipulate the decoding distribution, such as beam search, Nucleus sampling <ref type="bibr" target="#b20">(Holtzman et al., 2019)</ref>, or sampling with temperature <ref type="bibr" target="#b0">(Ackley et al., 1985)</ref>. To the best of our knowledge, sMIM is the current state-o-the-art for a single-task model for Ya-hooCQA, despite having simpler architecture and training procedure when compared to competing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>This paper introduces sMIM, a new probabilistic autoencoder for language modeling, trained with A-MIM learning. In particular, sMIM avoids posterior collapse, a challenging problem with VAEs applied to language, which enables the use of larger latent dimensions compared to sVAE, by orders of magnitude. While the reconstruction error is comparable to sAE, sMIM learns a latent representation with semantic structure, similar to sVAE, allowing for interpolation, perturbation, and sampling. In this sense, it achieves the best of both worlds: a semantically meaningful representation with high information content. We also use the structured latent representation for a downstream question-answering task on YahooCQA with state-of-the-art results. Importantly, the proposed framework has no hyperparameters in the loss, greatly simplifying the optimization procedure. In addition, sMIM benefits from a more expressive architecture, in contrast to sVAE, and demonstrates reduced susceptibility to over-fitting, compared to sAE. In future work, we will apply sMIM to more contemporary and powerful architectures like the Transformer.  <ref type="figure" target="#fig_3">Fig. 5</ref> shows histograms of sentence lengths. Notice that PTB sentences are significantly shorter that other datasets. As a result, sMIM is somewhat better able to learn a representation that is well suited for reconstruction. Other datasets, with longer sentences, are more challenging, especially with the simple architecture used here (i.e., 1 later GRU). We believe that implementing sMIM with an architecture that better handles long-term dependencies (e.g., transformers) might help. <ref type="figure">Figure 6</ref>. Histograms of reconstruction for sMIM and sVAE versus latent dimension for PTB. Dashed black line is the mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Distribution of Sentence Lengths</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison of Reconstruction in MIM and VAE</head><formula xml:id="formula_21">sMIM sVAE z ∈ R 16 z ∈ R 128 z ∈ R 512</formula><p>Figures 6-8 depict histograms of reconstruction values for sentences, for sVAE and sMIM with different latent dimensions. While a less expressive sMIM behaves much like sVAE, the difference is clearer as the expressiveness of the model increases.</p><p>Here, sVAE does not appear to effectively use the increased expressiveness for better modelling. We hypothesize that the added sVAE expressiveness is used to better match the posterior to the prior, resulting in posterior collapse. sMIM uses the increased expressiveness to increase mutual information.  there was no orders &lt;EOT&gt; there was no panic &lt;EOT&gt; (P) there was no panic &lt;EOT&gt; there was no shortage panic &lt;EOT&gt; (AE) there was no panic &lt;EOT&gt; (D) &lt;BOT&gt; the company did n't break out its fourth-quarter results (M) the company did n't break out its fourth-quarter results &lt;EOT&gt; the company did n't break out its results results &lt;EOT&gt; (R) the company did n't break out its results &lt;EOT&gt; the company did n't break out its results &lt;EOT&gt; (P) the company did n't break out its fourth-quarter results &lt;EOT&gt; the company did n't break out its results results &lt;EOT&gt; (AE) the company did n't break out results &lt;EOT&gt; (D)</p><formula xml:id="formula_22">sMIM sVAE + kl z ∈ R 32 z ∈ R 512 z ∈ R 1024</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Empirical Latent Entropy</head><p>&lt;BOT&gt; it had planned a strike vote for next sunday but that has been pushed back indefinitely (M) it had a weakening for promotional planned but that has pushed aside back but so far away &lt;EOT&gt; it had planned planned a planned for next week but that continues has been pushed back pushed &lt;EOT&gt; (R) it had a planned strike for energy gifts but so that has planned airlines but block after six months &lt;EOT&gt; it had planned a strike planned for next sunday but that has been pushed back culmination pushed &lt;EOT&gt; (P) it had a strike with stateswest airlines but so that it has slashed its spending but so far said he would be subject by far &lt;EOT&gt; it had planned a strike for hardcore but has been pushed every year that leaves back &lt;EOT&gt; (AE) it had been a five-year vote but for a week that drilling humana strike back back has planned back &lt;EOT&gt; Here we provide reconstruction results for PTB ( <ref type="figure">Fig. 12)</ref>, Yelp15 <ref type="figure">(Fig. 13)</ref>, and Yahoo Answers <ref type="figure" target="#fig_2">(Fig. 14)</ref>. Each figure shows (D) Data sample; (M) Mean (latent) reconstruction (i.e., z i = E [q θ (z|x i )]); (R) Reconstruction (i.e., z i ∼ q θ (z|x i )); (P) Perturbed (latent) reconstruction (i.e., z i ∼ q θ (z|x i ; µ i , 10σ i )); (AE) Reconstruction of AE. We compare the best performing sMIM model to an AE with the same architecture, and to sMIM (1024) † (i.e., the model trained on the Everything dataset).</p><p>Interestingly, AEs tend to perform worse for longer sentences, when compared to sMIM. We attribute this to the higher latent entropy, which leads to non-semantic errors (i.e., nearby latent codes are less similar compared to MIM). Another interesting point is how the reconstruction (R), is better in many cases than the reconstruction given the mean latent code from the encoder (M) (i.e., which have the highest probability density). We attribute that to the fact that most probability mass in a high dimensional Gaussian in d &gt;&gt; 1 dimensional space and σ standard deviation is concentrated in around a sphere of radius r ≈ σ √ d. As a result the probability mass around the mean is low, and sampling from the mean is less likely to represent the input sentence x i . This also explains how perturbations of up to 10 standard deviations might result in good reconstructions. Finally, we point how sMIM (1024) † , trained on Everything, does a better job handling longer sentences. <ref type="bibr">sMIM (1024)</ref> sMIM (1024) † (D) (3 stars) &lt;BOT&gt; decent price . fast . ok staff ... but it is fast food so i ca n't rate any higher than 3 .</p><p>(M) decent italians . fast . price ok ... but it is higher than any other fast food i ca n't rate so higher rate jusqu . &lt;EOT&gt; decent oxtail . ok . fast price ... but staff it is so fast i ca n't rate any food 3 . &lt;EOT&gt; (R) decent price . superior . decent staff ... but ok fast food is n't so it i ' d rate higher any higher quality than 3 . &lt;EOT&gt; decent price . fast staff . fast ok ... but it is so fast food i rate 3 higher than any . &lt;EOT&gt; (P) decent price . ok . fast food ... but it is ok . so i ca n't rate any higher rate as fast food is marginal . &lt;EOT&gt; decent price . fast . wu ... fast food ! but it staff so ok i ca n't rate 3 stars . . &lt;EOT&gt; (AE) decent price . fast staff . ok ... but it is fast food so i ca n't rate any rate than 3 . &lt;EOT&gt; (D) (4 stars) &lt;BOT&gt; excellent wings . great service . 100 % smoked wings . great flavor . big meaty . i will definitely be back . okra is great too . delicious ! the meat really are good and the quality is nice . it ' s also tempting top notch lovers from the roasters an item top . &lt;EOT&gt; delicious ! the sandwiches are really good and the quality is top notch . it ' s an exotic item popping also generates from the top spices . &lt;EOT&gt; (R) delicious ! the sandwiches are really good and the meat is quality . it ' s also nice dessert for shipping from the top floor an unhygienic machine . &lt;EOT&gt; delicious ! the sandwiches are really good and the quality is top notch . it ' s also charging an item assortment from the grocery store for dessert . &lt;EOT&gt; (P) delicious sandwiches ! the servers are really good and the quality is top notch . it ' s also an item for meat quality memories . &lt;EOT&gt; who ! the meat are really good and the quality is top notch ' s . it also seems top notch item has yet and an unexpected range for the pistachio . i do cross like john tomatoes from my experience . &lt;EOT&gt; (AE) delicious ! the sandwiches are really good and the quality is top notch . it ' s also caught meat also fixing an item from the top for nice hash . &lt;EOT&gt;  &lt;BOT&gt; thanks to modern medicine more couples are growing old together • to growing small businesses are growing more rapidly growing &lt;EOT&gt; • thanks to modern medicine more modern couples are growing together than &lt;EOT&gt; • growing to more areas are growing preventing black trends &lt;EOT&gt; • thanks to modern cancer more are growing peaceful couples form &lt;EOT&gt; • growing to the growing industry are growing more rapidly growing than &lt;EOT&gt; • thanks to medicine rosen modern more are growing together governing &lt;EOT&gt; • growing to the exact industry has been growing more sophisticated six months &lt;EOT&gt; • thanks to moolah the modern premises are more sensitive together &lt;EOT&gt; • politics the growing issue are not to mention closely although other prospective products &lt;EOT&gt;</p><p>• programm thanks to the cutbacks schedules is not an church system &lt;EOT&gt; • the system is growing enough to make not radical an article &lt;EOT&gt; • humana remains the loyalty to instituting dynamic is an orthodox montage &lt;EOT&gt; • the system is reducing compliance not to consider an article &lt;EOT&gt; • the strategies is not paying the non-food system an individual member &lt;EOT&gt; • the system is the problem system not an effective &lt;EOT&gt;</p><p>• the system is not the individual problem member an can &lt;EOT&gt; • the system is the system not knowing an individual &lt;EOT&gt;</p><p>• the system is not the individual problem an individual member &lt;EOT&gt; • the system is the system not an encouraging problem &lt;EOT&gt;</p><p>• the system is not the individual problem an individual member &lt;EOT&gt; &lt;BOT&gt; the system is the problem not an individual member • the system is the system not an investment fund &lt;EOT&gt;</p><p>• the system is the ringers not an individual member &lt;EOT&gt; • the system is the problem not an office &lt;EOT&gt;</p><p>• the system is not the problem an individual member &lt;EOT&gt; • the system is not the problem for an individual &lt;EOT&gt; • the problem is not the indies system an individual &lt;EOT&gt; • the system is not clear the veto &lt;EOT&gt; • the merksamer is not the problem system an individual &lt;EOT&gt; • the system is not encouraging to the securities &lt;EOT&gt; • mr . the herald is not an individual problem &lt;EOT&gt; • xtra the system is not even critical &lt;EOT&gt;</p><p>• qintex producers is the president's to comment &lt;EOT&gt; • sony denies the declines to secure &lt;EOT&gt; • sony preferences itself is the bidding to comment &lt;EOT&gt; • everyone brought the stock to comment &lt;EOT&gt; • sony sony itself is to comment &lt;EOT&gt; • sony which declines to comment &lt;EOT&gt; • sony sony itself to comment &lt;EOT&gt; • kellogg declines to induce itself &lt;EOT&gt; • sony declines itself to sony &lt;EOT&gt; &lt;BOT&gt; sony itself declines to comment <ref type="table">Table 15</ref>. Interpolation results between latent codes of input sentences (with gray) from PTB.</p><p>Here we provide interpolation results for PTB ( <ref type="figure" target="#fig_3">Fig. 15</ref>), Yelp15 <ref type="figure">(Fig. 16</ref>), and Yahoo Answers <ref type="figure">(Fig. 17)</ref>. We compare the best performing sMIM model to sMIM (1024) † . Interestingly, both models appear to have learned a dense latent space, with sMIM (1024) † roughly staying within the domain of each dataset. This is surprising since the latent space of sMIM (1024) † jointly represents all datasets. <ref type="bibr">sMIM (1024)</ref> sMIM <ref type="formula" target="#formula_1">(1024)  †</ref> (3 star) &lt;BOT&gt; as bbq in phoenix goes -this is one of the better ones . get there early -they fill up fast ! • as in china phoenix -this is one of the better ones fast get . fill there early -they fill up early ! &lt;EOT&gt;</p><p>• as in phoenix goes this is -better than one of the newest ones . get there earlythey fill up fast ! &lt;EOT&gt; • as far in san jose -this is one of the better ones . fast get up early ! there they fill up fast for u ! &lt;EOT&gt; • as shore goes in phoenix -this is one of the better bbq . fast ! they get up there early -men dinner . &lt;EOT&gt; • as pei wei goes in this phoenix --one of the best ones . get there early ! they picked up fast food items is better . &lt;EOT&gt; • as dean goes in phoenix this is the list of bbq . -one not goes fast -get there early ! they fill up fast . &lt;EOT&gt; • oxtail yo buffet in pittsburgh as the owners goes -better . this is not one of those fast food places . fill up there get the hot ! &lt;EOT&gt; • veal as rocks as this goes in the phoenix area . -one of food is not better quick enough they get . 2 enchiladas up ! &lt;EOT&gt; • ah circle k ! not as bad in the food . thankfully -this one is one of the best bbq joints here ! service was fast friendly . &lt;EOT&gt; • kohrs as molasses as comparing goes in the food . not sure is one of this better ones -the only ones for fat . thumbs squeeze there ! &lt;EOT&gt; • ehh = ciders as the food goes . not bad for service ! -in many fast the only ones available is this . you can get better steak anywhere else ! &lt;EOT&gt; • omg = rainbow not as the food goes . congrats service ! this is one of the hot spots for only frozen hot -you can . eat on carts there . &lt;EOT&gt; • bin spaetzle food not the best . wicked spoon ! service is brutal only fast for the hot mexican in lv . everything else on this planet as can you get . &lt;EOT&gt; • = frozen food ! not the best . only frozen hot as for you shall pick the ice cream -. loved everything else on wednesday ! &lt;EOT&gt; • frankie food not soo the best . service = horrible ! only drawback frozen for these hike . everything you can pass on the juke planet . &lt;EOT&gt; • = food not only the best . frozen service ! everything else for the frozen yogurt company . absolute hot tea during normal on as they can . &lt;EOT&gt; • food not the best service . knocking only 99 cents ! for the hot buffet everything . beef &amp; broccoli on the vip polo you can pass . &lt;EOT&gt; • = food not . the best frozen service ! only five stars for the water suppose . hot things you can smell on budget . &lt;EOT&gt; • food not the best . service = horrible ! only plopped for the paella everything &amp; rum . you can find everything on the strip . &lt;EOT&gt; • food = not the best . frozen service ! only $ 21 for the frozen hot chocolate . everything else can you tell on romance . &lt;EOT&gt; (2 star) &lt;BOT&gt; food = not the best . service = horrible ! only known for the frozen hot chocolate . everything else you can pass on . • food not the best . fuck service only ! ! horrible cannolis for the fajitas unusual known . everything you can pass on graduate . &lt;EOT&gt; • food = not the best . frozen hot service ! only website for the frozen hot chocolate . you can grab everything else on . &lt;EOT&gt; • food not suck . the best service ever ! just horrible everything for the frozen hot chocolate . you can probably survive on everything else . &lt;EOT&gt; • food = not the best . frozen service ! only for five stars during the san francisco frozen chicken . everything else on could not give thumbs . &lt;EOT&gt; • food = not ! service = the best . only organizations thing for chocolate lovers treats and green beans . everything you can taste on the planet . &lt;EOT&gt; • food = not ! the frozen yogurt . service only best for you ate here twice although the frozen yogurt . delicious atmosphere on everything else . &lt;EOT&gt; • blech food ! not the best dish anywhere else . service = &lt;unk&gt; for the frozen hot chocolate and dessert bartenders ! everything you can only expect better at this shuffle . &lt;EOT&gt; • gelato food ! not sure the best . frozen seared only wish you can mix for the frozen hot chocolate frozen . service on and everything else explains . &lt;EOT&gt; • 32 words ! not amazing food . the best &lt;unk&gt; music and service they had can earned a better meal at xs . everything else on bill for me . &lt;EOT&gt; • hilariously = ! food is not the best meal . hibachi cover service and they only wished a frozen yogurt for hot girl . better luck at &lt;unk&gt; and on the latter experience . &lt;EOT&gt; • snottsdale act ! ! rio mia &lt;unk&gt; at the food and wished you not a fan . delicious lunch &amp; dessert better choices for dessert but they had blackjack . &lt;EOT&gt; • blended ! wifey better food ! the service is not frozen hot . they redeemed a &lt;unk&gt; and only frozen someplace at horse's for frozen worms . &lt;EOT&gt; • husbands cher ! wish they had &lt;unk&gt; dessert at the bellagio and not a great lunch selection . food better tasting wise but sadly serves and dessert selection . &lt;EOT&gt; • wish ! methinks buffet is ingrediants at the &lt;unk&gt; food and a better tasting . they woulda frozen lunch but not memorable and satisfying tasting better ambiance . &lt;EOT&gt; • soooo ! pretzel panera &lt;unk&gt; they had at a better selection and the food sucked but nothing memorable a dessert . surely great value and better mayonnaise desserts . &lt;EOT&gt; • yummy ! wish they had &lt;unk&gt; at a buffet and netherlandish better tasting food . a renovation treasure and great value but not better than calories tasting . &lt;EOT&gt; • yummy ! wish they had &lt;unk&gt; at lunch and a dessert selection but a better value and great value than beef suggestion company . &lt;EOT&gt; • wish ! wish they had &lt;unk&gt; at 10am and a dessert selection but better food a better and better tasting selection . great value ! &lt;EOT&gt; • yummy ! wish they had &lt;unk&gt; dessert at lunch and a selection but a tiramisu better value and freshness value food taste better than ihop . &lt;EOT&gt; • wish ! wish they had lunch at &lt;unk&gt; and a dessert fountain but better than a selection and great tasting food servings better tasting . &lt;EOT&gt; (4 star) &lt;BOT&gt; yummy ! wish they had &lt;unk&gt; at lunch and a better dessert selection but a great value and better tasting food than wicked spoon . <ref type="table" target="#tab_5">Table 16</ref>. Interpolation results between latent codes of input sentences (with gray) from Yelp15.  <ref type="formula" target="#formula_1">(512)</ref> • instead the stock market is still being felt to &lt;unk&gt; those of our empty than in a bid &lt;EOT&gt; • he estimated the story will take &lt;unk&gt; of paper co . ' s $ n million in cash and social affairs to at the company a good share &lt;EOT&gt; • long-term companies while the company ' s &lt;unk&gt; provisions would meet there to n or n cents a share and some of costly fund &lt;EOT&gt; • time stocks the company explained him to sell &lt;unk&gt; properties of high-grade claims which has received a net loss in the firm &lt;EOT&gt; • what i had the recent competition of &lt;unk&gt; replies that is n't expected to draw a very big rise in tokyo &lt;EOT&gt; <ref type="table" target="#tab_6">Table 18</ref>. Samples from best performing model for dataset PTB.</p><p>sMIM <ref type="formula" target="#formula_1">(1024)</ref> • ben monkey gabi sister near the western fest . i ' ve been looking forward to this location , and each time i ' m in the 6th bunch i want to have a great visit experience . it was all kinds of fillers , owns and dressings non-asian with jalapeños &lt;unk&gt; does n't hold me for much healthier . front desk is not my favorite dinner place at the gates . they are closed on mondays , -lrb -it could affect a couple minutes more rocks -rrb -and then we said the bar was the real bold . i ' d rather go to firefly some bubble in greece . if you had a neighbourhood addiction &lt;unk&gt; c , take this look as most amazing . &lt;EOT&gt; • hello tanya stephen covering qualité . ugh haha , i was curious to consume that the white asian restaurants believes filled a mob and turkey melt departments for $ 9.99 . the &lt;unk&gt; of these were not intrusive , it was accepted in there . . i ' m sure this is n't one of my favorite places to go at night with here ! particularly speaking the italian cleaning tables . we also ordered some pina colada , which tasted exactly like they came out of a box and per endearing thick . pretty good food overall , and the pigeons self nightly . i ' d call it again just on halloween for a dependable lunch . but the statue sucks ? so if you have bouchon to inquire was good place . &lt;EOT&gt; • prada based pata based solely often inside . this place is unappealing horrific for the 50th and fries , i ' ve caught to have a ton of good reviews &lt;unk&gt; in buckeye , barnes knew . not bc that i was wrong with my team being kicked the whole thing at eggroll , it ' s like pulling out of the landmark . no luck on ketchup top crunch , if you are craving something simple and &lt;unk&gt; . we also tried the wild mushroom -lrb -it ' s burn , did n't go in disheveled -rrb -as a matter destination from flavor . the food was just ok and nothing to write home about . friend peeps i only had one beer , but this place does not deserve the same increase . &lt;EOT&gt; <ref type="table">Table 19</ref>. Samples from best performing model for dataset Yelp15.</p><p>sMIM <ref type="formula" target="#formula_1">(1024)</ref> • how does transformers send grow ina under pubs ? i found the suspension resides official game is exciting to withstand and what can a person do in that case ? brees fights , if it does 150 . the dre is tied ordered outlook &lt;unk&gt; 2005 . today had a migrane with limitation tops , because of his vr repeats , you are referring to review at the university of 1994 and have visited fortune . judy for websites &lt;unk&gt; website is beware confused . &lt;EOT&gt; • how do i download jesus gyno to woman whom ? being irvine in line is what you did a lot of oceanic denny in the middle east and spanish wallet or &lt;unk&gt; entity . plus , i'm aware of that , particularly do you have any insight insight ... if you are a hoe who's right click on it , and you can ' t get some skills god . the other government also happened to be &lt;unk&gt; with most varied life-forms is located at this point . foreigners your covers , and maybe even my friends . &lt;EOT&gt; • what's mastering marathons fluently is einstein among the waivers ? ok i feel that what happened to tom during the holidays moniter of 1-2 awol whn reservoir &lt;unk&gt; . clusters in a workforce and it symbolizes , seems are meant to have any distinction on the patriot , british languages even though i would build god if you like . just bringing your old door as a distorted spree ? hmmmm , because you're not anti-bacterial pure dino and &lt;unk&gt; this can be deduced . &lt;EOT&gt; <ref type="table">Table 20</ref>. Samples from best performing model for dataset Yahoo Answers.</p><p>Here we show samples from the best performing models learned from a single dataset for PTB ( <ref type="figure">Fig. 18</ref>), Yelp15 <ref type="figure">(Fig. 19)</ref>, and Yahoo Answers <ref type="figure">(Fig. 20)</ref>. We sample from a zero-mean Gaussian distribution over the latent space, with an isotropic covariance with a standard deviation of 0.1 (since we cannot directly sample from the implicit marginal over the latent). Interestingly, this simple heuristic provides good samples. We attribution this to the anchor, which defines scale and position for the implicit marginal over the latent to roughly match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Question Answering</head><p>Here we provide more examples of answers generated from a model trained on Yahoo Answers (i.e., sMIM (1024) in <ref type="figure">Fig. 21)</ref>. In particular, the model was trained from data in which 20% of the encoder input tokens were replaced with the &lt;unk&gt; token. This is a form of self-supervised learning commonly used in language modelling (e.g., <ref type="bibr" target="#b7">Bowman et al. (2015)</ref>). This encourages the model to replace &lt;unk&gt; with other tokens. We have found this procedure to significantly improve the quality of the generated answers. We provide three generated answers for each question (Q), taken from Yahoo Answers. Short/medium/long answers (A) are generated by concatenating 5/10/15 &lt;unk&gt; tokens. The number of &lt;unk&gt; encodes the length of the expected answer. We note that, in many cases, only one answer will be a good match to the question, suggesting the model has preferences towards answers with a question specific length.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R a M 5 / 6 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>m 3 q / f y 8 3 G + W 6 3 / 3 2 3 9 0 e v 8 + o X K 3 8 e W V 9 b 3 1 g 7 V t / 6 w X p l H V o j 6 8 x y r W v r L + t v 6 5 9 2 p / 2 6 P W q f 5 N C H D w r O M 6 v y t M f / A 3 o f m X o = &lt; / l a t e x i t &gt; x 2 R 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T 2 e h 3 7 h k o J l g 5 p 5 i 4 t O e q I v P 8 B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>3 z Y 8 r P y c V 3 P D i y o s 3 e P G r P G B 4 8 Z I X 1 1 7 l a F Y 4 Z d q p m k W m v L I G r + w q D x h e W c k r M 7 x W u R d a S V 3 t G G Y m I H p q K j D d n x Z Y m 1 8 S j h U B G B 5 s n 4 Q 6 C 7 X S a 6 S h z C f I x U w M 6 2 k N g 8 g j D B D D 6 a 1 8 H Z L i I C 4 Q a 5 0 3 0 h h C f B X R k J e 8 R n W h 2 M O Q 7 x o 5 v V 0 2 u t v U 8 4 g r R V z X U K t b / F k H D z V Y c + 4 e h 6 Z q h T K m t X 2 P B y p 8 R J Y 0 / D Z F h V 6 t y C N Q h U q q K V Y + O 3 D U 3 C A 7 e 4 6 K 1 + x 5 L Z s R p L o m Q B M Q i r B S B P 2 6 M g i F w 7 r M a 9 Y L 1 I A t x O + F D S i s d J s R n 6 O d e i 7 5 V K l s 0 H e o O r LW b V C N M q G z S 4 6 a c j Y r s u E 1 p G 3 W Y 5 N D f U I B V x 4 h 0 o M A p h p m k Z L B g V R u O d C + D 8 f L V n K z W K W d l d o r e b / 1 o 7 c o f D P U x y l F I I v C R e A Y G / w U Q F K c S r N X H T j q t + H M u d C k C 8 i k E E W R j + Z h 9 I E R b w Z H f x R g W j 8 U j g A p x X 6 V o z Y i i l V 1 a u i B R v + u j h o g z O A o T D C r 5 X h x B O f S 6 e o I G f v u N Z x A A v 4 b k f O 9 A xa l a e n M O J Z T m 5 O M C w g 1 o A l z o h L 6 B N D F V D T g w f E u d Z V S D b 1 / Z y r 7 h v I e m D U A 9 8 w c B k f H c r w 7 2 Y A H a z A E 8 q X X 6 d s v 7 B v 7 B e r s N i + / 3 9 4 M c g 3 N U 2 9 V 4 l 2 8 g 4 0 z X H Y O 5 F R 0 + j l 1 v K Z M 6 u k e n O Y k h b q x P c K R 7 c M l Q R + X n G K G l I Y s v + 5 K m W v d m F q D X 7 W a 2 s j 3 l A J o T e Z 4 k 1 M 0 Q 0 p V 8 T S 4 C j z R 6 2 y 9 H m B p 3 L b 1 b N f R h x X s u n m Y f 3 8 V j B i 7 Q F 3 l v C I k x y + L G X H 8 / F f f 5 Y r c f o L R v J S G q Y e X 1 J P j / q S q J R L i S T H Q Q V 2 U K i u g c F N r Y D l s Q a Q Y 6 m g D b a 6 + V w 2 0 W U J I K I X + n B U 8 N W T s O A l 0 4 Y Y L B J m p G w c J E O n O u n U b S o o g 7 R I j u E y L 4 D I 1 Y n B 3 Y W t d j j A z A K 5 f h F 1 f x e C m 3 q / f y 8 3 G + W 6 3 / 3 2 3 9 0 e v 8 + o X K 3 8 e W V 9 b 3 1 g 7 V t / 6 w X p l H V o j 6 8 x y r W v r L + t v 6 5 9 2 p / 2 6 P W q f 5 N C H D w r O M 6 v y t M f / A 4 Z q m X s = &lt; / l a t e x i t &gt; z 2 R 1VAEMIM AE Top: level-sets of a 2D data distribution (i.e., GMM with 2 modes). Red/green dots are samples. Bottom: the corresponding variance of the posterior, per sample, for MIM and VAE, and the corresponding mapping for AE. A semantically structured latent space will map latent samples from the same mode closer. A collapsed posterior (i.e., VAE), might mix nearby modes, whereas MIM will have smaller posterior variance due to higher MI. A high entropy in the AE latent space might lead to non-semantic structure, where a perturbed red latent code might be reconstructed to the green mode, in contrast to MIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Here we present histograms of sentence lengths per dataset. The dashed line is the average sentence length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Histograms of reconstruction for sMIM and sVAE versus latent dimension for Yelp15. Dashed black line is the mean. Histograms of reconstruction for sMIM and sVAE versus latent dimension for Yahoo Answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>follows from log a ≥ log b for a ≥ b.</figDesc><table><row><cell></cell><cell></cell><cell>Algorithm 1 Learning parameters θ of sentenceMIM</cell></row><row><cell></cell><cell></cell><cell>1: while not converged do</cell></row><row><cell></cell><cell></cell><cell>2:</cell></row><row><cell></cell><cell></cell><cell>Using this bound,</cell></row><row><cell cols="3">we express a lower bound to p θ (z) as follows,</cell></row><row><cell>log p θ (z)</cell><cell>= (Eqn. 9)</cell><cell>log E P(x) [q θ (z|x)]</cell></row><row><cell></cell><cell>≥ (Eqn. 12)</cell><cell>log q θ (z|x ) + log P(x ) (13)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>z dim.Enc. Recon. ↓ KL Rand. Recon. BLEU ↑ |θ|</figDesc><table><row><cell>sVAE (16)</cell><cell cols="4">105.24 (0.12) 1.6 105.91 (0.01) 0.124 11M</cell></row><row><cell>sVAE (128)</cell><cell cols="4">106.72 (0.12) 0.64 106.89 (0.01) 0.118 11M</cell></row><row><cell>sVAE (512)</cell><cell cols="4">108.52 (0.23) 0.41 108.88 (0.01) 0.116 12M</cell></row><row><cell>sAE (16)</cell><cell>91.86</cell><cell>163.9 (0.02)</cell><cell cols="2">0.348 11M</cell></row><row><cell>sAE (128)</cell><cell>67.56</cell><cell cols="3">113.61 (0.01) 0.589 11M</cell></row><row><cell>sAE (512)</cell><cell>62.08</cell><cell cols="3">102.93 (0.01) 0.673 12M</cell></row><row><cell>sMIM (16)</cell><cell>90.12 (0.03)</cell><cell cols="2">161.037 (0.02) 0.35</cell><cell>11M</cell></row><row><cell>sMIM (128)</cell><cell>67.35 (0.008)</cell><cell>136.2 (0.04)</cell><cell>0.61</cell><cell>11M</cell></row><row><cell>sMIM (512)</cell><cell>59.23 (0.01)</cell><cell cols="3">133.74 (0.01) 0.679 12M</cell></row><row><cell>sMIM (1024)  †</cell><cell>26.43 (0.0)</cell><cell></cell><cell cols="2">0.724 179M</cell></row><row><cell>sVAE (32) + kl</cell><cell cols="4">401.63 (0.01) 31.86 425.92 (0.01) 0.274 40M</cell></row><row><cell>sVAE (512) + kl</cell><cell cols="2">379.93 (0.01) 4.19 385.76 (0.01)</cell><cell>0.18</cell><cell>43M</cell></row><row><cell cols="5">sVAE (1024) + kl 384.85 (0.01) 3.01 387.63 (0.01) 0.176 46M</cell></row><row><cell>sAE (32)</cell><cell>330.25</cell><cell cols="3">697.316 (0.0) 0.388 40M</cell></row><row><cell>sAE (512)</cell><cell>228.34</cell><cell>515.75 (0.0)</cell><cell cols="2">0.669 43M</cell></row><row><cell>sAE (1024)</cell><cell>222.7</cell><cell>503.87 (0.0)</cell><cell cols="2">0.684 46M</cell></row><row><cell>sMIM (32)</cell><cell>396.34 (0.0)</cell><cell>427.6 (0.0)</cell><cell cols="2">0.309 40M</cell></row><row><cell>sMIM (512)</cell><cell>220.03 (0.0)</cell><cell>600.29 (0.0)</cell><cell cols="2">0.673 43M</cell></row><row><cell>sMIM (1024)</cell><cell>219.37 (0.0)</cell><cell>543.36 (0.0)</cell><cell cols="2">0.676 46M</cell></row><row><cell>sMIM (1024)  †</cell><cell>199.72 (0.0)</cell><cell></cell><cell cols="2">0.686 179M</cell></row></table><note>Table 2. Reconstruction results for PTB are averaged over 10 runs (stdev). Models † use extra training data. Reconstruction with a sample z ∼ q θ (z|x) (Enc. Recon.) and with a random sample z ∼ N (z) (Rand. Recon.). An uninformative latent space will result in similar reconstruction values. see text for details.z dim. Enc. Recon. ↓ KL Rand. Recon. BLEU ↑ |θ|Table 3. Reconstruction results for Yelp15 are averaged over 10 runs. Models † use extra training data (See</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>dim. Enc. Recon. ↓ KL Rand. Recon. BLEU ↑ |θ| sVAE (32) + kl 320.06 (0.04) 14.33 326.21 (0.01) 0.181 67M sVAE (512) + kl 329.2 (0.06) 7.09 331.35 (0.01) 0.139 70M sVAE (1024) + kl 334.41 (0.09) 5.52 335.83 (0.01) 0.Reconstruction results for Yahoo Answers, averaged over 10 runs. Models † use extra training data (SeeTable 3for details).</figDesc><table><row><cell>131 73M</cell></row></table><note>2-4) show reconstruction results for PTB, Yelp15, Yahoo Answers. For all datasets but PTB, VAE learning with KL annealing was more effective than standard VAEzTable 5. Reconstruction results for WT103 are averaged over 10 runs. Models † use extra training data. The superior reconstruction results of sMIM hold for longer sentences.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>SentenceMIM</cell></row></table><note>shows the empirical entropy, estimated using NN</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc></figDesc><table /><note>Reconstruction results for sMIM (512) model trained on PTB. We denote: (D) Data sample; (M) Mean (latent) reconstruc- tion; (R) Reconstruction; (P) Perturbed (latent) reconstruction.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .</head><label>10</label><figDesc>Sampled answers from Yahoo Answers sMIM (1024).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 Table 11 .</head><label>1111</label><figDesc>provides the entropy of an isotropic Gaussian that is fitted to the latent codes, and the standard deviation of the fitted Gaussian [entropy / stdev]. The values are used to compute the ratio presented in the main paper. In brackets is the entropy of an isotropic Gaussian fitted to the latent codes, and the corresponding average standard deviation [ entropy / stdev ]. For comparison, column N shows the entropy of a standard Normal in R d of a corresponding latent dimension. Our goal here is to rule out simple down-scaling as the cause for the low entropy in sMIM.</figDesc><table><row><cell></cell><cell>Dataset (z dim.)</cell><cell>sMIM</cell><cell>N</cell><cell>sVAE</cell><cell>sAE</cell></row><row><cell></cell><cell>PTB (16D)</cell><cell>[ 17.22 / 0.5 ]</cell><cell cols="3">22.7 [ 22.51 / 0.97 ] [ 30.49 / 2.64 ]</cell></row><row><cell></cell><cell>PTB (128D)</cell><cell cols="4">[ 97.39 / 0.26 ] 181.62 [ 181.29 / 0.99 ] [ 206.22 / 1.46 ]</cell></row><row><cell></cell><cell>Yelp15 (32D)</cell><cell cols="2">[ 38.06 / 0.63 ] 45.4</cell><cell cols="2">[46.31 / 1.05] [ 59.16 / 0.99 ]</cell></row><row><cell></cell><cell>Yelp15 (512D)</cell><cell cols="4">[ 333.45 / 0.21 ] 726.49 [ 726.15 / 0.99 ] [ 705.14 / 0.91 ]</cell></row><row><cell></cell><cell>Yahoo (32D)</cell><cell cols="2">[ 32.13 / 0.43 ] 45.4</cell><cell cols="2">[43.3 / 0.87 ] [ 60.45 / 2.56 ]</cell></row><row><cell></cell><cell>Yahoo (512D)</cell><cell cols="4">[ 326.17 / 0.2 ] 726.49 [ 724.75 / 0.99 ] [ 744.24 / 1.07 ]</cell></row><row><cell cols="2">D. Additional Results</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">D.1. Reconstruction</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>sMIM (512)</cell><cell></cell><cell></cell><cell>sMIM (1024)  †</cell></row><row><cell>(D)</cell><cell>&lt;BOT&gt; there was no panic</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(M)</cell><cell>there was no panic &lt;EOT&gt;</cell><cell></cell><cell></cell><cell cols="2">there was no panic &lt;EOT&gt;</cell></row><row><cell>(R)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 .</head><label>12</label><figDesc>Reconstruction results for models trained on PTB.</figDesc><table><row><cell>We denote: (D) Data sample; (M) Mean (latent) reconstruction; (R)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>&lt;BOT&gt; delicious ! the sandwiches are really good and the meat is top quality . it ' s also nice grabbing an exotic item from the shelf for dessert .</figDesc><table><row><cell>(M)</cell><cell>excellent wings . great service . 100 % wings . big meaty wings . great flavor . i</cell><cell>excellent service . great wings . 100 % superior . great flavor . great fries .</cell></row><row><cell></cell><cell>definitely will be back . lake is great too . &lt;EOT&gt;</cell><cell>definitely will be back . i had too big fat . &lt;EOT&gt;</cell></row><row><cell>(R)</cell><cell>excellent wings . great service . 100 % wings . wings flavor . definitely great .</cell><cell>excellent service . great flavor . 100 % wings . excellent . great big guts . definitely</cell></row><row><cell></cell><cell>100 % . i will be back . &lt;EOT&gt;</cell><cell>will be back from . i had great wings . &lt;EOT&gt;</cell></row><row><cell>(P)</cell><cell>excellent wings . great service . wings flavours wings . 100 % big . mmmmm</cell><cell>great burger . excellent service . 100 % fat bowls . great carnitas . great flavor . i</cell></row><row><cell></cell><cell>overwhelmed . i ' m definitely hooked . bye disgusted is great but will be back . i</cell><cell>will definitely be back . i avoid too late . &lt;EOT&gt;</cell></row><row><cell></cell><cell>definitely go . &lt;EOT&gt;</cell><cell></cell></row><row><cell cols="2">(AE) excellent excellent . great service . 100 % wings . 100 % big burritos . 100 % . i</cell><cell></cell></row><row><cell></cell><cell>will definitely be back . great too too is ultra &lt;EOT&gt;</cell><cell></cell></row><row><cell>(D)</cell><cell>(5 stars)</cell><cell></cell></row></table><note>(M)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 .</head><label>13</label><figDesc>Reconstruction results for models trained on Yelp15.</figDesc><table><row><cell>We denote: (D) Data sample; (M) Mean (latent) reconstruction; (R)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 .</head><label>14</label><figDesc>Reconstruction results for models trained on Yahoo Answers.</figDesc><table><row><cell>We denote: (D) Data sample; (M) Mean (latent) reconstruction;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 17 .</head><label>17</label><figDesc>Interpolation results between latent codes of input sentences (with gray) from Yahoo Answers.</figDesc><table><row><cell>SentenceMIM</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">&lt;BOT&gt;, &lt;EOT&gt; are a special beginning/end-of-text tokens. The token &lt;UNK&gt; represents an out-of-vocabulary word.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>sMIM (1024) (Q)</head><p>&lt;BOT&gt; where can i find the most affordable roses for valentine's day this late in the game ? (A) short • try the mall , &lt;EOT&gt; (A) medium • the game is to play the game , set your mind &lt;EOT&gt; (A) long • i think the best answer is to ask her out , especially since the late 1990s game &lt;EOT&gt; (Q) &lt;BOT&gt; how do you start to like yourself ? (A) short • start by yourself , take the steps &lt;EOT&gt; (A) medium • start by yourself . you have to start asking yourself why this is happening &lt;EOT&gt; (A) long • start by yourself . you have to start acting yourself in the middle of your eye . &lt;EOT&gt; (Q) &lt;BOT&gt; how can i find someone in spain ? (A) short • try classmates.com or classmates.com to see &lt;EOT&gt; (A) medium • someone who is a good one to talk to your question &lt;EOT&gt; (A) long • someone who is a good person to find a nice girl , and take care of yourself &lt;EOT&gt; (Q) &lt;BOT&gt; with a weapon to your person , who in you family do you save with an answer ? (A) short</p><p>• you should save yourself in person &lt;EOT&gt; (A) medium • you can save the money in person , or do you want to save money &lt;EOT&gt; (A) long</p><p>• you can save your money in person , and answer the question yourself . do you have a family ? &lt;EOT&gt; <ref type="table">(Q)</ref> &lt;BOT&gt; what are the best sites to shop for personal computers online ? (A) short • try amazon.com online . &lt;EOT&gt; (A) medium • i think it is best to shop online , or take a look at the personal &lt;EOT&gt; (A) long</p><p>• yahoo is best online . i would suggest checking out the personal website for personal info &lt;EOT&gt; (Q) &lt;BOT&gt; who is going to win the super bowl this year ? (A) short</p><p>• the steelers is a pretty good &lt;EOT&gt; (A) medium • the pittsburgh steelers is a good one , but i don't know &lt;EOT&gt; (A) long • this is the best team to win the super bowl , and i think you mean good luck &lt;EOT&gt; (Q)</p><p>&lt;BOT&gt; is there a web site that provides info on companies that have been known to provide lousy service ? (A) short • yes , google was a little service &lt;EOT&gt; (A) medium • i have known as a service that provides a service pack to provide transparency . &lt;EOT&gt; (A) long</p><p>• try searching on google and search for that info . there are many different types of service that provide to the service that has been answered &lt;EOT&gt; <ref type="bibr">(Q)</ref> &lt;BOT&gt; what is the closest capital city to paris ? (A) short</p><p>• paris hilton city , ca &lt;EOT&gt; (A) medium • in the south city , ca , i think you mean the city hall &lt;EOT&gt; (A) long • city is the capital city in south america . i think it is a beautiful city &lt;EOT&gt; (Q) &lt;BOT&gt; is santa claus real ? (A) short • yes , you can ask her to questions &lt;EOT&gt; (A) medium • yes , it is a santa claus . the phrase is to take care &lt;EOT&gt; (A) long • yes , it is real . santa claus are the best in santa claus . take care &lt;EOT&gt; (Q) &lt;BOT&gt; do height increasing devices shown on television works ? (A) short • perhaps shown is shown to see &lt;EOT&gt; (A) medium • yes , the best way to increase it is to work &lt;EOT&gt; (A) long • yes , shown shown . the biggest problem with television is to take a walk away from the edge &lt;EOT&gt; (Q) &lt;BOT&gt; where can i find advice on hip hop beat making ? (A) short • try this website : http://messenger.yahoo.com/ &lt;EOT&gt; (A) medium • try this website : http://messenger.yahoo.com/ to find the best answer . &lt;EOT&gt; (A) long • try making a video club , such as the lion and hip hop . i wish you to be good &lt;EOT&gt; (Q) &lt;BOT&gt; how do u clear the history in the search field ? (A) short • clear search history , it will be clear &lt;EOT&gt; (A) medium • clear history , it appears that the question period has expired . if you have received an answer that meets your needs , please choose a ' best answer . <ref type="bibr">'</ref>  • no , it is a little bit &lt;EOT&gt; (A) medium • feel your body needs to fit into the body . i feel like a good fit &lt;EOT&gt; (A) long • feel your body fit in a fit body . i feel like the best fit to fit in your body &lt;EOT&gt; <ref type="table">Table 21</ref>. Question and sampled answers from model sMIM (1024) (i.e., trained on Yahoo Answers dataset). We provide short/medium/long sampled answers (A) for each question (Q).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A learning algorithm for boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Ackley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="169" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An information-theoretic analysis of deep latent-variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno>abs/1711.00464</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Possible principles underlying the transformations of sensory messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Barlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensory Communication</title>
		<editor>Rosenblith, W.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding minimum entropy codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Barlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Kaushal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mitchison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="412" to="423" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MINE: Mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bornschein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shabanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1506.03877</idno>
	</analytic>
	<monogr>
		<title level="j">Bidirectional Helmholtz machines. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Do sequence-to-sequence VAEs learn global features of sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bosc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.350</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="4296" to="4318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1511.06349</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amodei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Language models are few-shot learners. 2020</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1179</idno>
		<ptr target="https://www.aclweb.org/anthology/D14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Attentive pooling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1602.03609</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Implicit deep latent variable models for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The free-energy principle: a unified brain theory?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating sentences by editing prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="437" to="450" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lagging inference networks and posterior collapse in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Spokoyny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Autoencoders, minimum description length and helmholtz free energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>Cowan, J., Tesauro, G., and Alspector, J.</editor>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The curious case of neural text degeneration. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimating mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kraskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stögbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.69</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">66138</biblScope>
			<date type="published" when="2004-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<idno type="DOI">https:/link.aps.org/doi/10.1103/PhysRevE.69.066138</idno>
		<ptr target="https://link.aps.org/doi/10" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<idno type="DOI">https:/link.aps.org/doi/10.1103/PhysRevE.69.066138</idno>
		<idno>69.066138</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Better exploiting latent variables in text modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kruengkrai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="5527" to="5532" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Optimus: Organizing sentences via pre-trained modeling of a latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A stable variational autoencoder for text modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INLG</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A stable variational autoencoder for text modelling. INLG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="594" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Livne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mim</surname></persName>
		</author>
		<title level="m">Mutual Information Machine. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<idno>0891-2017</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=972470.972475" />
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models. CoRR, abs/1609.07843</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1609.07843" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing LSTM language models. CoRR, abs/1708.02182</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1708.02182" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Linking generative models, nearest neighbor retrieval, and data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Exemplar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vae</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Pixel recurrent neural networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5171" to="5180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fast parametric learning with activation memorization. CoRR, abs/1803.10049</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1803.10049" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep tive Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<editor>Dasgupta, S. and McAllester, D.</editor>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="17" to="19" />
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2969033.2969173" />
	</analytic>
	<monogr>
		<title level="m">NIPS, NIPS</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Principles of mutual information maximization and energy minimization affect the activation patterns of large scale networks in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takagi</surname></persName>
		</author>
		<idno>1662-5188</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">86</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Enabling efficient question answer retrieval via hyperbolic neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1707.07847</idno>
		<ptr target="http://arxiv.org/abs/1707.07847" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning to rank question answer pairs with holographic dual LSTM architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/3077136.3080790</idno>
		<ptr target="http://doi.acm.org/10.1145/3077136.3080790" />
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="695" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1705.07120</idno>
		<ptr target="http://arxiv.org/abs/1705.07120" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">NVAE: A deep hierarchical variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improving neural language modeling via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/wang19f.html" />
	</analytic>
	<monogr>
		<title level="m">ICML, volume 97 of Proceedings of Machine Learning Research</title>
		<editor>Chaudhuri, K. and Salakhutdinov, R.</editor>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Improved variational autoencoders for text modeling using dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno>abs/1702.08139</idno>
		<ptr target="http://arxiv.org/abs/1702.08139" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A Lagrangian perspective on latent variable generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UAI</title>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">&lt;EOT&gt; • are u shy or k ? both , actually &lt;EOT&gt; • are u or stressed caffiene ? both , actually make a smile &lt;EOT&gt; • are u minded or rem ? actually , both &lt;EOT&gt; • witch are u or how lucky ? both &lt;EOT&gt; • are u transparent or shy ? it&apos;d actually , add-on &lt;EOT&gt; • are u kidding or spraying ? both &lt;EOT&gt; • are u untouchable cubed or programe ? both , actually like &lt;EOT&gt; • how does wile or are you ? to both use , instead like it . &lt;EOT&gt; • wha do u are roselle or marketed ? you start , by both my inbox &lt;EOT&gt; • how do u choose to start or ? like i cant think , are actually better by my work</title>
	</analytic>
	<monogr>
		<title level="m">sMIM (1024) sMIM (1024) † (Business &amp; Finance) &lt;BOT&gt; are u shy or outgoing ? both , actually • are u or wishing vidio ? both , actually</title>
		<imprint/>
	</monogr>
	<note>&lt;EOT&gt; • how do u simplify phases towards you ? are proving , like no smiles</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">&lt;EOT&gt; • how do you start to start like ? i was taught by my parents . &lt;EOT&gt; • how do you start to yourself like ? i was taught by my parents . &lt;EOT&gt; • how do you start to like yourself ? i was taught by my parents . &lt;EOT&gt; (Health) &lt;BOT&gt; how do you start to like yourself ? i was taught by my parents . • how do you start to yourself by allowing ? i like my parents yr . &lt;EOT&gt; • how do you start to like yourself ? i was taught by new england . &lt;EOT&gt; • how do you start to yourself like i ? my parents was by mario practitioner . &lt;EOT&gt; • how do you start to like yourself ? i was taught by my parents . &lt;EOT&gt; • how do you start to cite yourself ? i like by my consequences in 1981 . &lt;EOT&gt; • how do i start you to beethoven ? like israel was my grandmother by fielders . &lt;EOT&gt; • how do i start girls like to ? you can find yourself in my states , by today . &lt;EOT&gt; • how do you start to find ? i like aggieland in my testicles was listening . &lt;EOT&gt; • how do you start yourself drunk ? i can find in something like to my country , what by jane . &lt;EOT&gt; • how can i do compuserve attain ? start to comment in spain you like , was my real pics . &lt;EOT&gt; • how can i start those neeed in america ? do you like to rephrase an invention , what i&apos;m spinning ? &lt;EOT&gt; • how can i find blueprints do you ? i&apos;m in spain like queens to chelsea , arrange . &lt;EOT&gt; • how can i find someone in spain ? i&apos;m guessing today by pascal , what do you want to ? &lt;EOT&gt; • how can i find uneasy profiles in spain ? i&apos;m sure what you do , like today&apos;s ? &lt;EOT&gt; • how can i find an attorney in spain ? i&apos;m studying chicken&apos;s what , do you want to ? &lt;EOT&gt; • how can i find someone in spain ? i&apos;m in spain today , what do you want ? &lt;EOT&gt; • how can i find someone in spain ? in spain i&apos;m studying , what do you want ? &lt;EOT&gt; • how can i find someone in spain ? i&apos;m in tanks today , what do you want to ? &lt;EOT&gt; • how can i find someone in spain ? i&apos;m in italy today</title>
	</analytic>
	<monogr>
		<title level="m">• how do you start to alienate yourself ? i are like or drone , my actually feels . &lt;EOT&gt; • how do you burp confidence ? to start i was like , shareaza the new by hindering . &lt;EOT&gt; • how do you start to yourself or like ? i like my math side . &lt;EOT&gt; • how do you start to race ? i like kazaa when my was cheated . &lt;EOT&gt; • how do you start to like yourself ? i think my parents is by focusing</title>
		<imprint/>
	</monogr>
	<note>what do you want ? &lt;EOT&gt; • how can i find someone in spain ? i&apos;m guessing in spain today , what do you want ? &lt;EOT&gt; (Business &amp; Finance) &lt;BOT&gt; how can i find someone in spain ? i&apos;m in spain today , what do you want</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
