<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Summarization with Attention-Based Encoder-Decoder Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailin</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Xi&apos;an Institute of Optics and Precision Mechanics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Video Summarization with Attention-Based Encoder-Decoder Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video summarization</term>
					<term>LSTM</term>
					<term>encoder-decoder</term>
					<term>attention mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of supervised video summarization by formulating it as a sequence-to-sequence learning problem, where the input is a sequence of original video frames, the output is a keyshot sequence. Our key idea is to learn a deep summarization network with attention mechanism to mimic the way of selecting the keyshots of human. To this end, we propose a novel video summarization framework named Attentive encoder-decoder networks for Video Summarization (AVS), in which the encoder uses a Bidirectional Long Short-Term Memory (BiLSTM) to encode the contextual information among the input video frames. As for the decoder, two attention-based LSTM networks are explored by using additive and multiplicative objective functions, respectively. Extensive experiments are conducted on two video summarization benchmark datasets, i.e., SumMe, and TVSum. The results demonstrate the superiority of the proposed AVS-based approaches against the state-of-the-art approaches, with remarkable improvements from 0.8% to 3% on two datasets, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-This paper addresses the problem of supervised video summarization by formulating it as a sequence-to-sequence learning problem, where the input is a sequence of original video frames, the output is a keyshot sequence. Our key idea is to learn a deep summarization network with attention mechanism to mimic the way of selecting the keyshots of human. To this end, we propose a novel video summarization framework named Attentive encoder-decoder networks for Video Summarization (AVS), in which the encoder uses a Bidirectional Long Short-Term Memory (BiLSTM) to encode the contextual information among the input video frames. As for the decoder, two attention-based LSTM networks are explored by using additive and multiplicative objective functions, respectively. Extensive experiments are conducted on two video summarization benchmark datasets, i.e., SumMe, and TVSum. The results demonstrate the superiority of the proposed AVS-based approaches against the state-of-the-art approaches, with remarkable improvements from 0.8% to 3% on two datasets, respectively.</p><p>Index Terms-Video summarization, LSTM, encoder-decoder, attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>V IDEO is inundating the Internet social platform. There are more than 300 hours video upload per minute to YouTube. It is awfully time-consuming to browse these videos. According to Ciscos 2015 Visual Networking Index, it will take over 500 million years to watch all videos uploaded to Internet per month in the year of 2020! It is therefore becoming increasingly important to efficiently browse, manage, and retrieve these videos.</p><p>Video summarization is one of the promising techniques to address this challenge <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Its goal is to produce a compact yet comprehensive summary to enable an efficient browsing experience. An ideal video summarization is that can provide users the maximum information of the target video with the shortest time. It is also useful for many other practical applications, such as video indexing <ref type="bibr" target="#b6">[7]</ref>, video retrieval <ref type="bibr" target="#b7">[8]</ref>, and event detection <ref type="bibr" target="#b8">[9]</ref>.</p><p>Generally, there are two types of video summarization: storyboard and video skim. Specifically, a storyboard is based on a set of keyframes, and a video skim is composed of a number of representative video segments, called keyshots. In this work, we focus on video skim. However, it can be easily converted to the form of storyboard by selecting one or several keyframes from each keyshot.</p><p>Video summarization has been studied over two decades <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b5">[6]</ref>. During these years, many approaches have been developed by exploring cues ranging from low-level visual inconsistency <ref type="bibr" target="#b9">[10]</ref>  <ref type="bibr" target="#b10">[11]</ref>, attention <ref type="bibr" target="#b2">[3]</ref> [5] <ref type="bibr" target="#b25">[26]</ref>, to high-level semantic change of concepts <ref type="bibr" target="#b5">[6]</ref>  <ref type="bibr" target="#b12">[13]</ref> and entities in videos <ref type="bibr" target="#b11">[12]</ref>  <ref type="bibr" target="#b33">[34]</ref>. However, most of these studies focus on unsupervised leaning technique. Recently, the research focus has been extending to supervised learning approaches <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b18">[19]</ref>, which aims at explicitly learning the summarizing capability from the human labels. Usually, supervised approaches have better performance than unsupervised ones.</p><p>Among the previous supervised approaches, studies in <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b17">[18]</ref> are attractive ones. They treat video summarization as a sequence-to-sequence learning problem, where the input is the original video frame sequence and the output is the keyframe/keyshot sequence. To obtain a good video summarization, the complex and heterogeneous inter-dependency should be well considered. Both studies explore the encoderdecoder framework with Long Short-Term Memory (LSTM) technique to model the variable-range dependencies in video summarization. As a specific type of Recurrent Neural Network (RNN), LSTM has shown its effectiveness in modeling long-range dependencies where the influence by the distant states on the present and future states can be adaptively adjusted and data-dependent <ref type="bibr" target="#b19">[20]</ref>. Therefore, both <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b17">[18]</ref> achieve state-of-the-art performances.</p><p>However, one main drawback in such an encoder-decoder framework <ref type="bibr" target="#b16">[17]</ref>  <ref type="bibr" target="#b17">[18]</ref> is that it encodes all the necessary information in one single context vector no matter how long the input sequence is. Thus, the length of the intermediate code is fixed in their encoder-decoder models, which incapacitates it to give different weights to different frames in the input sequence explicitly. In this situation, all the shots/frames in the input video sequence have the same importance no matter what kind of output shots/frames are to be predicted. Due to this indiscriminate averaging of all the frames, both approaches <ref type="bibr" target="#b16">[17]</ref>  <ref type="bibr" target="#b17">[18]</ref> risks ignoring much of the temporal structure underlying the video. For example, considering summarizing a video "leave home to walk dog and then come back". Since the video frames related to the "home scene" are visually similar, it is hard for both approaches to tell the order of appearances from the collapsed vectors.</p><p>To this end, we explore the attentive encoder-decoder framework to tackle this problem in video summarization. The framework employs attention mechanism in the encoderdecoder framework by conditioning the generative process in the decoder on the encoder hidden states, rather than on one single context vector only. We name this framework Attentive encoder-decoder networks for Video Summarization (AVS). In specific, we use attention mechanism <ref type="bibr" target="#b20">[21]</ref>  <ref type="bibr" target="#b21">[22]</ref> in the AVS framework, which can assign importance weights to different shots/frames of the input instead of treating all the input ones equally. In this way, it provides the inherent relations between the input video sequence and the output keyshots. <ref type="figure" target="#fig_1">Figure  1</ref> shows an overview of AVS framework. Compared with previous work, this paper has several essential characteristics worth being highlighted: 1) It proposes an Attentive encoder-decoder framework for Video Summarization, named AVS. It is a supervised-based video summarization framework, which mimics the way of selecting the keyshots of human. To the best of our knowledge, this attentive encoder-decoder framework has not previously proposed for implementing video summarization.</p><p>2) It investigates the attention-based LSTM mechanism in the AVS framework, and develops two approaches to generate the video summarization. One is based on additive attention mechanism named A-AVS, the other is based on multiplicative attention mechanism, named M-AVS.</p><p>3) Extensive experiments are conducted on two popular video summarization datasets, including both the edited and raw video datasets. The results show the proposed M-AVS and A-AVS approaches consistently outperform the state-of-the-art ones by at least 0.8%, 3.1%on SumMe and TVSum datasets, respectively. These promising results verify the effectiveness of the proposed AVS framework.</p><p>The remainder of this paper is organized as follows. Section II reviews the related video summarization methods. Section III introduces the proposed AVS framework and two specific approaches. Section IV presents the experimental results and analysis. Finally, conclusions and future work are provided in Section V.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>According to the number of videos to be summarized, there are Single-Video Summarization (SVS) and Multi-Video Summarization (MVS). Specifically, SVS aims at digesting one individually long video <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref>, while MVS aims at summarizing a large number of short videos obtained by a query to web videos <ref type="bibr" target="#b3">[4]</ref> [31] [32] <ref type="bibr" target="#b35">[37]</ref>. MVS may also called query-based video summarization, and its processing method is generally different from that of SVS since it has to handle diverse query-based videos and can take advantage of the query information. Furthermore, there is a direction of studying multi-view video summarization, which mainly used in surveillance scenarios to compact the videos captured from different cameras <ref type="bibr" target="#b38">[40]</ref>. In our work, we focus on SVS.</p><p>From the perspective of the learning model, there are unsupervised and supervised video summarization approaches. In the following, we will introduce their related work in detail. In particular, our work is a supervised approach. Additionally, since our work applies attention-based LSTM network and LSTM is a special type of RNN, we will further review the existing RNN-based and attention-based video summarization approaches, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unsupervised and Supervised Video Summarization</head><p>Unsupervised approaches dominate the field of video summarization for a long time. They are generally designed to make the summarization meets the desired properties, such as conciseness, representativeness, and informativeness. Thus, the corresponding selection criteria for summaries include content frequency <ref type="bibr" target="#b26">[27]</ref>  <ref type="bibr" target="#b27">[28]</ref>, coverage <ref type="bibr" target="#b28">[29]</ref>  <ref type="bibr" target="#b29">[30]</ref>, relevance <ref type="bibr" target="#b3">[4]</ref> [24] <ref type="bibr" target="#b24">[25]</ref>, and user's attention <ref type="bibr" target="#b2">[3]</ref> [26], etc. According to these different criteria, numerous approaches have been developed. Among them, clustering-based methods are the most popular ones <ref type="bibr" target="#b26">[27]</ref>  <ref type="bibr" target="#b27">[28]</ref>. It clusters the visually similar frames or shots into groups, in which the group centers are considered as the representative elements of the video and therefore selected as the keyframes or keyshots. Dictionary learning is another popular technique used in unsupervised video summarization <ref type="bibr" target="#b28">[29]</ref>  <ref type="bibr" target="#b29">[30]</ref>. It regards the base vectors in the dictionary model as the keyframes or keyshots since they can maximally reconstruct the visual content of the original video.</p><p>Recently, supervised video summarization approach has also received much research focus. It takes videos and their humanlabeled summaries as training data to seek supervised learning methods to explicitly learn how human would summarize videos. For example, Gong et al. <ref type="bibr" target="#b13">[14]</ref> treat video summarization as a supervised subset selection problem, and present a probabilistic model called sequential Determinatal Point Process (seqDPP) to learn how a diverse and representative subset is selected from the training set. Potapov et al. <ref type="bibr" target="#b23">[24]</ref> train a set of SVM classifiers to score each segment in a video with importance score, and those segments with higher scores constitute a video summary.</p><p>Besides, some work tend to directly optimize the multiple objectives for video summarization. For instance, Gygli et al. <ref type="bibr" target="#b14">[15]</ref> learn to combine the criteria of representative, relevance, and uniformity to ensure the generated summaries are the most consistent with the reference ones. Specifically, they develop several submodular functions for these criteria and learn a linear combination of them using structured learning with a large margin formulation. Similarly, Li et al. <ref type="bibr" target="#b18">[19]</ref> design four functions for the criteria of representativeness, importance, diversity and storyness, respectively. And then, they build a score function to linearly combine the four functions with the maximum margin algorithm. Particularly, their proposed framework is general for both edited and raw video summarization. More recently, some deep architectures with RNN network for supervised video summarization have also been proposed <ref type="bibr" target="#b16">[17]</ref>  <ref type="bibr" target="#b17">[18]</ref>, which will be introduced in the next sub-section in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RNN-Based Video Summarization Approaches</head><p>To the best of our knowledge, the only existing RNN-based video summarization approaches are <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b17">[18]</ref>. In <ref type="bibr" target="#b16">[17]</ref>, video summarization is considered as a structured prediction problem on sequential data, and a bidirectional LSTM is used to model the variable-range dependency in the video. The method is called vsLSTM. Specifically, its input is a sequence of video frames and its output is a binary indicator vector (being selected or not) or frame-level importance scores. To enhance the diversity, the authors further introduce Determinatal Point Process (DPP) algorithm to vsLSTM, which is called dppLSTM. In <ref type="bibr" target="#b17">[18]</ref>, an unsupervised generative adversarial learning model is presented, which is called SUM-GAN. Specifically, the generator is an autoencoder LSTM. Its goal is to select video frames and decode the obtained summarization for reconstructing the input video. In contrast, the discriminator is another LSTM network aiming at distinguishing between the original video and its reconstruction from the generator. Furthermore, the authors also extend SUM-GAN method to a supervised setting by adding a sparse regularization with the ground-truth summarization labels, the corresponding method is named SUM-GAN sup . Both methods achieve the state-ofthe-art performances in the field of video summarization. In this paper, we treat video summarization as a sequential encoder-decoder problem, and formulate it with an attentionbased LSTM framework.</p><p>Video highlight <ref type="bibr" target="#b22">[23]</ref> and storyline <ref type="bibr" target="#b31">[32]</ref> have similar goals to video summarization, thus we also give brief views for existing methods using RNN network in both directions. Specifically, video highlight is a moment of major or special interest in a video. Yang et al. <ref type="bibr" target="#b22">[23]</ref> cast it as an outlier detection problem where the non-highlights are considered as outliers. Then, they apply recurrent autoencoder with LSTM cells to model temporal dependencies to identify video highlights. Sigurdsson et al. <ref type="bibr" target="#b31">[32]</ref> propose a Skipping Recurrent Neural Network (S-RNN) to learn a storyline from a photo stream. The goal of storyline is to learn the underlying visual appearances and temporal dynamics simultaneously when given hundreds of albums for a concept. Specifically, S-RNN skips through the photo sequences to extract the common latent stories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attention-Based Video Summarization Approaches</head><p>Users attention implies the concentration of mental powers upon a video segment <ref type="bibr" target="#b2">[3]</ref> [5] <ref type="bibr" target="#b25">[26]</ref>. If a video segment captures much attention of a user, it is more important and more likely to be a keyshot. Existing methods usually apply low-level features, such as motion and face to score the importance of video segments by modeling the users attention. These scores join together to form an attention cue, and those on the curve crests are extracted as the keyshots to construct the summarization.</p><p>For example, Ma et al. <ref type="bibr" target="#b2">[3]</ref> present a set of attention models via multiple sensory perceptions, such as motion, static, face, camera attention, and audio saliency. Then, these models are fused linearly and nonlinearly, respectively. Ejaz et al. <ref type="bibr" target="#b25">[26]</ref> explore the static attention by using the image signature based saliency detection method, and model the dynamic attention with temporal gradients. Then, they combine both attention models non-linearly to build video summarization. Ngo et al. <ref type="bibr" target="#b9">[10]</ref> represent a video with a temporal graph of scenes, shots and sub-shots, where motion-based attention values are attached to each node. By modeling the evolution of a video through the temporal graph, the scene changes can be detected and the summary can be generated. More recently, to reduce the computational cost on computing the attention clues, Zhang et al. <ref type="bibr" target="#b4">[5]</ref> propose a simple but effective motion state change model by using a spatiotemporal slice to analyze the attention curve.</p><p>Although these attention modeling schemes have proved to be effective in video summarization, there are still some drawbacks. On the one hand, the attention curve is usually constructed with one or several low-level features. However, one feature cannot well reflect the users attention, and several features cannot typically guarantee a correlation with what the user is interested in <ref type="bibr" target="#b34">[35]</ref>. On the other hand, due to the unsupervised characteristics, the existing attention-based approaches cannot take advantage of human guidance. In contrast, our proposed AVS framework can well utilize this guidance since it learns the attention mechanism in a supervised manner. Moreover, its deep neural network framework also guarantee it can capture the complex attention mechanism of viewers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED AVS FRAMEWORK</head><p>We formulate video summarization as a sequence-tosequence learning problem, where the input is a sequence of video frames , and the output is a sequence of keyshot. The flowchart of AVS framework is illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>. It consists of two components: an encoder-decoder model and a keyshot selection model. Particularly, the encoder-decoder model consists of an encoder and a decoder. It measures the importance of each frame. The key shots selection model aims at converting the frame-level importance scores into shot-level scores and generating summary with a length budget <ref type="bibr" target="#b24">[25]</ref>.</p><p>In this section, we first introduce the encoder network with a bidirectional LSTM, and then present the decoder network with attention mechanism, finally introduce the keyshot selection model briefly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Encoder with Bidirectional LSTM Network</head><p>In a common encoder-decoder framework, an encoder converts the input sequence X = {x 1 , x 2 , ..., x T } into a repre- where h t ∈ R n is a hidden state at time t. The architecture of an encoder φ depends on the input in a specific application. For instance, in the application of image caption <ref type="bibr" target="#b32">[33]</ref>, Convolutional Neural Network (CNN) is a good choice. In the case of machine translation [21] <ref type="bibr" target="#b21">[22]</ref>, it is natural to use a RNN as the encoder, since its input is a variable-length sequence of symbols. When applied to video summarization, LSTM is the most suitable algorithm <ref type="bibr" target="#b16">[17]</ref> [18] since the contextual information around a specific frame is necessary for generating a video summary. It is because human relies on high-level semantic understanding of the video contents, usually after viewing the whole sequence can she/he decide which frame or shot should be selected into the summary. For example, considering summarizing a basketball game video, only a key ball that affects the game process should be selected into the summary. However, there are many goals in a basketball game, thus it is necessary to combine the scene before and after the goal to determine whether a goal is a key ball.</p><formula xml:id="formula_0">sentation vector v= {v 1 ,v 2 , · · · ,v T }. v t h t = φ(x t ),<label>(1)</label></formula><formula xml:id="formula_1">… … Backward LSTM Forward LSTM 2 v 1 v 3 v T v 1 h  2 h  3 h  T h  1 h  2 h  3 h  T h  1 x 2 x 3 x T x</formula><p>Inspired by outstanding performance of Bidirectional Long Short-term Memory (BiLSTM) to encode the necessary information in a sequence <ref type="bibr" target="#b36">[38]</ref>, we select it as an encoder for taking the temporal relation of video frames into consideration. The principle of BiLSTM is to split the neurons of a regular LSTM <ref type="bibr" target="#b29">[30]</ref> into two directions, one for positive time direction (forward states), and the other for negative time direction (backward states). Moreover, those two states outputs are not connected. By utilizing the two-time directions, the sequential information from the past and future of the current frame can be used.</p><p>The flowchart of BiLSTM is shown in the encoder part of <ref type="figure" target="#fig_2">Fig. 2</ref>. First, the forward LSTM reads the input sequence in its forward direction (from x 1 to x T ) and calculates the forward hidden states ( − → h 1 , · · · , − → h T ). Meanwhile, the backward LSTM reads the sequence in the reverse order, resulting in a sequence of backward hidden states( ← − h 1 , · · · , ← − h T ). Then we obtain an annotation v t for each x t by concatenating the forward hidden state − → h t and the backward one ← − h t . That is to say, the annotation v t incorporates the information of both the preceding frames and the following frames. Due to the time tendency of an LSTM, the annotation v t can focus on the frames around x t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Decoder with Attention Mechanism</head><p>A decoder generates the corresponding output sequence Y = {y 1 , · · · , y m } with the representation vector from the encoder. Similar to that in the encoder, the architecture of the decoder ψ is determined by the output in a specific application. In the application of video summarization, LSTM is the preferred decoder model since it runs sequentially over the output sequence <ref type="bibr" target="#b17">[18]</ref>. Generally speaking, there is a contextual relationship for each frame in a video. Due to the importance scores among frames are basically continuous in a video shot and varied among the shots, a decoder should learn the long term and short term dependency among these scores. An LSTM decoder can be written as:</p><formula xml:id="formula_2">p(y t |{y i |i &lt; t}, v) s t = ψ(s t−1 , y t−1 , v).<label>(2)</label></formula><p>However, the representation vector v in Eq. (2) is a fixed length encoding vector and cannot accurately describe the temporal characteristics of a video. To exploit the temporal ordering across the entire video, we introduce attention mechanism <ref type="bibr" target="#b20">[21]</ref> [22] to it. Then the decoder can be changed as:</p><formula xml:id="formula_3">V t = n i=1 α i t v i , s.t. n i=1 α i t = 1,<label>(3)</label></formula><formula xml:id="formula_4">p(y t |{y i |i &lt; t}, V t ) s t = ψ(s t−1 , y t−1 , V t ),<label>(4)</label></formula><p>where V t stands for the attention vector at moment t. The attention weight α i t is a parameter to trade-off the inputs and the encoder vector. The attention mechanism allows the decoder to selectively focus on only a subset of inputs by increasing their attention weights. The attention mechanism in the LSTM decoder is shown in <ref type="figure">Fig. 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. The attention weight</head><formula xml:id="formula_5"> 2 t  n t  ... ... 1 t s  1 V n i t t i i v     2 t s  1 2 T x x x  Fig. 3.</formula><p>Illustration of the proposed attention mechanism in the LSTM decoder. To generate decoder output yt at time t, a score function first combines the i-th encoder output v i and the last hidden state of the decoder s t−1 to obtain the relevance score e i t . Second, e i t is normalized to gain the attention weight α i t . Finally, the decoder input is obtained by weighted sum.</p><p>α i t is computed at each time step t , and it reflects the attention degree of the i-th temporal feature in the input video. To obtain α i t , the relevance score e i t should be computed. This is because that it combines the previous hidden state s t−1 in the LSTM decoder and the output of the encoder at time step i . The score function that computes the relevance score e i t can be written as:</p><formula xml:id="formula_6">e i t = score(s t−1 , v i ).<label>(5)</label></formula><p>The score function in Eq. (5) decides the relationship between the i-th visual features v i and the output scores at time t. It can be implemented in variable ways. Concretely, we develop two models: A-AVS and M-AVS, respectively. As shown in <ref type="figure" target="#fig_5">Fig.  4.(a)</ref>, the A-AVS model applies an additive score function:</p><formula xml:id="formula_7">e i t = w T tanh(w a s t−1 + U a v i + b a ),<label>(6)</label></formula><p>where w, w a , U a are the weights of the additive score function and b a is the bias. These parameters are estimated together with all other parameters of the encoder and decoder networks.</p><p>The A-AVS model simply concatenates the video frames and the hidden states of the decoder. Considering a special condition that the outputs of the decoder and visual frames are matched in video summarization. That is to say, a video frame feature v i corresponds to the hidden state s t−1 of the decoder. However, the additive function does not take full advantage of this relationship. To take a better use of the relationship between the outputs of the decoder and the visual frames, we further present an M-AVS model by exploring a multiplicative score function.</p><formula xml:id="formula_8">e i t = v T i W a s t−1 .<label>(7)</label></formula><p>M-AVS model is shown in <ref type="figure" target="#fig_5">Fig. 4. (b)</ref>. Once the relevance socres e i t for all frames i = 1, · · · , n are computed, we normalize them to obtain the α i t by:</p><formula xml:id="formula_9">α i t = exp(e i t )/ n j=1</formula><p>exp(e j t ).</p><p>Intuitively, this implements an attention mechanism in the decoder. The decoder decides which parts of the source frames to pay attention to. Then the importance score of each frame can be computed.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Keyshots Selection</head><p>Once obtained the predicted importance scores for all frames, the remaining work is to select the keyshots to generate the video summarization. Specifically, we apply the Kernel Temporal Segmentation (KTS) proposed by Potapov et al. <ref type="bibr" target="#b23">[24]</ref> to segment the visually coherent frames into shots. Then it computes shot-level importance scores by taking an average of the frame importance scores within each shot. To generate keyshot-based summary, we need to solve the following optimization problem:</p><formula xml:id="formula_11">max m i=1 u i w i , s.t. m i=1 u i l i ≤ 1, u i ∈ {0, 1} ,<label>(9)</label></formula><p>where s is the number of shots, w i is the importance score of the i-th shot, and l i is the length of the i-th shot. Note that this is exactly the 0/1 knapsack problem, which can be solved by the dynamic programming method <ref type="bibr" target="#b24">[25]</ref>. The summary is then created by concatenating those shots with u i = 0 in a chronological order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND ANALYSIS</head><p>This section first introduces the implementation details, including the datasets, evaluation metrics, and experimental settings. Then, we provide the main experimental results and parameter analysis. Next, we provide additional experiments with data argumentation. Finally, qualitative results are provided.</p><p>A. Implementation Details 1) Datasets: We evaluate the proposed AVS framework on two publicly available benchmark datasets: SumMe <ref type="bibr" target="#b15">[16]</ref>, and TVSum <ref type="bibr" target="#b24">[25]</ref>. Most of the videos in these datasets are 1 to 10 minutes in length. Specifically, SumMe <ref type="bibr" target="#b15">[16]</ref> consists of 25 raw videos recording a variety of events such as holidays and sports. TVSum <ref type="bibr" target="#b24">[25]</ref> contains 50 edited videos downloaded from YouTube in 10 categories, such as changing vehicle tire, getting vehicle unstuck, grooming an animal. The video contents in both datasets are diverse and include both egocentric and third-person camera. In addition, both of SumMe and TVSum datasets provide frame-level importance scores for each video, which are used as the ground-truth labels. For both the two datasets, we follow the steps in <ref type="bibr" target="#b16">[17]</ref> to convert frame level scores to keyshot summaries. <ref type="table" target="#tab_0">Table I</ref> summarizes the key characteristics of these datasets.</p><p>2) Evaluation Metrics: We apply the popular F-measure as the evaluation metric <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b18">[19]</ref>. Similar to <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b17">[18]</ref>, our methods generate a summary S which is less than 15% in duration of the original. Given a generated summary S and the ground-truth summary G, we compute the precision P and the recall R for each pair of S and G based on the temporal overlaps between them, as follows:</p><formula xml:id="formula_12">P = overlaped duration of S and G duration of S ,<label>(10)</label></formula><p>R = overlaped duration of S and G duration of G .</p><p>Finally, the F-measure is computed as: User generated videos of events 1.5-6.5 Frame-level importance scores TVSum <ref type="bibr" target="#b24">[25]</ref> 50 Edited videos (10 categories) 1-5 Frame-level importance scores 3) Experimental Settings: We downsample the videos into frame sequences in 2 fps. For fair comparison with <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b17">[18]</ref>, we choose to use the output of pool5 layer of the GoogLeNet <ref type="bibr" target="#b37">[39]</ref> (1024 dimensionality), trained on ImageNet, as the visual feature for each video frame. Both proposed models have three LSTM layers, and each layer contains 256 units. The attention scale of the decoder is set as 9. As for the training/testing data, we apply the same standard supervised learning setting as <ref type="bibr" target="#b16">[17]</ref>  <ref type="bibr" target="#b17">[18]</ref> where the training and testing are from the disjoint part of the same dataset. We randomly leave 20% for testing and the remaining 80% for training.</p><formula xml:id="formula_14">F = 2 × P × R (P + R) × 100%.<label>(12)</label></formula><p>To learn parameters in the LSTM layers, we use annotations in the forms of the frame-level importance scores. For both A-AVS and M-AVS, we stop training after 5 consecutive epochs with descending summarization F-score. The network is trained using gradient descent with a learning rate 0.15. We set attention scales to 9 and the mini-batch size to 16. For fair comparison, we run both A-AVS and M-AVS for 5 times and report the average performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison and Analysis</head><p>1) Comparison with State-of-the-art Approaches: Eight state-of-the-art video summarization approaches are selected for comparison with our AVS framework, including both unsupervised and supervised approaches. The performance results of the selected approaches are all from the original papers. Particularly, we are interested in comparing our performance in contrast with prior supervised approaches within the deep encoder-decoder framework, i.e., vsLSTM <ref type="bibr" target="#b16">[17]</ref>, dppLSTM <ref type="bibr" target="#b16">[17]</ref>, and SUM-GAN sup <ref type="bibr" target="#b17">[18]</ref>. We also choose three additional supervised approaches for comparison.The first one is Li et al. <ref type="bibr" target="#b18">[19]</ref>, which is a general framework designed for both edited and raw videos with the idea of property-weight learning. The second one is Gygli et al. <ref type="bibr" target="#b14">[15]</ref>, which learns submodular mixtures of objectives for different criteria directly. The third one is Zhang et al. <ref type="bibr" target="#b39">[41]</ref>, which learns nonparametrically to transfer summary structures from training videos to test ones. Moreover, two unsupervised approaches, SUM-GAN dpp <ref type="bibr" target="#b17">[18]</ref> and TVSum <ref type="bibr" target="#b24">[25]</ref> are chosen for comparison. <ref type="table" target="#tab_0">Table II</ref> shows the comparison results. We can observe that both A-AVS and M-AVS clearly outperform all the competitors in all the datasets. Specifically, on TVSum dataset, our approaches outperform the others in at least 3 absolute points. On SumMe dataset, there are almost 1 absolute points better than the state-of-the-arts. The significant improvements on TVSum against SumMe mainly lies in the fact that the association within each category of videos in TVSum is closer than that in SumMe. Thus, it is more suitable for attention mechanism to focus on the common important part of a video, which leads to a better performance on TVSum dataset.</p><p>In addition, it can be seen that the M-AVS model performs better than the A-AVS model on the two benchmark datasets in about 0.5%-1.6%. This is mainly due to that the multiplicative score layer makes better use of the relationship between the hidden states of the decoder and the visual feature than the additive score one. Even A-AVS has inferior performance, it outperforms SUM-GAN dpp , the prior best method with deep encoder-decoder framework, in 0.8%, 3.1% on SumMe and TVSum datasets, respectively. The promising results prove the effectiveness and superiority of our proposed AVS framework.</p><p>2) Importance Evaluation of Attention Mechanism: To better verify the effectiveness of the attention mechanism in AVS framework, we abandon the attention layer in AVS to build a baseline named LSTM-VS. <ref type="figure" target="#fig_6">Figure 5</ref> illustrates the performance comparison. It is clear to see that AVS framework outperforms the non-attention based LSTM-VS model noticeably (6%-10%), which also demonstrates the effectiveness of attention mechanism. 3) Parameter Sensitive Analysis: We evaluate the performances of our methods with different attention scales. <ref type="figure" target="#fig_7">Figure  6</ref> shows the F-score values on two different datasets. It can be seen that the performances reach their peaks when the attention scale is around 9. It is maybe due to the fact that each shot is around 9 frames on average when we perform KTS to segment the video into shots. Therefore, we can conclude that the proposed methods will perform better when their attention scales are close to the length of shots. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Augmentation Experiments</head><p>Zhang et al. <ref type="bibr" target="#b16">[17]</ref> and Mahasseni et al. <ref type="bibr" target="#b17">[18]</ref> augment the SumMe and TVSum datasets with OVP [36] and YouTube <ref type="bibr" target="#b26">[27]</ref> datasets to further improve the performance on SumMe and TVSum. YouTube <ref type="bibr" target="#b26">[27]</ref> contains 50 videos selected from Open Video Project (OVP) <ref type="bibr">[36]</ref>. The video contents include cartoons, news and sports. This dataset provides multiple userannotated subsets of keyframes for each video, and we follow the standard approach described in <ref type="bibr" target="#b16">[17]</ref> to create a single ground truth set for evaluation. Following their settings, we implement the augmented experiments in AVS framework. Particularly, for a given dataset, we randomly leave 20% of it for testing and augment the remaining 80% with the other three datasets to form an augmented training dataset. The results in <ref type="table" target="#tab_0">Table III</ref> clearly indicates that augmenting the training dataset with annotated data from other datasets improves summarization performance. For SumMe, the performances of both proposed methods rise about 0.7%. For TVSum, the performance of A-AVS method has been improved by 1.4%, while that of M-AVS method has been slightly improved by 0.8%. Moreover, the augmented performances for both datasets outperform the comparative approaches. These results confirm that our models are still effective and competitive when performing data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results</head><p>To better illustrate the temporal selection pattern of different variations of our approach, we demonstrate the selected frames on an example video in <ref type="figure" target="#fig_8">Fig. 7</ref>. It shows the results from vsLSTM, LSTM-VS, A-AVS, and M-AVS models on the 48th video of the TVSum dataset. The ground-truth frame-level importance scores of the video are represented by the blue blocks. The marked orange intervals are the ones selected by vsLSTM, LSTM-VS, A-AVS, and M-AVS model respectively. We can see that the summaries generated by our methods are more uniform distribution in time than that generated by vsLSTM model. Besides, our A-AVS and M-AVS approaches select more shots with larger importance scores than the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND FUTURE WORK</head><p>We propose a deep attentive framework for supervised video summarization. Specifically, two attention-based deep models named A-AVS and M-AVS are developed, respectively. To the best of our knowledge, our work is the first attempt to apply attention mechanism in deep models for video summarization. The proposed models outperform the competing methods on two benchmark datasets by 0.8%-3%. We also provide the qualitative analysis and parameter sensitive analysis. In addition, the augmentation experiments also verify the effectiveness and superiority of AVS framework when applied augmented data.</p><p>In our future work, we will explore more sophisticated attention mechanism in the proposed AVS framework to obtain richer contextual information. Moreover, the existing datasets are not large enough in scale. Thus, the insufficient training data restrict the performance and development of supervised video summarization approaches. To address this problem, we will apply transfer learning <ref type="bibr" target="#b39">[41]</ref> and Generative Adversarial Network (GAN) <ref type="bibr" target="#b17">[18]</ref> techniques to the proposed AVS framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>An overview of the proposed AVS framework. It includes an encoderdecoder model and a keyshot selection model. The encoder first reads the sequence of frames and then the attention based decoder generates a sequence of importance scores. Finally, the keyshot selection model generates the keyshots based on the visual sequence and the output of the decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of the BiLSTM in the proposed AVS framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of the proposed score functions in the A-AVS and M-AVS model to score the relationship between the input and the output, where v i represents the i-th vector encoded by the encoder and s t−1 stands for the hidden state of the decoder at time t − 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of the proposed methods with/without attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>F-score results of A-AVS model for different values of attention scales on SumMe, and TVSum datasets, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Exemplar video summaries (orange intervals) from a sample video (the 48th video of TVSum) along with the ground-truth importance scores (blue background).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DESCRIPTIVE</head><label>I</label><figDesc>STATISTICS OF THE TWO DATASETS.</figDesc><table><row><cell>Dataset</cell><cell>#Video</cell><cell>Descriptions</cell><cell>Duration(Min)</cell><cell>Annotations</cell></row><row><cell>SumMe [16]</cell><cell>25</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>COMPARISON (F-SCORE) WITH STATE-OF-THE-ART METHODS. BEST RESULTS ARE DENOTED IN BOLD.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Feature</cell><cell cols="2">Supervised/unsupervised F-score</cell></row><row><cell></cell><cell>SUM-GAN dpp [18]</cell><cell>GoogleNet</cell><cell>unsupervised</cell><cell>39.1</cell></row><row><cell></cell><cell>Gygli et al. [15]</cell><cell>DeCAF</cell><cell>supervised</cell><cell>39.7</cell></row><row><cell></cell><cell>Zhang et al. [41]</cell><cell>AlexNet</cell><cell>supervised</cell><cell>40.9</cell></row><row><cell></cell><cell>vsLSTM [17]</cell><cell>GoogleNet</cell><cell>supervised</cell><cell>37.6</cell></row><row><cell>SumMe</cell><cell>dppLSTM [17]</cell><cell>GoogleNet</cell><cell>supervised</cell><cell>38.6</cell></row><row><cell></cell><cell>SUM-GANsup [18]</cell><cell>GoogleNet</cell><cell>supervised</cell><cell>41.7</cell></row><row><cell></cell><cell>Li et al. [19]</cell><cell>VGGNet-16</cell><cell>supervised</cell><cell>43.1</cell></row><row><cell></cell><cell>A-AVS(ours)</cell><cell>GoogleNet</cell><cell>supervised</cell><cell>43.9</cell></row><row><cell></cell><cell>M-AVS(ours)</cell><cell>GoogleNet</cell><cell>supervised</cell><cell>44.4</cell></row><row><cell></cell><cell>TVSum [25]</cell><cell>HoG+GIST+SIFT</cell><cell>unsupervised</cell><cell>51.3</cell></row><row><cell></cell><cell>SUM-GAN dpp [18]</cell><cell>GoogleNet</cell><cell>unsupervised</cell><cell>51.7</cell></row><row><cell></cell><cell>vsLSTM [17]</cell><cell>GoogleNet</cell><cell>supervised</cell><cell>54.2</cell></row><row><cell>TVSum</cell><cell>dppLSTM [17] SUM-GANsup [18]</cell><cell>GoogleNet GoogleNet</cell><cell>supervised supervised</cell><cell>54.7 56.3</cell></row><row><cell></cell><cell>Li et al. [19]</cell><cell>VGGNet-16</cell><cell>supervised</cell><cell>52.7</cell></row><row><cell></cell><cell>A-AVS(ours)</cell><cell>GoogleNet</cell><cell>supervised</cell><cell>59.4</cell></row><row><cell></cell><cell>M-AVS(ours)</cell><cell>GoogleNet</cell><cell>supervised</cell><cell>61.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III SUMMARIZATION</head><label>III</label><figDesc>RESULTS (F-SCORE) WITH OUR AVS FRAMEWORK IN THE AUGMENTED SETTING. BEST RESULTS ARE DENOTED IN BOLD.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="2">Canonical Augmented</cell></row><row><cell></cell><cell>dppLSTM [17]</cell><cell>38.6</cell><cell>42.9</cell></row><row><cell>SumMe</cell><cell>SUM-GANsup [18] A-AVS(ours)</cell><cell>41.7 43.9</cell><cell>43.6 44.6</cell></row><row><cell></cell><cell>M-AVS(ours)</cell><cell>44.4</cell><cell>46.1</cell></row><row><cell></cell><cell>dppLSTM [17]</cell><cell>54.7</cell><cell>59.6</cell></row><row><cell>TVSum</cell><cell>SUM-GANsup [18] A-AVS(ours)</cell><cell>56.3 59.4</cell><cell>61.2 60.8</cell></row><row><cell></cell><cell>M-AVS(ours)</cell><cell>61.0</cell><cell>61.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video abstraction: a systematic review and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Ba Tu Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Comput., Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video summarisation: a conceptual framework and survey of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><forename type="middle">G</forename><surname>Money</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Agius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="143" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A user attention model for video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="533" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Event driven web video summarization by tag localization and keyshot identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="975" to="985" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Motion-state-adaptive video summarization via spatiotemporal analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1340" to="1352" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discovery of shared semantic spaces for multiscene video query and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1353" to="1367" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coherent semantic-visual indexing for large-scale image retrieval in the cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4128" to="4138" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pairwise relationship guided deep hashing for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Art. Intell</title>
		<meeting>AAAI Conf. Art. Intell</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1618" to="1625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning deep event models for crowd anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yachuang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomput</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="page" from="548" to="556" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video summarization and scene detection by graph modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Chong Wah Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="296" to="305" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Information theory-based shot cut/fade detection and video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuzana</forename><surname>Cernekova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Pitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophoros</forename><surname>Nikou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="90" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1346" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale video summarization using web-image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffay</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2698" to="2705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diverse sequential subset selection for supervised video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2069" to="2077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video summarization by learning submodular mixtures of objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3090" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Creating summaries from user videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayko</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="505" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video summarization with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="766" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised video summarization with adversarial LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A general framework for edited video and raw video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xuelong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3652" to="3664" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Kyungh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Empi. Meth. Natural Lan. Proc</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised extraction of video highlights via robust recurrent auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4633" to="4641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Category-specific video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danila</forename><surname>Potapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="540" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">TVSum: summarizing web videos using titles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Vallmitjana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5179" to="5187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient visual attention based framework for extracting key frames from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Ejaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Mehmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Wook</forename><surname>Baik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Commun</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="44" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Signal Process</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">VSUMM: a mechanism designed to produce static video summaries and a novel evaluation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">Eliza</forename><surname>Fontes De Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Paula Brando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaldo</forename><surname>Da Luz</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Albuquerque Arajo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="68" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video key frame extraction through dynamic delaunay clustering with a structural constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Kuanar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananda</forename><forename type="middle">S</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1212" to="1227" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video summarization via minimum sparse reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genliang</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David Dagan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="522" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards scalable summarization of consumer videos via sparse dictionary selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="75" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Query-aware sparse coding for multi-video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaru</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1707.04021" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning visual storylines with skipping recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="71" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show, attend and tell: neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keivin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd Int. Conf. Mach. Learn</title>
		<meeting>32nd Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bayesian modeling of temporal coherence in videos for entity discovery and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adway</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soma</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiranjib</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="430" to="443" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatically creating adaptive video summaries using constraint satisfaction programming: Application to sport content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haykel</forename><surname>Boukadida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sid-Ahmed</forename><surname>Berrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="920" to="934" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Perceptual attributes optimization for multivideo summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2991" to="3003" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Joint Conf. on Neural Net</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2047" to="2052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-View Video Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanshu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="717" to="729" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Summary transfer: exemplar-based subset selection for video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1059" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
