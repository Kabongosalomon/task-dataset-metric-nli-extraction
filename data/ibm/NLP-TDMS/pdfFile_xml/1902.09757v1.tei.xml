<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interaction-aware Factorization Machines for Recommender Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxing</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Advertising and Marketing Services</orgName>
								<orgName type="department" key="dep2">Corporate Development Group</orgName>
								<address>
									<region>Tencent Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Ge</roleName><forename type="first">Dongbo</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Advertising and Marketing Services</orgName>
								<orgName type="department" key="dep2">Corporate Development Group</orgName>
								<address>
									<region>Tencent Inc</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
							<email>gechen@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Advertising and Marketing Services</orgName>
								<orgName type="department" key="dep2">Corporate Development Group</orgName>
								<address>
									<region>Tencent Inc</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Interaction-aware Factorization Machines for Recommender Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Factorization Machine (FM) is a widely used supervised learning approach by effectively modeling of feature interactions. Despite the successful application of FM and its many deep learning variants, treating every feature interaction fairly may degrade the performance. For example, the interactions of a useless feature may introduce noises; the importance of a feature may also differ when interacting with different features. In this work, we propose a novel model named Interaction-aware Factorization Machine (IFM) by introducing Interaction-Aware Mechanism (IAM), which comprises the feature aspect and the field aspect, to learn flexible interactions on two levels. The feature aspect learns feature interaction importance via an attention network while the field aspect learns the feature interaction effect as a parametric similarity of the feature interaction vector and the corresponding field interaction prototype. IFM introduces more structured control and learns feature interaction importance in a stratified manner, which allows for more leverage in tweaking the interactions on both feature-wise and field-wise levels. Besides, we give a more generalized architecture and propose Interaction-aware Neural Network (INN) and DeepIFM to capture higher-order interactions. To further improve both the performance and efficiency of IFM, a sampling scheme is developed to select interactions based on the field aspect importance. The experimental results from two well-known datasets show the superiority of the proposed models over the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Learning the effects of feature conjugations, especially degree-2 interactions, is important for prediction accuracy <ref type="bibr" target="#b0">(Chang et al. 2010)</ref>. For instance, people often download apps for food delivery at meal-time, which suggests that the (order-2) interaction between the app category and the time-stamp is an important signal for prediction <ref type="bibr" target="#b11">(Guo et al. 2017)</ref>. Applying a linear model on the explicit form of degree-2 mappings can capture the relationship between features, where feature interactions can be easily understood and domain knowledge can be absorbed. The widely used generalized linear models (e.g., logistic regression) with cross features are effective for learning on a massive scale.</p><p>Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p><p>Although the feature vector might have billions of dimensions, each instance will typically have only hundreds of non-zero values, and FTRL <ref type="bibr" target="#b10">(McMahan et al. 2013</ref>) can save both time and memory when making predictions. However, feature engineering is an important but labor-intensive and time-consuming work, and the "cold-start" problem may hurt performance, especially in a sparse dataset, where only a few cross features are observed; the parameters for unobserved cross features cannot be estimated.</p><p>To address the generalization issue, factorization machines (FMs)(Rendle 2010) were proposed, which factorizes coefficients into a product of two latent vectors to utilize collaborative information and demonstrate superior performance to a linear model based on the explicit form of degree-2 mappings. In FM, unseen feature interactions can be learned from other pairs, which is useful, as demonstrated by the effectiveness of latent factor models <ref type="bibr" target="#b1">(Chen et al. 2014;</ref><ref type="bibr" target="#b7">Hong, Zheng, and Chen 2016)</ref>. In fact, by specifying the input feature vector, FM can achieve the same express capacity of many factorization models, such as matrix factorization, the pairwise interaction tensor factorization model(Rendle and Schmidt-Thieme 2010), and SVD++ <ref type="bibr" target="#b9">(Koren 2008)</ref>.</p><p>Despite the successful application of FM, two-folds significant shortcomings still exist. (1) Feature aspect. On one hand, the interactions of a useless feature may introduce noises. On the other hand, treating every feature interaction fairly may degrade the performance. (2) Field 1 aspect. A latent factor 2 may also have different importance in feature interactions from different fields. Assuming that there is a latent factor indicating the quality of a phone, this factor may be more important to the interaction between a phone brand and a location than the interaction between gender and a location. To solve the above problems, we propose a novel model called Interaction-aware Factorization Machine (IFM) to learn flexible interaction importance on both feature aspect and field aspect.</p><p>Meanwhile, as a powerful approach to learning feature representation, deep neural networks are becoming increasingly popular and have been employed in predictive models.</p><p>For example, <ref type="bibr">Wide&amp;Deep(Cheng et al. 2016</ref>) extends generalized linear models with a multi-layer perceptron (MLP) on the concatenation of selected feature embedding vectors to learn more sophisticated feature interactions. However, in the wide part of the Wide&amp;Deep model, feature engineering is also required and drastically affects the model performance.</p><p>To eliminate feature engineering and capture sophisticated feature interactions, many works <ref type="bibr" target="#b0">(Cao et al. 2016;</ref>) are proposed and some of them have fused FM with MLP. FNN <ref type="bibr" target="#b12">(Zhang, Du, and Wang 2016)</ref> initializes parts of the feed-forward neural network with FM pretrained latent feature vectors, where FM is used as a feature transformation method. <ref type="bibr">PNN(Qu et al. 2016</ref>) imposes a product layer between the embedding layer and the first hidden layer and uses three different types of product operations to enhance the model capacity. Nevertheless, both FNN and PNN capture only high-order feature interactions and ignore low-order feature interactions. DeepFM <ref type="bibr" target="#b11">(Guo et al. 2017)</ref> shares the feature embedding between the FM and deep component to make use of both low-and high-order feature interactions; however, simply concatenating <ref type="bibr" target="#b3">(Cheng et al. 2016;</ref> or averaging embedding vectors <ref type="bibr" target="#b11">(Wang et al. 2015;</ref><ref type="bibr" target="#b3">Chen et al. 2017)</ref> does not account for any interaction between features. In contrast to that, NFM(He and Chua 2017) uses a bi-interaction operation that models the second-order feature interactions to maintain more feature interaction information. Unfortunately, the pooling operation in NFM may also cause information loss. To address this problem, interaction importance on both feature aspect and field aspect is encoded to facilitate the MLP to learn feature interactions more accurately.</p><p>The main contributions of the paper include the following:</p><p>• To the best of our knowledge, this work represents the first step towards absorbing field information into interaction importance learning.</p><p>• The proposed interaction-aware models can effectively learn interaction importance and require no feature engineering.</p><p>• The proposed IFM provides insight into which feature interactions contribute more to the prediction at the field level.</p><p>• A sampling scheme is developed to further improve both the performance and efficiency of IFM.</p><p>• The experimental results on two well-known datasets show the superiority of the proposed interaction-aware models over the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factorization Machines</head><p>We assume that each instance has attributions x = {x 1 , x 2 , ..., x m } from n fields and a target y, where m is the number of features and x i is the real valued feature in the ith category. Let V ∈R K×m be the latent matrix, with column vector V i representing the K-dimensional feature-specific latent feature vector of feature i. Then pair-wise enumera-tion of non-zero features can be defined as</p><formula xml:id="formula_0">X = {(i, j) | 0 &lt; i ≤ m, 0 &lt; j ≤ m, j &gt; i, x i = 0, x j = 0}.</formula><p>(1) Factorization Machine (FM)(Rendle 2010) is a widely used model that captures all interactions between features using the factorized parameters:</p><formula xml:id="formula_1">y = w 0 + m i=1 w i x i + (i,j)∈X w ij x i x j pair-wise feature interactions ,<label>(2)</label></formula><p>where w 0 is the global bias, and w i models the strength of the i-th variable. In addition, FM captures pairwise (order-2) feature interactions effectively as w ij = V i , V j , where ·,· is the inner product of two vectors; therefore, the parameters for unobserved cross features can also be estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Approach</head><p>In this section, we first present the interaction-aware mechanism. Subsequently, we detail the proposed Interactionaware Factorization Machine (IFM). Finally, we propose a generalized interaction-aware model and its neural network specialized versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction-Aware Mechanism (IAM)</head><p>The pair-wise feature interaction part of FM can be reformulated as</p><formula xml:id="formula_2">m i=1 m j=i+1 1 · 1, V i V j x i x j ,<label>(3)</label></formula><p>where 1 is a K-dimensional vector with all entries one and denotes the Hadamard product. Then we introduce the Interaction-Aware Mechanism (IAM) to discriminate the importance of feature interactions, which simultaneously considers field information as auxiliary information,</p><formula xml:id="formula_3">m i=1 m j=i+1 T ij F fi,fj field aspect , V i V j x i x j ,<label>(4)</label></formula><p>where f i is the field of feature i, F fi,fj is the K-dimensional field-aware factor importance vector of the interaction between feature i and feature j modeling the field aspect; thus, both factors from the same feature interaction and the same factor of interactions from different fields can have significantly different influences on the final prediction. T ij is the corresponding attention score modeling the feature aspect; thus, the importance of feature interactions can be significantly different, which is defined as</p><formula xml:id="formula_4">a ij = h T Relu(W (V i V j )x i x j + b), T ij = exp(a ij /τ ) (i,j)∈X exp(a ij /τ ) ,<label>(5)</label></formula><p>where K a is the hidden layer size of the attention network, b∈R Ka , h∈R Ka , W ∈R Ka×K , and τ is a hyperparameter</p><formula xml:id="formula_5">0 x 1 1 x 2 0 x 3 0.2 x 4 0 x 5 1 x 6 V 2 · x 2 V 4 · x 4 V 6 · x 6 . . . (V 2 V 4 )x 2 x 4 (V 2 V 6 )x 2 x 6 (V 4 V 6 )x 4 x 6 Attention Net T 24 T 26 T 46 U f2 U f4 U f6 U f2 U f4 U f4 U f6 U f4 U f6</formula><p>Projection Matrix that was originally used to control the randomness of predictions by scaling the logits before applying softmax(Hinton, Vinyals, and Dean 2015). Here we use τ to control the effectiveness strength of the feature aspect. For a high temperature(τ → ∞), all interactions have nearly the same importance, and the feature aspect has a limited impact on the final prediction. For low temperatures (τ → 0), the probability of the interaction vector with the highest expected reward tends to 1 and the other interactions are ignored. The raw presentation of F has n(n−1)/2×K parameters, where n is the number of fields, so the space complexity of IAM is quadratic in the field number. We further factorize tensor F using canonical decomposition(Kolda and Bader 2009):</p><formula xml:id="formula_6">F fi,fj = D T (U fi U fj ),<label>(6)</label></formula><p>where U ∈R K F ×n and D∈R K F ×K , and K F is the number of latent factors of both U and D. Therefore, the space complexity is reduced to O(nK F + K F K), which is linear in the field number. From another perspective, field aspect learns feature interaction effect as a parametric similarity of the feature interaction vector (V i V j )x i x j and the corresponding field interaction prototype U fi U fj , which has a bi-linear form <ref type="bibr" target="#b0">(Chechik et al. 2010)</ref>,</p><formula xml:id="formula_7">sim D (c, e) = c T De,<label>(7)</label></formula><formula xml:id="formula_8">with c = U fi U fj , e = (V i V j )x i x j .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction-aware Factorization Machines (IFMs)</head><p>Interaction-aware Factorization Machine (IFM) models feature interaction importance more precisely by introducing IAM. For simplicity, we omit linear terms and the bias term in the remaining parts. <ref type="figure" target="#fig_0">Figure 1</ref> shows the neural network architecture of IFM, which comprises 6 layers. In the following, several layers are detailed:</p><p>• Embedding layer. The embedding layer is a fully connected layer that projects each feature to a dense vector representation. IFM employs two embedding matrices V and U for feature embedding and field embedding querying, respectively. • Pair-wise interaction layer. The pair-wise interaction layer enumerates interacted latent vectors, each of which is a element-wise product of two embedding vectors from the embedding layer. Let the feature aspect pair-wise interaction set P F and the field aspect pair-wise interaction set P I be</p><formula xml:id="formula_9">P F = {(V i V j )x i x j | (i, j)∈X }, P I = {U fi U fj | (i, j)∈X },<label>(8)</label></formula><p>then each has no information overlap; the former only depends on the feature embedding matrix V , while the latter only comes from the field embedding matrix U . • Inference layer. The inference layer calculates the feature aspect importance and the field aspect importance according to Equation 5 and Equation 6, respectively. To summarize, we give the overall formulation of IFM as:</p><formula xml:id="formula_10">y = m i=1 m j=i+1 T ij (U fi U fj ) T D(V i V j )x i x j + m i=1 w i x i + w 0 .<label>(9)</label></formula><p>We also apply L 2 regularization on U and D with λ F controling the regularization strength and employ dropout(Srivastava et al. 2014) on the pair-wise interaction layer to prevent overfitting. Note that U ∈R K F ×n and V ∈R K×m can have different dimensions; each latent vector of U only needs to learn the effect with a specific field, so usually,</p><formula xml:id="formula_11">K F K.<label>(10)</label></formula><p>Complexity Analysis. Feature embedding matrix V require m × K parameters and field-aware factor importance matrix F requires n × K F + K F × K parameters after applying Equation 6. Besides, the parameters of attention network is K a × K + 2K a . Thus, the overall space complexity is O(nK F + (K F + m + K a )K + 2K a ), where K F , K a , K and n are small compared to m, so the space complexity is similar to that of FM, which is O(mK).</p><p>The cost of computing P F (Equation 8) and feature aspect importance are O(|X |K) and O(|X |KK a ), respectively. For prediction, because the field-aware factor importance matrix F can be pre-calculated by Equation 6 and the fusion layer only involves the inner product of two vectors, for which the complexity is O(|X |K), the overall time complexity is O(|X |KK a ).</p><p>Sampling. We dynamically sample c feature interactions according to the norms of field-aware factor importance vectors (F fi,fj ) and attention scores are only computed for the sampled interactions. The cost of sampling is O(n 2 K F K) for a mini-batch data and the computation cost of attention scores is O(cKK a ) for every instance. By sampling, the selection frequency for useless interactions is reduced and the overall time complexity is reduced to O(cKK a + n 2 batchSize K F K).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalized Interaction-aware Model (GIM)</head><p>We present a more generalized architecture named Generalized Interaction-aware Model (GIM) in this section and derive its neural network versions to effectively learn higher order interactions. Let feature aspect embedding set F X and field aspect embedding set I X be</p><formula xml:id="formula_12">F X = {T ij V i V j x i x j | (i, j)∈X }, I X = {D T (U fi U fj ) | (i, j)∈X },<label>(11)</label></formula><p>Then, the final prediction can be calculated by introducing function G as y = G(F X , I X ).</p><p>Let F X i,j and I X i,j be the element with index (i, j) in F X and I X , respectively. Then IFM can be seen as a special case of GIM using the following,</p><formula xml:id="formula_14">G IF M (F X , I X ) = {I X T i,j F X i,j | (i, j)∈X }.<label>(13)</label></formula><p>Besides, G can be a more complex function to capture the non-linear and complex inherent structure of real-world data. Let</p><formula xml:id="formula_15">h 0 = concate{I X i,j F X i,j | (i, j)∈X }, h l =f l (Q l h l−1 + z l ),<label>(14)</label></formula><p>where n l is the number of nodes in the l-th hidden layer; then, Q l ∈R n l ×n l−1 , z l ∈R n l are parameters for the l-th hidden layer, f l is the activation function for the l-th hidden layer, and h l ∈R n l is the output of the l-th hidden layer. Specially, Interaction-aware Neural Network (INN) is defined as where L denotes the number of hidden layers and f L is the identity function. For hidden layers, we use Relu as the activation function, which empirically shows good performance.</p><formula xml:id="formula_16">G IN N (F X , I X ) = h L ,<label>(15)</label></formula><p>To learn both high-and low-order feature interactions, the wide component of DeepFM ) is replaced by G IF M (F X , I X ) and named as DeepIFM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental results</head><p>In this section, we evaluate the performance of the proposed IFM, INN and DeepIFM on two real-world datasets and examine the effect of different parts of IFM. We conduct experiments with the aim of answering the following questions:</p><p>• RQ1 How do IFM and INN perform compared to the state-of-the-art methods?</p><p>• RQ2 How do the feature aspect and the field aspect (with sampling) impact the prediction accuracy?</p><p>• RQ3 How dose factorization of field-aware factor importance matrix F impact the performance of IFM?</p><p>• RQ4 How do the hyper-parameters of IFM impact its performance?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Settings</head><p>Datasets and Evaluation. We evaluate our models on two real-world datasets, MovieLens 3 <ref type="bibr" target="#b4">(Harper and Konstan 2015)</ref> and Frappe(Baltrunas et al. 2015), for personalized tag recommendation and context-aware recommendation. We follow the experimental settings in the previous works(Xiao et al. 2017; He and Chua 2017) and use the optimal parameter settings reported by the authors to have fair comparisons. The datasets are divided into a training set (70%), a probe set (20%), and a test set (10%). All models are trained on the training set, and the optimal parameters are obtained on the held-out probe set. The performance is evaluated by the root mean square error (RMSE), where a lower score indicates better performance, on the test set with the optimal parameters. Both datasets contain only positive records, so we generate negative samples by randomly pairing two negative samples with each log and converting each log into a feature vector via one-hot encoding. <ref type="table" target="#tab_0">Table 1</ref> shows a description of the datasets after processing, where the sparsity level is the ratio of observed to total features(Lee, Sun, and Lebanon 2012).</p><p>Baselines. We compare our models with the following methods:</p><p>• FM(Rendle 2010). As described in Equation 2. In addition, dropout is employed on the feature interactions to further improve its performance.</p><p>• FFM <ref type="bibr" target="#b8">(Juan et al. 2016)</ref>. Each feature has separate latent vectors to interact with features from different fields.</p><p>• AFM <ref type="bibr" target="#b11">(Xiao et al. 2017</ref>). AFM learns one coefficient for every feature interaction to enable feature interactions that contribute differently to the prediction.</p><p>• Neural Factorization Machines (NFMs) . NFM performs a non-linear transformation on the latent space of the second-order feature interactions. Batch normalization(Ioffe and Szegedy 2015) is also employed to address the covariance shift issue.</p><p>• DeepFM(Guo et al. 2017). DeepFM shares the feature embedding between the FM and the deep component.</p><p>Regularization. We use L 2 regularization, dropout, and early stopping.</p><p>Hyperparameters. The model-independent hyperparameters are set to the optimal values reported by the previous works <ref type="bibr" target="#b11">(Xiao et al. 2017;</ref>. The embedding size of features is set to 256, and the batch size is set to 4096 and 128 for MovieLens and Frappe, respectively. We also pre-train the feature embeddings with FM to get better results. For IFM and INN, we set τ = 10 and tune the other hyperparameters on the probe set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Performance (RQ1)</head><p>The performance of different models on the MovieLens dataset and the Frappe dataset is shown in <ref type="table" target="#tab_1">Table 2</ref>, from which the following observations may be made:</p><p>• Learning the importance of different feature interactions improves performance. This observation is derived from the fact that both AFM and the IAM-based models (IFM and INN) perform better than FM does. As the best model, INN outperforms FM by more than 10% and 7% on the MovieLens and Frappe datasets, respectively.</p><p>• IFM makes use of field information and can model feature interactions more precisely. To verify the effectiveness of field information, we conduct experiments with FFM and FFM-style AFM, where each feature has separate latent vectors to interact with features from different fields, on the MovieLens dataset. As expected, the utilization of field information brings improvements of approximately 2% and 3% with respect to FM and AFM.</p><p>• INN outperforms IFM by using a more complex function G, as described in Equation 15, which captures more complex and non-linear relations from IAM encoded vectors.</p><p>• Overall, our proposed IFM model outperforms the competitors by more than 4.8% and 1.2% on the Movie-Lens and Frappe datasets, respectively. The proposed INN model performs even better, which achieves an improvement of approximately 6% and 1.5% on the MovieLens and Frappe datasets, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of different aspects and sampling (RQ2)</head><p>IFM discriminates feature interaction importance on feature aspect and field aspect. To study how each aspect influences IFM prediction, we keep only one aspect and monitor how IFM performs. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, feature-aspect-only IFM (FA-IFM) performs better than field-aspect-only IFM (IA-IFM) does. We explain this phenomenon by examining the models. The FA-IFM modeling of feature interaction importance is more detailed for each individual interacted vectors; thus, it can make use of the feature interaction information precisely, whereas IA-IFM utilizes only field-level interaction information and lacks the capacity to distinguish feature interactions from the same fields. Although FA-IFM models feature interactions in a more precise way, IFM still achieves a significant improvement by incorporating field information, which can be seen as auxiliary information, to give more structured control and allow for more leverage when tweaking the interaction between features. We now focus on analyzing the different role of field aspect in different datasets. We calculated the ratio of the improvements of FA-IFM over IA-IFM, which were 9:1 and 1.7:1 on the Frappe and MovieLens datasets, respectively. It is determined that field information plays a more significant role in the MovieLens dataset. We explain this phenomenon by examining the datasets. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the Movie-Lens dataset is sparser than the Frappe dataset, where the field information brings more benefit <ref type="bibr" target="#b8">(Juan et al. 2016)</ref>.</p><p>Field importance Analysis. Field aspect not only improves the model performance but also gives the ability to interpret the importance of feature interactions at the fieldfactor level. Besides, the norm of field aspect importance vector provides insight into interaction importance at the field level. To demonstrate this, we investigate field aspect importance vectors on the MovieLens dataset. As shown in <ref type="table" target="#tab_2">Table 3</ref>, the movie-tag interaction is the most important while the user-movie interaction has a negligible impact on the prediction because tags link users and items as a bridge ) and directly modeling semantic correlation between them is less effective.</p><p>Sampling. To examine how sampling affects the performance of IFM, an experiment was conducted on Frappe dataset and because there are only three interactions in MovieLens dataset, sampling is meaningless. As shown in <ref type="table" target="#tab_1">Table 2</ref>, IFM with sampling achieves a similar level of performance. To verify how sampling performs when the   dataset is large, we compare the performance 4 on clickthrough prediction for advertising in Tencent video, which has around 10 billion instances. As shown in <ref type="table" target="#tab_3">Table 4</ref>, sampling reduce the training time with no significant loss to the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of factorization (RQ3)</head><p>As described in Equation 6, IAM factorizes field-aware factor importance matrix F ∈R n(n−1)/2×K to get a more compact representation. We conduct experiments with both the factorized version and the non-factorized version (indicated as IFM − ) to determine how factorization affects the performance. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, factorization can speed up the convergence of both datasets. However, it also has a significantly different impact on the performance of the two datasets. For the MovieLens dataset, both versions achieve similar levels of performance but IFM outperforms IFM − by a large margin on the Frappe dataset, where the performance of IFM − is degraded from epoch 50 because of an overfitting issue 5 . We explain this phenomenon by comparing the number of entries of field-aware factor importance matrix F . For the Frappe dataset, IFM − and IFM have 11,520 and 6,370 entries with the optimal settings with K = 256 and 4 Feature interactions from the same field are discarded and the activation of attention network is set to tanh.</p><p>5 Early stopping is disabled in this experiment.   adaptations on training data. We apply dropout to FM on feature interaction vectors and obtain better performance as a benchmark. As shown in <ref type="figure">Figure 4</ref>, we set the keep probability from 0.1 to 1.0 with increments of 0.1 and it significantly affects the performance of both FM and IFM. When the keep probability tends to zero, the performance of both models is poor due to the underfitting issue. When the keep probability tends to 1, i.e., no dropout is employed, both models also cannot achieve the best performance. Both IFM and FM achieve the best performance when the keep probability is properly set due to the extreme bagging effect. For nearly all keep probabilities, IFM outperforms FM, which shows the effectiveness of IAM.</p><p>L 2 regularization. <ref type="figure">Figure 5</ref> shows how IFM performs when the L 2 regularization hyperparameter λ F varies while keeping the dropout ratio constant (optimal value from the validation dataset). IFM performs better when L 2 regularization is applied and it achieves an improvement of approximately 1.4% in the MovieLens dataset. We explain this phenomenon as the following. Using dropout on the pair-wise interaction layer only prevent overfitting for the feature aspect and λ F controls the regularization strength of factorization parameters for the field aspect importance learning.</p><p>The number of hidden factors K F . <ref type="figure" target="#fig_3">Figure 6</ref> shows how IFM performs when the number of hidden factors K F varies. IFM cannot effectively capture the field-aware factor importance when K F is small and it also can not achieve the best performance when K F is large due to the overfitting issue. An interesting phenomenon is that the best K F for the MovieLens dataset is much smaller than that for the Frappe dataset. We explain this phenomenon by looking into the datasets. Because the number of fields n is 10 for the Frappe dataset, the field-aware factor importance matrix captures the importance of factors from 45 interacted vectors. While the MovieLens dataset contains only 3 interactions and the field-aware factor importance matrix keeps much less information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>In the introduction section, factorization machine and its many neural network variants are already mentioned, thus we do not discuss them here. In what follows, we briefly recapitulate the two most related models, i.e., <ref type="bibr">AFM(Xiao et al. 2017</ref>) and FFM <ref type="bibr" target="#b8">(Juan et al. 2016)</ref>.</p><p>AFM learns one coefficient for every feature interaction to enable feature interactions that contribute differently to the final prediction and the importance of a feature interaction is automatically learned from data without any human domain knowledge. However, the pooling layer of AFM lacks the capacity of discriminating factor importance in feature interactions from different fields. In contrast, IFM models feature interaction importance at interaction-factor level; thus, the same factor in different interactions can have significantly different influences on the final prediction.</p><p>In FMs, every feature has only one latent vector to learn the latent effect with any other features. FFM utilizes field information as auxiliary information to improve model performance and introduces more structured control. In FFM, each feature has separate latent vectors to interact with features from different fields, thus the effect of a feature can differ when interacting with features from different fields. However, modeling feature interactions without discriminating importance is unreasonable. IFM learns flexible interaction importance and outperforms FFM by more than 6% and 7% on the Frappe and MovieLens datasets, respectively. Moreover, FFM requires O(mnK) parameters, while the space complexity of IFM is O(mK).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Directions</head><p>In this paper, we proposed a generalized interaction-aware model and its specialized versions to improve the representation ability of FM. They gain performance improvement based on the following advantages. (1) All models can effectively learn both the feature aspect and the field aspect interaction importance. (2) All models can utilize field information that is usually ignored but useful. (3) All models apply factorization in a stratified manner. (4) INN and Deep-IFM can learn jointly with deep representations to capture the non-linear and complex inherent structure of real-world data.</p><p>The experimental results on two well-known datasets show the superiority of the proposed models over the stateof-the-art methods. To the best of our knowledge, this work represents the first step towards absorbing field information into feature interaction importance learning.</p><p>In the future, we would like to generalize the field-aware importance matrix to a more flexible structure by applying neural architecture search ).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The neural network architecture of the proposed Interaction-aware Factorization Machine (IFM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of test RMSE by using only one aspect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Performance comparison on the test set w.r.t. IFM and the non-factorization version IFM-.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of test RMSE by varying K F .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset Description.</figDesc><table><row><cell>DATA SET</cell><cell>MOVIELENS</cell><cell>FRAPPE</cell></row><row><cell>ORIGIN RECORDS</cell><cell>668,953</cell><cell>96,203</cell></row><row><cell>FEATURES</cell><cell>90,445</cell><cell>5,382</cell></row><row><cell>EXPERIMENTAL RECORDS</cell><cell>2,006,859</cell><cell>288,609</cell></row><row><cell>FIELDS</cell><cell>3</cell><cell>10</cell></row><row><cell>SPARSITY LEVEL</cell><cell>0.01%</cell><cell>0.19%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test RMSE from different models.</figDesc><table><row><cell></cell><cell>FRAPPE</cell><cell></cell><cell>MOVIELENS</cell><cell></cell></row><row><cell>METHOD</cell><cell cols="2">#PARAM RMSE</cell><cell>#PARAM</cell><cell>RMSE</cell></row><row><cell>FM</cell><cell>1.38M</cell><cell>0.3321</cell><cell>23.24M</cell><cell>0.4671</cell></row><row><cell>DEEPFM</cell><cell>1.64M</cell><cell>0.3308</cell><cell>23.32M</cell><cell>0.4662</cell></row><row><cell>FFM</cell><cell>13.8M</cell><cell>0.3304</cell><cell>69.55M</cell><cell>0.4568</cell></row><row><cell>NFM</cell><cell>1.45M</cell><cell>0.3171</cell><cell>23.31M</cell><cell>0.4549</cell></row><row><cell>AFM</cell><cell>1.45M</cell><cell>0.3118</cell><cell>23.25M</cell><cell>0.4430</cell></row><row><cell>IFM-SAMPLING</cell><cell>1.46M</cell><cell>0.3085</cell><cell>-</cell><cell>-</cell></row><row><cell>IFM</cell><cell>1.46M</cell><cell>0.3080</cell><cell>23.25M</cell><cell>0.4213</cell></row><row><cell>INN</cell><cell>1.46M</cell><cell>0.3071</cell><cell>23.25M</cell><cell>0.4188</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The norm of field aspect importance vector of each feature interaction on the MovieLens dataset.</figDesc><table><row><cell></cell><cell cols="3">USER-MOVIE USER-TAG MOVIE-TAG</cell></row><row><cell>NORM</cell><cell>0.648</cell><cell>5.938</cell><cell>9.985</cell></row><row><cell>PROPORTION</cell><cell>3.9 %</cell><cell>35.8 %</cell><cell>60.3 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The performance on click-through prediction for advertising in Tencent video.</figDesc><table><row><cell cols="3">METHOD</cell><cell></cell><cell>AUC</cell><cell>TIME</cell></row><row><cell cols="3">DEEPIFM</cell><cell></cell><cell>0.8436 16HRS, 18MINS</cell></row><row><cell cols="5">DEEPIFM-SAMPLING(10%) 0.8420</cell><cell>3HRS, 49MINS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>IA-IFM</cell></row><row><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell>FA-IFM IFM</cell></row><row><cell></cell><cell>0.48</cell><cell></cell><cell></cell></row><row><cell>RMSE</cell><cell>0.46</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.44</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.42</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>20</cell><cell>40 Epoch 60</cell><cell>80 100</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A field can be viewed as a class of features. For instance, two features male and female belong to the field gender. 2 A variable in a latent vector corresponding to an abstract concept.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">grouplens.org/datasets/movielens/latest</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Training and testing low-degree polynomial data mappings via linear svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baltrunas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.03014</idno>
	</analytic>
	<monogr>
		<title level="m">Frappe: Understanding the usage and perception of mobile app recommendations in-the-wild</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010-03" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1109" to="1135" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Journal of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Context-aware collaborative topic regression with social matrix factorization for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Capturing semantic correlation for item recommendation in tagging systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="108" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1725" to="1731" />
		</imprint>
	</monogr>
	<note>Proceedings of the 26th International Joint Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<title level="m">The movielens datasets: History and context</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural factorization machines for sparse predictive analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinyals</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dean ; Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen ;</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>Latent space regularization for recommender systems</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Field-aware factorization machines for ctr prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Juan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="455" to="500" />
		</imprint>
	</monogr>
	<note>Tensor decompositions and applications</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pairwise interaction tensor factorization for personalized tag recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1205.3193</idno>
		<idno>1222-1230. ACM</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>IEEE 10th International Conference on. Srivastava et al. 2014. Dropout: A simple way to prevent neural networks from overfitting</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attentional factorization machines: Learning the weight of feature interactions via attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17). Morgan Kaufmann</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17). Morgan Kaufmann</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
	<note>Proceedings of the 38th International ACM SIGIR conference on Research and Development in Information Retrieval</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning over multi-field categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang ;</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
