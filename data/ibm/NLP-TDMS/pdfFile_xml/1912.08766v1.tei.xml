<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RealMix: Towards Realistic Semi-Supervised Deep Learning Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Nair</surname></persName>
							<email>varun.nair1@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
								<address>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Uizard Technologies</orgName>
								<address>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Fuentes Alonso</surname></persName>
							<email>javier@uizard.io</email>
							<affiliation key="aff1">
								<orgName type="institution">Uizard Technologies</orgName>
								<address>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Beltramelli</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Uizard Technologies</orgName>
								<address>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RealMix: Towards Realistic Semi-Supervised Deep Learning Algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semi-Supervised Learning (SSL) algorithms have shown great potential in training regimes when access to labeled data is scarce but access to unlabeled data is plentiful. However, our experiments illustrate several shortcomings that prior SSL algorithms suffer from. In particular, poor performance when unlabeled and labeled data distributions differ. To address these observations, we develop RealMix, which achieves state-of-the-art results on standard benchmark datasets across different labeled and unlabeled set sizes while overcoming the aforementioned challenges. Notably, RealMix achieves an error rate of 9.79% on CIFAR10 with 250 labels, and is the only SSL method tested able to surpass baseline performance when there is significant mismatch in the labeled and unlabeled data distributions. RealMix demonstrates how SSL can be used in real world situations with limited access to both data and compute and guides further research in SSL with practical applicability in mind.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent progress in deep learning has largely been driven by the development of specialized hardware and the abundance of large, labeled datasets. While applicable in learning tasks when data is widely and cheaply available, these techniques are impractical to solve real world problems where collecting data is both time-consuming and expensive. Typical examples of such problems include diagnosis from medical imaging and robotic perception problems.</p><p>To combat challenges in these domains, Semi-Supervised Learning (SSL) algorithms have emerged as a useful tool <ref type="bibr" target="#b2">[3]</ref>. SSL algorithms seek to learn the underlying structure of data by utilizing large amounts of unlabeled data, which can often be more readily available than labeled data. Recent work in SSL <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> has progressed using a number of assumptions. First, that model outputs on unlabeled data should be invariant to small perturbations (i.e. consistency training). Second, that encouraging model outputs to be more confident will steer decision boundaries away from high-density regions (i.e. entropy minimization <ref type="bibr" target="#b5">[6]</ref>). Finally, that the training data distribution can be extended using linear interpolations of data points (i.e. MixUp <ref type="bibr" target="#b17">[18]</ref>).</p><p>SSL algorithms are typically evaluated by taking a standard benchmarking dataset (e.g. CIFAR10 <ref type="bibr" target="#b6">[7]</ref>, SVHN <ref type="bibr" target="#b9">[10]</ref>) and discarding a significant fraction of the labels. This results in a small labeled dataset and a larger unlabeled dataset that both come from the same distribution. The current state-of-the-art SSL technique MixMatch <ref type="bibr" target="#b1">[2]</ref> is able to recover over 92% of the test accuracy on CIFAR10 using 200 times fewer labels than the supervised baseline. These advances prompt the following question: Can SSL algorithms sufficiently alleviate the need for labeled data in real-world settings?</p><p>Oliver et al. <ref type="bibr" target="#b10">[11]</ref> argues that the current approach to evaluating SSL algorithms is inadequate and raises several questions about SSL's real-world applicability. In particular, they find that performance of SSL techniques suffer when there is a significant mismatch in the unlabeled and labeled data distributions and that transfer learning can often outperform SSL with labeled data alone. We reevaluate these findings on sections 4.2.2 and 4.2.3, showing that this is no longer true.</p><p>These problems have, up until now, been a major drawback on the adoption of SSL techniques in realistic setups. We can define a realistic setup for SSL as one in which a practitioner compares SSL performance with transfer learning using limited labeled samples (given its success in pretraining classifiers <ref type="bibr" target="#b0">[1]</ref>) and where unlabeled data samples are not guaranteed to come from the labeled data distribution. Our goal is to develop a deep SSL algorithm that unites successful practices in SSL and is viable in realistic setups.</p><p>We present RealMix, an SSL algorithm depicted in fig. 1 that unites ("mixes") the most successful approaches in SSL to set state-of-the-art results on benchmark datasets while surpassing baseline performance when there is significant mismatch in the unlabeled and labeled datasets. Our contributions can be summarized as follows:</p><p>• We perform experiments to show that RealMix sets state-of-the-art results on CIFAR10 and SVHN, achieving an error rate of 9.79% on CIFAR10 using 250 labels.</p><p>• We experimentally demonstrate that RealMix is applicable in real-world settings by showing that when the unlabeled distribution is significantly different from the labeled distribution, we can still improve on the supervised baseline performance. Notably, RealMix is the only SSL approach tested that is able to surpass baseline performance when there is significant or complete mismatch in the labeled and unlabeled distributions.</p><p>• We demonstrate that RealMix (in addition to Mix-Match <ref type="bibr" target="#b1">[2]</ref>) surpasses transfer learning, and that transfer learning is complementary to SSL. We show this experimentally by pre-training a classifier on ILSVRC-2012 <ref type="bibr" target="#b12">[13]</ref> and applying RealMix to further reduce the error on CIFAR10 with 250 labels to just 8.48%.</p><p>• We also perform an ablation study on RealMix to identify the components that lead to its success in realistic scenarios.</p><p>• We provide our implementation source code as a publicly available repository 1 to foster future research.</p><p>We continue our discussion of RealMix in the next section by detailing successful approaches in SSL, how RealMix unites these approaches, and what new elements are introduced by RealMix to make it work in realistic scenarios. In section 4, we carry out several experiments with RealMix that lead to state-of-the-art results on benchmark image classification datasets and demonstrate its effectiveness when unlabeled and labeled data distributions mismatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>While SSL techniques have a rich history <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19]</ref>, we focus on describing methods that recent deep variants utilize to achieve state-of-the-art and literature that considers SSL in realistic setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Consistency Training and Data Augmentation</head><p>Chapelle et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> describe the cluster assumption, in which data samples that belong to the same cluster structure are likely to belong to the same class. Unlabeled data points can then be used to better define the boundaries of these clusters, where the class of each cluster is defined by the labeled data points within. This assumption is also equivalent to the low-density assumption, in which the decision boundaries should lie in low-density regions. Consistency training (also consistency regularization) can be formulated by combining these assumptions into a regularization task: given an unlabeled data point x u , a classifier f θ (x), and a perturbation δ, then f θ (x u ) = f θ (x u + δ). In other words, a classifier should be invariant to small perturbations applied on the input, which is typically enforced by an additional loss term.</p><p>The choice of perturbation (δ) induced on an unlabeled sample has varied across SSL techniques. The Γ-Model <ref type="bibr" target="#b11">[12]</ref>, Π-Model <ref type="bibr" target="#b7">[8]</ref>, and Mean Teacher <ref type="bibr" target="#b13">[14]</ref> perturb unlabeled samples using Gaussian noise and simple data augmentations (e.g. random translation and horizontal flips), while VAT <ref type="bibr" target="#b8">[9]</ref> applies noise that adversarially affects classifier outputs. UDA <ref type="bibr" target="#b15">[16]</ref> applies a more diverse range of augmentations, and ICT <ref type="bibr" target="#b14">[15]</ref> and MixMatch <ref type="bibr" target="#b1">[2]</ref> both use MixUp <ref type="bibr" target="#b17">[18]</ref> to train an SSL classifier to output consistent predictions on linear interpolations of data points.</p><p>RealMix performs consistency training by applying MixUp, horizontal flips, and random translation on labeled and unlabeled samples (detailed in section 3.3) We also extend our unlabeled sample distribution by creating several augmented copies (augmented using CutOut <ref type="bibr" target="#b4">[5]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Entropy Minimization</head><p>Entropy Minimization (EM) <ref type="bibr" target="#b5">[6]</ref> has been applied in SSL to encourage high-confidence classifier outputs. This approach is also inspired by the low-density assumption, as a classifier with a decision boundary passing through highdensity regions would make low-confidence predictions on a number of samples. VAT <ref type="bibr" target="#b8">[9]</ref> incorporates EM as a loss for b in 1, . . . , num batches do 12:</p><formula xml:id="formula_0">x l,b = Augment(x l,b ) 13:x u,b , y u,b = generateT argets(x u,b ) 14: end for 15:X l+u =X l +X u 16: Y l+u = Y l + Y u 17: X l , Y l = M ixU p α ((X l , Y l ), (X l+u , Y l+u )) 18: X u , Y u = M ixU p α ((X u , Y u ), (X l+u , Y l+u )) 19: L sup = CrossEntropy(f θ (X l ), Y l ) 20:</formula><p>if T SA then 21:</p><formula xml:id="formula_1">L sup = T SA(L sup , schedule) 22: end if 23: L unsup = M SE(f θ (X u ), Y u ) 24: L unsup = OODM ask γ (L unsup ) 25: L = L sup + λL unsup 26: θ = ExponentialM ovingAverage(θ) 27:</formula><p>perform gradient descent update on θ using L 28: end for 29: return θ term to further improve results, and MixMatch <ref type="bibr" target="#b1">[2]</ref> and UDA <ref type="bibr" target="#b15">[16]</ref> apply EM by sharpening the targets of unlabeled samples. We also apply EM through a sharpening function (described in section 3.2), as we find it to work well experimentally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">SSL in Realistic Contexts</head><p>Oliver et al. <ref type="bibr" target="#b10">[11]</ref> described a number of pitfalls of current SSL algorithms and provided recommendations to practitioners for when SSL may be appropriate. We do not investigate all of their findings, but instead focus on those most pertinent to our work. Specifically, these include that SSL is most likely applicable if:</p><p>• Transfer learning from similar domains using labeled datasets is not feasible.</p><p>• The labeled and unlabeled data samples are drawn from the same distribution.</p><p>With the two above-mentioned points in mind, we find that:</p><p>• RealMix surpasses performance compared to transfer learning and fine-tuning even when transfer learning from similar domains is feasible, including when the target and transfer domains share classes. We show this experimentally in section 4.2.3.</p><p>• RealMix is capable of surpassing baseline performance even when upwards of 75% of the unlabeled data comes from a different distribution than the labeled data. We accomplish this using out-ofdistribution masking, which prevents our classifier from learning on examples that are out-of-distribution. This is detailed further in section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RealMix</head><p>As discussed in section 1 and 2, RealMix unites the most successful approaches in SSL and adapts them to work in realistic contexts. An overview for RealMix is presented in <ref type="figure" target="#fig_0">fig. 1</ref> and algorithm 1.</p><p>Algorithm 2 Pseudocode for generating targets 1: Require: Sharpen(d, t): entropy minimization function <ref type="bibr">2:</ref> Require:x u,b : batch of unlabeled samples 3:</p><formula xml:id="formula_2">x u,b,aug1 = Augment(x u,b ) 4:x u,b,aug2 = Augment(x u,b ) 5: y u,b,aug1 = f θ (x u,b,aug1 ) 6: y u,b,aug2 = f θ (x u,b,aug2 ) 7:ȳ u,b = 1 2 (y u,b,aug1 + y u,b,aug2 ) 8:ȳ u,b = Sharpen(ȳ u,b , 0.5) 9: returnȳ u,b</formula><p>Formally, given labeled samples (X l , Y l ), unlabeled samples X u , MixUp beta distribution parameter α, out-ofdistribution masking parameter γ, and consistency training (unlabeled loss) weight λ, we can obtain a classifier f θ that minimizes eq. <ref type="formula">(2)</ref>:</p><formula xml:id="formula_3">f θ = RealM ix(X l , Y l , X u , α, γ, λ) (1) L = L sup + λL unsup (2)</formula><p>where L sup is the standard cross-entropy loss on labeled samples, λ is the consistency training (unlabeled loss) weight, and L unsup is computed using MSE and out-ofdistribution masking (see section 4.2.2) on targets of unlabeled samples. The generation of unlabeled targets is presented in algorithm 2 and we discuss hyperparameters α and γ in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Augmentation</head><p>Following UDA <ref type="bibr" target="#b15">[16]</ref>, we first extend our unlabeled set X u by applying 50 rounds of augmentations toX u using Extend(x), where Extend(x) can include cropping, flipping, or stronger augmentations such as CutOut <ref type="bibr" target="#b4">[5]</ref>. By using several augmented copies of unlabeled data, we provide our classifier with a wide range of perturbations that give more inductive biases about the data distribution.</p><p>As a part of consistency training, we compute targets y u,b for each unlabeled batchx u,b by averaging the classifier's predicted distribution over two additional augmentations created by a separate augmentation function, denoted Augment(x) (as shown in algorithm 2). We settled on two augmentations as additional augmentations significantly increased training time without significantly improving results. Note that Extend(x) produces many copies of unlabeled data, whereas Augment(x) produces just 1 copy for use in generating targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Entropy Minimization</head><p>MixMatch <ref type="bibr" target="#b1">[2]</ref> and UDA <ref type="bibr" target="#b15">[16]</ref> both implement entropy minimization through a sharpening function, which we also find to be helpful. By applying this function (line 8 of algorithm 2) on the unlabeled targetsȳ u,b , we encourage our classifier to produce low entropy predictions on unlabeled data. That is, for each class c i ∈ C:</p><formula xml:id="formula_4">Sharpen(p, t) i := p 1 T i |C| k=1 p 1 T k (3)</formula><p>where p is the average predicted class and t is the temperature of the sharpened distribution. Intuitively, the distribution approaches a one-hot distribution as t goes to 0. We find t = 0.5 to be a good value across multiple benchmark datasets and use it in all reported experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">MixUp</head><p>MixUp was proposed by Zhang et al. <ref type="bibr" target="#b17">[18]</ref> as a regularization technique to encourage high-margin decision boundaries and was utilized in SSL by ICT <ref type="bibr" target="#b14">[15]</ref> and MixMatch <ref type="bibr" target="#b1">[2]</ref>. Given two samples (x 1 , y 1 ), (x 2 , y 2 ) and Beta distribution parameter α, our MixUp function generates a new sample (x 3 , y 3 ) as follows:</p><formula xml:id="formula_5">φ ∼ Beta(α, α) (4) φ = max(1 − φ, φ) (5) x 3 = φ x 1 + (1 − φ )x 2 (6) y 3 = φ y 1 + (1 − φ )y 2<label>(7)</label></formula><p>Following data augmentation and the generation of unlabeled sample targets, we apply MixUp separately to both the labeled samples (X l , Y l ) and unlabeled samples (X u , Y u ) (see lines 17-18 of algorithm 1). As in MixMatch, the resulting samples (X l , Y l ) and (X u , Y u ) are linear interpolations of samples from both the labeled and unlabeled collections but are weighted to more closely resemble their "original" distribution (eq. (6)). In other words, (X l , Y l ) are more similar to the original labeled points and (X u , Y u ) are more similar to the original unlabeled points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Out-of-Distribution Masking</head><p>To combat the effects of labeled and unlabeled samples coming from different distributions on current SSL methods  <ref type="table">Table 1</ref>: Results comparing error of RealMix to other SSL methods on the distribution mismatch experiment. 0% mismatch serves as the baseline in which the labeled and unlabeled data are drawn from the same distribution. While other methods steadily increase in error as amount of mismatch increases, RealMix is surprisingly able to surpass baseline performance when there is over 75% mismatch.</p><p>(see results of Mean Teacher and MixMatch in table 1), we introduce out-of-distribution masking. The goal of out-ofdistribution masking is to mask out the unlabeled samples that the classifier has the least confidence in, computing gradients only on samples that have a confidence above a moving threshold and are thus likely in-distribution samples (see <ref type="figure">fig. 4</ref>).</p><p>It is important that the threshold for masking samples is not static, as over the course of training, we found that entropy minimization tended to force confidence values on most unlabeled samples above a specified static threshold and render the threshold useless. To find a dynamic threshold for each training step, we specify a hyperparameter 0 ≤ γ ≤ 1 that dictates what percentage of unlabeled samples to mask. We then exclude samples that have confidence values in the bottom γ * 100% from training. Intuitively, γ can be thought of as the level of "noise" present in the unlabeled dataset.</p><p>Out-of-distribution masking helps to make RealMix extremely effective at mitigating unlabeled data mismatch, as RealMix is able to maintain performance above a supervised baseline no matter the amount of induced mismatch (see <ref type="table">table 1</ref> and <ref type="figure" target="#fig_2">fig. 2</ref>). We also perform an ablation on γ in table 6 to show that out-of-distribution masking boosts performance even if the optimal γ value is not found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training Signal Annealing</head><p>Semi-supervised learning algorithms have been evaluated on labeled data set sizes as few as 250 labels while the unlabeled data collections are often orders of magnitude larger <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. To mitigate the effects of overfitting to such small quantities of labeled data samples, Xie et al. <ref type="bibr" target="#b15">[16]</ref> introduces training signal annealing (TSA). TSA delays the release of training signal based on a training schedule (logistic, linear, exponential) to limit training on labeled samples that the classifier is already confident about. We find TSA to help with training on 250 labeled samples or less using a linear schedule. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In the following sections, we show RealMix's performance on benchmark datasets, a distribution mismatch experiment, comparison to transfer learning, and an ablation study on its components.</p><p>To allow for comparison with prior SSL techniques, we follow the WRN-28-2 architecture <ref type="bibr" target="#b16">[17]</ref>, hyperparameter selection (for α and λ), and evaluation procedure described by Berthelot at al. <ref type="bibr" target="#b1">[2]</ref> (which uses weight decay and an exponential moving average of model parameters). A key difference is that we train only for 500k iterations and use only 1 GPU, similarly to Oliver et al. <ref type="bibr" target="#b10">[11]</ref> to emulate a more realistic training setup. We report uncertainty values according to the standard deviation across 2 random seeds where possible. We also base our code implementation of RealMix and other SSL methods presented in this paper off of those created by Berthelot at al. <ref type="bibr" target="#b1">[2]</ref> in order to provide the research community with reproducible results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baselines</head><p>We report baseline results for the CIFAR10 and SVHN experiments from the Π-Model <ref type="bibr" target="#b7">[8]</ref>, VAT <ref type="bibr" target="#b8">[9]</ref>, Mean Teacher <ref type="bibr" target="#b13">[14]</ref> from those presented in <ref type="bibr" target="#b1">[2]</ref>, and re-run Mix-Match <ref type="bibr" target="#b1">[2]</ref> and RealMix according the settings described in the previous section. For the distribution mismatch experiment (illustrated in <ref type="figure" target="#fig_2">fig. 2</ref>), we additionally re-run and report results for Mean Teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">CIFAR10 and SVHN</head><p>We compare RealMix and prior SSL methods on the benchmark datasets CIFAR10 and SVHN, with results visible in table 2, <ref type="figure" target="#fig_3">fig. 3 and table 3</ref>. The typical evaluation method for SSL methods is to discard all but a number of labels, reporting performance across varying labeled set sizes. For CIFAR10, we evaluate RealMix and MixMatch [2] on 4 labeled set sizes (250, 500, 1000, 4000) and present the results found by Berthelot et al. <ref type="bibr" target="#b1">[2]</ref> for Π-Model, VAT, Mean Teacher. For SVHN, we evaluate RealMix and MixMatch on 2 labeled set sizes (250, 4000) and compare them with results found by Berthelot et al. <ref type="bibr" target="#b1">[2]</ref> for Π-Model, VAT, Mean Teacher. Note that these 3 models are run for 500k iterations more than the RealMix and MixMatch experiments, leaving room for further improvement on RealMix given a larger training budget.</p><p>We find that RealMix sets a new state-of-the-art on CIFAR10 with 250 labels, with an error rate of 9.79% and 17% reduction in error from the current state-ofthe-art MixMatch. Compared to the fully-supervised baseline with an error rate of 4.48%, RealMix is able to use 200x fewer labels to capture over 94% of the test accuracy. We also find that RealMix is competitive with MixMatch on SVHN across labeled set sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>250 Labels 4000 Labels Π-Model <ref type="bibr" target="#b7">[8]</ref> 53.02 18.13 VAT <ref type="bibr" target="#b8">[9]</ref> 36.03 11.32 Mean Teacher <ref type="bibr" target="#b13">[14]</ref> 47.32 10.72 MixMatch <ref type="bibr" target="#b1">[2]</ref> 11.78 6.45 RealMix 9.79 ± 0.75 6.39 ± 0.27 3.63 ± 0.24 3.07 ± 0.14 RealMix</p><p>3.53 ± 0.38 3.13 ± 0.11  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Distribution Mismatch</head><p>Oliver et al. <ref type="bibr" target="#b10">[11]</ref> introduced a distribution mismatch experiment using CIFAR10 to evaluate the robustness of SSL methods to out-of-distribution examples in unlabeled data. By evaluating robustness to mismatch, a practitioner can determine in which situations SSL may be preferable to using labeled samples alone. CIFAR10 contains two sets of classes: animals (bird, cat, deer, dog, frog, horse) and transportation (airplane, automobile, ship, truck). We simulate a mismatch by making the labeled distribution consist of the 6 animal classes each with 400 labels and varying the overlap of animal classes that make up the unlabeled distribution. For example, at 0% mismatch the unlabeled distribution consists of 4 classes that are all animals and at 100% mismatch, the unlabeled distribution consists of the 4 transportation classes. We evaluate RealMix, MixMatch, and Mean Teacher on varying levels of mismatch (0%, 25%, 50%, 75%, 100%) and present our results in <ref type="figure" target="#fig_2">fig. 2</ref>.</p><p>Surprisingly, RealMix is able to surpass baseline performance on the 6 animal classes alone at all levels of mismatch. Our ablation study (results in table 6) shows that RealMix is robust to unlabeled distribution mismatch as a result of out-of-distribution masking. Both MixMatch and Mean Teacher are able to surpass baseline performance with limited mismatch, but perform far worse when more significant amounts of mismatch (75% and 100%) are introduced.</p><p>Notably, RealMix is able to surpass baseline perfomance even when the unlabeled classes share no overlap <ref type="figure">Figure 4</ref>: Illustration of the out-of-distribution masking process. RealMix produces both the confidence on each of the images and the threshold that should be applied to them based on γ and the confidence values of that given batch (In this example γ = 0.66) . Only the images with a confidence above this dynamic threshold contribute to the unsupervised loss.</p><p>with labeled classes. This would suggest that the classifier is able to learn from unlabeled data that is outof-distribution, which we hypothesize to be the result of MixUp <ref type="bibr" target="#b17">[18]</ref> generating new samples that are still "slightly" in-distribution. We also selected values of the hyperparameter γ for OODM ask γ (x) as 0, 0.20, 0.40, 0.60, and 0.85 respectively for the levels of mismatch to represent the expected percentage of unlabeled mismatch. These γ values were not tuned and RealMix's performance on this experiment could presumably improve further. We also hope that future work in SSL considers out-of-distribution robustness as a key evaluation, as it not always true in real-world settings that unlabeled and labeled data arise from the same distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Transfer Learning</head><p>Transfer learning is often an attractive first option when faced with limited quantities of labeled data, which we study following the findings of Oliver et al. <ref type="bibr" target="#b10">[11]</ref> that transfer learning may be a preferable alternative to SSL. We pretrained a classifer on ILSVRC-2012 <ref type="bibr" target="#b12">[13]</ref> downsampled to 32x32 and then fine-tuned it on CIFAR10 at 250 and 4000 labels.</p><p>We find that RealMix (as well as MixMatch) outperform transfer learning and finetuning on labeled data alone, even when there is overlap in the CIFAR10 and ILSVRC-2012. This suggests that the error rates of 20.60 and 8.45 are upper bounds on the performance using transfer learning and fine-tuning. We also find transfer learning can be complementary to SSL. Specifically, we set a new state-of-the-art on CIFAR10 with 250 labels and reduce the error rate to just 8.48%. We also attempted transfer learning on SVHN, and found that SSL methods performed far better than transfer learning -likely because the datasets are quite different.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Ablation</head><p>We finally perform an ablation study on two components of the RealMix algorithm: data augmentation and outof-distribution masking (referred to as Extend(x) and OODM ask(x) respectively, in section 3.1 and algorithm 1). RealMix extends unlabeled samples using 50 copies of samples augmented with CutOut <ref type="bibr" target="#b4">[5]</ref>, which gives us the state-of-the-art error rate on CIFAR10 with 250 labels of 9.79%. Using a simpler augmentation (random translation and horizontal flips) and using fewer augmented copies both give slightly weaker results (as listed in table 5), suggesting that performing targeted augmentations and making more augmented copies of unlabeled data may further improve results.</p><p>In section 4.2.2 we study the effects of distribution mismatch on RealMix and claim that this is due to our use of out-of-distribution masking. In fact, table 6 shows that RealMix's ability to meet or decrease the baseline error rate of 20.32% is indeed linked to out-of-distribution masking, and without it, error increases markedly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>CIFAR10 on 250 Labels RealMix 9.79 RealMix w/ Simple Aug 10.42 RealMix w/ 25 Augs 10.80  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we presented RealMix, a novel semisupervised learning technique to improve classification performance even under situations when there is a significant shift between the distributions of the unlabeled and the labeled data. RealMix is, to the best of our knowledge, the only SSL approach that is able to maintain baseline performance when there is a complete mismatch in the labeled and unlabeled distributions. This is a particularly important contribution when considering the applicability of semisupervised learning outside of academic settings where data is scarce and often noisy.</p><p>We demonstrated that RealMix achieves state-of-the-art performance on common semi-supervised learning benchmarks such as CIFAR10 and SVHN, notably achieving an error rate of 9.79% on CIFAR10 using 250 labels.</p><p>Additionally, we showed that using transfer learning techniques compliments our method to further reduce the error on CIFAR10 with 250 labels to just 8.48%.</p><p>We hope that these results illustrate the practicality of semi-supervised learning in real world settings, and alongside the provided source code, will foster future research to further advance semi-supervised learning techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A high-level illustration overview of RealMix, a novel semi-supervised learning technique improving classification performance when there is a significant shift between the distributions of the unlabeled and the labeled data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Pseudocode for RealMix Algorithm 1: Require: f θ (x): deep neural network with trainable parameters θ 2: Require: X l , Y l : set of labeled data points 3: Require: X u : set of unlabeled data points 4: Require: Extend(x): stochastic data augmentation function for unlabeled data 5: Require: Augment(x): stochastic data augmentation function for consistency training 6: Require: M ixU p α (A, B), α: MixUp function and Beta distribution parameter 7: Require: T SA(L sup ), schedule: Training signal annealing function for supervised loss and annealing schedule. 8: Require: OODM ask γ (L sup ), γ Out-of-distribution masking function for unsupervised loss and masking parameter. 9:X u = Extend(X u ) 10: for t in 1, . . . , num epochs do 11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Error rate comparison of RealMix to other state of the art methods on the distribution mismatch experiment. All of the experiments are run using 6 animal classes from CIFAR10 with 400 samples per class as labeled data, and varying the overlap of animal classes that make up the unlabeled data. For example, at 0% mismatch the unlabeled distribution is made up of 4 animal classes and at 100% mismatch the unlabeled distribution is made up of 4 nonanimal classes. We present supervised baseline results using the 2400 labeled samples, which achieves an error rate of 20.32%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Results of SSL algorithms on CIFAR10 across varying labeled set sizes. Note that results Π-Model, VAT, Mean Teacher come from Berthelot et al.<ref type="bibr" target="#b1">[2]</ref> which are run for 500k iterations more than RealMix and MixMatch experiments. RealMix achieves state-of-the-art performance on CIFAR10 with 250 labels with an error rate of 9.79%, while the supervised baseline trained on all 50000 CIFAR10 samples achieves error of 4.48%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>17.85 17.25 18.95 20.14 20.57 MM [2] 16.75 18.14 21.01 21.07 22.08 RealMix 16.41 16.60 16.51 16.99 17.62</figDesc><table><row><cell>Method</cell><cell>0%</cell><cell>25%</cell><cell>50%</cell><cell>75% 100%</cell></row><row><cell>MT [14]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results comparing error of RealMix to other SSL methods on CIFAR10 with 250 and 4000 labeled samples.</figDesc><table><row><cell cols="3">The supervised baseline trained on all 50000 CIFAR10</cell></row><row><cell cols="2">samples achieves error of 4.48%.</cell><cell></cell></row><row><cell>Method</cell><cell>250 Labels</cell><cell>4000 Labels</cell></row><row><cell>Π-Model [8]  †</cell><cell>17.65 ± 0.27</cell><cell>5.57 ± 0.14</cell></row><row><cell>VAT [9]  †</cell><cell>8.41 ± 1.01</cell><cell>4.20 ± 0.15</cell></row><row><cell>Mean Teacher [14] †</cell><cell>6.45 ± 2.43</cell><cell>3.39 ± 0.11</cell></row><row><cell>MixMatch [2]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results comparing error of RealMix to other SSL methods on SVHN with 250 and 4000 labeled samples. The supervised baseline trained on all 73257 SVHN samples achieves error of 2.72%.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Results comparing error of RealMix to transfer</cell></row><row><cell>learning (from ILSVRC-2012) on CIFAR10 with 250 and</cell></row><row><cell>4000 labeled samples. We find that not only are recent</cell></row><row><cell>SSL methods and RealMix able to surpass transfer learning</cell></row><row><cell>alone, but combining transfer learning with RealMix can</cell></row><row><cell>further improve results.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results from ablation experiments on the augmentation type and amount from CIFAR10 on 250 labels. RealMix uses CutOut<ref type="bibr" target="#b4">[5]</ref> to generate 50 copies of unlabeled data.</figDesc><table><row><cell>Method</cell><cell>OOD w/ 75% Mismatch</cell></row><row><cell>RealMix (γ = 0.60)</cell><cell>16.99</cell></row><row><cell>RealMix (γ = 0.3)</cell><cell>20.73</cell></row><row><cell>RealMix w/o OODMask</cell><cell>22.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results from ablation experiments on out-ofdistribution masking on the experiment from table 1 with 75% mismatch. Using OODMask, RealMix meets or surpasses the supervised baseline performance (error of 20.32%) at multiple values of γ.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Available at https://github.com/uizard-technologies/realmix</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Unsupervised and Transfer Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Introduction to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semi-Supervised Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cluster kernels for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno>abs/1610.02242</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: A regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semisupervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Wide residual networks. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno>abs/1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization. ArXiv</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
