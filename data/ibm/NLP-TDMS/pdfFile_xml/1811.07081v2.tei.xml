<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Skeleton-based Gesture Recognition Using Several Fully Connected Layers with Path Signature Features and Temporal Transformer Module</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhang</surname></persName>
							<email>eexinzhang@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lufan</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Skeleton-based Gesture Recognition Using Several Fully Connected Layers with Path Signature Features and Temporal Transformer Module</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The skeleton based gesture recognition is gaining more popularity due to its wide possible applications. The key issues are how to extract discriminative features and how to design the classification model. In this paper, we first leverage a robust feature descriptor, path signature (PS), and propose three PS features to explicitly represent the spatial and temporal motion characteristics, i.e., spatial PS (S PS), temporal PS (T PS) and temporal spatial PS (T S PS). Considering the significance of fine hand movements in the gesture, we propose an "attention on hand" (AOH) principle to define joint pairs for the S PS and select single joint for the T PS. In addition, the dyadic method is employed to extract the T PS and T S PS features that encode global and local temporal dynamics in the motion. Secondly, without the recurrent strategy, the classification model still faces challenges on temporal variation among different sequences. We propose a new temporal transformer module (TTM) that can match the sequence key frames by learning the temporal shifting parameter for each input. This is a learning-based module that can be included into standard neural network architecture. Finally, we design a multi-stream fully connected layer based network to treat spatial and temporal features separately and fused them together for the final result. We have tested our method on three benchmark gesture datasets, i.e., ChaLearn 2016, ChaLearn 2013 and MSRC-12. Experimental results demonstrate that we achieve the state-of-the-art performance on skeleton-based gesture recognition with high computational efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>stream RNN  and Long Short-Term Memory (LSTM) <ref type="bibr" target="#b40">(Weng et al. 2018)</ref>, are popular choice to explore the temporal dependency for recognition. These frameworks have reached state-of-the-art recognition results but the computational complexity may be unacceptable in real-world applications. Hence, we need simple, compact and explicit features to represent global body movements and fine hand motions. Also, the classification model should be simple with temporal dependency.</p><p>In this paper, we propose the path-signature feature based hand gesture recognition framework with only few fully connected layers. The flowchart of our algorithm is shown in <ref type="figure">Fig. 1</ref>. The main contributions are as follows: • We introduce the three different path signature (PS) features , i.e., spatial (S PS), temporal (T PS) and temporal spatial PS (T S PS) features, to explicitly characterize spatial configuration and temporal dependency of hand gestures. We also propose an AOH principle to define joint pairs for the S PS and select single joint for the T PS. In addition, the dyadic method is employed to extract the T PS and T S PS features that encode both global and local temporal dynamics. • We propose the temporal transformer module (TTM) that can actively produce an appropriate temporal transformation for each input sequence. This is a learning-based module that can be included into standard neural network architecture. • We propose an extremely simple multi-streams architecture as the classifier with only several fully connected (FC) layers. Different features have their own channels and the FC layer are used for final fusion. • By only using skeleton data, our method obtains the state-of-the-art results on three major benchmarks, i.e., ChaLearn 2013, ChaLearn 2016 and MSRC 12. Further, our model requires less floating-point multiplication-adds and training memory.</p><p>2 Related Work 2.1 Gesture recognition Skeleton based hand gesture recognition methods are much less than those dealing with the full body skeleton based action recognition . It is limited by the dataset availability and gesture unique property. Hand gesture mainly involves the finger, palm and hand motion, which only has 1-3 joints in the skeleton obtained from depth data. In (De Smedt, Wannous, and Vandeborre 2016), several skeleton-based features are used together as temporal pyramid, including shape of connected joints, histogram of hand direction and histogram of wrist rotation. In 2D skeleton are superimposed onto original image as dynamic signatures. These features aim to describe the hand motion in detail but the description and representation abilities are limited.</p><p>Deep learning have made great process in the area of action recognition. Considering the sequential property, it is natural to apply the RNN, LSTM and their extensions to learn temporal dynamics. In 2017 ChaLearn LAP RGB-D isolated gesture recognition competition <ref type="bibr" target="#b32">(Wan et al. 2017)</ref>, the largest hand gesture recognition contest, most participants (including the winner) used C3D and/or LSTM neural networks. C3D architecture has been widely used in the action recognition for appearance and motion modeling because it is more suitable for spatial-temporal feature extraction than 2D CNN. The model inputs are multimodalities including RGB, depth, optical flow and/or skeleton. Recently, <ref type="bibr" target="#b40">(Weng et al. 2018</ref>) proposed a deformable pose traversal convolution method based on 1D convolution and LSTM. These recognition networks usually have multiple LSTMs and temporal streams channels and the final result is multi-stream average fusion. In , RNN architecture not only characterizes the temporal dynamics but also considers the spatial configuration in the two-stream architecture. With the proper modeling of skeleton structure and spatial dependency of the action, recognition accuracy increased. In the latest hand gesture recognition research <ref type="bibr" target="#b26">(Narayana, Beveridge, and Draper 2018)</ref>, by using RGB-D and their flow as inputs, the network has 12 channels representing the large body movement and fine hand motions individually. The fusion channel is a sparsely connected network with one weight per gesture and channel. The RNN/LSTM frameworks deliver the stateof-the-art performance on most action and gesture recognition datasets, indicating the excellent feature and dependency learning capabilities. The only concerns are the architecture complexity, training data requirement and computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Path signature feature</head><p>The path signature (PS) was first proposed in <ref type="bibr" target="#b1">(Chen 1958)</ref> in the form of noncommutative formal power series. After that PS was used to solve differential equations driven by rough paths <ref type="bibr" target="#b24">(Lyons 1998;</ref><ref type="bibr" target="#b10">Garrido 2010)</ref>. Recently, the path signature has been successfully used as a trajectory descriptor and applied to many tasks in the field of machine learning and pattern recognition, such as quantitative finance <ref type="bibr" target="#b11">(Gyurkó et al. 2013;</ref><ref type="bibr" target="#b23">Lyons, Ni, and Oberhauser 2014)</ref>, handwriting recognition <ref type="bibr" target="#b17">(Lai, Jin, and Yang 2017;</ref>, writer identification <ref type="bibr" target="#b44">(Yang, Jin, and Liu 2015;</ref><ref type="bibr" target="#b21">Liu, Jin, and Xie 2017)</ref>, human action ) and hand gesture recognition <ref type="bibr" target="#b19">(Li, Zhang, and Jin 2017)</ref>. ) is the pioneer work employing the path signature feature for skeleton-based action recognition. All joint pairs and temporal joint evolution are considered as path and the corresponding path signatures are computed as features. The concatenation of all path signatures are the input vector for classification. In <ref type="bibr" target="#b19">(Li, Zhang, and Jin 2017)</ref>, the path signature is the firstly used in the gesture recognition by defining the hand trajectory as the path. Path signature can provide the informative representation of sequential data but how to define proper paths and how to deal with their high dimensionality is worthy to be explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of Path Signature</head><p>In this section, we will briefly introduce the mathematical definition, geometric interpretation and some properties of path signature (PS), which is mainly referred to <ref type="bibr" target="#b2">(Chevyrev and Kormilitzin 2016)</ref>.</p><p>Assume a path P :</p><formula xml:id="formula_0">[t 1 , t 2 ] → R d , where [t 1 , t 2 ]</formula><p>is a time interval. The coordinate paths are denoted by (P 1 t , ..., P d t ), where each P i : [t 1 , t 2 ] → R is a real-value path. For an integer k ≥ 1 and the collection of indices i 1 , ..., i k ∈ {1, ..., d}, the k-fold iterated integral of the path along indices i 1 , ..., i k can be defined as:</p><formula xml:id="formula_1">S(P ) i1,...,i k t1,t2 = t1&lt;a k &lt;t2 ... t1&lt;a1&lt;a2 dP i1 a1 ...dP i k a k (1) where t 1 &lt; a 1 &lt; a 2 &lt; ... &lt; a k &lt; t 2 .</formula><p>The signature of path P , denoted by S(P ) t1,t2 , is the collection (infinite series) of all the iterated integrals of P :</p><formula xml:id="formula_2">S(P ) t1,t2 =(1, S(P ) 1 t1,t2 , S(P ) 2 t1,t2 , ..., S(P ) d t1,t2 , S(P ) 1,1 t1,t2 , ..., S(P ) 1,d t1,t2 , ..., S(P ) d,d t1,t2 , ..., S(P ) 1,...,1 t1,t2 , ..., S(P ) i1,...,i k t1,t2</formula><p>, ..., S(P ) d,...,d t1,t2 , ...)</p><p>(2)</p><p>The k-th level PS is the collection (finite series) of all the kfold iterated integral of path P . The 1-st and 2-nd level represents path displacement and path curvature respectively. By increasing k, higher levels of path information can be extracted, but the dimensionality of iterated integrals enlarge rapidly as well. Note that the 0-th level PS of path P is equal to 1 by convention.</p><p>In practice, we often truncate the S(P ) t1,t2 at level m to ensure the dimensionality of the PS feature in a reasonable range. The dimensionality of S(P ) t1,t2 truncated at level m is calculated through M = d + · · · + d m (without zeroth term).</p><p>The path is considered as the piecewise linear path after sampling. The PS of a discrete path with finite length can be easily calculate based on linear interpolation and Chen's identity <ref type="bibr" target="#b1">(Chen 1958)</ref>. For each straight line of a path, the element of its PS can be calculates by:</p><formula xml:id="formula_3">S(P ) i1,...,i k t,t+1 = 1 k! k j=1 S(P ) ij t,t+1<label>(3)</label></formula><p>For the entire path, Chen's identity states that for any</p><formula xml:id="formula_4">(t s , t m , t u ) satisfying: t s &lt; t m &lt; t u , then, S(P ) i1,...,i k ,...,in ts,tu = n k=0 S(P ) i1,...,i k ts,tm S(P ) i k+1 ,i k+2 ,...,in tm,tu</formula><p>(4) PS has two excellent properties for path expression. First, PS is the unique representation of a non tree-like path (Hambly and Lyons 2010). A tree-like path is a trajectory that retraces itself (such as clapping). For time-sequential data, it's natural and effective to add an extra time dimension into the original path to avoid the tree-like situation. Second, shuffle product identity <ref type="bibr" target="#b22">(Lyons, Caruana, and Lévy 2004)</ref> indicates that the product of two signature of lower level can be expressed as a linear combination of some higher level terms. Hence, adoption of higher level terms of PS actually brings more nonlinear prior knowledge, which reduces the need for the classifier of high complexity. More properties and related details can be found in <ref type="bibr" target="#b2">(Chevyrev and Kormilitzin 2016)</ref>.</p><p>We recommend an open-source python library named iisignature, which can be easily installed through pip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>In this section, we first introduce an "attention on hand" (AOH) principle for PS extraction, which deals with global body movements and fine hand motions. Then we propose a novel temporal transformer module (TTM) to alleviate the sequence temporal variation. Finally a multi-stream architecture is presented to fuse different types of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">AOH principle and PS extraction</head><p>AOH principle Before calculating the PS feature, we need to consider about what path to be used and how to design paths efficiently for the recognition. In , the first work leveraging PS features in the human action recognition problem, single joint, joint pair and joint triple are utilized to define paths. They use all N joints and exhaust all the possible pairs and triples (i.e., C 2 N and C 3 N ), which brings performance improvement but increases dimensionality dramatically. In the context of gesture recognition, we propose the AOH principle to select single joints and joint pairs.</p><p>For the single joint, only the joint belongs to the hand part (including elbow, wrist and hand 3 joints, i.e., N J = 3 · 2 = 6) are selected, as <ref type="figure" target="#fig_0">Fig. 2 (a)</ref> shows. For the joint pair, three kinds of pairs are considered (as depicted in <ref type="figure" target="#fig_0">Fig. 2</ref> </p><formula xml:id="formula_5">(b) 1)- 3)</formula><p>). The first kind of joint pair is inside the same hand part, describing the geometric characteristics of hand explicitly. The second kind of joint pair is from two hand parts, indicating the relative state of two hands. The third kind of joint pair is across hand part and body part (upper body joints except hand part), characterizing the related location of hand and body. The selected single joint and joint pairs defined by AOH principle can not only model global hand relative body movements and fine hand motions but also make the PS features more compact.</p><p>Path definition and PS feature extraction Based on the selected single joint and joint pairs obtained by AOH principle, we further define one spatial path and two kinds of temporal paths for PS feature extraction. We regard each joint pair as a spatial path for the PS feature extraction. The first type of temporal path is the evolution of each selected single joint along the time, as shown in <ref type="figure" target="#fig_0">Fig. 2 (c)</ref>. Another type of temporal path is the evolution of spatial correlations among joints, as shown in <ref type="figure" target="#fig_0">Fig. 2 (d)</ref>. The summary of three PS features are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Spatial PS features The fundamental description of spatial structure is the d-dimensional raw coordinates. We concatenate the coordinates of single joints in each frame as a RC vector obeying chronological order as <ref type="figure" target="#fig_0">Fig. 2 (c)</ref> shows. Further, due to the poor characterization ability of RC and noise interference, we extract PS features over selected joint pairs to explore the spatial relation between joints. The implementations of spatial PS feature extractor (Box A in <ref type="figure" target="#fig_0">Fig. 2</ref>) are as follows: i) Select elements that need to be calculated in Eq.1 according to the truncated level m S . ii) Calculate the truncated spatial PS of a joint pair (a straight line in Eq.3) by Eq.1 and Eq.3 (The start and end points are defined according to the predefined order in <ref type="figure" target="#fig_0">Fig. 2)</ref>. iii) Finally concatenate the truncated spatial PS of all input joint pairs as the spatial PS (S PS) feature.</p><p>Dyadic temporal PS features The dyadic method with PS was firstly used in  for the writer ID identification. Since the gesture always contain global and local variation, we employ the dyadic method for the temporal PS feature extraction. The dyadic method divides the entire path into dyadic pieces and set up a hierarchical representation of the path, which can extract both the global and local feature of entire path, and reduce the feature dimensionality as well. If the dyadic level is L D , then an entire path can be divided into 2 (L D +1) − 1 subpaths.</p><p>To characterize the temporal dynamic of single joint, the evolution of each single joint is treated as an entire temporal path , as shown in <ref type="figure" target="#fig_0">Fig. 2 (c)</ref>. An extra monotone time dimension is added to ensure the path uniqueness (i.e., to avoid tree-like path as discussed in Section 3).</p><p>To further explore kinematic constraints of the joint pairs, the evolution of each dimension in the S PS of every frame also can be regarded as another kind of entire temporal path, as <ref type="figure" target="#fig_0">Fig. 2 (d)</ref> shows. As a result, we acquire a series of 1D paths. However, the signature of a 1D path is just the increments to a certain power, which can be easily get from Eq. 3. To alleviate this problem, we use the lead-lag trans-formation as ) does over the 1D path to enrich the temporal contextual information.</p><p>The implementations of temporal PS feature extractor (Box B in <ref type="figure" target="#fig_0">Fig. 2</ref>) are as follows: i) Select elements that need to be calculated in Eq.1 according to the truncated level m T or m T S . ii) Every subpath generated by dyadic method is an entire path (consist of several straight lines) in Eq. 4. According to Eq. 1, Eq. 3 and Eq. 4, the truncated temporal PS of a subpath can be calculated. iii) Concatenate the truncated temporal PS of all subpaths as temporal PS (T PS) or temporal spatial PS (T S PS) feature.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Temporal Transformer Module (TTM)</head><p>Motivation Although deep neural network achieves break through in the sequential recognition task, it's still limited by the lack of ability to be temporally invariant to the input sequence in a computationally and parameter efficient manner. In the context of action recognition, the time-stamps of keyframes are variant among different clips, which makes the model difficult to catch the key information along time dimension.</p><p>There are mainly two existing types of methods to solve this problem: structure driven method and data driven method. The structure driven method mostly use LSTM to model the temporal contextual dependence of sequence data. The data driven method is to provide more diverse samples by temporal shift data enhancement. However, LSTM model requires large training data and unnegligible training cost. If we used a simple network like FC layer as the classifier, the temporal consistency is also learned as part of features, which is the unwanted result. For example, we visualize the weight matrix of the first FC layer of a trained one-stream network (will be introduced in the following), which takes RC as input, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The x-axis denotes the input dimensionality (obey chronological order), and the y-axis denotes the neuron number (64 in the first FC layer). The brighter position means the corresponding weight is larger, that is, this connection is more important. The FC layer pay more attention on several time stamp, indicating the position of key frames. If there is the temporal variation between the model and testing sequence, the recognition result is worse. Even if the training data is augmented by temporal shift, the model capacity is too small to fit any arbitrary temporal situation.</p><p>Recent work ) proposed a spatiotemporal transform method to deal with the spatiotemporal variation, but their method is for RGB-D video. As mentioned in Section 4.1, we have employed the temporal PS features to represent temporal dynamics within the sequence. This is what exactly ) has done. The inter-sequence temporal difference might be alleviated by the temporal transformation. To this end, we design a differentiable module called temporal transformer module (TTM). This module can actively transform the input data temporally and finally adjust the key frame to the best time stamp for the network.</p><p>Proposed TTM Inspired by STN <ref type="bibr" target="#b15">(Jaderberg et al. 2015)</ref>, TTM is a differentiable module that applies a temporal transform to RC. The TTM contains two steps: Localization network (LN) and temporal shifting.</p><p>Firstly, we use LN to learn the temporal transform factor delta (∆), as shown in <ref type="figure">Fig.4 (b)</ref>. It takes the input vector I ∈ R D RC (D RC denotes the dimensionality of RC) and output ∆ as shown in <ref type="figure">Fig. 4 (b)</ref>, i.e., ∆ = f LN (I). Note that the network function f LN () can take any form, such as FC layer or 1D convolution layer, but should finally regress to one neuron.</p><p>Secondly, the input vector I is reshaped as a matrix V i ∈ R d·N J ×F , where N J is the number of single joints (i.e., 6 in <ref type="figure" target="#fig_0">Fig. 2)</ref> and F denotes the frame number. Each column of V i is a vector v i x which consists of the coordinates of single </p><formula xml:id="formula_6">v o x = f I (x − ∆, V i ) = (1 − α) · v i x−∆ + α · v i x−∆ (5) Here, because ∆ is a decimal, we use linear interpola- tion function f I () to generate the V o . α is calculated by (x − ∆) − x − ∆ . Note that x − ∆ is clip by value [1, F]. Eventually, V o is reshaped back as a output vector O.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-stream Architecture</head><p>As discussed in Section 4.1, we define spatial and temporal path based on AOH principle. Corresponding PS features encode the spatial and temporal information of the action. Previous work <ref type="bibr" target="#b19">Li, Zhang, and Jin 2017)</ref> concatenates these features together as whole and one classification model is employed, e.g, FC layers. We believe temporal and spatial features should be treated separately because one represents the joint evolution and the other describes the body configuration. The final result is fusion of these multiple channels.</p><p>So we utilize the multi-stream architecture to process different kinds of information separately. As a result, we design three kinds of network architectures, one-stream network (1s net), two-stream network (2s net) and three-stream network (3s net), as shown in <ref type="figure">Fig. 4(c)</ref>-(e) . The 1s net directly concatenates all the features as one input vector and feed it to a 2-fc-layer network, similar to ). The 2s net has two inputs, RC and PS, representing basic information and extracted compact information. As defined in <ref type="table" target="#tab_0">Table 1</ref>, the T PS and T S PS represent temporal information and S PS is spatial feature. Hence, the 3s net has three streams with two FC layers separately. The final fusion result is obtained through a FC layer as the weighted summation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>ChaLearn 2013 dataset: It is the ChaLearn 2013 Multimodel gesture dataset <ref type="bibr" target="#b6">(Escalera et al. 2013)</ref>, which contains 23 hours of Kinect data with 27 persons performing 20 Italian gestures. This dataset provides RGB, depth, foreground segmentation and Kinect skeletons. Here, we only use skeletons for gesture recognition as done in the literature . ChaLearn 2016 dataset: ChaLearn Isolated dataset <ref type="bibr" target="#b31">(Wan et al. 2016)</ref>, the largest gesture recognition dataset consisting of RGB and depth videos, includes 35,878 training, 5,784 validation and 6,271 test videos for totally 249 gestures. We use Openpose <ref type="bibr" target="#b39">(Wei et al. 2016)</ref> to estimate the skeleton joints in all videos as <ref type="bibr" target="#b20">(Lin et al. 2018</ref>) did. It can be downloaded from http://www.hcii-lab.net/data/. <ref type="figure">Figure 4</ref>: Three different types of network architectures: one-stream network (1s net), two-stream network (2s net) and threestream network (3s net). TTM is the temporal transformer module. LN is the localization network that generates the transformation parameters ∆. and denote temporal shift and weighted sum. The fc2 in each stream denotes a FC layer with a softmax activation function.</p><p>MSRC-12 dataset: MSRC-12 gesture dataset <ref type="bibr" target="#b8">(Fothergill et al. 2012)</ref> includes 6 iconic and 6 metaphoric gestures performed by 30 people. We use 6 iconic gestures from the dataset that amounts to 3,034 instances and employ 5fold leave-person-out cross-validation as in <ref type="bibr" target="#b16">(Jung and Hong 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data Preprocessing and Network Setting</head><p>We first normalize skeletons by subtracting the central joint, which is the average position of all joints in a video clip. Then all coordinates are further normalized to the range of [-1, 1] over the entire video clip. Finally, we sample all videos clips to 39 frames by linear interpolation or uniform sampling. The data enhancement methods we use are three-fold. The first one is temporal augmentation by randomly temporal shift the frame in range of <ref type="bibr">[-5, 5</ref>]. The second one is adding Gaussian noise with a standard deviation of 0.001 to joints coordinates. The last one is rotating coordinates along x, y, z three axes in range of [−π/36, π/36], [−π/18, π/18] and [−π/36, π/36].</p><p>For ChaLearn 2013 and MSRC-12 two datasets, we set the neuron number of each 2-fc-layer stream to 64 and C (64 fc-C fc), where C is the gesture class number. For the largest dataset ChaLearn 2016, we use 256 fc-C fc. We adopt 64 fc-1 fc architecture for f LN (). DropOut <ref type="bibr" target="#b13">(Hinton et al. 2012</ref>) layer is added after the first FC layer of each stream to avoid over fitting. The mini-batch size and dropout rate are set to 56 and 0.5. We use the method of stochastic gradient descent with a momentum value equal to 0.7. The learning rate updates in accordance to α(n) = α(0) · exp(−λn), where n is the cumulative mini-batch number. α(0) and λ are set to 0.01 and 0.001. The L D for T PS and T S PS calculation are set to 3 and 2. The lead-lag dimensionality is set to 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>We do some ablation experiments on the ChaLearn 2013 dataset to explore the truncated level of PS and examine the effectiveness of PS, TTM and multi-stream architecture.</p><p>Investigation of the truncated level of PS We utilize the one-stream network without TTM (1s net w/o TTM) to explore the contributions of different truncated level PS. The validation accuracy rate is 77.84% with only RC. The accuracy rate after adding different PS are shown in <ref type="table" target="#tab_2">Table 2</ref>. It is noted that the T S PS is calculated from S PS truncated at level 2 (The abbreviation can be referred to <ref type="table" target="#tab_0">Table 1</ref>). The performance improves after adding any type of PS truncated at any level, which indicates the effectiveness of PS. The last column of each row illustrates that all types of PS feature are complementary.</p><p>It is worthy to note that the contributions trend to be negligible and even vanish when truncated level is greater than a certain value. There is a trade-off between validation performance and feature dimensionality. As a result, we choose to set m T , m S , m T S as 4, 2, 3.  Investigation of the TTM We use the 1s net with RC as input to examine the effectiveness of TTM. Results are presented in <ref type="table" target="#tab_4">Table 3</ref>. 1s net w/o TTM can be roughly regarded as the method proposed by ). Firstly, we use temporal augmentation to test whether the data driven methods can make the improvement. We shift the samples in range of <ref type="bibr">[-5, 5]</ref> to provide more diverse samples. However, it doesn't improve the performance. Hence, directly data driven methods cannot work well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PS truncated level +T PS +S PS +T S PS</head><p>Then we directly add TTM to the 1s net (1s net TTM), and the result improves from 77.84% to 80.55%. The temporal transformation parameter learned by TTM can fit the key moment of an action and the active part of the FC layer.</p><p>At last we add the same temporal augmentation for 1s net TTM, and the accuracy rate increases from 80.55% to 81.33%. This attractive observation indicates that TTM makes good use of the diverse samples provided by temporal enhancement. In other words, the network is more adaptive after adding TTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Components</head><p>Accuracy  Investigation of different network architectures For the estimation of different architecture, we use network without TTM, with RC and all PS selected above as input, which can be roughly regarded as the architecture utilized by . As shown in <ref type="table" target="#tab_6">Table 4</ref>, the performance improves clearly, which indicates that the multi-stream architectures allow each stream to dig deeply into one type of feature and finally provide more discriminative information.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with the State-of-the-arts</head><p>In this subsection, we use the best parameter setting and network structure getting from our ablation study. We also do all the data augmentation methods mentioned above for our network.</p><p>ChaLearn 2013 The results on the ChaLearn 2013 dataset are shown in <ref type="table" target="#tab_8">Table 5</ref>. Currently, methods achieving the best performance are mainly benefited from powerful characterization ability of CNN and LSTM models. <ref type="bibr" target="#b4">(Du, Fu, and Wang 2015)</ref> organizes the raw coordinates as the spatial temporal feature maps then feeds it to the hierarchical spatial-temporal adaptive filter banks CNN architecture.  propose a two-stream LSTM to model temporal dynamics and spatial configurations separately. Compared with these method, our FC based network achieves the best results with less multiplication-adds operation as <ref type="table" target="#tab_9">Table 6</ref> shows. Note that the AOH principle dramatically reduces the Multiplication-Adds compared with the method without AOH .    <ref type="table" target="#tab_11">Table 7</ref>. Our model outperforms the skeleton based method <ref type="bibr" target="#b20">(Lin et al. 2018</ref>) by around 4.5%. We also notice that the accuracies of skeleton based methods are inferior to video frame based models. The reasons are mainly two-fold. Firstly, the precision of OpenPose is affected by the drastic background and illumination changes. Secondly, a lot of gesture classes requires recognizing the static hand gesture instead of dynamic hand motion. The recognition performance on these classes mainly depends on the estimation precision of hand joints. It is worth noting that our model is the simplest one. We argue that the performance will improve if more accurate joints locations are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test acc.</p><p>Modality Model R D O S AMRL <ref type="bibr">[1]</ref> 65.59 √ √ 8*CNN+4ConvLSTM ASU <ref type="bibr">[2]</ref> 67.71 √ √ √ 4*C3D+2*TSN+1*SVM FOANet <ref type="bibr">[3]</ref>   MSRC-12 <ref type="bibr" target="#b35">(Wang et al. 2016)</ref> proposed Joint Trajectory Maps (JIM), which are a set of 2D images that encode spatiotemporal information carried by 3D skeleton sequences in three orthogonal planes. In <ref type="bibr" target="#b16">(Jung and Hong 2014)</ref>, a novel framework called Enhanced Sequence Matching (ESM) is leveraged to align and compare action sequences based on a set of elementary Moving Poses (eMP). (Garcia-Hernando and Kim 2017) proposed "transition forests", an ensemble of randomized tree classifiers that learnt both static pose information and temporal transitions. All these methods show the importance of spatio-temporal information modelling. Our method extracts spatial, temporal and joint spatialtemporal features and achieves the state-of-the-art accuracy of 99.01%, as shown in <ref type="table" target="#tab_13">Table 8</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we first leverage S PS, T PS and T S PS three PS features to explicitly represent the spatial and temporal motion characteristics. In the path definition, we propose the AOH principle to select single joint and joint pairs, which ensures the feature robust and compact. Furthermore, the dyadic method employed to extract the T PS and T S PS features that encode global and local temporal dynamics with less dimensionality. Secondly, we propose a differentiable module TTM to match the sequence key frames by learning the temporal shifting parameter for each input. Finally, we design a multi-stream FC layer based network to treat spatial and temporal features separately. The ablation study has shown the effective of every contribution. We have achieved the best result on skeleton-based gesture recognition with high computational efficiency on three benchmarks. We will explore the possible combination of the attention scheme and PS features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The illustration of PS features extraction (T PS, S PS and T S PS). (a) and (b) are single joints and joint pairs selected following AOH. (c) and (d) are the temporal paths of single joint and each dimension of S PS. Note that TTM (proposed in Section 4.2 and illustrated inFig. 4) should be implemented between AOH and PS extractors, but we omit it here for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>each predefined joint pair truncated at level m S . Temporal PS (T PS) The PS over the temporal evolution of each single joint truncated at level m T . Temporal Spatial PS (T S PS) The PS over the temporal evolution of each dimension of S PS truncated at level m T S .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The visualization of the weight matrix of first fully connected layer in a trained one-stream network. joints in the same frame, x ∈ [1, F ]. And if we denote the matrix after shifting as V o , then each column of it can be calculate by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>The proposed feature for skeletal hand gesture recognition.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The ablation study of PS features on ChaLearn 2013. The truncated level of PS can be referred to Section 3.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The ablation study of TTM on ChaLearn 2013. Temporal Enhancement is abbreviated to Temp. Enh.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The ablation study of network architectures.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison of methods on the ChaLearn 2013.</figDesc><table><row><cell cols="4">Method CNN [1] 2s LSTM [2] 3s net w/o AOH 3s net</cell></row><row><cell>Mult-adds (Million)</cell><cell>11.54</cell><cell>358.34</cell><cell>14.89 1 +15.06 2.69 1 +2.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison of the Multiplication-Adds. [1] : Du, Fu, and Wang [2] : Wang and Wang 1 : PS calculation.</figDesc><table><row><cell>ChaLearn 2016 The results on ChaLearn 2016 are sum-</cell></row><row><cell>marized in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Comparison on ChaLearn 2016 dataset. [1] : Wang et al. [2] : Miao et al. [3] : Narayana, Beveridge, and Draper [4] : Lin et al. RGB, depth, optical flow and skeleton are abbreviated as R, D, O and S.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Comparison of methods on the MSRC-12 dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>This work is supported by GD-NSF (2016A010101014,  2017A030312006, 2018A030313295), the Science and Technology Program of Guangzhou (2018-1002-SF-0561), the National Key Research and Development Program of China (2016YFB1001405), the MSRA Research Collaboration Funds (FY18-Research-Theme-022), Fundamental Research Funds for Central Universities of China (2017MS050). We thank Prof. Terry Lyons from University of Oxford and Dr. Hao Ni from UCL for their great help. We thank anonymous reviewers for their careful reading and insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Egocentric gesture recognition using recurrent 3d convolutional neural networks with spatiotemporal transformer modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3763" to="3771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Integration of paths-a faithful representation of paths by noncommutative formal power series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="395" to="407" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A primer on the signature method in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chevyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kormilitzin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03788</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skeletonbased dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>De Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vandeborre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ACPR), 2015 3rd IAPR Asian Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="579" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-modal gesture recognition challenge 2013: Dataset and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzàlez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Escalante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM on International conference on multimodal interaction</title>
		<meeting>the 15th ACM on International conference on multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="445" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Instructing people for training gestural interactive systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fothergill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mentis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1737" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transition forests: Learning discriminative temporal transitions for action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garcia-Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="432" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">System control and rough paths (oxford mathematical monographs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garrido</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Gyurkó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kontkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Field</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.7244</idno>
		<title level="m">Extracting information from the signature of a financial data stream</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Uniqueness for the signature of a path of bounded variation and the reduced path group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hambly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics</title>
		<imprint>
			<biblScope unit="page" from="109" to="167" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2466" to="2472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhanced sequence matching for action recognition from 3d skeletal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="226" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward high-performance online hccr: A cnn approach with dropdistortion, path signature and spatial stochastic max-pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="60" to="66" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient nonlinear markov models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1314" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lpsnet: A novel log path signature feature based hand gesture recognition framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="631" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale isolated gesture recognition using a refined fused model based on masked res-c3d network and skeleton lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition (FG 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="58" />
		</imprint>
	</monogr>
	<note>13th IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ps-lstm: Capturing essential sequential online information with path signature and lstm for writer identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="664" to="669" />
		</imprint>
	</monogr>
	<note>14th IAPR International Conference on</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Differential equations driven by rough paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lévy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecole dEté de Probabilités de Saint-Flour</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="93" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A feature set for streams and an application to high-frequency financial tick data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oberhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 International Conference on Big Data Science and Computing, 5</title>
		<meeting>the 2014 International Conference on Big Data Science and Computing, 5</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Differential equations driven by rough signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Revista Matemática Iberoamericana</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="310" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal gesture recognition based on the resc3d network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3047" to="3055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gesture recognition: Focus on the hands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Narayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5235" to="5244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative dimensionality reduction for multi-dimensional sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="91" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4041" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Chalearn looking at people rgb-d isolated and continuous datasets for gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="56" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Results and analysis of chalearn lap multi-modal isolated and continuous gesture recognition, and real versus fake expressed emotions challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Anbarjafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gorbova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3189" to="3197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling temporal dynamics and spatial configurations of actions using two-stream recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">e Conference on Computer Vision and Pa ern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Action recognition based on joint trajectory maps using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large-scale multimodal gesture recognition using heterogeneous networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3129" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Rgb-d-based human motion recognition with deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Computer Vision and Image Understanding</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical motion evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ACPR), 2015 3rd IAPR Asian Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="574" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deformable pose traversal convolution for 3d action and gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
	<note>Europen Conference on Computer Vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improved deep convolutional neural network for online handwritten chinese character recognition using domain-specific knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07675</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rotation-free online handwritten character recognition using dyadic path signature features, hanging normalization, and deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2016 23rd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4083" to="4088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Leveraging the path signature for skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03993</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Chinese character-level writer identification using path signature feature, dropstroke and deep cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="546" to="550" />
		</imprint>
	</monogr>
	<note>13th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deepwriterid: An end-to-end online text-independent writer identification system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="45" to="53" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
