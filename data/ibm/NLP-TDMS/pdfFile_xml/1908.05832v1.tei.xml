<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transferable Contrastive Network for Generalized Zero-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajie</forename><surname>Jiang</surname></persName>
							<email>huajie.jiang@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>200050</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>200031</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<email>xlchen@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transferable Contrastive Network for Generalized Zero-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Zero-shot learning (ZSL) is a challenging problem that aims to recognize the target categories without seen data, where semantic information is leveraged to transfer knowledge from some source classes. Although ZSL has made great progress in recent years, most existing approaches are easy to overfit the sources classes in generalized zero-shot learning (GZSL) task, which indicates that they learn little knowledge about target classes. To tackle such problem, we propose a novel Transferable Contrastive Network (TCN) that explicitly transfers knowledge from the source classes to the target classes. It automatically contrasts one image with different classes to judge whether they are consistent or not. By exploiting the class similarities to make knowledge transfer from source images to similar target classes, our approach is more robust to recognize the target images. Experiments on five benchmark datasets show the superiority of our approach for GZSL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object recognition is one of the basic issues in computer vision. It has made great progress in recent years with the rapid development of deep learning approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b12">13]</ref>, where large numbers of labeled images are required, such as ImageNet <ref type="bibr" target="#b27">[28]</ref>. However, collecting and annotating large numbers of images are difficult, especially for fine-grained categories in specific domains. Moreover, such supervised learning approaches can only recognize a fixed number of categories, which is not flexible. In contrast, humans can learn from only a few samples or even recognize unseen objects. Therefore, learning visual classifiers with no need of human annotation is becoming a hot topic in recent years.</p><p>Zero-shot learning (ZSL) aims to learn classifiers for the target categories where no labeled images are accessible. It is accomplished by transferring knowledge from the source categories with the help of semantic information. Semantic information can build up the relations among different classes thus to enable knowledge transfer from source classes to target classes. Currently the most widely used semantic information includes attributes <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b8">9]</ref> and word vectors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2]</ref>. Traditional ZSL approaches usually learn universal visual-semantic transformations among the source classes and then apply them to the target classes. In this way, the visual samples and class semantics can be projected into a common space, where zero-shot recognition is conducted by the nearest neighbor approach. Although ZSL has made great progress in recent years, the strong assumption that the test images only come from the target classes is not realistic for practical applications. Therefore, generalized zero-shot learning (GZSL) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">39]</ref> draws much attention recently, where test samples may come from either source or target classes. However, most existing ZSL approaches perform badly on GZSL task because they are easy to overfit the source classes, which indicates that they learn little knowledge about the target classes. These approaches learn the models only on the source categories and ignore the targets. Since the domain shift problem exists <ref type="bibr" target="#b10">[11]</ref>, the models learned on the source classes may not be suitable to the target classes, which results in overfitting the source categories in the GZSL task.</p><p>In order to tackle such problem, we propose to explicitly transfer the knowledge from the source classes to the target categories. The key problem for ZSL is that no labeled images are available for the target categories so we could not directly train the target image classifiers. An intuitive idea is to learn target classifiers from similar source images. For example, we could leverage the source images 'horse' to learn the target class 'zebra'. Based on this idea, we propose a novel transferable contrastive network for generalized zero-shot learning. It automatically contrasts the images with class semantics to judge whether they are consistent or not. <ref type="figure">Figure 1</ref> shows the motivations of our approach, where two key properties for ZSL are considered in the contrastive learning process: discriminative property and transferable property. We maximize the contrastive values of images with corresponding class semantics and minimize the inconsistent ones among source classes thus to ensure that our model is discriminative enough to recognize different classes. Furthermore, to make the contrast transferable to the target classes, we utilize the class similarities to transfer knowledge from the source-class images to similar target classes. In this way, the model will be more robust to the target categories though no labeled target images are available to learn the model.</p><p>The main contributions of this paper are in two aspects. First, we propose a novel transferable contrastive network for GZSL, where a new network structure is designed for contrastive learning. Second, we consider both the discriminative property and transferable property in the contrastive learning procedure, where the discriminative property ensures to effectively discriminate different classes and the transferable property guarantees the robustness to the target classes. Experiments on five benchmark datasets show the superiority of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic Information</head><p>Semantic information is the key to ZSL. It builds up the relations between the source and target classes thus to enable knowledge transfer. Recently, the most widely used semantic information in ZSL is attributes <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b8">9]</ref> and word vectors <ref type="bibr" target="#b21">[22]</ref>. Attributes are general descriptions of objects. They are accurate but need human experts for definition and annotation. Word vectors are automatically learned from large numbers of text corpus which reduces human labor. However, there is much noise in the texts, which restricts their performance. In this paper, we use the attributes as the semantic information since they are more accurate to bridge the source and target classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Visual-Semantic Transformations</head><p>Visual-semantic transformations establish relationships between the visual space and the semantic space. According to different projection directions, current ZSL approaches can be grouped into three types: visual to semantic embeddings, semantic to visual embeddings, latent space embeddings. We will introduce them in detail below.</p><p>Visual to semantic embeddings. These approaches learn the transformations from the visual space to the semantic space and perform image recognition in the semantic space. In the early age of ZSL, <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b8">9]</ref> propose to learn attribute classifiers to transfer knowledge from the source to the target classes. They train each attribute classifier independently, which is time-consuming. To tackle such problem, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> consider all attributes as a whole and learn label embedding functions to maximize the compatibilities between images and corresponding class semantics. Furthermore, <ref type="bibr" target="#b23">[24]</ref> proposes to synthesize the semantic representations of test images by a convex combination of source-class semantics using the probability outputs of source classifiers. To learn more robust transformations, <ref type="bibr" target="#b22">[23]</ref> proposes a deep neural network to combine attribute classifier learning and semantic label embedding.</p><p>Semantic to visual embeddings. These approaches learn the transformations from semantic space to the visual space and perform image recognition in the visual space, which can effectively tackle the hubness problem in ZSL <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">29]</ref>. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">40]</ref> predict the visual samplers by learning embedding functions from the semantic space to the visual space. <ref type="bibr" target="#b24">[25]</ref> adds some regularizers to learn the embedding function from class semantic to corresponding visual classifiers and <ref type="bibr" target="#b35">[35]</ref> utilizes knowledge graphs to learn the same embedding functions. Some other works directly synthesize the target-class classifiers <ref type="bibr" target="#b3">[4]</ref> or learn the target-class prototypes <ref type="bibr" target="#b13">[14]</ref> in the visual space by utilizing the class structure information. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7]</ref> exploit the auto-encoder framework to learn both the semantic to visual and visual to semantic embeddings simultaneously. Inspired by the generative adversarial networks, <ref type="bibr" target="#b38">[38]</ref> generates the target-class samples in the feature space and directly learns the target classifiers.</p><p>Latent space embedding. These approaches encode the visual space and semantic space into a latent space for more effective image recognition. Since the predefined semantic information may be not discriminative enough to classify different classes, <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42]</ref> propose to use class similarities as the embedding space and <ref type="bibr" target="#b15">[16]</ref> proposes discriminative latent attributes for zero-shot recognition. Moreover, <ref type="bibr" target="#b2">[3]</ref> ex-ploits metric learning techniques, where relative distance is utilized, to improve the embedding models. In order to learn robust visual-semantic transformations, <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23]</ref> utilize deep neural networks to project the visual space and the semantic space into a common latent space and align the representations of the same class.</p><p>Our approach belongs to the latent space embedding, but there is a little difference. Traditional methods aim to minimize the distance of images and corresponding class semantics in the latent space for image recognition, while our approach fuses their information for contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Zero-Shot Recognition</head><p>Zero-shot recognition is the last step for ZSL, most of which can be grouped into two categories. The distancebased approaches usually exploit the nearest neighbour approach to recognize the target-class samples <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b15">16]</ref> and the classifier-based approaches directly learn the visual classifiers to recognize the target-class images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b38">38]</ref>. Our approach utilizes contrastive values for image recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Discussions about Relevant Works</head><p>Most existing approaches ignore the target classes when learning the recognition model, so they are prone to overfitting the source classes in GZSL task. To tackle this problem, <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b43">43]</ref> leverage the semantic information of target classes to generate image features for training target classifiers. Although satisfactory performance has been achieved, it is difficult to train and use the generative models. While our approach is easy to learn. Moreover, it is complementary to such generative approaches. <ref type="bibr" target="#b19">[20]</ref> proposes a calibration network that calibrates the confidence of source classes and uncertainty of target classes. Different from it, we directly transfer knowledge to the target classes, which is more effective for GZSL. <ref type="bibr" target="#b10">[11]</ref> uses all the unlabeled target images to adjust the models in transductive ZSL settings. However, these images are often unavailable in practical conditions, so we perform the inductive ZSL task. <ref type="bibr" target="#b14">[15]</ref> proposes adaptive metric learning to make the model suitable for the target classes. However, the linear model restricts its performance. Another relevant work is <ref type="bibr" target="#b32">[32]</ref>, which also studies the relations between images and class semantics. Compared with <ref type="bibr" target="#b32">[32]</ref>, we design a novel network structure for TCN. Moreover, we explicitly transfer knowledge from the source images to similar target classes, which makes our model more robust to the target categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The objective of our approach is learning how to contrast the image with the class semantics. <ref type="figure" target="#fig_1">Figure 2</ref> shows the general framework of the proposed transferable contrastive network (TCN). It contains two parts: information fusion and contrastive learning. Instead of computing the distance between images and class semantics using fixed metric for recognition, we fuse their information and learn a metric that automatically judges whether the fusions are consistent or not, where high contrastive values should be obtained between images and corresponding class semantics. In order to make the contrastive mechanism suitable to the target classes, we explicitly transfer knowledge from source images to similar target classes since no target images are available for training. More details will be described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Settings</head><p>In zero-shot learning, we are given K source classes (denoted as Y s ) and L target classes (denoted as Y t ), where the source and target classes are disjoint, i.e. Y s ∩ Y t = ∅. We use the index {1, ..., K} to represent the source classes and {K + 1, ..., K + L} to represent the target classes. The source classes contain</p><formula xml:id="formula_0">N labeled images D = {(x i , y i )|x i ∈ X , y i ∈ Y s } N i=1</formula><p>, while no labeled images are available for the target classes. X represents the visual sample space. To build up the relations between the source and target classes,</p><formula xml:id="formula_1">semantic information A = {a c } K+L c=1 is provided for each class c ∈ Y s ∪ Y t .</formula><p>The goal of ZSL is to learn visual classifiers of target classes f zsl : X → Y t and the goal of GZSL is to learn more general visual classifiers of all classes f gzsl :</p><formula xml:id="formula_2">X → Y s ∪ Y t .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contrastive Network</head><p>Information Fusion. An intuitive way of contrasting an image with one class is to fuse their information and judge how consistent the fusion is. Therefore, we first encode the images and class semantics into the same latent feature space to fuse their information. As is shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we use two branches of neural network to encode the image and the class semantics into the same feature space respectively, where convolutional neural network (CNN) is utilized to encode the images and the multilayer perceptrons (MLP) is utilized to encode the class semantic information (attributes or word vectors). Then an element-wise product operation (⊗) is exploited to fuse the information from these two domains. Let f (x i ) denote the coding feature of the ith image and g(a j ) represent the coding feature of the jth class semantic, we can get the fused feature z ij as:</p><formula xml:id="formula_3">z ij = f (x i ) ⊗ g(a j )<label>(1)</label></formula><p>where a j is the class semantic of the jth class. Then we can feed z ij to the next stage to judge how well the image i is consistent with class j. Contrastive Learning. Different from previous approaches that use fixed distance, such as Euclidean distance or cosine distance, to compute the similarities between images and classes for image recognition, we design a contrastive network that automatically judges how well the image is consistent with a specific class. Let v ij denote the contrastive value between image i and class j, we can obtain it from the fused feature z ij as:</p><formula xml:id="formula_4">v ij = h(z ij )<label>(2)</label></formula><p>where h is the contrastive learning function.</p><p>In the contrastive learning phase, we should consider two characters: discriminative property and transferable property. Discriminative property indicates that the contrastive model should be discriminative enough to classify different classes. Transferable property means that the contrastive model should be generalized to the target classes.</p><p>In order to enable the discriminative property, we utilize the semantic information of source classes as supervision, where the contrastive values of consistent fusions are maximized and those of inconsistent ones are minimized. The loss function can be formulated by the cross-entropy loss:</p><formula xml:id="formula_5">L D = − N i=1 K j=1 m ij log v ij + (1 − m ij ) log(1 − v ij ) (3)</formula><p>where m ij is a class indicator. Let y i be the class label for the ith image, then m ij can be obtained by:</p><formula xml:id="formula_6">m ij = 1, y i = j 0, y i = j<label>(4)</label></formula><p>The goal of ZSL is to recognize the target classes. If we only use the source classes in the contrastive learning phase, it is easy to overfit and the model would be less transferable to the target classes. This is the problem that exists in most ZSL approaches. Unfortunately, we don't have labeled target images to take part in the contrastive learning process. To tackle such problem, we explicitly transfer knowledge from source images to the target classes by class similarities. In other words, the source images could also be utilized to learn similar target classes. Let s kj denote the similarity of source class k (k=1,...,K) to target class j (j=K+1,...,K+L) and then the loss function for transferable property is formulated as:</p><formula xml:id="formula_7">L T = − N i=1 K+L j=K+1 s yij log v ij + (1 − s yij ) log(1 − v ij )<label>(5)</label></formula><p>To summarize, our full loss function is:</p><formula xml:id="formula_8">L = L D + αL T<label>(6)</label></formula><p>where α is a parameter that controls the relative importance of discriminative property and transferable property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Class Similarity</head><p>In order to accomplish the contrastive learning approach proposed above, the similarities between the source and target classes should be obtained. Inspired by the sparse coding approach, we utilize the target classes to reconstruct a source class and the reconstruction coefficients are viewed as the similarity of the source class to the target classes. The objective function is:</p><formula xml:id="formula_9">s k = arg min s k ||a k − K+L j=K+1 a j s kj || 2 2 + β||s k || 2<label>(7)</label></formula><p>where a k is the semantic information of class k and s kj is the jth element of s k , which denotes the similarities of source class k to target class j. β is the regularization parameter. Then we normalize the similarity by </p><formula xml:id="formula_10">s kj = s kj K+L j=K+1 s kj<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Zero-Shot Recognition</head><p>We conduct zero-shot recognition by comparing the contrastive values of one image with all the class semantics.</p><p>For ZSL, we classify one image to the class which has the largest contrastive value among target classes, which can be formulated as:</p><formula xml:id="formula_11">P zsl (x i ) = max j {v ij } K+L j=K+1<label>(9)</label></formula><p>For GZSL, we classify one image to the class which has the largest contrastive value among all classes, which can be formulated as:</p><formula xml:id="formula_12">P gzsl (x i ) = max j {v ij } K+L j=1 (10)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Settings</head><p>We conduct experiments on five widely used ZSL datasets: APY <ref type="bibr" target="#b8">[9]</ref>, AWA (2 versions AWA1 <ref type="bibr" target="#b18">[19]</ref> and AWA2 <ref type="bibr" target="#b37">[37]</ref>), CUB <ref type="bibr" target="#b34">[34]</ref>, SUN <ref type="bibr" target="#b25">[26]</ref>. APY is a small-scale coarsegrained dataset with 64 attributes, which contains 20 object classes of aPascal and 12 object classes of aYahoo. AWA1 is a medium-scale animal dataset which contains 50 animal classes with 85 attributes annotated. AWA2 is collected by <ref type="bibr" target="#b37">[37]</ref>, which has the same classes as AWA1. CUB is a finegrained and medium-scale dataset, which contains 200 different types of birds annotated with 312 attributes. SUN is a medium-scale dataset containing 717 types of scenes where 102 attributes are annotated. In order to make fair comparisons with other approaches, we conduct our experiment on the more reasonable pure ZSL settings recently proposed by <ref type="bibr" target="#b37">[37]</ref>. The details of each dataset and class splits for source and target classes are shown in <ref type="table">Table 1</ref>.</p><p>Implementation Details. We extract the image features f (x) by the ResNet101 model <ref type="bibr" target="#b12">[13]</ref> and use class attributes as the semantic information. The class semantic transformation g(a) is implemented by a two-layer fully connected neural network, where the hidden layer dimension is set to 1024 and the output size is 2048. The contrastive learning h(z) is also implemented by the fully connected neural network, where the hidden dimension is 1024 and the output </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance on ZSL and GZSL</head><p>To demonstrate the effectiveness of the transferable contrastive network, we compare our approach with several state-of-the-art approaches. <ref type="table" target="#tab_1">Table 2</ref> shows the comparison results of ZSL, where the performance is evaluated by the average per-class top-1 accuracy. It can be seen that our approach achieves the best performance on three datasets and is comparable to the best approach on SUN, which indicates that transferable contrastive network can make good knowledge transfer to the target classes. Our approach is effective to perform fine-grained recognition, as can be seen by the good performance on CUB. We owe the success to two aspects. First, the discriminative property of contrastive learning ensures the contrastive network to effectively discriminate the fine-grained classes. Second, the fine-grained images are more effective to transfer the knowledge since the classes are similar, which makes our model more robust to the target classes. A little lower performance is obtained on APY probably due to the weak relations between the source and target classes. APY is a small-scale coarse-grained dataset, where the categories are very different. Therefore, the relations between source and target classes are weak. That's why most approaches could not perform well on this simple dataset. Since we utilize the class similarities to transfer the knowledge, our model may be influenced by the weak relations.</p><p>We argue that traditional approaches usually tend to overfit the source classes since they ignore the target in the model learning process, which will result in the projection domain shift problem. While TCN could alleviate this problem since our model explicitly transfers the knowledge. To demonstrate this viewpoint, we perform GZSL task on these datasets. <ref type="table" target="#tab_2">Table 3</ref> shows the comparison results, where 'ts' is average per-class top-1 accuracy of target classes and 'tr' is the same evaluation results on source classes. 'H' is the harmonic mean that evaluates the total performance. It can be seen that most approaches achieve very high performance on the source classes and extremely low performance on the target classes, which indicates that these approaches learn little knowledge about the target classes. Compared with the results in <ref type="table" target="#tab_1">Table 2</ref>, the performance of target classes drops greatly for GZSL because most target-class images are recognized as source classes. This indicates that previous approaches are easy to overfit the source classes. While TCN can effectively alleviate the overfitting problem, as can be seen by the more balanced performance on source and target classes for our approach. We owe the success to the transferable property of the contrastive network, which makes our model more robust to recognize the target images. Although the generative approaches <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b43">43]</ref> are also very effective in GZSL, they need to learn the complicated generative models. While our approach is very simple to learn. Moreover, our approach is well complementary to the generative approaches since the generated features can also be utilized to learn our model. Some other approaches  <ref type="table">Table 4</ref>. Comparison with the baseline approach where the knowledge transfer item (LT ) is removed. 'Base' represents the baseline approach. 'TCN' is our approach. 'ZSL' is the accuracy of zeroshot recognition. 'ts', 'tr' and 'H' are the target-class accuracy, source-class accuracy and harmonic mean in GZSL. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref> also adapt the models to the target classes. Compared with them, our approach is more effective. We also tried other information fusion approaches and more details are shown in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Importance of Knowledge Transfer</head><p>Explicit knowledge transfer is an important part of our framework. It is intuitive that similar objects should play a more important role in transfer learning. Therefore, we use class similarities to explicitly transfer the knowledge from source images to similar target classes. In this way, our model will be more robust to the target classes. More- over, it should also have the ability to prevent the model from overfitting the source classes. To demonstrate these assumptions, we compare our approach with the basic model, where the knowledge transfer term (L T ) is removed. <ref type="table">Table 4</ref> shows the recognition results. Although only small improvements are achieved for ZSL, the improvements for GZSL are significant. This phenomenon demonstrates that explicit knowledge transfer can effectively tackle the overfitting problem, which enables the model to learn the knowledge about the target classes. Another factor that deserves to be explored is how important the knowledge transfer is. Therefore, we analyze the influence of parameter α to our model and the recognition results on CUB are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. It can be seen that TCN achieves its best performance when α equals to 0.01. We can infer that α should be small in order to get good performance. This may be caused by two reasons. First, the class similarities are fuzzy measures and there is no accurate definitions. Second, the source images do not absolutely match with the target classes. When α increases, the performance of source classes drops, as can be seen by the results of 'tr', because the model pays more attention to the target classes and neglects the accurate source classes. Since the loss on the source classes ensures the discriminative property of contrastive learning and the loss on the target classes guarantees the transferable property, we must balance these terms to obtain a robust recognition model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization of Class Similarities</head><p>The transferable property of our approach is accomplished by leveraging the class similarities to make knowledge transfer in the model learning process. To see what knowledge has been transferred, we show the class similarities of AWA1 in <ref type="figure" target="#fig_4">Figure 4</ref>. Because of space constraints, we select 15 source classes and visualize their similarities to the target classes. It can be figured out that leopard is similar to bobcat so the training samples of leopard can also be utilized to learn the target class bobcat in the training phase,   thus to enable knowledge transfer. It effectively tackles the problem that no training images are available for the target classes. Through such explicit knowledge transfer, our model would be more robust to the target. Other class similarities, i.e. killer+whale is similar to blue+whale, seal, walrus and dolphin, are also useful knowledge to transfer in the contrastive learning process.</p><p>The foundation on which our approach works well is that reasonable class similarities are obtained for knowledge transfer. However, the class similarities may be very rough for some coarse-grained dataset, such as APY, so it becomes difficult to transfer knowledge from source classes to the target classes. That is why low zero-shot recognition accuracy is obtained on APY for all approaches, as can be seen from <ref type="table" target="#tab_1">Table 2</ref>. To make it intuitive, we show the class similarities for APY in <ref type="figure" target="#fig_5">Figure 5</ref>. It can be figured out that the relations between source and target classes are less reli-  able. For example, among the target classes, the most similar one to the source class building is the train. However, buildings and trains are very different in reality. Therefore, using the training images of building to learn the target train would degrade our model. This may be the reason why TCN achieves lower performance than the state-of-the-art approach on APY. Although some incomprehensible similarities exist, there are also some useful relations, i.e. bicycle is similar to motorbike and bus is similar to train, which ensures the relative good performance of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization of Contrastive Values</head><p>Different from the visual-semantic embedding approaches that use fixed distance to conduct zero-shot recognition, our transferable contrastive network automatically contrasts one image with every class and outputs the contrastive values for image recognition. <ref type="figure" target="#fig_6">Figure 6</ref> shows the contrastive values of some test samples obtained on AWA1. In order to make it intuitive, we normalize the contrastive values and show five most similar classes, where the target classes are marked with red and the source classes are marked with green. We can figure out that most images are consistent with their corresponding classes and dissimilar with other classes. For ZSL, we recognize the test samples among the target classes. As can be seen, the image 'giraffe' has high contrastive value with its class and has low contrastive values with other ones. For GZSL, we recognize the test samples among all classes. Although we only have source-class images for training, our model can effectively recognize the target-class samples in the test procedure. For example, 'bobcat' is effectively discriminated with sourceclass leopard in GZSL task though these two classes are very similar. We owe this success to the explicit knowledge transfer by the class similarities. It prevents our model from overfitting the source classes and ensures the transferable ability to target classes, thus the target-class images would be effectively recognized when they are encountered. Moreover, one image may also have relatively high contrastive values with similar classes. For example, 'rat' has relative strong activations on hamster. This shows that TCN is not only discriminative enough to classify different classes but also transferable to novel classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel transferable contrastive network for generalized zero-shot learning. It automatically contrasts the images with the class semantics to judge how consistent they are. We consider two key properties in contrastive learning, where the discriminative property ensures the contrastive network to effectively classify different classes and the transferable property makes the contrastive network more robust to the target classes. By explicitly transferring knowledge from source images to similar target classes, our approach can effectively tackle the problem of overfitting the source classes in GZSL task. Extensive experiments on five benchmark datasets show the superiority of the proposed approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>0 1 0Figure 1 .</head><label>11</label><figDesc>The domestic dog is a member of the genus Canis (canines), which forms part of the wolf-like canids ... ...The horse is one of two extant subspecies of Equus ferus. It is an oddtoed ungulate mammal … … Birds, also known as Aves, are characterised by feathers, toothless beaked jaws … …HorseDogBirdZebras are several species of African equids (horse family) united by their distinctive black and white striped coats… …The lion is a species in the family Felidae; it is a muscular, deep-chested cat with a short, rounded head … … Illustration diagram that shows the motivations of transferable contrastive learning. The training images should not only match their class semantics (discriminative property) but also have relatively high contrastive values with similar target classes (transfer property). 'D' represents discriminative learning and 'T' represents transfer learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The framework of transferable contrastive network. The information fusion module merges the image information with the class semantic information. The contrastive learning module automatically judges whether the fusion is consistent or not. ' ' denotes the element-wise product operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The recognition results on CUB with different value of α. 'ZSL' is the accuracy of zero-shot recognition. 'ts', 'tr' and 'H' are the target-class accuracy, source-class accuracy and harmonic mean in GZSL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The class similarities in AWA1, where 15 source classes are selected. Each row represents the similarities of one source class to the target classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>The class similarities in APY, where each row shows the similarities of one source class to the target classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>The normalized contrastive values of some test samples obtained on AWA1, where five most similar classes are shown. The source classes are marked with green and the target classes are marked with red. The first row in GZSL shows the target-class samples and the second shows the source-class samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell>APY</cell><cell cols="3">AWA1 AWA2 CUB</cell><cell>SUN</cell></row><row><cell>DAP [19]</cell><cell>33.8</cell><cell>44.1</cell><cell>46.1</cell><cell>40.0</cell><cell>39.9</cell></row><row><cell>IAP [19]</cell><cell>36.6</cell><cell>35.9</cell><cell>35.9</cell><cell>24.0</cell><cell>19.4</cell></row><row><cell>CONSE [24]</cell><cell>26.9</cell><cell>45.6</cell><cell>44.5</cell><cell>34.3</cell><cell>38.8</cell></row><row><cell>CMT [31]</cell><cell>28.0</cell><cell>39.5</cell><cell>37.9</cell><cell>34.6</cell><cell>39.9</cell></row><row><cell>SSE [41]</cell><cell>34.0</cell><cell>60.1</cell><cell>61.0</cell><cell>43.9</cell><cell>51.5</cell></row><row><cell>LATEM [36]</cell><cell>35.2</cell><cell>55.1</cell><cell>55.8</cell><cell>49.3</cell><cell>55.3</cell></row><row><cell>ALE [1]</cell><cell>39.7</cell><cell>59.9</cell><cell>62.5</cell><cell>54.9</cell><cell>58.1</cell></row><row><cell>DEVISE [10]</cell><cell>39.8</cell><cell>54.2</cell><cell>59.7</cell><cell>52.0</cell><cell>56.5</cell></row><row><cell>SJE [2]</cell><cell>32.9</cell><cell>65.6</cell><cell>61.9</cell><cell>53.9</cell><cell>53.7</cell></row><row><cell>EZSL [25]</cell><cell>38.3</cell><cell>58.2</cell><cell>58.6</cell><cell>53.9</cell><cell>54.5</cell></row><row><cell>SYNC [4]</cell><cell>23.9</cell><cell>54.0</cell><cell>46.6</cell><cell>55.6</cell><cell>56.3</cell></row><row><cell>SAE [17]</cell><cell>8.3</cell><cell>53.0</cell><cell>54.1</cell><cell>33.3</cell><cell>40.3</cell></row><row><cell>CDL [14]</cell><cell>43.0</cell><cell>69.9</cell><cell>-</cell><cell>54.5</cell><cell>63.6</cell></row><row><cell>RNet [32]</cell><cell>-</cell><cell>68.2</cell><cell>64.2</cell><cell>55.6</cell><cell>-</cell></row><row><cell>FGN [38]</cell><cell>-</cell><cell>68.2</cell><cell>-</cell><cell>57.3</cell><cell>60.8</cell></row><row><cell>GAZSL [43]</cell><cell>41.1</cell><cell>68.2</cell><cell>70.2</cell><cell>55.8</cell><cell>61.3</cell></row><row><cell>DCN [20]</cell><cell>43.6</cell><cell>65.2</cell><cell>-</cell><cell>56.2</cell><cell>61.8</cell></row><row><cell>TCN (ours)</cell><cell>38.9</cell><cell>70.3</cell><cell>71.2</cell><cell>59.5</cell><cell>61.5</cell></row></table><note>. Zero-shot recognition results on APY, AWA1, AWA2, CUB and SUN (%). '-' denotes that the results are not reported.size is 1. We use Leaky ReLU as the nonlinear activation function for all the hidden layers and sigmoid function for the last layer 1 . The hyperparameter α is fine-tuned in the range [0.001, 0.01, 0.1, 1] by the validation set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell>ts</cell><cell>APY tr</cell><cell>H</cell><cell>ts</cell><cell>AWA1 tr</cell><cell>H</cell><cell>ts</cell><cell>AWA2 tr</cell><cell>H</cell><cell>ts</cell><cell>CUB tr</cell><cell>H</cell><cell>ts</cell><cell>SUN tr</cell><cell>H</cell></row><row><cell>DAP [19]</cell><cell>4.8</cell><cell cols="2">78.3 9.0</cell><cell>0.0</cell><cell cols="2">88.7 0.0</cell><cell>0.0</cell><cell cols="2">84.7 0.0</cell><cell>1.7</cell><cell cols="2">67.9 3.3</cell><cell>4.2</cell><cell cols="2">25.1 7.2</cell></row><row><cell>IAP [19]</cell><cell>5.7</cell><cell cols="3">65.6 10.4 2.1</cell><cell cols="2">78.2 4.1</cell><cell>0.9</cell><cell cols="2">87.6 1.8</cell><cell>0.2</cell><cell cols="2">72.8 0.4</cell><cell>1.0</cell><cell cols="2">37.8 1.8</cell></row><row><cell>CONSE [24]</cell><cell>0.0</cell><cell cols="2">91.2 0.0</cell><cell>0.4</cell><cell cols="2">88.6 0.8</cell><cell>0.5</cell><cell cols="2">90.6 1.0</cell><cell>1.6</cell><cell cols="2">72.2 3.1</cell><cell>6.8</cell><cell cols="2">39.9 11.6</cell></row><row><cell>CMT [31]</cell><cell>1.4</cell><cell cols="2">85.2 2.8</cell><cell>0.9</cell><cell cols="2">87.6 1.8</cell><cell>0.5</cell><cell cols="2">90.0 1.0</cell><cell>7.2</cell><cell cols="3">49.8 12.6 8.1</cell><cell cols="2">21.8 11.8</cell></row><row><cell>SSE [41]</cell><cell>0.2</cell><cell cols="2">78.9 0.4</cell><cell>7.0</cell><cell cols="3">80.5 12.9 8.1</cell><cell cols="3">82.5 14.8 8.5</cell><cell cols="3">46.9 14.4 2.1</cell><cell cols="2">36.4 4.0</cell></row><row><cell>LATEM [36]</cell><cell>0.1</cell><cell cols="2">73.0 0.2</cell><cell>7.3</cell><cell cols="11">71.7 13.3 11.5 77.3 20.0 15.2 57.3 24.0 14.7 28.8 19.5</cell></row><row><cell>ALE [1]</cell><cell>4.6</cell><cell cols="2">73.7 8.7</cell><cell cols="12">16.8 76.1 27.5 14.0 81.8 23.9 23.7 62.8 34.4 21.8 33.1 26.3</cell></row><row><cell>DEVISE [10]</cell><cell>4.9</cell><cell cols="2">76.9 9.2</cell><cell cols="12">13.4 68.7 22.4 17.1 74.7 27.8 23.8 53.0 32.8 16.9 27.4 20.9</cell></row><row><cell>SJE [2]</cell><cell>3.7</cell><cell cols="2">55.7 6.9</cell><cell cols="4">11.3 74.6 19.6 8.0</cell><cell cols="8">73.9 14.4 23.5 59.2 33.6 14.1 30.5 19.8</cell></row><row><cell>EZSL [25]</cell><cell>2.4</cell><cell cols="2">70.1 4.6</cell><cell>6.6</cell><cell cols="3">75.6 12.1 5.9</cell><cell cols="8">77.8 11.0 12.6 63.8 21.0 11.0 27.9 15.8</cell></row><row><cell>SYNC [4]</cell><cell>7.4</cell><cell cols="3">66.3 13.3 8.9</cell><cell cols="9">87.3 16.2 10.0 90.5 18.0 11.5 70.9 19.8 7.9</cell><cell cols="2">43.3 13.4</cell></row><row><cell>SAE [17]</cell><cell>0.4</cell><cell cols="2">80.9 0.9</cell><cell>1.8</cell><cell cols="2">77.1 3.5</cell><cell>1.1</cell><cell cols="2">82.2 2.2</cell><cell>7.8</cell><cell cols="3">54.0 13.6 8.8</cell><cell cols="2">18.0 11.8</cell></row><row><cell>CDL [14]</cell><cell cols="7">19.8 48.6 28.1 28.1 73.5 40.6 -</cell><cell>-</cell><cell>-</cell><cell cols="6">23.5 55.2 32.9 21.5 34.7 26.5</cell></row><row><cell>RNet [32]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">31.4 91.3 46.7 30.0 93.4 45.3 38.1 61.4 47.0 -</cell><cell>-</cell><cell>-</cell></row><row><cell>FGN [38]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">57.9 61.4 59.6 -</cell><cell>-</cell><cell>-</cell><cell cols="6">43.7 57.7 49.7 42.6 36.6 39.4</cell></row><row><cell>GAZSL [43]</cell><cell cols="15">14.2 78.6 24.0 29.6 84.2 43.8 35.4 86.9 50.3 31.7 61.3 41.8 22.1 39.3 28.3</cell></row><row><cell>DCN [20]</cell><cell cols="7">14.2 75.0 23.9 25.5 84.2 39.1 -</cell><cell>-</cell><cell>-</cell><cell cols="6">28.4 60.7 38.7 25.5 37.0 30.2</cell></row><row><cell>TCN (ours)</cell><cell cols="15">24.1 64.0 35.1 49.4 76.5 60.0 61.2 65.8 63.4 52.6 52.0 52.3 31.2 37.3 34.0</cell></row></table><note>. GZSL results on APY, AWA1, AWA2, CUB and SUN. ts = Top-1 accuracy of the target classes, tr = Top-1 accuracy of the source classes, H = harmonic mean. We measure average per-class top-1 accuracy in %. '-' represents that the results are not reported.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source code is available at http://vipl.ict.ac.cn/resources/codes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label-embedding for attribute-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="819" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for finegrained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2927" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving semantic embedding consistency by metric learning for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Herbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="730" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5327" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting visual exemplars of unseen classes for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Computer Vision</title>
		<meeting>of International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3496" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An empirical study and analysis of generalized zeroshot learning for object recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="52" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Zero-shot visual recognition using semanticspreserving adversarial embedding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1043" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations workshops</title>
		<meeting>of International Conference on Learning Representations workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcaurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
		<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transductive multi-view zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2332" to="2345" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zero-shot object recognition by semantic manifold distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Yong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><forename type="middle">A</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2635" to="2644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning class prototypes via structure alignment for zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="118" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive metric learning for zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1270" to="1274" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning discriminative latent attributes for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Computer Vision</title>
		<meeting>of International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4233" to="4242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic autoencoder for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4447" to="4456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
		<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by betweenclass attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalized zero-shot learning with deep calibration network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
		<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2005" to="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Zero-shot learning using synthesised unseen visual data with diffusion regularisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2498" to="2512" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
		<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantically consistent regularization for zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2037" to="2046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations</title>
		<meeting>of International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romera</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning</title>
		<meeting>of International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2152" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The SUN attribute database: Beyond categories for deeper scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="59" to="81" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<publisher>Andrej Karpathy</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ridge regression, hubness, and zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaro</forename><surname>Shigeto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikumi</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuo</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamsa</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osbert</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
		<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via semantic embeddings and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6857" to="6866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Latent embeddings for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quynh</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Zero-shot learning -a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2251" to="2265" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature generating networks for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5542" to="5551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Zeroshot learning -the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3077" to="3086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3010" to="3019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Zero-shot learning via semantic similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Computer Vision</title>
		<meeting>of International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4166" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Zero-shot learning via joint latent similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6034" to="6042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A generative adversarial approach for zero-shot learning from noisy texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">M</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1004" to="1013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
