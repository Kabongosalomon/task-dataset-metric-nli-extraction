<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VoxelPose: Towards Multi-Camera 3D Human Pose Estimation in Wild Environment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyue</forename><surname>Tu</surname></persName>
							<email>tuhanyue@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
							<email>wezeng@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VoxelPose: Towards Multi-Camera 3D Human Pose Estimation in Wild Environment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>3D Human Pose Estimation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present VoxelPose to estimate 3D poses of multiple people from multiple camera views. In contrast to the previous efforts which require to establish cross-view correspondence based on noisy and incomplete 2D pose estimates, VoxelPose directly operates in the 3D space therefore avoids making incorrect decisions in each camera view. To achieve this goal, features in all camera views are aggregated in the 3D voxel space and fed into Cuboid Proposal Network (CPN) to localize all people. Then we propose Pose Regression Network (PRN) to estimate a detailed 3D pose for each proposal. The approach is robust to occlusion which occurs frequently in practice. Without bells and whistles, it outperforms the previous methods on several public datasets. The code is available at https://github.com/microsoft/voxelpose-pytorch</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Estimating 3D human pose from multiple cameras separated by wide baselines <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> has been a longstanding problem in computer vision. The goal is to predict 3D positions of the landmark joints for all people in a scene. The successful resolution of the task can benefit many applications such as intelligent sports <ref type="bibr" target="#b4">[5]</ref> and retail analysis.</p><p>The previous works such as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> propose to address the problem in three steps. They first estimate 2D poses in each camera view independently, for example, by Convolutional Neural Networks (CNN) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Then, in the second step, the poses that correspond to the same person in different views are grouped into clusters according to appearance and geometry cues. The final step is to estimate a 3D pose for each person (i.e. each cluster) by standard methods such as triangulation <ref type="bibr" target="#b9">[10]</ref> or pictorial structure models <ref type="bibr" target="#b10">[11]</ref>.</p><p>In spite of the fact that 2D pose estimation has quickly matured due to the development of CNN models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, the estimation results are still unsatisfactory for challenging cases especially when occlusion occurs which is often This work is done when Hanyue Tu is an intern at Microsoft Research Asia.  <ref type="figure">Fig. 1</ref>: Overview of our approach. It consists of three modules: (a) we first estimate 2D pose heatmaps for all views; (b) we warp the heatmaps to a common 3D space and construct a feature volume which is fed into a Cuboid Proposal Network to localize all people instances; (c) for each proposal, we construct a finer-grained feature volume and estimate a 3D pose.</p><p>the case for natural scenes. See <ref type="figure" target="#fig_3">Figure 5</ref> for some example 2D poses estimated by the state-of-the-art method <ref type="bibr" target="#b12">[13]</ref>. In addition, it is very difficult to establish cross-view correspondence when 2D poses are inaccurate. All these pose a serious challenge for 3D pose estimation in the wild. To avoid making incorrect decisions for each camera view, we propose a 3D pose estimator which directly operates in the 3D space by gathering information from all camera views. <ref type="figure">Figure 1</ref> shows an overview of our approach. It first estimates 2D heatmaps for each view to encode per-pixel likelihood of all joints as shown in <ref type="figure">Figure 1 (a)</ref>. Different from the previous works, we do not determine the locations of joints (e.g., by finding the maximum response) nor group the joints into different instances because estimated heatmaps are usually very noisy and incomplete. Instead, we project the heatmaps of all views to a common 3D space as in <ref type="bibr" target="#b5">[6]</ref> and obtain a more complete feature volume which allows us to accurately estimate the 3D positions of all joints.</p><p>We first present Cuboid Proposal Network (CPN), as shown in <ref type="figure">Figure 1</ref> (b), to coarsely localize all people in the scene by predicting a number of 3D cuboid proposals from the 3D feature volume. Then for each proposal, we construct a separate finer-grained feature volume centered at each proposal, and feed it into a Pose Regression Network (PRN) to estimate a detailed 3D pose. See <ref type="figure">Figure  1</ref> (c) for illustration. The two networks are composed of basic 3D convolution layers and can be jointly trained.</p><p>It is worth noting that our approach implicitly accomplishes two types of association which was previously addressed by post-processing methods. Firstly, the joints of the same person in a single camera view are implicitly associated by the cuboid proposal. This was previously addressed in the 2D space either by bottom-up approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref> or by top-down approaches <ref type="bibr" target="#b12">[13]</ref> which would suffer when occlusion occurs. Secondly, the joints that correspond to the same person in different camera views are also implicitly associated based on the fact that the 2D poses which overlap with the projections of a 3D pose belong to the same person. Our approach allows us to avoid the challenging association tasks therefore significantly improves the robustness.</p><p>We evaluate our approach on three public datasets including Campus <ref type="bibr" target="#b1">[2]</ref>, Shelf <ref type="bibr" target="#b1">[2]</ref> and CMU Panoptic <ref type="bibr" target="#b14">[15]</ref>. It outperforms the state-of-the-arts on the first two datasets. Since no work has reported numerical results on Panoptic, we conduct a series of ablation studies by comparing our approach to several baselines. In addition, we find that CPN and PRN can be accurately trained on automatically generated synthetic heatmaps. They achieve similar results as the models trained on realistic images. This is possible mainly because the heatmap based 3D feature volume representation is a high level abstraction that is disentangled from appearance/lighting, etc. This favorable property dramatically enhances the practical values of the approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we briefly review the related works on 3D pose estimation for single and multiple people scenarios, respectively. We discuss their main difference from our work and summarize our contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Single Person 3D Pose Estimation</head><p>We briefly classify the existing works into analytical and predictive approaches based on whether they have learnable parameters. Analytical methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref> explicitly model the relationship between a 2D and 3D pose according to the camera geometry. On one hand, when multiple cameras are available, the 3D pose can be fully determined by simple geometry methods such as triangulation <ref type="bibr" target="#b9">[10]</ref> based on the 2D poses in each view. So the bottleneck lies in the inaccuracy of 2D pose estimations. Some works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref> propose to model the conditional dependence between the joints and jointly infer their 3D positions to improve their robustness to errors in 2D poses. On the other hand, when only one camera is available, the problem is under-determined because multiple 3D poses may correspond to the same 2D pose. The previous works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref> propose to use low-dimensional pose representations to reduce ambiguities. They optimize the low-dimensional parameters of the representation such that the discrepancy between its projection and the 2D pose is minimized. The improvement in 2D pose estimation has boosted 3D accuracy.</p><p>The predictive models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> are mainly proposed for the single camera setup aiming to address the ambiguity issue by powerful neural networks. The pioneer works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> propose to regress 3D pose from 2D joint locations by various networks. Some recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26]</ref> also propose to regress a volumetric 3D pose representation from images. In particular, in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref>, the authors project 2D features or pose heatmaps to 3D space and estimate 3D positions of the body joints. The approach achieves better performance than the triangulation and pictorial structure models on single person pose estimation.</p><p>However, it requires to address the challenging association problem in order to apply to scenes with multiple people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multiple Person 3D Pose Estimation</head><p>There are two challenging association problems in this task. First, it needs to associate the joints of the same person by either top-down <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b8">9]</ref> or bottomup <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b13">14]</ref> strategies. Second, it needs to associate the 2D poses of the same person in different views based on appearance features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> which are unstable when people are occluded. The pictorial structure model is extended to deal with multiple people in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>. The number of people is assumed to be known which is difficult by itself. Besides, the interactions between different people introduce loops into the graph model and complicate the optimization problem. These challenges limit the 3D pose estimation accuracy.</p><p>Our work differs from the previous methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> in that it elegantly avoids the two challenging association problems. This is because a 3D cuboid proposal already naturally associates the joints of the same person in the same and different views by projecting the proposals to image space. Different from the pictorial structure models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>, our approach does not suffer from the local optimum and does not need the number of people in each frame to be known as an input. We find in our experiments that our approach outperforms the previous methods on several public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cuboid Proposal Network</head><p>The overview of our approach is shown in <ref type="figure">Figure 1</ref>. It first estimates 2D pose heatmaps for every camera view independently by HRNet <ref type="bibr" target="#b12">[13]</ref>. Then we introduce Cuboid Proposal Network (CPN) to localize all people by a number of cuboid proposals. Finally, we present Pose Regression Network (PRN) to regress a detailed 3D pose for each proposal. In this section, we focus on the details of CPN including the input, output and network structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Volume</head><p>The input to CPN is a 3D feature volume which contains rich information for detecting people in the 3D space. The feature volume is constructed by projecting the 2D pose heatmaps in all camera views to a common discretized 3D space as will be detailed later. Since the 2D pose heatmaps encode location information of the joints, the resulting 3D feature volume also carries rich information for detecting 3D poses.</p><p>We discretize the 3D space, in which people can freely move, by X × Y × Z discrete locations {G x,y,z }. Each location can be regarded as an anchor of people. In order to reduce the quantization error, we set the distance between the neighboring anchors to be small by adjusting the values of X, Y and Z, respectively. In general, the space is about 8m × 8m × 2m on the public datasets 7×7×7 K×80×80×20 pool 16×80×80×20 32×80×80×20 <ref type="figure">Fig. 2</ref>: Network structure of CPN. The input is a feature volume (see section 3.1) and the output is the probability map V (see section 3.2). The yellow arrow represents a standard 3D convolutional layer and the blue arrow represents a Residual Block of two 3D convolutional layers as shown in the legend. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b1">2]</ref>. So we set X, Y and Z to be 80, 80 and 20, respectively, to strike a good balance between speed and precision. The distance between two neighboring bins is about 100mm which is sufficiently accurate for coarsely localizing people. Note that we will obtain finer-grained 3D poses in PRN.</p><formula xml:id="formula_0">32×40×40×10 64×40×40×10 64×20×20×5 pool 128×20×20×5 128×20×20×5 128×20×20×5 64×40×40×10 64×40×40×10 deconv deconv 32×80×80×20 1×80×80×20 Conv3D n×n×n Conv3D 3×3×3 Conv3D 3×3×3 1×1×1</formula><p>We compute a feature vector for each anchor by sampling and fusing the 2D heatmap values at its projected locations in all camera views. Denote the 2D heatmap of view v as M v ∈ R K×H×W where K is the number of body joints. For each anchor location G x,y,z , we compute its projected location in view v as P x,y,z v . The heatmap values at P x,y,z v is denoted as M x,y,z v ∈ R K . Then we compute a feature vector for the anchor as the average heatmap values in all camera views:</p><formula xml:id="formula_1">F x,y,z = 1 V V v=1 M x,y,z v</formula><p>where V is the number of cameras. More advanced fusion strategies, for example, assigning a data-dependent weight to reflect the heatmap estimation quality in each camera view, could be explored in the future work. In this work, we stick to the the approach of computing average in order to keep the overall approach as simple as possible. We can see that F x,y,z actually encodes the likelihood that the K joints are at G x,y,z which is sufficient to infer people presence. In the following sections, we will describe how we estimate cuboid proposals from the feature volume F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cuboid Proposals</head><p>We represent a cuboid proposal by a 3D bounding box whose orientation and size are fixed in our experiments. This is a reasonable simplification because the sizes of people in 3D space have limited variations which differs from 2D proposals in object detection <ref type="bibr" target="#b31">[32]</ref>. So the main task in CPN is to determine the people presence likelihood at each anchor location.</p><p>To generate proposals, we slide a small network over the feature volume F. Each sliding window centered at an anchor is mapped to a lower-dimensional feature which is fed to a fully connected layer to regress a confidence score V x,y,z representing the likelihood of people presence at the location. The likelihood at all anchors form a 3D heatmap V ∈ R X,Y,Z . Since we use fixed box orientation and size, we do not estimate them as the 2D object detectors <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. Neither do we estimate center offsets relative to the anchor locations for precise people locations because coarse locations are sufficient.</p><p>We compute a Ground-Truth (GT) heatmap score V x,y,z * for every anchor according to its distance to the GT poses. Specifically, for each pair of GT pose (root joint) and anchor, we compute a Gaussian score according to their distance. The score decreases exponentially when the distance increases. Note that there could be multiple scores for one anchor if there are multiple people in the scene and we simply keep the largest one. We train CPN by minimizing:</p><formula xml:id="formula_2">L CPN = X x=1 Y y=1 Z z=1 V x,y,z * − V x,y,z 2 (1)</formula><p>The edge length of every proposal is set to be 2000mm which is sufficiently large to cover people in arbitrary poses. The orientations of the proposals are aligned with the world coordinate axes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Non-Maximum Suppression</head><p>We select the anchors with large regression confidence values as the proposals. On top of the 3D heatmap, we perform Non-Maximum Suppression (NMS) based on the heatmap scores to extract local peaks. Then, we keep the locations of peaks whose heatmap scores are larger than a threshold. Similar to <ref type="bibr" target="#b33">[34]</ref>, IOU based NMS is not needed for generating proposals because we only have one positive anchor per pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Network Structures of CPN</head><p>Inspired by the voxel-to-voxel prediction network in <ref type="bibr" target="#b34">[35]</ref>, we also adopt the 3D convolutions as the basic building blocks for CPN. Since the input feature volume is sparse and has clear semantic meanings, we propose a simpler structure than <ref type="bibr" target="#b34">[35]</ref> which is shown in <ref type="figure">Figure 2</ref>. In some scenarios such as football court, the motion capture space could be larger than that of the public datasets, which would lead to larger feature volume, thus notably reducing the inference speed. We solve the problem by using sparse 3D convolutions <ref type="bibr" target="#b35">[36]</ref> because the feature volume only has a small number of non-zero values.</p><p>We visualize some estimated proposals in <ref type="figure" target="#fig_1">Figure 3</ref>. We project the 3D proposals to 2D images using the camera parameters for the sake of simplicity. We can see that most people instances can be accurately retrieved even though some of them are severely occluded in the current view. This is mainly due to the effective fusion of multiview features in a common 3D space. We will numerically evaluate CPN in more details in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Pose Regression Network</head><p>In this section, we present the details of Pose Regression Network (PRN) which, for each proposal, predicts a complete 3D pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Constructing Feature Volume</head><p>Recall that we have already constructed a big feature volume in the previous CPN step, which covers the whole motion space, to coarsely localize people in the environment. However, we do NOT reuse it here in PRN because it is too coarse to accurately estimate the 3D positions of all joints. Instead, we construct a separate finer-grained feature volume centered at each proposal. The size of the feature volume is set to be 2000mm × 2000mm × 2000mm, which is much smaller than that of CPN (8m × 8m × 2m), but is still large enough to cover people in arbitrary poses. This volume is divided into a discrete grid with X ×Y ×Z bins where X = Y = Z = 64. The edge length of a bin is about 2000 64 = 31.25mm. Note that the precision of our approach is not bounded to 31.25mm because we will use the integration trick <ref type="bibr" target="#b21">[22]</ref> to reduce the impact of quantization error as will be described in more detail later. With these definitions, we compute the feature volume following the descriptions in section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Regression of Human Poses</head><p>We estimate a 3D heatmap H k ∈ R X ×Y ×Z for each joint k based on the constructed feature volume. Then the 3D location J k of the joint can be obtained by computing the center of mass of H k according to the following formula:</p><formula xml:id="formula_3">J k = X x=1 Y y=1 Z z=1 (x, y, z) · H k (x, y, z)<label>(2)</label></formula><p>Note that we do not obtain the location J k by finding the maximum of H k because the quantization error of 31.25mm is still large. Computing the expectation as in the above equation effectively reduces the error. This technique is frequently used in the previous works such as <ref type="bibr" target="#b21">[22]</ref>.</p><p>The estimated joint location is compared to the ground-truth location J * to train PRN. Specifically, the L 1 loss is used:</p><formula xml:id="formula_4">L PRN = K k=1 J k * − J k 1 (3)</formula><p>The network of PRN is kept the same as CPN as shown in <ref type="figure">Figure 2</ref> except that the input and output are different. The network weights are shared for different joints. We conducted experiments using different weights but that did not make much difference on current datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Strategies</head><p>We first train the 2D pose estimation network for 20 epochs. The initial learning rate is 1e-4, and decreases to 1e-5 and 1e-6 at the 10 th and 15 th epochs, respectively. Then we jointly train the whole network including CPN and PRN for 10 epochs to convergence. The learning rate is set to be 1e-4. In some experiments (which will be described clearly), we directly use the backbone network learned on the COCO dataset without finetuning on target datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Datasets and Metrics</head><p>The Campus Dataset <ref type="bibr" target="#b1">[2]</ref> This dataset captures three people interacting with each other in an outdoor environment by three cameras. We follow <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref> to split the dataset into training and testing subsets. To avoid over-fitting to this small training data, we directly use the 2D pose estimator trained on the COCO dataset and only train CPN and PRN. The Shelf Dataset <ref type="bibr" target="#b1">[2]</ref> It captures four people disassembling a shelf by five cameras. Similar to what we do for Campus, we use the 2D pose estimator trained on COCO and only train CPN and PRN. The CMU Panoptic Dataset <ref type="bibr" target="#b14">[15]</ref> It captures people doing daily activities by dozens of cameras among which five HD cameras <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23)</ref> are used in our experiments. We also report results for fewer cameras. Following <ref type="bibr" target="#b36">[37]</ref>, the training set consists of the following sequences: ''160422 ultimatum1'',''16022 4 haggling1'',''160226 haggling1'',''161202 haggling1'',''160906 ian1'',''160906 ian2'',''160906 ian3'',''160906 band1'',''160906 band2'',''160906 band3''. The testing set consists of :''160906 pizza1'',''160422 haggling1'',''160906 ian5'',and ''160906 band4''. The Proposal Evaluation Metric We compute recall of proposals at different proposal-groundtruth-distance. It is noteworthy that this metric is only loosely related to the 3D estimation accuracy. We keep ten proposals after NMS for evaluation on the three datasets. The 3D Pose Estimation Metric Following <ref type="bibr" target="#b3">[4]</ref>, we use the Percentage of Correct Parts (PCP3D) metric to evaluate the estimated 3D poses. Specifically, for each ground-truth 3D pose, it finds the closest pose estimation and computes the percentage of correct parts. This metric does not penalize false positive pose estimations. To overcome the limitation, we also extend the Average Precision (AP K ) metric <ref type="bibr" target="#b37">[38]</ref> to the multi-person 3D pose estimation task which is more comprehensive than PCP3D. If the Mean Per Joint Position Error (MPJPE) of a pose is smaller than K millimeters, we think the pose is accurately estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation of CPN</head><p>We first study the impact of the space division granularity to the proposals by setting different values to the X, Y and Z parameters. The results are shown in <ref type="figure" target="#fig_2">Figure 4 (a)</ref>. When we increase the number of bins from 48 × 48 × 12 to 80 × 80 × 20, the recall improves significantly for small thresholds. This is mainly because the quantization error is effectively reduced and the locations of the proposals become more precise. However, the gap becomes smaller for large thresholds. In our experiments, we use 80 × 80 × 20 bins to strike a good balance between the accuracy and speed. We also consider a practical situation where we do not have sufficient data to train CPN. We propose to address the problem by generating many synthetic heatmaps: we place a number of 3D poses (sampled from the motion capture datasets) at random locations in the space and project them to all views to get the respective 2D locations. Then we generate 2D heatmaps from the locations to train CPN. The experimental results are shown in <ref type="figure">Figure 2 (b)</ref>. We can see that the performance is on par with the model trained on the real images. This significantly improves the general applicability of CPN in the wild (we may also need to address the domain adaptation problem in 2D heatmap estimation but it is beyond the scope of this work).</p><p>Finally, we study the impact of the number of cameras to the proposals. In general, the recall decreases when fewer cameras are used. In particular, the results of a single camera are shown in <ref type="figure" target="#fig_2">Figure 4 (c)</ref>. We can see that the recall rates at different thresholds are consistently lower than those of the five-camera  <ref type="table">Table 1</ref>: 2D pose estimation accuracy on the Panoptic dataset. Ours are obtained by projecting the estimated 3D poses to the images. setup in (a). However, it is still larger than 95% at the threshold of 175mm. It means that it can coarsely retrieve most people using a single camera which demonstrates its practical feasibility. We will report the ultimate 3D pose error using a single camera in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation of PRN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">2D Pose Estimation Accuracy</head><p>We project the 3D poses estimated by our approach to 2D and compare them to the results of HRNet <ref type="bibr" target="#b12">[13]</ref>. Since our approach also uses HRNet to estimate heatmaps, they are comparable. The two models are both trained on the Panoptic dataset <ref type="bibr" target="#b14">[15]</ref>. The results are shown in <ref type="table">Table 1</ref>. The AP of HRNet is only 55.8% because there is severe occlusion. <ref type="figure" target="#fig_3">Figure 5</ref> shows some 2D poses estimated by HRNet and our approach, respectively. HRNet gets accurate estimates when there is no occlusion which validates its effectiveness. However, it gets inaccurate estimates when people are occluded. In addition, as a top-down method, it may generate false positives if object detector fails. For example, there are two poses mistakenly detected at the dome entrance area in the fourth example. Ours are obtained by projecting the 3D poses to the images. Note that this is only proof-of-concept result rather than rigorous fair comparison as our approach uses multiview images as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Ablation Study on 3D Pose Estimation</head><p>We conduct ablation studies to evaluate a variety of factors of our approach. The results on the Panoptic dataset are shown in <ref type="table">Table 2</ref>.</p><p>Space Division Granularity of CPN By comparing the results of (a) and (b), we can see that increasing the number of bins from 64×64×16 to 80×80×20 improves accuracy in general. In particular, the AP 25 metric improves most significantly whereas AP 50 improves only marginally. The results represent that using finer-grained grids improves the precision but not accuracy which agrees with our expectation. Further increasing the grid size only slightly decreases the error but notably increases the computation time. To strike a good balance, we use 80 × 80 × 20 for the rest of the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Cameras</head><p>As shown in (b-d) of <ref type="table">Table 2</ref>, reducing the number of cameras generally increases the 3D error because the information in the feature volume becomes less complete. In extreme cases, when there is only one camera, the 3D error increases dramatically to 66.95mm as shown in row (d). This is mainly because there is severe ambiguity in monocular 3D pose estimation. If we align the pelvis joints of the estimated poses to the ground-truth (as the previous methods), the 3D pose error decreases to 51.14mm as shown in <ref type="table">Table  2</ref> (j). This is comparable to the state-of-the-art monocular 3D pose estimation methods such as <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b25">26]</ref>. In addition, we find that AP 25 drops dramatically but AP 150 only drops slightly when we reduce the number of cameras from five to one. This means that it can estimate coarse 3D poses using a single camera although they are not as precise as the multiview setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization to Different Cameras</head><p>We train and test our approach on different sets of cameras. Specifically, we randomly select a few cameras from the remaining HD cameras for training and test on the selected five cameras. The 3D error is about 25.51mm (f) which is larger than the situation where training and testing are on the same cameras. But this is still a reasonably good result demonstrating that the approach has strong generalization capability.</p><p>Impact of Heatmaps By comparing the results in (b) and (g), we can see that getting accurate 2D heatmaps is critical to the 3D accuracy. When the heatmaps are the ground-truth, the AP s at a variety of thresholds are very high suggesting that the estimated poses are accurate. The MPJPE remarkably decreases to 11.77mm. The main reason for this remaining small error is the quantization error caused by space discretization.</p><p>Impact of Proposals By comparing (b) and (h), we can see that replacing CPN by ground-truth proposals does not notably improve the results. The results suggest that the estimated proposals are already very accurate and more attention should be spent on improving the heatmaps and PRN. We do not compute APs when using ground-truth proposals because the confidence scores of all proposals are all set to be one.  <ref type="table">Table 2</ref>: Ablation study on the Panoptic dataset. "*" means that CPN and PRN are trained on synthetic heatmaps. "+" means that CPN and PRN are trained and tested with different cameras. "rel" represents that we align the root joints of the estimated poses to the ground-truth.</p><p>Qualitative Study We show the estimated 3D poses of three examples in <ref type="figure" target="#fig_4">Figure 6</ref>. We can see that there are severe occlusions in the images of all camera views. However, by fusing the noisy and incomplete heatmaps from multiple cameras, our approach obtains more comprehensive features which allows us to successfully estimate the 3D poses without bells and whistles. It is noteworthy that we do not need to associate 2D poses in different views based on noisy observations by combining a number of sophisticated techniques. This significantly improves the robustness of the approach. Please see the supplementary video for more examples 3 . <ref type="figure" target="#fig_5">Figure 7 (B)</ref> shows two examples where our approach did not obtain accurate estimations in the three-camera setup. In the first example, most joints of the lady can be seen from two of the three cameras and our approach accurately estimates the 3D pose. However, the little child is only visible in the first view and, even in that view, many joints are actually occluded by its body. So the resulting 3D pose has large errors. The second example is also interesting. The person is only visible in one view but, fortunately, most joints are visible. We can see that our approach estimates a 3D pose which seems like a translated version of the ground-truth pose plotted in dashed lines. This is reasonable because there is ambiguity for 3D pose estimation from a single image.</p><p>Computational Complexity It takes about 300ms on a single Titan X GPU to estimate 3D poses in a five-camera setup. In particular, 93ms is spent on estimating heatmaps and 24ms is spent on generating proposals. The time spent on regressing poses depends on the number of proposals (people). In particular, it takes about 46ms to process one proposal. The inference time has the potential to be further reduced by using sparse 3D convolutions <ref type="bibr" target="#b35">[36]</ref>. <ref type="table" target="#tab_3">Table 3</ref> shows the results of the state-of-the-art methods on the Campus and the Shelf datasets in the top and bottom sections, respectively. On the Campus dataset, we can see that our approach improves PCP3D from 96.3% of <ref type="bibr" target="#b3">[4]</ref> to 96.7% which is a decent improvement considering the already very high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Comparison to the State-of-the-arts</head><p>As discussed in Section 5, the PCP3D metric does not penalize false positive estimates. However, it is also meaningless to report AP scores because the GT pose annotations in this dataset are incomplete. So we propose to visualize and publish all of our estimated poses 4 . We find that our approach usually gets accurate estimates as long as joints are visible in at least two views.</p><p>Our approach also achieves better results than <ref type="bibr" target="#b3">[4]</ref> on the Shelf dataset. In particular, it gets fewer false positives. For example, in <ref type="figure" target="#fig_5">Figure 7 (A.2)</ref>, there is a false positive pose in the pink dashed circle estimated by <ref type="bibr" target="#b3">[4]</ref>. In contrast, our approach can suppress most false positives. We find that most errors of our approach are caused by inaccurate GT annotations. For example, as shown in the first column of <ref type="figure" target="#fig_5">Figure 7 (A.1)</ref>, the GT joint locations within the red circle are incorrect. In summary, 66 out of the 301 frames have completely correct annotations and our approach gets accurate estimates on them.</p><p>The previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> did not report numerical results on the large scale Panoptic dataset. We encourage future works to do so as in <ref type="table">Table 2</ref> (b). We also evaluate our approach on the single person dataset Human3.6M <ref type="bibr" target="#b40">[41]</ref>. The MPJPE of our approach is about 19mm which is comparable to <ref type="bibr" target="#b25">[26]</ref>. We also visualize and publish our estimated poses 5 .   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>we present a novel approach for multi-person 3D pose estimation. Different from the previous methods, it only makes hard decisions in the 3D space which allows to avoid the challenging association problems in the 2D space. In particular, noisy and incomplete information of all camera views are warped to a common 3D space to form a comprehensive feature volume which is used for 3D estimation. The experimental results on the benchmark datasets validate that the approach is robust to occlusion which has practical values. In addition, the approach has strong generalization capability to different camera setups.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Estimated cuboid proposals. We project the eight corners of each proposal to the image and compute the minimum and maximum coordinates along the x and y-axis, respectively, which form a bounding box. The numbers represent the estimated confidence scores. The gray boxes denote low confidence proposals. The dashed boxes are the ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Recall curves when the motion space is discretized with different numbers of bins on the Panoptic dataset. (a) CPN is trained/tested on the real images of five cameras from the Panoptic dataset. (b) CPN is trained on the synthetic heatmaps and tested on the real images of five cameras. (c) CPN is trained/tested on the real images of one camera from the Panoptic dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison of 2D poses estimated by HRNet [13] (top row) and our approach (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Estimated 3D poses and their projections in images. The last column shows the estimated 3D poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>(A) shows the 3D poses of ground-truth (A.1), estimated by [4] (A.2) and ours (A.3), respectively. The joints in the dashed circles represent the locations are incorrect. (B) shows two typical cases where our approach makes mistakes. The pose plotted by dashed lines in B.2 is the ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison to the state-of-the-art methods on the Campus and the Shelf datasets. The metric is PCP3D.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://youtu.be/qZAyHUzdpgw</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://youtu.be/AgDQFIlL5IM 5 https://youtu.be/S6G3TXaBukw</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d pictorial structures revisited: Multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1929" to="1942" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiple human pose estimation with temporally consistent 3d pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="742" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fast and robust multi-person 3d pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7792" to="7801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-person 3d pose estimation and tracking in sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bridgeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4342" to="4351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">4d association graph for realtime multi-person motion capture using multiple video cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1324" to="1333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multi-view pictorial structures for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<title level="m">Panoptic studio: A massively multiview system for social interaction capture. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2361" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sparse representation for 3d shape estimation: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1648" to="1661" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="7307" to="7316" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR, IEEE</publisher>
			<biblScope unit="page" from="1561" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="529" to="545" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7718" to="7727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lightweight multi-view 3d pose estimation through camera-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Remelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6040" to="6049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR, IEEE</publisher>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Lcr-net++: Multi-person 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11977" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="5079" to="5088" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page" from="10965" to="10974" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiple human 3d pose estimation from multiview images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ershadi-Nasab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="15573" to="15601" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. T-PAMI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
