[{"caption":"Table 2. Zero-shot code clone detection with cosine similarity probe. Contrastive and hybrid representations improve clone detection \nAUROC on unmodified (natural) HackerRank programs by +8% and +10% AUROC over a heuristic textual similarity probe, respectively, \nsuggesting they are predictive of functionality. Contrastive representations are also the most robust to adversarial code transformations. \nNatural code \nAdversarial (N \u003d4) Adversarial (N \u003d16) \nAUROC \nAP \nAUROC \nAP \nAUROC \nAP \n\nEdit distance heuristic \n69.55±0.81 73.75 31.63±0.82 42.85 12.11±0.54 \n32.46 \nRandomly initialized Transformer \n72.31±0.79 75.82 22.72±0.20 37.73 \n3.09±0.28 \n30.95 \n+ RoBERTa MLM pre-train \n74.04±0.77 77.65 25.83±0.21 39.46 \n4.51±0.33 \n31.17 \n+ ContraCode pre-train \n75.73±0.75 78.02 64.97±0.24 66.23 58.32±0.88 \n59.66 \n+ ContraCode + RoBERTa MLM 79.39±0.70 81.47 37.81±0.24 51.42 10.09±0.50 \n32.52 \n\n","rows":["75 . 73±0 . 75","72 . 31±0 . 79","31 . 63±0 . 82","22 . 72±0 . 20","12 . 11±0 . 54","+ ContraCode + RoBERTa MLM","3 . 09±0 . 28","64 . 97±0 . 24","69 . 55±0 . 81","79 . 39±0 . 70","4 . 51±0 . 33","10 . 09±0 . 50","25 . 83±0 . 21","74 . 04±0 . 77","Edit distance heuristic","+ ContraCode pre - train","37 . 81±0 . 24","Randomly initialized Transformer","+ RoBERTa MLM pre - train","58 . 32±0 . 88"],"columns":["Natural code","Table 2 . Zero - shot code clone detection with cosine similarity probe . Contrastive and hybrid representations improve clone detection","Adversarial ( N \u003d4 )","Adversarial ( N \u003d16 )","AP"],"mergedAllColumns":["suggesting they are predictive of functionality . Contrastive representations are also the most robust to adversarial code transformations ."],"numberCells":[{"number":"42.85","isBolded":false,"associatedRows":["Edit distance heuristic","69 . 55±0 . 81","31 . 63±0 . 82"],"associatedColumns":["Table 2 . Zero - shot code clone detection with cosine similarity probe . Contrastive and hybrid representations improve clone detection","Adversarial ( N \u003d4 )","AP"],"associatedMergedColumns":["suggesting they are predictive of functionality . Contrastive representations are also the most robust to adversarial code transformations ."]},{"number":"81.47","isBolded":true,"associatedRows":["Randomly initialized Transformer","+ ContraCode + RoBERTa MLM","79 . 39±0 . 70"],"associatedColumns":["Table 2 . Zero - shot code clone detection with cosine similarity probe . Contrastive and hybrid representations improve clone detection","Natural code","AP"],"associatedMergedColumns":["suggesting they are predictive of functionality . Contrastive representations are also the most robust to adversarial code transformations ."]},{"number":"59.66","isBolded":true,"associatedRows":["+ ContraCode pre - train","75 . 73±0 . 75","64 . 97±0 . 24","58 . 32±0 . 88"],"associatedColumns":["Table 2 . Zero - shot code clone detection with cosine similarity probe . Contrastive and hybrid representations improve clone detection","Adversarial ( N \u003d16 )","AP"],"associatedMergedColumns":["suggesting they are predictive of functionality . Contrastive representations are also the most robust to adversarial code transformations ."]},{"number":"77.65","isBolded":false,"associatedRows":["+ RoBERTa MLM pre - train","74 . 04±0 . 77"],"associatedColumns":["Table 2 . Zero - shot code clone detection with cosine similarity probe . Contrastive and hybrid representations improve clone detection","Natural code","AP"],"associatedMergedColumns":["suggesting they are predictive of functionality . Contrastive representations are also the most robust to adversarial code transformations ."]},{"number":"51.42","isBolded":false,"associatedRows":["Randomly initialized Transformer","+ ContraCode + RoBERTa MLM","79 . 39±0 . 70","37 . 81±0 . 24"],"associatedColumns":["Table 2 . Zero - shot code clone detection with cosine similarity probe . Contrastive and hybrid representations improve clone detection","Adversarial ( N \u003d4 )","AP"],"associatedMergedColumns":["suggesting they are predictive of functionality . Contrastive representations are also the most robust to adversarial code transformations ."]},{"number":"32.52","isBolded":false,"associatedRows":["Randomly initialized Transformer","+ ContraCode + RoBERTa MLM","79 . 39±0 . 70","37 . 81±0 . 24","10 . 09±0 . 50"],"associatedColumns":["Table 2 . Zero - shot code clone detection with cosine similarity probe . Contrastive and hybrid representations improve clone detection","Adversarial ( N \u003d16 )","AP"],"associatedMergedColumns":["suggesting they are predictive of functionality . Contrastive representations are also the most robust to adversarial code transformations ."]},{"number":"30.95","isBolded":false,"associatedRows":["Randomly initialized Transformer","72 . 31±0 . 79","22 . 72±0 . 20","3 . 09±0 . 28"],"associatedColumns":["Table 2 . Zero - shot code clone detection with cosine similarity probe . Contrastive and hybrid representations improve clone detection","Adversarial ( N \u003d16 )","AP"],"associatedMergedColumns":["suggesting they are predictive of functionality . Contrastive representations are also the most robust to adversarial code transformations ."]},{"number":"32.46","isBolded":false,"associatedRows":["Edit distance heuristic","69 . 55±0 . 81","31 . 63±0 . 82","12 . 11±0 . 54"],"associatedColumns":["Table 2 . Zero - shot code clone detection with cosine similarity probe . Contrastive and hybrid representations improve clone detection","Adversarial ( N \u003d16 )","AP"],"associatedMergedColumns":["suggesting they are predictive of functionality . Contrastive representations are also the most robust to adversarial code transformations ."]},{"number":"39.46","isBolded":false,"associatedRows":["+ RoBERTa MLM pre - train","74 . 04±0 . 77","25 . 83±0 . 21"],"associatedColumns":["Table 2 . Zero - shot code clone detection with cosine similarity probe . Contrastive and hybrid representations improve clone detection","Adversarial ( N \u003d4 )","AP"],"associatedMergedColumns":["suggesting they are predictive of functionality . Contrastive representations are also the most robust to adversarial code transformations ."]},{"number":"78.02","isBolded":false,"associatedRows":["+ ContraCode pre - train","75 . 73±0 . 75"],"associatedColumns":["Table 2 . Zero - shot code clone detection with cosine similarity probe . Contrastive and hybrid representations improve clone detection","Natural code","AP"],"associatedMergedColumns":["suggesting they are predictive of functionality . Contrastive representations are also the most robust to adversarial code transformations ."]},{"number":"73.75","isBolded":false,"associatedRows":["Edit distance heuristic","69 . 55±0 . 81"],"associatedColumns":["Table 2 . Zero - shot code clone detection with cosine similarity probe . Contrastive and hybrid representations improve clone detection","Natural code","AP"],"associatedMergedColumns":["suggesting they are predictive of functionality . Contrastive representations are also the most robust to adversarial code transformations ."]},{"number":"37.73","isBolded":false,"associatedRows":["Randomly initialized Transformer","72 . 31±0 . 79","22 . 72±0 . 20"],"associatedColumns":["Table 2 . Zero - shot code clone detection with cosine similarity probe . Contrastive and hybrid representations improve clone detection","Adversarial ( N \u003d4 )","AP"],"associatedMergedColumns":["suggesting they are predictive of functionality . Contrastive representations are also the most robust to adversarial code transformations ."]},{"number":"31.17","isBolded":false,"associatedRows":["+ RoBERTa MLM pre - train","74 . 04±0 . 77","25 . 83±0 . 21","4 . 51±0 . 33"],"associatedColumns":["Table 2 . Zero - shot code clone detection with cosine similarity probe . Contrastive and hybrid representations improve clone detection","Adversarial ( N \u003d16 )","AP"],"associatedMergedColumns":["suggesting they are predictive of functionality . Contrastive representations are also the most robust to adversarial code transformations ."]},{"number":"75.82","isBolded":false,"associatedRows":["Randomly initialized Transformer","72 . 31±0 . 79"],"associatedColumns":["Table 2 . Zero - shot code clone detection with cosine similarity probe . Contrastive and hybrid representations improve clone detection","Natural code","AP"],"associatedMergedColumns":["suggesting they are predictive of functionality . Contrastive representations are also the most robust to adversarial code transformations ."]},{"number":"66.23","isBolded":true,"associatedRows":["+ ContraCode pre - train","75 . 73±0 . 75","64 . 97±0 . 24"],"associatedColumns":["Table 2 . Zero - shot code clone detection with cosine similarity probe . Contrastive and hybrid representations improve clone detection","Adversarial ( N \u003d4 )","AP"],"associatedMergedColumns":["suggesting they are predictive of functionality . Contrastive representations are also the most robust to adversarial code transformations ."]}]},{"caption":"Table 3. Type inference accuracy on TypeScript programs in \nthe Hellendoorn et al. (2018) dataset. ContraCode (BiLSTM) \noutperforms baseline top-1 accuracies by 2.28% to 13.16%. As \nContraCode does not modify model architecture, contrastive pre-\ntraining can be combined with each baseline. Compared with \nTypeScript\u0027s built-in type inference, we improve accuracy by 8.9%. \n\n","rows":["DeepTyper ( supervised )","( BiLSTM )","analysis","+ RoBERTa MLM pre - train ( 10K steps )","Name only ( Hellendoorn et al . , 2018 )","TypeScript CheckJS ( Bierman et al . , 2014 )","Transformer ( supervised )","+ ContraCode pre - train ( hybrid )","+ ContraCode pre - train ( + SW reg ft )","Transformer ( RoBERTa MLM pre - train )","Static","DeepTyper","+ ContraCode pre - train"],"columns":["Acc@1","Acc@5"],"mergedAllColumns":["Transformer","RoBERTa - 6","-"],"numberCells":[{"number":"45.11%","isBolded":false,"associatedRows":["Static","TypeScript CheckJS ( Bierman et al . , 2014 )"],"associatedColumns":["Acc@1"],"associatedMergedColumns":[]},{"number":"84.60%","isBolded":false,"associatedRows":["( BiLSTM )","+ ContraCode pre - train"],"associatedColumns":["Acc@5"],"associatedMergedColumns":["RoBERTa - 6"]},{"number":"52.65%","isBolded":false,"associatedRows":["( BiLSTM )","+ ContraCode pre - train"],"associatedColumns":["Acc@1"],"associatedMergedColumns":["RoBERTa - 6"]},{"number":"50.24%","isBolded":false,"associatedRows":["DeepTyper","+ RoBERTa MLM pre - train ( 10K steps )"],"associatedColumns":["Acc@1"],"associatedMergedColumns":["RoBERTa - 6"]},{"number":"85.55%","isBolded":true,"associatedRows":["DeepTyper","+ ContraCode pre - train ( + SW reg ft )"],"associatedColumns":["Acc@5"],"associatedMergedColumns":["RoBERTa - 6"]},{"number":"40.85%","isBolded":false,"associatedRows":["DeepTyper","Transformer ( RoBERTa MLM pre - train )"],"associatedColumns":["Acc@1"],"associatedMergedColumns":["Transformer"]},{"number":"75.76%","isBolded":false,"associatedRows":["DeepTyper","Transformer ( RoBERTa MLM pre - train )"],"associatedColumns":["Acc@5"],"associatedMergedColumns":["Transformer"]},{"number":"80.08%","isBolded":false,"associatedRows":["DeepTyper","Transformer ( supervised )"],"associatedColumns":["Acc@5"],"associatedMergedColumns":["-"]},{"number":"81.85%","isBolded":true,"associatedRows":["DeepTyper","+ ContraCode pre - train"],"associatedColumns":["Acc@5"],"associatedMergedColumns":["-"]},{"number":"81.44%","isBolded":true,"associatedRows":["DeepTyper","+ ContraCode pre - train ( hybrid )"],"associatedColumns":["Acc@5"],"associatedMergedColumns":["Transformer"]},{"number":"46.86%","isBolded":true,"associatedRows":["DeepTyper","+ ContraCode pre - train"],"associatedColumns":["Acc@1"],"associatedMergedColumns":["-"]},{"number":"82.71%","isBolded":false,"associatedRows":["DeepTyper","DeepTyper ( supervised )"],"associatedColumns":["Acc@5"],"associatedMergedColumns":["RoBERTa - 6"]},{"number":"45.66%","isBolded":false,"associatedRows":["DeepTyper","Transformer ( supervised )"],"associatedColumns":["Acc@1"],"associatedMergedColumns":["-"]},{"number":"54.01%","isBolded":true,"associatedRows":["DeepTyper","+ ContraCode pre - train ( + SW reg ft )"],"associatedColumns":["Acc@1"],"associatedMergedColumns":["RoBERTa - 6"]},{"number":"70.07%","isBolded":false,"associatedRows":["analysis","Name only ( Hellendoorn et al . , 2018 )"],"associatedColumns":["Acc@5"],"associatedMergedColumns":["-"]},{"number":"51.73%","isBolded":false,"associatedRows":["DeepTyper","DeepTyper ( supervised )"],"associatedColumns":["Acc@1"],"associatedMergedColumns":["RoBERTa - 6"]},{"number":"47.16%","isBolded":true,"associatedRows":["DeepTyper","+ ContraCode pre - train ( hybrid )"],"associatedColumns":["Acc@1"],"associatedMergedColumns":["Transformer"]},{"number":"28.94%","isBolded":false,"associatedRows":["analysis","Name only ( Hellendoorn et al . , 2018 )"],"associatedColumns":["Acc@1"],"associatedMergedColumns":["-"]},{"number":"82.85%","isBolded":false,"associatedRows":["DeepTyper","+ RoBERTa MLM pre - train ( 10K steps )"],"associatedColumns":["Acc@5"],"associatedMergedColumns":["RoBERTa - 6"]}]},{"caption":"Table 4. Results for different settings of the code summariza-\ntion task: supervised training with 81K functions, masked lan-\nguage model pre-training, training from scratch and contrastive \npre-training with fine-tuning. \nMethod \nPrecision Recall \nF1 \n\ncode2vec (Alon et al., 2019b) \n10.78% 8.24% 9.34% \ncode2seq (Alon et al., 2019a)  12.17% 7.65% 9.39% \nRoBERTa MLM (Liu et al., 2019) 15.13% 11.47% 12.45% \nTransformer (Vaswani et al., 2017) 18.11% 15.78% 16.86%   ","rows":["code2seq ( Alon et al . , 2019a )","Transformer ( Vaswani et al . , 2017 )","code2vec ( Alon et al . , 2019b )","RoBERTa MLM ( Liu et al . , 2019 )"],"columns":["Precision","Table 4 . Results for different settings of the code summariza -","Recall","F1"],"mergedAllColumns":["pre - training with fine - tuning ."],"numberCells":[{"number":"12.45%","isBolded":false,"associatedRows":["RoBERTa MLM ( Liu et al . , 2019 )"],"associatedColumns":["Table 4 . Results for different settings of the code summariza -","F1"],"associatedMergedColumns":["pre - training with fine - tuning ."]},{"number":"15.78%","isBolded":false,"associatedRows":["Transformer ( Vaswani et al . , 2017 )"],"associatedColumns":["Table 4 . Results for different settings of the code summariza -","Recall"],"associatedMergedColumns":["pre - training with fine - tuning ."]},{"number":"10.78%","isBolded":false,"associatedRows":["code2vec ( Alon et al . , 2019b )"],"associatedColumns":["Table 4 . Results for different settings of the code summariza -","Precision"],"associatedMergedColumns":["pre - training with fine - tuning ."]},{"number":"8.24%","isBolded":false,"associatedRows":["code2vec ( Alon et al . , 2019b )"],"associatedColumns":["Table 4 . Results for different settings of the code summariza -","Recall"],"associatedMergedColumns":["pre - training with fine - tuning ."]},{"number":"9.34%","isBolded":false,"associatedRows":["code2vec ( Alon et al . , 2019b )"],"associatedColumns":["Table 4 . Results for different settings of the code summariza -","F1"],"associatedMergedColumns":["pre - training with fine - tuning ."]},{"number":"7.65%","isBolded":false,"associatedRows":["code2seq ( Alon et al . , 2019a )"],"associatedColumns":["Table 4 . Results for different settings of the code summariza -","Recall"],"associatedMergedColumns":["pre - training with fine - tuning ."]},{"number":"16.86%","isBolded":false,"associatedRows":["Transformer ( Vaswani et al . , 2017 )"],"associatedColumns":["Table 4 . Results for different settings of the code summariza -","F1"],"associatedMergedColumns":["pre - training with fine - tuning ."]},{"number":"12.17%","isBolded":false,"associatedRows":["code2seq ( Alon et al . , 2019a )"],"associatedColumns":["Table 4 . Results for different settings of the code summariza -","Precision"],"associatedMergedColumns":["pre - training with fine - tuning ."]},{"number":"9.39%","isBolded":false,"associatedRows":["code2seq ( Alon et al . , 2019a )"],"associatedColumns":["Table 4 . Results for different settings of the code summariza -","F1"],"associatedMergedColumns":["pre - training with fine - tuning ."]},{"number":"11.47%","isBolded":false,"associatedRows":["RoBERTa MLM ( Liu et al . , 2019 )"],"associatedColumns":["Table 4 . Results for different settings of the code summariza -","Recall"],"associatedMergedColumns":["pre - training with fine - tuning ."]},{"number":"18.11%","isBolded":false,"associatedRows":["Transformer ( Vaswani et al . , 2017 )"],"associatedColumns":["Table 4 . Results for different settings of the code summariza -","Precision"],"associatedMergedColumns":["pre - training with fine - tuning ."]},{"number":"15.13%","isBolded":false,"associatedRows":["RoBERTa MLM ( Liu et al . , 2019 )"],"associatedColumns":["Table 4 . Results for different settings of the code summariza -","Precision"],"associatedMergedColumns":["pre - training with fine - tuning ."]}]},{"caption":"Table 5. On two tasks, compiler data augmentations degrade per-\nformance when training supervised models from scratch. \nMethod for code summarization \nF1 \n\nTransformer (Table 4) \n16.86 \n+ LS,SW,VR,DCI augmentations \n15.65 \n\nMethod for type inference \nAcc@1 \n\nTransformer (Table 3) \n45.66 \n+ SW regularization \n43.96 \n+ LS,SW augmentations \n44.14 \n\nDeepTyper (Table 3) \n51.73 \n+ SW regularization \n49.93 \n+ LS,SW augmentations \n50.93 \n+ stronger LS,SW augmentations \n50.33 \n\n","rows":["DeepTyper ( Table 3 )","+ LS , SW augmentations","+ SW regularization","+ LS , SW , VR , DCI augmentations","+ stronger LS , SW augmentations","Transformer ( Table 3 )","Transformer ( Table 4 )"],"columns":["Acc@1","Table 5 . On two tasks , compiler data augmentations degrade per -","F1"],"mergedAllColumns":["formance when training supervised models from scratch ."],"numberCells":[{"number":"43.96","isBolded":false,"associatedRows":["+ SW regularization"],"associatedColumns":["Table 5 . On two tasks , compiler data augmentations degrade per -","F1","Acc@1"],"associatedMergedColumns":["formance when training supervised models from scratch ."]},{"number":"44.14","isBolded":false,"associatedRows":["+ LS , SW augmentations"],"associatedColumns":["Table 5 . On two tasks , compiler data augmentations degrade per -","F1","Acc@1"],"associatedMergedColumns":["formance when training supervised models from scratch ."]},{"number":"45.66","isBolded":true,"associatedRows":["Transformer ( Table 3 )"],"associatedColumns":["Table 5 . On two tasks , compiler data augmentations degrade per -","F1","Acc@1"],"associatedMergedColumns":["formance when training supervised models from scratch ."]},{"number":"50.33","isBolded":false,"associatedRows":["+ stronger LS , SW augmentations"],"associatedColumns":["Table 5 . On two tasks , compiler data augmentations degrade per -","F1","Acc@1"],"associatedMergedColumns":["formance when training supervised models from scratch ."]},{"number":"16.86","isBolded":true,"associatedRows":["Transformer ( Table 4 )","+ LS , SW , VR , DCI augmentations"],"associatedColumns":["Table 5 . On two tasks , compiler data augmentations degrade per -","F1"],"associatedMergedColumns":["formance when training supervised models from scratch ."]},{"number":"15.65","isBolded":false,"associatedRows":["Transformer ( Table 4 )","+ LS , SW , VR , DCI augmentations"],"associatedColumns":["Table 5 . On two tasks , compiler data augmentations degrade per -","F1"],"associatedMergedColumns":["formance when training supervised models from scratch ."]},{"number":"50.93","isBolded":false,"associatedRows":["+ LS , SW augmentations"],"associatedColumns":["Table 5 . On two tasks , compiler data augmentations degrade per -","F1","Acc@1"],"associatedMergedColumns":["formance when training supervised models from scratch ."]},{"number":"51.73","isBolded":true,"associatedRows":["DeepTyper ( Table 3 )"],"associatedColumns":["Table 5 . On two tasks , compiler data augmentations degrade per -","F1","Acc@1"],"associatedMergedColumns":["formance when training supervised models from scratch ."]},{"number":"49.93","isBolded":false,"associatedRows":["+ SW regularization"],"associatedColumns":["Table 5 . On two tasks , compiler data augmentations degrade per -","F1","Acc@1"],"associatedMergedColumns":["formance when training supervised models from scratch ."]}]},{"caption":"Table 6. Ablating compiler transformations used during contrastive \npre-training. The DeepTyper BiLSTM is pre-trained with con-\nstrastive learning for 20K steps, then fine-tuned for type inference. \nAugmentations are only used during pre-training. Each transfor-\nmation contributes to accuracy. \nAugmentations used for pre-training \nAcc@1 Acc@5 \n\nAll augmentations (Table 3) \n52.65% 84.60% \nw/o identifier modification (-VR, -IM) 51.94% 84.43% \nw/o line subsampling (-LS) \n51.05% 81.63% \nw/o code compression (-T,C,DCE,CF) 50.69% 81.95% \n\n","rows":["All augmentations ( Table 3 )","w / o identifier modification ( - VR , - IM )","w / o code compression ( - T , C , DCE , CF )","w / o line subsampling ( - LS )"],"columns":["Acc@1","Acc@5","The DeepTyper BiLSTM is pre - trained with con -","Table 6 . Ablating compiler transformations used during contrastive"],"mergedAllColumns":["mation contributes to accuracy ."],"numberCells":[{"number":"51.94%","isBolded":false,"associatedRows":["w / o identifier modification ( - VR , - IM )"],"associatedColumns":["Table 6 . Ablating compiler transformations used during contrastive","The DeepTyper BiLSTM is pre - trained with con -","Acc@1"],"associatedMergedColumns":["mation contributes to accuracy ."]},{"number":"52.65%","isBolded":true,"associatedRows":["All augmentations ( Table 3 )","w / o identifier modification ( - VR , - IM )"],"associatedColumns":["Table 6 . Ablating compiler transformations used during contrastive","The DeepTyper BiLSTM is pre - trained with con -","Acc@1"],"associatedMergedColumns":["mation contributes to accuracy ."]},{"number":"84.43%","isBolded":false,"associatedRows":["w / o identifier modification ( - VR , - IM )"],"associatedColumns":["Table 6 . Ablating compiler transformations used during contrastive","The DeepTyper BiLSTM is pre - trained with con -","Acc@5"],"associatedMergedColumns":["mation contributes to accuracy ."]},{"number":"50.69%","isBolded":false,"associatedRows":["w / o code compression ( - T , C , DCE , CF )"],"associatedColumns":["Table 6 . Ablating compiler transformations used during contrastive","The DeepTyper BiLSTM is pre - trained with con -","Acc@1"],"associatedMergedColumns":["mation contributes to accuracy ."]},{"number":"81.95%","isBolded":false,"associatedRows":["w / o code compression ( - T , C , DCE , CF )"],"associatedColumns":["Table 6 . Ablating compiler transformations used during contrastive","The DeepTyper BiLSTM is pre - trained with con -","Acc@5"],"associatedMergedColumns":["mation contributes to accuracy ."]},{"number":"51.05%","isBolded":false,"associatedRows":["w / o line subsampling ( - LS )"],"associatedColumns":["Table 6 . Ablating compiler transformations used during contrastive","The DeepTyper BiLSTM is pre - trained with con -","Acc@1"],"associatedMergedColumns":["mation contributes to accuracy ."]},{"number":"84.60%","isBolded":true,"associatedRows":["All augmentations ( Table 3 )","w / o identifier modification ( - VR , - IM )"],"associatedColumns":["Table 6 . Ablating compiler transformations used during contrastive","The DeepTyper BiLSTM is pre - trained with con -","Acc@5"],"associatedMergedColumns":["mation contributes to accuracy ."]},{"number":"81.63%","isBolded":false,"associatedRows":["w / o line subsampling ( - LS )"],"associatedColumns":["Table 6 . Ablating compiler transformations used during contrastive","The DeepTyper BiLSTM is pre - trained with con -","Acc@5"],"associatedMergedColumns":["mation contributes to accuracy ."]}]},{"caption":"Table 8. Contrasting global, sequence-level representations outperforms contrasting local representations. We compare using the terminal \n(global) hidden states of the DeepTyper BiLSTM and the mean pooled token-level (local) hidden states. \nRepresentation Optimization \nAcc@1 Acc@5 \n\nGlobal \nInfoNCE with terminal hidden state, 20K steps 52.65% 84.60% \nInfoNCE with terminal hidden state, 10K steps 51.70% 83.03% \n\nLocal \nInfoNCE with mean token rep., 10K steps \n49.32% 80.03% \n\nTable 9. Training time and decoder depth ablation on the method name prediction task. Longer pre-training significantly improves \ndownstream performance when a shallow, 1 layer decoder is used. \n\n","rows":["Original set","InfoNCE with mean token rep . , 10K steps","InfoNCE with terminal hidden state , 20K steps","Transformer , 4 layers","Local","Transformer , 1 layer","InfoNCE with terminal hidden state , 10K steps","MoCo , 45k steps","MoCo , 10k steps"],"columns":["Acc@1","Acc@5","Precision","( 81k programs )","Recall","F1","Supervision","Table 8 . Contrasting global , sequence - level representations outperforms contrasting local representations . We compare using the terminal"],"mergedAllColumns":["downstream performance when a shallow , 1 layer decoder is used .","( global ) hidden states of the DeepTyper BiLSTM and the mean pooled token - level ( local ) hidden states .","Global"],"numberCells":[{"number":"51.70%","isBolded":false,"associatedRows":["Transformer , 1 layer","InfoNCE with terminal hidden state , 10K steps"],"associatedColumns":["Table 8 . Contrasting global , sequence - level representations outperforms contrasting local representations . We compare using the terminal","Acc@1"],"associatedMergedColumns":["( global ) hidden states of the DeepTyper BiLSTM and the mean pooled token - level ( local ) hidden states ."]},{"number":"13.21%","isBolded":true,"associatedRows":["Transformer , 4 layers","MoCo , 45k steps","Original set"],"associatedColumns":["Table 8 . Contrasting global , sequence - level representations outperforms contrasting local representations . We compare using the terminal","Acc@1","Supervision","Recall","( 81k programs )"],"associatedMergedColumns":["downstream performance when a shallow , 1 layer decoder is used ."]},{"number":"83.03%","isBolded":false,"associatedRows":["Transformer , 1 layer","InfoNCE with terminal hidden state , 10K steps"],"associatedColumns":["Table 8 . Contrasting global , sequence - level representations outperforms contrasting local representations . We compare using the terminal","Acc@5"],"associatedMergedColumns":["( global ) hidden states of the DeepTyper BiLSTM and the mean pooled token - level ( local ) hidden states ."]},{"number":"5.96%","isBolded":false,"associatedRows":["Transformer , 1 layer","MoCo , 10k steps","Original set"],"associatedColumns":["Table 8 . Contrasting global , sequence - level representations outperforms contrasting local representations . We compare using the terminal","Acc@1","Supervision","Recall","( 81k programs )"],"associatedMergedColumns":["downstream performance when a shallow , 1 layer decoder is used ."]},{"number":"12.57%","isBolded":true,"associatedRows":["Transformer , 1 layer","MoCo , 45k steps","Original set"],"associatedColumns":["Table 8 . Contrasting global , sequence - level representations outperforms contrasting local representations . We compare using the terminal","Acc@1","Supervision","Recall","( 81k programs )"],"associatedMergedColumns":["downstream performance when a shallow , 1 layer decoder is used ."]},{"number":"13.79%","isBolded":true,"associatedRows":["Transformer , 1 layer","MoCo , 45k steps","Original set"],"associatedColumns":["Table 8 . Contrasting global , sequence - level representations outperforms contrasting local representations . We compare using the terminal","Acc@5","Supervision","F1","( 81k programs )"],"associatedMergedColumns":["downstream performance when a shallow , 1 layer decoder is used ."]},{"number":"11.91%","isBolded":false,"associatedRows":["Transformer , 1 layer","MoCo , 10k steps","Original set"],"associatedColumns":["Table 8 . Contrasting global , sequence - level representations outperforms contrasting local representations . We compare using the terminal","Acc@1","Supervision","Precision","( 81k programs )"],"associatedMergedColumns":["downstream performance when a shallow , 1 layer decoder is used ."]},{"number":"84.60%","isBolded":true,"associatedRows":["Transformer , 1 layer","InfoNCE with terminal hidden state , 20K steps"],"associatedColumns":["Table 8 . Contrasting global , sequence - level representations outperforms contrasting local representations . We compare using the terminal","Acc@5"],"associatedMergedColumns":["( global ) hidden states of the DeepTyper BiLSTM and the mean pooled token - level ( local ) hidden states ."]},{"number":"49.32%","isBolded":false,"associatedRows":["Local","InfoNCE with mean token rep . , 10K steps"],"associatedColumns":["Table 8 . Contrasting global , sequence - level representations outperforms contrasting local representations . We compare using the terminal","Acc@1"],"associatedMergedColumns":["Global"]},{"number":"80.03%","isBolded":false,"associatedRows":["Local","InfoNCE with mean token rep . , 10K steps"],"associatedColumns":["Table 8 . Contrasting global , sequence - level representations outperforms contrasting local representations . We compare using the terminal","Acc@5"],"associatedMergedColumns":["Global"]},{"number":"18.21%","isBolded":true,"associatedRows":["Transformer , 4 layers","MoCo , 45k steps","Original set"],"associatedColumns":["Table 8 . Contrasting global , sequence - level representations outperforms contrasting local representations . We compare using the terminal","Acc@1","Supervision","Precision","( 81k programs )"],"associatedMergedColumns":["downstream performance when a shallow , 1 layer decoder is used ."]},{"number":"17.71%","isBolded":true,"associatedRows":["Transformer , 1 layer","MoCo , 45k steps","Original set"],"associatedColumns":["Table 8 . Contrasting global , sequence - level representations outperforms contrasting local representations . We compare using the terminal","Acc@1","Supervision","Precision","( 81k programs )"],"associatedMergedColumns":["downstream performance when a shallow , 1 layer decoder is used ."]},{"number":"7.49%","isBolded":false,"associatedRows":["Transformer , 1 layer","MoCo , 10k steps","Original set"],"associatedColumns":["Table 8 . Contrasting global , sequence - level representations outperforms contrasting local representations . We compare using the terminal","Acc@5","Supervision","F1","( 81k programs )"],"associatedMergedColumns":["downstream performance when a shallow , 1 layer decoder is used ."]},{"number":"52.65%","isBolded":true,"associatedRows":["Local","InfoNCE with terminal hidden state , 20K steps"],"associatedColumns":["Table 8 . Contrasting global , sequence - level representations outperforms contrasting local representations . We compare using the terminal","Acc@1"],"associatedMergedColumns":["( global ) hidden states of the DeepTyper BiLSTM and the mean pooled token - level ( local ) hidden states ."]},{"number":"14.56%","isBolded":true,"associatedRows":["Transformer , 4 layers","MoCo , 45k steps","Original set"],"associatedColumns":["Table 8 . Contrasting global , sequence - level representations outperforms contrasting local representations . We compare using the terminal","Acc@5","Supervision","F1","( 81k programs )"],"associatedMergedColumns":["downstream performance when a shallow , 1 layer decoder is used ."]}]}]