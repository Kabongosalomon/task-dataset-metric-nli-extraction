[{"caption":"76.9 \nViT-B-16 (Dosovitskiy et al., 2021) 77.9 \nResNet-50 (RGB+FF) \n73.5 \nViT-B-16 (RGB+FF) \n76.7 \nTransformer (64x64) \n57.0 \nPerceiver \n76.4 \n\nTable 1. Top-1 validation accuracy (in %) on ImageNet. Methods \nshown in red exploit domain-specific grid structure, while methods \nin blue do not. The first block reports standard performance from \npixels -these numbers are taken from the literature. The second \nblock shows performance when the inputs are RGB values con-\ncatenated with Fourier features (FF) of the xy positions -the same \nthat the Perceiver receives. This block uses our implementation of \nthe baselines. The Perceiver is competitive with standard baselines \non ImageNet while not relying on domain-specific architectural \nassumptions. \n\n","rows":["ResNet - 50 ( RGB+FF )","ViT - B - 16 ( RGB+FF )","Transformer ( 64x64 )","ViT - B - 16 ( Dosovitskiy et al . , 2021 )","Perceiver"],"columns":[],"mergedAllColumns":[],"numberCells":[{"number":"76.9","isBolded":false,"associatedRows":["ViT - B - 16 ( Dosovitskiy et al . , 2021 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"73.5","isBolded":false,"associatedRows":["ResNet - 50 ( RGB+FF )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"77.9","isBolded":true,"associatedRows":["ViT - B - 16 ( Dosovitskiy et al . , 2021 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"57.0","isBolded":false,"associatedRows":["Transformer ( 64x64 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"76.7","isBolded":false,"associatedRows":["ViT - B - 16 ( RGB+FF )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"76.4","isBolded":false,"associatedRows":["Perceiver"],"associatedColumns":[],"associatedMergedColumns":[]}]},{"caption":"Table 2. Top-1 validation accuracy (in %) on permuted ImageNet. \n\"Fixed\" \u003d permuted with a constant permutation for all images over \nthe dataset. \"Random\" \u003d random, per-example permutation. Meth-\nods that make strong assumptions about the structure of 2D data \nfare poorly when this structure is removed. All methods receive \nidentical input features (RGB+FF). We also show the receptive \nfield of the input units for each model on the right, in pixels. Note \nthat both Transformer and Perceiver have a global view of all inputs \nin each first layer unit. ResNet-50 starts with a 7x7 convolution, \nhence each unit sees 49 pixels, and ViT-B-16 inputs 16x16 patches, \nhence 256 pixels are seen by each first layer unit. \n\n","rows":["ResNet - 50 ( RGB+FF )","ViT - B - 16 ( RGB+FF )","Transformer ( 64x64 )","Perceiver"],"columns":["Random","Fixed"],"mergedAllColumns":["256","49","4 , 096"],"numberCells":[{"number":"57.0","isBolded":false,"associatedRows":["Transformer ( 64x64 )"],"associatedColumns":["Random"],"associatedMergedColumns":["256"]},{"number":"39.4","isBolded":false,"associatedRows":["ResNet - 50 ( RGB+FF )"],"associatedColumns":["Fixed"],"associatedMergedColumns":[]},{"number":"76.4","isBolded":true,"associatedRows":["Perceiver"],"associatedColumns":["Fixed"],"associatedMergedColumns":["4 , 096"]},{"number":"76.4","isBolded":true,"associatedRows":["Perceiver"],"associatedColumns":["Random"],"associatedMergedColumns":["4 , 096"]},{"number":"57.0","isBolded":false,"associatedRows":["Transformer ( 64x64 )"],"associatedColumns":["Fixed"],"associatedMergedColumns":["256"]},{"number":"61.7","isBolded":false,"associatedRows":["ViT - B - 16 ( RGB+FF )"],"associatedColumns":["Fixed"],"associatedMergedColumns":["49"]},{"number":"16.1","isBolded":false,"associatedRows":["ViT - B - 16 ( RGB+FF )"],"associatedColumns":["Random"],"associatedMergedColumns":["49"]},{"number":"14.3","isBolded":false,"associatedRows":["ResNet - 50 ( RGB+FF )"],"associatedColumns":["Random"],"associatedMergedColumns":[]}]},{"caption":"38.4 \n25.7 \n46.2 \nPerceiver \n44.9 \n38.0 \n47.3 \n\nTable 3. Perceiver performance on AudioSet, compared to state-of-the-art models (mAP, larger is better). Best current results in bold. \n\n","rows":["Perceiver","G - blend ( Wang et al . , 2020b )"],"columns":[],"mergedAllColumns":[],"numberCells":[{"number":"44.9","isBolded":true,"associatedRows":["Perceiver"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"38.0","isBolded":true,"associatedRows":["Perceiver"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"46.2","isBolded":false,"associatedRows":["G - blend ( Wang et al . , 2020b )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"32.4","isBolded":false,"associatedRows":["G - blend ( Wang et al . , 2020b )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"25.7","isBolded":false,"associatedRows":["G - blend ( Wang et al . , 2020b )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"40.2","isBolded":false,"associatedRows":["G - blend ( Wang et al . , 2020b )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"38.4","isBolded":false,"associatedRows":["G - blend ( Wang et al . , 2020b )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"18.8","isBolded":false,"associatedRows":["G - blend ( Wang et al . , 2020b )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"47.3","isBolded":true,"associatedRows":["Perceiver"],"associatedColumns":[],"associatedMergedColumns":[]}]},{"caption":"Table 4. Top-1 test-set classification accuracy (in %) on Model-\nNet40. Higher is better. We report best result per model class, \nselected by test-set score. There are no RGB features nor a natural \ngrid structure on this dataset. We compare to the generic baselines \nconsidered in previous sections with fourier feature embeddings of \npositions, as well as to a specialized model: PointNet++ ","rows":["ViT - B - 16 ( FF )","ViT - B - 8 ( FF )","PointNet++ ( Qi et al . , 2017 )","ViT - B - 2 ( FF )","ResNet - 50 ( FF )","Transformer ( 44x44 )","ViT - B - 4 ( FF )","Perceiver"],"columns":["Accuracy"],"mergedAllColumns":[],"numberCells":[{"number":"65.3","isBolded":false,"associatedRows":["ViT - B - 8 ( FF )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"82.1","isBolded":false,"associatedRows":["Transformer ( 44x44 )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"73.4","isBolded":false,"associatedRows":["ViT - B - 4 ( FF )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"78.9","isBolded":false,"associatedRows":["ViT - B - 2 ( FF )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"91.9","isBolded":true,"associatedRows":["PointNet++ ( Qi et al . , 2017 )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"59.6","isBolded":false,"associatedRows":["ViT - B - 16 ( FF )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"85.7","isBolded":true,"associatedRows":["Perceiver"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"66.3","isBolded":false,"associatedRows":["ResNet - 50 ( FF )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]}]},{"caption":"64 \n\n66 \n\n68 \n\n70 \n\n72 \n\n400 \n600 \n800 \n1000 \n\n#latent dimensions \n\n64 \n\n66 \n\n68 \n\n70 \n\n72 \n\n74 \n\n400 \n600 \n800 \n1000 \n\n#latents \n\n62 \n\n64 \n\n66 \n\n68 \n\n70 \n\n72 \n\n74 \n\n1 \n2 \n3 \n4 \n\n#attends \n\n64 \n\n66 \n\n68 \n\n70 \n\n72 \n\n2 \n4 \n6 \n8 \n\n#transformers per attend \n\nFigure 5. Ablations around a basic Perceiver architecture. Increasing the number of latents, attends and transformers per attends always \nseems to help. \n\nValid Train Params \nNo weight sharing 72.9 \n87.7 \n331.3M \nW/ weight sharing 76.4 \n79.7 \n43.9M \n\nTable 5. Weight sharing mitigates overfitting and leads to better \nvalidation performance on ImageNet. We show results (top-1 accu-\nracy) for the best-performing ImageNet architecture (reported in \nTables 1-2 of the main paper) on train and validation sets. This ar-\nchitecture uses 8 cross-attends and 6 blocks per latent transformer. \nThe model labeled \"W/ weight sharing\" shares weights between \ncross-attention modules 2-8 and between the corresponding blocks \nof latent transformers 2-8. The first cross-attention module and \ntransformer use their own, unshared weights. \n\n","rows":["W / weight sharing","No weight sharing"],"columns":["400","Valid","#latents","600","800","1000","#transformers per attend","Params","Train","#latent dimensions","#attends"],"mergedAllColumns":["seems to help ."],"numberCells":[{"number":"72","isBolded":false,"associatedRows":[],"associatedColumns":["#latent dimensions"],"associatedMergedColumns":[]},{"number":"64","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#latents"],"associatedMergedColumns":[]},{"number":"66","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#latents"],"associatedMergedColumns":[]},{"number":"62","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#attends"],"associatedMergedColumns":[]},{"number":"66","isBolded":false,"associatedRows":[],"associatedColumns":["#latent dimensions"],"associatedMergedColumns":[]},{"number":"72","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#latents"],"associatedMergedColumns":[]},{"number":"74","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#latents"],"associatedMergedColumns":[]},{"number":"72","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#attends"],"associatedMergedColumns":[]},{"number":"79.7","isBolded":true,"associatedRows":["W / weight sharing"],"associatedColumns":["#latents","400","1000","Train"],"associatedMergedColumns":["seems to help ."]},{"number":"64","isBolded":false,"associatedRows":[],"associatedColumns":["#latent dimensions"],"associatedMergedColumns":[]},{"number":"70","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#transformers per attend"],"associatedMergedColumns":[]},{"number":"87.7","isBolded":true,"associatedRows":["No weight sharing"],"associatedColumns":["#latents","400","1000","Train"],"associatedMergedColumns":["seems to help ."]},{"number":"70","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#latents"],"associatedMergedColumns":[]},{"number":"68","isBolded":false,"associatedRows":[],"associatedColumns":["#latent dimensions"],"associatedMergedColumns":[]},{"number":"72","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#transformers per attend"],"associatedMergedColumns":[]},{"number":"72.9","isBolded":true,"associatedRows":["No weight sharing"],"associatedColumns":["#latent dimensions","400","800","Valid"],"associatedMergedColumns":["seems to help ."]},{"number":"66","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#transformers per attend"],"associatedMergedColumns":[]},{"number":"64","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#attends"],"associatedMergedColumns":[]},{"number":"70","isBolded":false,"associatedRows":[],"associatedColumns":["#latent dimensions"],"associatedMergedColumns":[]},{"number":"331.3M","isBolded":true,"associatedRows":["No weight sharing"],"associatedColumns":["#latents","600","1000","Params"],"associatedMergedColumns":["seems to help ."]},{"number":"66","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#attends"],"associatedMergedColumns":[]},{"number":"43.9M","isBolded":true,"associatedRows":["W / weight sharing"],"associatedColumns":["#latents","400","1000","Params"],"associatedMergedColumns":["seems to help ."]},{"number":"76.4","isBolded":true,"associatedRows":["W / weight sharing"],"associatedColumns":["#latent dimensions","400","800","Valid"],"associatedMergedColumns":["seems to help ."]},{"number":"68","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#attends"],"associatedMergedColumns":[]},{"number":"74","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#attends"],"associatedMergedColumns":[]},{"number":"70","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#attends"],"associatedMergedColumns":[]},{"number":"68","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#transformers per attend"],"associatedMergedColumns":[]},{"number":"64","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#transformers per attend"],"associatedMergedColumns":[]},{"number":"68","isBolded":false,"associatedRows":["No weight sharing"],"associatedColumns":["#latents"],"associatedMergedColumns":[]}]}]