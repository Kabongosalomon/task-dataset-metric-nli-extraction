[{"caption":"Model \nPrecision Recall \nF1 \nAccuracy \n\nSTAGG (Yih et al., 2016) \n70.9 \n80.3 \n71.7 \n63.9 \nGraftNet (Sun et al., 2018) \n-\n-\n66.4 (Hits@1) \n-\nPullNet (Sun et al., 2019a) \n-\n-\n68.1 (Hits@1) \n-\nEmbedKGQA (Saxena et al., 2020) \n-\n-\n66.6 (Hits@1) \n-\n\nT5-11B (Raffel et al., 2020) \n62.1 \n62.6 \n61.5 \n56.5 \nT5-11B + Revise \n63.6 \n64.3 \n63.0 \n57.7 \nCBR-KBQA (Ours) \n73.1 \n75.1 \n72.8 \n70.0 \n\nTable 1. Performance on the WebQSP dataset. GraftNet, PullNet and EmbedKGQA produces a ranking of KG entities hence evaluation is \nin Hits@k (see text for description). CBR-KBQA significantly outperforms baseline models in the strict exact match accuracy metric. \n\n","rows":["PullNet ( Sun et al . , 2019a )","GraftNet ( Sun et al . , 2018 )","T5 - 11B + Revise","STAGG ( Yih et al . , 2016 )","EmbedKGQA ( Saxena et al . , 2020 )","WebQSP","CWQ","-","T5 - 11B ( Raffel et al . , 2020 )","CBR - KBQA ( Ours )"],"columns":["Accuracy","Precision","Model","Recall","F1"],"mergedAllColumns":["in Hits@k ( see text for description ) . CBR - KBQA significantly outperforms baseline models in the strict exact match accuracy metric .","-"],"numberCells":[{"number":"61.5","isBolded":false,"associatedRows":["WebQSP","T5 - 11B ( Raffel et al . , 2020 )","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":["-"]},{"number":"72.8","isBolded":true,"associatedRows":["WebQSP","CBR - KBQA ( Ours )","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":["-"]},{"number":"0.796","isBolded":false,"associatedRows":["CWQ","CBR - KBQA ( Ours )"],"associatedColumns":["Precision","F1"],"associatedMergedColumns":["in Hits@k ( see text for description ) . CBR - KBQA significantly outperforms baseline models in the strict exact match accuracy metric ."]},{"number":"68.1(Hits@1)","isBolded":false,"associatedRows":["WebQSP","PullNet ( Sun et al . , 2019a )","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":["-"]},{"number":"66.4(Hits@1)","isBolded":false,"associatedRows":["WebQSP","GraftNet ( Sun et al . , 2018 )","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":[]},{"number":"70.0","isBolded":true,"associatedRows":["WebQSP","CBR - KBQA ( Ours )","-","-"],"associatedColumns":["Accuracy"],"associatedMergedColumns":["-"]},{"number":"0.789","isBolded":false,"associatedRows":["WebQSP","CBR - KBQA ( Ours )"],"associatedColumns":["Precision","F1"],"associatedMergedColumns":["in Hits@k ( see text for description ) . CBR - KBQA significantly outperforms baseline models in the strict exact match accuracy metric ."]},{"number":"66.6(Hits@1)","isBolded":false,"associatedRows":["WebQSP","EmbedKGQA ( Saxena et al . , 2020 )","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":["-"]},{"number":"63.0","isBolded":false,"associatedRows":["WebQSP","T5 - 11B + Revise","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":["-"]},{"number":"62.1","isBolded":false,"associatedRows":["WebQSP","T5 - 11B ( Raffel et al . , 2020 )"],"associatedColumns":["Precision"],"associatedMergedColumns":["-"]},{"number":"63.9","isBolded":false,"associatedRows":["WebQSP","STAGG ( Yih et al . , 2016 )","-","-"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"0.707","isBolded":false,"associatedRows":["CWQ"],"associatedColumns":["Model","Precision"],"associatedMergedColumns":["in Hits@k ( see text for description ) . CBR - KBQA significantly outperforms baseline models in the strict exact match accuracy metric ."]},{"number":"64.3","isBolded":false,"associatedRows":["WebQSP","T5 - 11B + Revise","-"],"associatedColumns":["Recall"],"associatedMergedColumns":["-"]},{"number":"63.6","isBolded":false,"associatedRows":["WebQSP","T5 - 11B + Revise"],"associatedColumns":["Precision"],"associatedMergedColumns":["-"]},{"number":"0.910","isBolded":false,"associatedRows":["CWQ"],"associatedColumns":["Model","Recall"],"associatedMergedColumns":["in Hits@k ( see text for description ) . CBR - KBQA significantly outperforms baseline models in the strict exact match accuracy metric ."]},{"number":"71.7","isBolded":false,"associatedRows":["WebQSP","STAGG ( Yih et al . , 2016 )","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":[]},{"number":"0.761","isBolded":false,"associatedRows":["WebQSP"],"associatedColumns":["Model","Precision"],"associatedMergedColumns":["in Hits@k ( see text for description ) . CBR - KBQA significantly outperforms baseline models in the strict exact match accuracy metric ."]},{"number":"73.1","isBolded":true,"associatedRows":["WebQSP","CBR - KBQA ( Ours )"],"associatedColumns":["Precision"],"associatedMergedColumns":["-"]},{"number":"75.1","isBolded":false,"associatedRows":["WebQSP","CBR - KBQA ( Ours )","-"],"associatedColumns":["Recall"],"associatedMergedColumns":["-"]},{"number":"0.819","isBolded":false,"associatedRows":["WebQSP"],"associatedColumns":["Model","Recall"],"associatedMergedColumns":["in Hits@k ( see text for description ) . CBR - KBQA significantly outperforms baseline models in the strict exact match accuracy metric ."]},{"number":"80.3","isBolded":true,"associatedRows":["WebQSP","STAGG ( Yih et al . , 2016 )","-"],"associatedColumns":["Recall"],"associatedMergedColumns":[]},{"number":"57.7","isBolded":false,"associatedRows":["WebQSP","T5 - 11B + Revise","-","-"],"associatedColumns":["Accuracy"],"associatedMergedColumns":["-"]},{"number":"70.9","isBolded":false,"associatedRows":["WebQSP","STAGG ( Yih et al . , 2016 )"],"associatedColumns":["Precision"],"associatedMergedColumns":[]},{"number":"62.6","isBolded":false,"associatedRows":["WebQSP","T5 - 11B ( Raffel et al . , 2020 )","-"],"associatedColumns":["Recall"],"associatedMergedColumns":["-"]},{"number":"56.5","isBolded":false,"associatedRows":["WebQSP","T5 - 11B ( Raffel et al . , 2020 )","-","-"],"associatedColumns":["Accuracy"],"associatedMergedColumns":["-"]}]},{"caption":"Dataset \nPrecision Recall \nF1 \n\nWebQSP \n0.761 \n0.819 0.789 \nCWQ \n0.707 \n0.910 0.796 \n\nTable 2. Entity linking performance on various datasets \n\n","rows":["PullNet ( Sun et al . , 2019a )","GraftNet ( Sun et al . , 2018 )","T5 - 11B + Revise","STAGG ( Yih et al . , 2016 )","EmbedKGQA ( Saxena et al . , 2020 )","WebQSP","CWQ","-","T5 - 11B ( Raffel et al . , 2020 )","CBR - KBQA ( Ours )"],"columns":["Accuracy","Precision","Model","Recall","F1"],"mergedAllColumns":["in Hits@k ( see text for description ) . CBR - KBQA significantly outperforms baseline models in the strict exact match accuracy metric .","-"],"numberCells":[{"number":"66.4(Hits@1)","isBolded":false,"associatedRows":["WebQSP","GraftNet ( Sun et al . , 2018 )","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":[]},{"number":"70.9","isBolded":false,"associatedRows":["WebQSP","STAGG ( Yih et al . , 2016 )"],"associatedColumns":["Precision"],"associatedMergedColumns":[]},{"number":"0.819","isBolded":false,"associatedRows":["WebQSP"],"associatedColumns":["Model","Recall"],"associatedMergedColumns":["in Hits@k ( see text for description ) . CBR - KBQA significantly outperforms baseline models in the strict exact match accuracy metric ."]},{"number":"56.5","isBolded":false,"associatedRows":["WebQSP","T5 - 11B ( Raffel et al . , 2020 )","-","-"],"associatedColumns":["Accuracy"],"associatedMergedColumns":["-"]},{"number":"63.9","isBolded":false,"associatedRows":["WebQSP","STAGG ( Yih et al . , 2016 )","-","-"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"64.3","isBolded":false,"associatedRows":["WebQSP","T5 - 11B + Revise","-"],"associatedColumns":["Recall"],"associatedMergedColumns":["-"]},{"number":"80.3","isBolded":true,"associatedRows":["WebQSP","STAGG ( Yih et al . , 2016 )","-"],"associatedColumns":["Recall"],"associatedMergedColumns":[]},{"number":"63.6","isBolded":false,"associatedRows":["WebQSP","T5 - 11B + Revise"],"associatedColumns":["Precision"],"associatedMergedColumns":["-"]},{"number":"0.796","isBolded":false,"associatedRows":["CWQ","CBR - KBQA ( Ours )"],"associatedColumns":["Precision","F1"],"associatedMergedColumns":["in Hits@k ( see text for description ) . CBR - KBQA significantly outperforms baseline models in the strict exact match accuracy metric ."]},{"number":"0.761","isBolded":false,"associatedRows":["WebQSP"],"associatedColumns":["Model","Precision"],"associatedMergedColumns":["in Hits@k ( see text for description ) . CBR - KBQA significantly outperforms baseline models in the strict exact match accuracy metric ."]},{"number":"0.910","isBolded":false,"associatedRows":["CWQ"],"associatedColumns":["Model","Recall"],"associatedMergedColumns":["in Hits@k ( see text for description ) . CBR - KBQA significantly outperforms baseline models in the strict exact match accuracy metric ."]},{"number":"0.707","isBolded":false,"associatedRows":["CWQ"],"associatedColumns":["Model","Precision"],"associatedMergedColumns":["in Hits@k ( see text for description ) . CBR - KBQA significantly outperforms baseline models in the strict exact match accuracy metric ."]},{"number":"73.1","isBolded":true,"associatedRows":["WebQSP","CBR - KBQA ( Ours )"],"associatedColumns":["Precision"],"associatedMergedColumns":["-"]},{"number":"75.1","isBolded":false,"associatedRows":["WebQSP","CBR - KBQA ( Ours )","-"],"associatedColumns":["Recall"],"associatedMergedColumns":["-"]},{"number":"70.0","isBolded":true,"associatedRows":["WebQSP","CBR - KBQA ( Ours )","-","-"],"associatedColumns":["Accuracy"],"associatedMergedColumns":["-"]},{"number":"0.789","isBolded":false,"associatedRows":["WebQSP","CBR - KBQA ( Ours )"],"associatedColumns":["Precision","F1"],"associatedMergedColumns":["in Hits@k ( see text for description ) . CBR - KBQA significantly outperforms baseline models in the strict exact match accuracy metric ."]},{"number":"66.6(Hits@1)","isBolded":false,"associatedRows":["WebQSP","EmbedKGQA ( Saxena et al . , 2020 )","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":["-"]},{"number":"68.1(Hits@1)","isBolded":false,"associatedRows":["WebQSP","PullNet ( Sun et al . , 2019a )","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":["-"]},{"number":"62.6","isBolded":false,"associatedRows":["WebQSP","T5 - 11B ( Raffel et al . , 2020 )","-"],"associatedColumns":["Recall"],"associatedMergedColumns":["-"]},{"number":"63.0","isBolded":false,"associatedRows":["WebQSP","T5 - 11B + Revise","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":["-"]},{"number":"61.5","isBolded":false,"associatedRows":["WebQSP","T5 - 11B ( Raffel et al . , 2020 )","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":["-"]},{"number":"71.7","isBolded":false,"associatedRows":["WebQSP","STAGG ( Yih et al . , 2016 )","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":[]},{"number":"62.1","isBolded":false,"associatedRows":["WebQSP","T5 - 11B ( Raffel et al . , 2020 )"],"associatedColumns":["Precision"],"associatedMergedColumns":["-"]},{"number":"72.8","isBolded":true,"associatedRows":["WebQSP","CBR - KBQA ( Ours )","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":["-"]},{"number":"57.7","isBolded":false,"associatedRows":["WebQSP","T5 - 11B + Revise","-","-"],"associatedColumns":["Accuracy"],"associatedMergedColumns":["-"]}]},{"caption":"Table 2. \n\n","rows":["PullNet ( Sun et al . , 2019a )","DynAS ( Anonymous )","T5 - 11B + Revise","KBQA - GST ( Lan et al . , 2019 )","QGG ( Lan \u0026 Jiang , 2020 )","-","T5 - 11B ( Raffel et al . , 2020 )","CBR - KBQA ( Ours )"],"columns":["Acc","P","R","F1"],"mergedAllColumns":[],"numberCells":[{"number":"70.0","isBolded":true,"associatedRows":["CBR - KBQA ( Ours )","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":[]},{"number":"58.2","isBolded":false,"associatedRows":["T5 - 11B + Revise","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":[]},{"number":"55.2","isBolded":false,"associatedRows":["T5 - 11B ( Raffel et al . , 2020 )"],"associatedColumns":["P"],"associatedMergedColumns":[]},{"number":"58.7","isBolded":false,"associatedRows":["T5 - 11B + Revise"],"associatedColumns":["P"],"associatedMergedColumns":[]},{"number":"67.1","isBolded":true,"associatedRows":["CBR - KBQA ( Ours )","-","-","-"],"associatedColumns":["Acc"],"associatedMergedColumns":[]},{"number":"52.4","isBolded":false,"associatedRows":["T5 - 11B ( Raffel et al . , 2020 )","-","-","-"],"associatedColumns":["Acc"],"associatedMergedColumns":[]},{"number":"59.6","isBolded":false,"associatedRows":["T5 - 11B + Revise","-"],"associatedColumns":["R"],"associatedMergedColumns":[]},{"number":"45.9","isBolded":false,"associatedRows":["PullNet ( Sun et al . , 2019a )","-","-","-"],"associatedColumns":["Acc"],"associatedMergedColumns":[]},{"number":"71.9","isBolded":true,"associatedRows":["CBR - KBQA ( Ours )","-"],"associatedColumns":["R"],"associatedMergedColumns":[]},{"number":"70.4","isBolded":true,"associatedRows":["CBR - KBQA ( Ours )"],"associatedColumns":["P"],"associatedMergedColumns":[]},{"number":"50.0","isBolded":false,"associatedRows":["DynAS ( Anonymous )","-","-","-"],"associatedColumns":["Acc"],"associatedMergedColumns":[]},{"number":"54.6","isBolded":false,"associatedRows":["T5 - 11B ( Raffel et al . , 2020 )","-","-"],"associatedColumns":["F1"],"associatedMergedColumns":[]},{"number":"55.6","isBolded":false,"associatedRows":["T5 - 11B + Revise","-","-","-"],"associatedColumns":["Acc"],"associatedMergedColumns":[]},{"number":"39.4","isBolded":false,"associatedRows":["KBQA - GST ( Lan et al . , 2019 )","-","-","-"],"associatedColumns":["Acc"],"associatedMergedColumns":[]},{"number":"44.1","isBolded":false,"associatedRows":["QGG ( Lan \u0026 Jiang , 2020 )","-","-","-"],"associatedColumns":["Acc"],"associatedMergedColumns":[]},{"number":"55.4","isBolded":false,"associatedRows":["T5 - 11B ( Raffel et al . , 2020 )","-"],"associatedColumns":["R"],"associatedMergedColumns":[]}]},{"caption":"Model \nMCD1 MCD2 MCD3 MCD-mean \n\nT5-11B \n72.9 \n69.2 \n62.0 \n67.7 \nCBR-KBQA \n87.9 \n61.3 \n60.6 \n69.93 \n\nTable 4. Accuracy on the CFQ dataset. Our CBR approach (with \norder of magnitude less parameters) outperforms a massive T5 \nmodel. \n\n","rows":["T5 - 11B","CBR - KBQA"],"columns":["MCD2","MCD3","MCD1","MCD - mean"],"mergedAllColumns":[],"numberCells":[{"number":"67.7","isBolded":false,"associatedRows":["T5 - 11B"],"associatedColumns":["MCD - mean"],"associatedMergedColumns":[]},{"number":"87.9","isBolded":false,"associatedRows":["CBR - KBQA"],"associatedColumns":["MCD1"],"associatedMergedColumns":[]},{"number":"61.3","isBolded":false,"associatedRows":["CBR - KBQA"],"associatedColumns":["MCD2"],"associatedMergedColumns":[]},{"number":"69.2","isBolded":false,"associatedRows":["T5 - 11B"],"associatedColumns":["MCD2"],"associatedMergedColumns":[]},{"number":"72.9","isBolded":false,"associatedRows":["T5 - 11B"],"associatedColumns":["MCD1"],"associatedMergedColumns":[]},{"number":"69.93","isBolded":false,"associatedRows":["CBR - KBQA"],"associatedColumns":["MCD - mean"],"associatedMergedColumns":[]},{"number":"62.0","isBolded":false,"associatedRows":["T5 - 11B"],"associatedColumns":["MCD3"],"associatedMergedColumns":[]},{"number":"60.6","isBolded":false,"associatedRows":["CBR - KBQA"],"associatedColumns":["MCD3"],"associatedMergedColumns":[]}]},{"caption":"WebQSP \nAccuracy(%) \n∆ \n\nCBR-KBQA (before Revise) \n69.43 \n-\n+Revise (Roberta) \n69.49 \n+0.06 \n+Revise (TransE) \n70.00 \n+0.57 \n\nCWQ \nAccuracy(%) \n∆ \n\nCBR-KBQA (before Revise) \n65.95 \n-\n+Revise (Roberta) \n66.32 \n+0.37 \n+Revise (TransE) \n67.11 \n+1.16 \n\nTable 5. Impacts of the revise step. We show that the revise step \nconsistently improves the accuracy on WebQSP and CWQ, espe-\ncially with the TransE pretrained embeddings. \n\n","rows":["T5 - 11B","CBR - KBQA"],"columns":["MCD2","MCD3","MCD1","MCD - mean"],"mergedAllColumns":[],"numberCells":[{"number":"87.9","isBolded":false,"associatedRows":["CBR - KBQA"],"associatedColumns":["MCD1"],"associatedMergedColumns":[]},{"number":"61.3","isBolded":false,"associatedRows":["CBR - KBQA"],"associatedColumns":["MCD2"],"associatedMergedColumns":[]},{"number":"69.2","isBolded":false,"associatedRows":["T5 - 11B"],"associatedColumns":["MCD2"],"associatedMergedColumns":[]},{"number":"72.9","isBolded":false,"associatedRows":["T5 - 11B"],"associatedColumns":["MCD1"],"associatedMergedColumns":[]},{"number":"62.0","isBolded":false,"associatedRows":["T5 - 11B"],"associatedColumns":["MCD3"],"associatedMergedColumns":[]},{"number":"60.6","isBolded":false,"associatedRows":["CBR - KBQA"],"associatedColumns":["MCD3"],"associatedMergedColumns":[]},{"number":"67.7","isBolded":false,"associatedRows":["T5 - 11B"],"associatedColumns":["MCD - mean"],"associatedMergedColumns":[]},{"number":"69.93","isBolded":false,"associatedRows":["CBR - KBQA"],"associatedColumns":["MCD - mean"],"associatedMergedColumns":[]}]},{"caption":"Scenario \nInitial Set Held-Out \n\nTransformer \n59.6 \n0.0 \n+ Fine-tune on additional cases only (100 steps/50sec) \n1.3 \n76.3 \n+ Fine-tune on additional cases and original data (300 steps/150sec) \n53.1 \n57.6 \n\nCBR-KBQA (Ours) \n69.4 \n0.0 \n+ Adding additional cases to index (2 sec) \n69.4 \n70.6 \n\nTable 6. Robustness and controllability of our method against transformer. We can easily and quickly adopt to new relations given cases \nabout it, whereas heavily parameterized transformer is finicky, not stable, and can undergo catastrophic forgetting when we try to add new \nrelation information intro its parameters. \n\n","rows":["Transformer","+ Adding additional cases","+ Fine - tune on additional cases and original data ( 300 steps / 150sec )","+ Fine - tune on additional cases only ( 100 steps / 50sec )","+ Adding additional cases to index ( 2 sec )","CBR - KBQA ( Ours )"],"columns":["Accuracy","Initial Set","Precision","Held - Out","Recall","F1"],"mergedAllColumns":["relation information intro its parameters ."],"numberCells":[{"number":"36.54","isBolded":false,"associatedRows":["+ Adding additional cases"],"associatedColumns":["Initial Set","Precision"],"associatedMergedColumns":["relation information intro its parameters ."]},{"number":"59.6","isBolded":false,"associatedRows":["Transformer","+ Fine - tune on additional cases and original data ( 300 steps / 150sec )"],"associatedColumns":["Initial Set"],"associatedMergedColumns":[]},{"number":"53.1","isBolded":false,"associatedRows":["+ Fine - tune on additional cases and original data ( 300 steps / 150sec )"],"associatedColumns":["Initial Set"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["CBR - KBQA ( Ours )"],"associatedColumns":["Held - Out","Accuracy"],"associatedMergedColumns":["relation information intro its parameters ."]},{"number":"0.0","isBolded":false,"associatedRows":["Transformer","+ Fine - tune on additional cases and original data ( 300 steps / 150sec )"],"associatedColumns":["Held - Out"],"associatedMergedColumns":[]},{"number":"69.4","isBolded":false,"associatedRows":["+ Adding additional cases to index ( 2 sec )"],"associatedColumns":["Initial Set"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["CBR - KBQA ( Ours )"],"associatedColumns":["Initial Set","Recall"],"associatedMergedColumns":["relation information intro its parameters ."]},{"number":"0.0","isBolded":false,"associatedRows":["CBR - KBQA ( Ours )"],"associatedColumns":["Initial Set","F1"],"associatedMergedColumns":["relation information intro its parameters ."]},{"number":"1.3","isBolded":false,"associatedRows":["+ Fine - tune on additional cases only ( 100 steps / 50sec )"],"associatedColumns":["Initial Set"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["CBR - KBQA ( Ours )"],"associatedColumns":["Initial Set","Precision"],"associatedMergedColumns":["relation information intro its parameters ."]},{"number":"0.0","isBolded":false,"associatedRows":["CBR - KBQA ( Ours )"],"associatedColumns":["Held - Out"],"associatedMergedColumns":[]},{"number":"70.6","isBolded":false,"associatedRows":["+ Adding additional cases to index ( 2 sec )"],"associatedColumns":["Held - Out"],"associatedMergedColumns":[]},{"number":"38.59","isBolded":false,"associatedRows":["+ Adding additional cases"],"associatedColumns":["Initial Set","Recall"],"associatedMergedColumns":["relation information intro its parameters ."]},{"number":"57.6","isBolded":false,"associatedRows":["+ Fine - tune on additional cases and original data ( 300 steps / 150sec )"],"associatedColumns":["Held - Out"],"associatedMergedColumns":[]},{"number":"69.4","isBolded":false,"associatedRows":["CBR - KBQA ( Ours )"],"associatedColumns":["Initial Set"],"associatedMergedColumns":[]},{"number":"76.3","isBolded":false,"associatedRows":["+ Fine - tune on additional cases only ( 100 steps / 50sec )"],"associatedColumns":["Held - Out"],"associatedMergedColumns":[]},{"number":"32.89","isBolded":false,"associatedRows":["+ Adding additional cases"],"associatedColumns":["Held - Out","Accuracy"],"associatedMergedColumns":["relation information intro its parameters ."]},{"number":"36.39","isBolded":false,"associatedRows":["+ Adding additional cases"],"associatedColumns":["Initial Set","F1"],"associatedMergedColumns":["relation information intro its parameters ."]}]},{"caption":"Table 7. Results for H-I-T-L experiment. After adding a few cases , we see that we can get the accuracy of OOV questions to improve \nconsiderably, without needing to re-train the model. \n\n","rows":["+ Adding additional cases","CBR - KBQA ( Ours )"],"columns":["Accuracy","Precision","Recall","F1"],"mergedAllColumns":[],"numberCells":[{"number":"36.39","isBolded":false,"associatedRows":["+ Adding additional cases"],"associatedColumns":["F1"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["CBR - KBQA ( Ours )"],"associatedColumns":["Precision"],"associatedMergedColumns":[]},{"number":"38.59","isBolded":false,"associatedRows":["+ Adding additional cases"],"associatedColumns":["Recall"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["CBR - KBQA ( Ours )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["CBR - KBQA ( Ours )"],"associatedColumns":["F1"],"associatedMergedColumns":[]},{"number":"36.54","isBolded":false,"associatedRows":["+ Adding additional cases"],"associatedColumns":["Precision"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["CBR - KBQA ( Ours )"],"associatedColumns":["Recall"],"associatedMergedColumns":[]},{"number":"32.89","isBolded":false,"associatedRows":["+ Adding additional cases"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]}]},{"caption":"Data \n# Total Q \n# Q that need \n# Correct \ncomp. reasoning \nT5 \nCBR \n\nCWQ \n3531 \n639 \n205 \n270 \nCFQ \n11968 \n6541 \n3351 3886 \n\nTable 8. Analysis of Compositional Reasoning. We compare the \nperformance of models on questions that need novel combinations \nof relations not seen during training. \n\n","rows":["11968","CWQ","CFQ"],"columns":["T5","CBR","# Correct","205","# Q that need","comp . reasoning","270","639","# Total Q"],"mergedAllColumns":[],"numberCells":[{"number":"6541","isBolded":false,"associatedRows":["CFQ","11968"],"associatedColumns":["# Q that need","# Total Q","comp . reasoning","639"],"associatedMergedColumns":[]},{"number":"3886","isBolded":true,"associatedRows":["CFQ","11968"],"associatedColumns":["# Correct","# Total Q","CBR","270"],"associatedMergedColumns":[]},{"number":"3351","isBolded":false,"associatedRows":["CFQ","11968"],"associatedColumns":["# Correct","# Total Q","T5","205"],"associatedMergedColumns":[]},{"number":"3531","isBolded":false,"associatedRows":["CWQ"],"associatedColumns":["# Q that need","# Total Q","comp . reasoning"],"associatedMergedColumns":[]}]},{"caption":"SimpleQuestions (SQ) is a large dataset containing \nmore than 100K NL questions that are \u0027simple\u0027 in nature -i.e. each NL query maps to a single relation (fact) in the \n\nCases Added via \nDataset \n# missing relations # questions H-I-T-L SimpleQuestions Avg. # cases per relation \n\nWebQSP \n94 \n79 \n72 \n292 \n3.87 \n\nTable 10. Number of questions in the evaluation set that needs a relation which is not seen in the training set. Note that, there can be \nmultiple relations in a question that might not be seen during training. The last two columns show the number of cases added both via \nhuman-in-the-loop (H-I-T-L) annotation and automatically from SimpleQuestions dataset. \n","rows":["WebQSP","292"],"columns":["# missing relations","H - I - T - L","# questions","Avg . # cases per relation","SimpleQuestions ( SQ ) is a large dataset containing"],"mergedAllColumns":["Cases Added via"],"numberCells":[{"number":"79","isBolded":false,"associatedRows":["WebQSP"],"associatedColumns":["SimpleQuestions ( SQ ) is a large dataset containing","# questions"],"associatedMergedColumns":["Cases Added via"]},{"number":"94","isBolded":false,"associatedRows":["WebQSP"],"associatedColumns":["SimpleQuestions ( SQ ) is a large dataset containing","# missing relations"],"associatedMergedColumns":["Cases Added via"]},{"number":"3.87","isBolded":false,"associatedRows":["WebQSP","292"],"associatedColumns":["SimpleQuestions ( SQ ) is a large dataset containing","Avg . # cases per relation"],"associatedMergedColumns":["Cases Added via"]},{"number":"72","isBolded":false,"associatedRows":["WebQSP"],"associatedColumns":["SimpleQuestions ( SQ ) is a large dataset containing","H - I - T - L"],"associatedMergedColumns":["Cases Added via"]}]},{"caption":"NL Query \nSPARQL \nSource \n\nWhat is the Mexican Peso called? \nselect ?x where { m.012ts8 finance.currency.currency code ?x .} \nManual \nWho invented the telephone? \nselect ?x where { m.0g 3r base.argumentmaps.original idea.innovator ?x .} Manual \nwhat area is wrvr broadcated in? \nselect ?x where { m.025z9rx broadcast.broadcast.area served ?x .} \nSQ \nWhere are Siamese cats originally from? select ?x where { m.012ts8 biology.animal breed.place of origin ?x .} \nManual \n\nTable 11. Examples of few added questions and their corresponding SPARQL queries. Notice that the SPARQL queries are very simple \nto create once we know the name of the missing relation. The source column indicate whether the question was manually created or \nautomatically added from Simple Questions (SQ) dataset. \n\n","rows":["Baseline ( K \u003d 0 )","CBR - KBQA ( K \u003d 20 )"],"columns":["WebQSP","CWQ"],"mergedAllColumns":[],"numberCells":[{"number":"67.2","isBolded":false,"associatedRows":["Baseline ( K \u003d 0 )"],"associatedColumns":["WebQSP"],"associatedMergedColumns":[]},{"number":"67.1","isBolded":true,"associatedRows":["CBR - KBQA ( K \u003d 20 )"],"associatedColumns":["CWQ"],"associatedMergedColumns":[]},{"number":"65.8","isBolded":false,"associatedRows":["Baseline ( K \u003d 0 )"],"associatedColumns":["CWQ"],"associatedMergedColumns":[]},{"number":"70.0","isBolded":true,"associatedRows":["CBR - KBQA ( K \u003d 20 )"],"associatedColumns":["WebQSP"],"associatedMergedColumns":[]}]},{"caption":"WebQSP CWQ \n\nBaseline (K \u003d 0) \n67.2 \n65.8 \nCBR-KBQA (K \u003d 20) \n70.0 \n67.1 \n\nTable 12. Comparison with a baseline model that do not use cases. The numbers denote exact match accuracy. \n\n","rows":["Baseline ( K \u003d 0 )","CBR - KBQA ( K \u003d 20 )"],"columns":["WebQSP","CWQ"],"mergedAllColumns":[],"numberCells":[{"number":"65.8","isBolded":false,"associatedRows":["Baseline ( K \u003d 0 )"],"associatedColumns":["CWQ"],"associatedMergedColumns":[]},{"number":"70.0","isBolded":true,"associatedRows":["CBR - KBQA ( K \u003d 20 )"],"associatedColumns":["WebQSP"],"associatedMergedColumns":[]},{"number":"67.1","isBolded":true,"associatedRows":["CBR - KBQA ( K \u003d 20 )"],"associatedColumns":["CWQ"],"associatedMergedColumns":[]},{"number":"67.2","isBolded":false,"associatedRows":["Baseline ( K \u003d 0 )"],"associatedColumns":["WebQSP"],"associatedMergedColumns":[]}]},{"caption":"CWQ \nAttempted revisions \n247 \n1128 \nRevised program covers more target clauses \n29 \n114 \nRevised program produces an answer \n13 \n27 \nRevised clauses match target \n9 \n41 \n\nTable 15. We report the outcomes of the revise step on various datasets. We attempt revision if the predicted program produces no answer. \nIf the revision step aligns some but not all clauses, it is not guanranteed to produce the answer. \n\n","rows":["Revised program produces an answer","Revised clauses match target","Revised program covers more target clauses","9"],"columns":["1128","247","CWQ"],"mergedAllColumns":["114"],"numberCells":[{"number":"41","isBolded":false,"associatedRows":["Revised clauses match target","9"],"associatedColumns":["CWQ","1128"],"associatedMergedColumns":["114"]},{"number":"27","isBolded":false,"associatedRows":["Revised program produces an answer"],"associatedColumns":["CWQ","1128"],"associatedMergedColumns":["114"]},{"number":"13","isBolded":false,"associatedRows":["Revised program produces an answer"],"associatedColumns":["CWQ","247"],"associatedMergedColumns":["114"]},{"number":"29","isBolded":false,"associatedRows":["Revised program covers more target clauses"],"associatedColumns":["CWQ","247"],"associatedMergedColumns":[]}]}]