[{"caption":"84.5 \n84.2 \n2.8 \n-\n300D Directional self-attention network (Shen et al., 2018a)  91.1 \n85.6 \n2.4 \n587 \n600D Gumbel TreeLSTM (Choi et al., 2018)  93.1 \n86.0 \n10.0 \n-\n600D Residual stacked encoders (Nie and Bansal, 2017)  91.0 \n86.0 \n29.0 \n-\n300D Reinforced self-attention network (Shen et al., 2018b)  92.6 \n86.3 \n3.1 \n622 \n1200D Distance-based self-attention network (Im and Cho, 2017)  89.6 \n86.3 \n4.7 \n693 \n600D CNN (Dense) with self-attention \n88.7 \n84.6 \n2.4 \n121 \nours (600D Single DSA) \n87.3 \n86.8 \n2.1 \n135 \nours (2400D Multiple DSA) \n89.0 \n87.4 \n7.0 \n198 \n\nTable 1: SNLI Results. The values in T(s)/epoch come from original papers and are experimented on the \nsame graphic card to ours (single Nvidia GTX 1080Ti). Word embedding is not counted in parameters. \n\n","rows":["600D Gumbel TreeLSTM ( Choi et al . , 2018 )","600D CNN ( Dense ) with self - attention","300D Directional self - attention network ( Shen et al . , 2018a )","ours ( 2400D Multiple DSA )","300D Reinforced self - attention network ( Shen et al . , 2018b )","ours ( 600D Single DSA )","1200D Distance - based self - attention network ( Im and Cho , 2017 )","600D Residual stacked encoders ( Nie and Bansal , 2017 )"],"columns":["-"],"mergedAllColumns":["693","121","135","587","622","-"],"numberCells":[{"number":"87.3","isBolded":false,"associatedRows":["ours ( 600D Single DSA )"],"associatedColumns":["-"],"associatedMergedColumns":["121"]},{"number":"86.3","isBolded":false,"associatedRows":["300D Reinforced self - attention network ( Shen et al . , 2018b )"],"associatedColumns":["-"],"associatedMergedColumns":["-"]},{"number":"3.1","isBolded":false,"associatedRows":["300D Reinforced self - attention network ( Shen et al . , 2018b )"],"associatedColumns":["-"],"associatedMergedColumns":["-"]},{"number":"29.0","isBolded":false,"associatedRows":["600D Residual stacked encoders ( Nie and Bansal , 2017 )"],"associatedColumns":["-"],"associatedMergedColumns":["-"]},{"number":"4.7","isBolded":false,"associatedRows":["1200D Distance - based self - attention network ( Im and Cho , 2017 )"],"associatedColumns":["-"],"associatedMergedColumns":["622"]},{"number":"85.6","isBolded":false,"associatedRows":["300D Directional self - attention network ( Shen et al . , 2018a )"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"86.0","isBolded":false,"associatedRows":["600D Residual stacked encoders ( Nie and Bansal , 2017 )"],"associatedColumns":["-"],"associatedMergedColumns":["-"]},{"number":"92.6","isBolded":false,"associatedRows":["300D Reinforced self - attention network ( Shen et al . , 2018b )"],"associatedColumns":["-"],"associatedMergedColumns":["-"]},{"number":"86.3","isBolded":false,"associatedRows":["1200D Distance - based self - attention network ( Im and Cho , 2017 )"],"associatedColumns":["-"],"associatedMergedColumns":["622"]},{"number":"2.4","isBolded":false,"associatedRows":["600D CNN ( Dense ) with self - attention"],"associatedColumns":["-"],"associatedMergedColumns":["693"]},{"number":"89.6","isBolded":false,"associatedRows":["1200D Distance - based self - attention network ( Im and Cho , 2017 )"],"associatedColumns":["-"],"associatedMergedColumns":["622"]},{"number":"7.0","isBolded":false,"associatedRows":["ours ( 2400D Multiple DSA )"],"associatedColumns":["-"],"associatedMergedColumns":["135"]},{"number":"84.5","isBolded":false,"associatedRows":["300D Directional self - attention network ( Shen et al . , 2018a )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"87.4","isBolded":true,"associatedRows":["ours ( 2400D Multiple DSA )"],"associatedColumns":["-"],"associatedMergedColumns":["135"]},{"number":"91.0","isBolded":false,"associatedRows":["600D Residual stacked encoders ( Nie and Bansal , 2017 )"],"associatedColumns":["-"],"associatedMergedColumns":["-"]},{"number":"84.6","isBolded":false,"associatedRows":["600D CNN ( Dense ) with self - attention"],"associatedColumns":["-"],"associatedMergedColumns":["693"]},{"number":"84.2","isBolded":false,"associatedRows":["1200D Distance - based self - attention network ( Im and Cho , 2017 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"91.1","isBolded":false,"associatedRows":["300D Directional self - attention network ( Shen et al . , 2018a )"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"2.4","isBolded":false,"associatedRows":["300D Directional self - attention network ( Shen et al . , 2018a )"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"93.1","isBolded":false,"associatedRows":["600D Gumbel TreeLSTM ( Choi et al . , 2018 )"],"associatedColumns":["-"],"associatedMergedColumns":["587"]},{"number":"88.7","isBolded":false,"associatedRows":["600D CNN ( Dense ) with self - attention"],"associatedColumns":["-"],"associatedMergedColumns":["693"]},{"number":"86.0","isBolded":false,"associatedRows":["600D Gumbel TreeLSTM ( Choi et al . , 2018 )"],"associatedColumns":["-"],"associatedMergedColumns":["587"]},{"number":"86.8","isBolded":false,"associatedRows":["ours ( 600D Single DSA )"],"associatedColumns":["-"],"associatedMergedColumns":["121"]},{"number":"89.0","isBolded":false,"associatedRows":["ours ( 2400D Multiple DSA )"],"associatedColumns":["-"],"associatedMergedColumns":["135"]},{"number":"2.8","isBolded":false,"associatedRows":["1200D Distance - based self - attention network ( Im and Cho , 2017 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"2.1","isBolded":true,"associatedRows":["ours ( 600D Single DSA )"],"associatedColumns":["-"],"associatedMergedColumns":["121"]},{"number":"10.0","isBolded":false,"associatedRows":["600D Gumbel TreeLSTM ( Choi et al . , 2018 )"],"associatedColumns":["-"],"associatedMergedColumns":["587"]}]},{"caption":"Table 2: Test accuracy with SST dataset. \n\n","rows":["BiLSTM ( Cho et al . , 2014 )","BiLSTM with self - attention","ours ( Single DSA )","CNN ( Dense ) with self - attention"],"columns":[],"mergedAllColumns":[],"numberCells":[{"number":"88.2","isBolded":false,"associatedRows":["BiLSTM with self - attention"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"48.0","isBolded":false,"associatedRows":["CNN ( Dense ) with self - attention"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"50.6","isBolded":true,"associatedRows":["CNN ( Dense ) with self - attention"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"88.5","isBolded":true,"associatedRows":["ours ( Single DSA )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"49.5","isBolded":false,"associatedRows":["BiLSTM ( Cho et al . , 2014 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"50.6","isBolded":true,"associatedRows":["ours ( Single DSA )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"50.4","isBolded":false,"associatedRows":["BiLSTM with self - attention"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"87.5","isBolded":false,"associatedRows":["BiLSTM ( Cho et al . , 2014 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"88.3","isBolded":false,"associatedRows":["CNN ( Dense ) with self - attention"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"87.2","isBolded":false,"associatedRows":["CNN ( Dense ) with self - attention"],"associatedColumns":[],"associatedMergedColumns":[]}]},{"caption":"Table 2. We \ncompare single DSA with four baseline models: \n\n1 https://nlp.stanford.edu/projects/snli/ \n\nBiLSTM ","rows":["600D Gumbel TreeLSTM ( Choi et al . , 2018 )","600D CNN ( Dense ) with self - attention","300D Directional self - attention network ( Shen et al . , 2018a )","ours ( 2400D Multiple DSA )","300D Reinforced self - attention network ( Shen et al . , 2018b )","ours ( 600D Single DSA )","1200D Distance - based self - attention network ( Im and Cho , 2017 )","600D Residual stacked encoders ( Nie and Bansal , 2017 )"],"columns":["-"],"mergedAllColumns":["693","121","135","587","622","-"],"numberCells":[{"number":"84.2","isBolded":false,"associatedRows":["1200D Distance - based self - attention network ( Im and Cho , 2017 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"86.3","isBolded":false,"associatedRows":["300D Reinforced self - attention network ( Shen et al . , 2018b )"],"associatedColumns":["-"],"associatedMergedColumns":["-"]},{"number":"3.1","isBolded":false,"associatedRows":["300D Reinforced self - attention network ( Shen et al . , 2018b )"],"associatedColumns":["-"],"associatedMergedColumns":["-"]},{"number":"29.0","isBolded":false,"associatedRows":["600D Residual stacked encoders ( Nie and Bansal , 2017 )"],"associatedColumns":["-"],"associatedMergedColumns":["-"]},{"number":"86.8","isBolded":false,"associatedRows":["ours ( 600D Single DSA )"],"associatedColumns":["-"],"associatedMergedColumns":["121"]},{"number":"89.6","isBolded":false,"associatedRows":["1200D Distance - based self - attention network ( Im and Cho , 2017 )"],"associatedColumns":["-"],"associatedMergedColumns":["622"]},{"number":"84.5","isBolded":false,"associatedRows":["300D Directional self - attention network ( Shen et al . , 2018a )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"10.0","isBolded":false,"associatedRows":["600D Gumbel TreeLSTM ( Choi et al . , 2018 )"],"associatedColumns":["-"],"associatedMergedColumns":["587"]},{"number":"85.6","isBolded":false,"associatedRows":["300D Directional self - attention network ( Shen et al . , 2018a )"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"88.7","isBolded":false,"associatedRows":["600D CNN ( Dense ) with self - attention"],"associatedColumns":["-"],"associatedMergedColumns":["693"]},{"number":"89.0","isBolded":false,"associatedRows":["ours ( 2400D Multiple DSA )"],"associatedColumns":["-"],"associatedMergedColumns":["135"]},{"number":"4.7","isBolded":false,"associatedRows":["1200D Distance - based self - attention network ( Im and Cho , 2017 )"],"associatedColumns":["-"],"associatedMergedColumns":["622"]},{"number":"86.3","isBolded":false,"associatedRows":["1200D Distance - based self - attention network ( Im and Cho , 2017 )"],"associatedColumns":["-"],"associatedMergedColumns":["622"]},{"number":"93.1","isBolded":false,"associatedRows":["600D Gumbel TreeLSTM ( Choi et al . , 2018 )"],"associatedColumns":["-"],"associatedMergedColumns":["587"]},{"number":"7.0","isBolded":false,"associatedRows":["ours ( 2400D Multiple DSA )"],"associatedColumns":["-"],"associatedMergedColumns":["135"]},{"number":"2.4","isBolded":false,"associatedRows":["300D Directional self - attention network ( Shen et al . , 2018a )"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"87.4","isBolded":true,"associatedRows":["ours ( 2400D Multiple DSA )"],"associatedColumns":["-"],"associatedMergedColumns":["135"]},{"number":"2.4","isBolded":false,"associatedRows":["600D CNN ( Dense ) with self - attention"],"associatedColumns":["-"],"associatedMergedColumns":["693"]},{"number":"84.6","isBolded":false,"associatedRows":["600D CNN ( Dense ) with self - attention"],"associatedColumns":["-"],"associatedMergedColumns":["693"]},{"number":"86.0","isBolded":false,"associatedRows":["600D Gumbel TreeLSTM ( Choi et al . , 2018 )"],"associatedColumns":["-"],"associatedMergedColumns":["587"]},{"number":"87.3","isBolded":false,"associatedRows":["ours ( 600D Single DSA )"],"associatedColumns":["-"],"associatedMergedColumns":["121"]},{"number":"86.0","isBolded":false,"associatedRows":["600D Residual stacked encoders ( Nie and Bansal , 2017 )"],"associatedColumns":["-"],"associatedMergedColumns":["-"]},{"number":"91.0","isBolded":false,"associatedRows":["600D Residual stacked encoders ( Nie and Bansal , 2017 )"],"associatedColumns":["-"],"associatedMergedColumns":["-"]},{"number":"91.1","isBolded":false,"associatedRows":["300D Directional self - attention network ( Shen et al . , 2018a )"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"2.1","isBolded":true,"associatedRows":["ours ( 600D Single DSA )"],"associatedColumns":["-"],"associatedMergedColumns":["121"]},{"number":"2.8","isBolded":false,"associatedRows":["1200D Distance - based self - attention network ( Im and Cho , 2017 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"92.6","isBolded":false,"associatedRows":["300D Reinforced self - attention network ( Shen et al . , 2018b )"],"associatedColumns":["-"],"associatedMergedColumns":["-"]}]}]