[{"caption":"Table 1: Test error rates (%) on the 20 QA tasks for models using 1k training examples (mean \ntest errors for 10k training examples are shown at the bottom). Key: BoW \u003d bag-of-words \nrepresentation; PE \u003d position encoding representation; LS \u003d linear start training; RN \u003d random \ninjection of time index noise; LW \u003d RNN-style layer-wise weight tying (if not stated, adjacent \nweight tying is used); joint \u003d joint training on all tasks (as opposed to per-task training). \n\n","rows":["The best MemN2N models are reasonably close to the supervised models ( e . g .","17 : positional reasoning","Mean error ( % )","2 : 2 supporting facts","8 : lists / sets","11 : basic coreference","10 : indefinite knowledge","Jittering the time index with random empty memories ( RN ) as described in Section","20 : agent \u0027 s motivation","14 : time reasoning","12 : conjunction","1 : 1 supporting fact","4 : 2 argument relations","9 : simple negation","â€¢","13 : compound coreference","1k :","7 : counting","19 : path finding","6 : yes / no questions","PE alone gets","3 : 3 supporting facts","trained and 10k :","MemNN vs","15 : basic deduction","18 : size reasoning","5 : 3 argument relations","16 : basic induction"],"columns":["[ 22 ]","joint","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","4 , although the supervised models are still superior .","LS","BoW","LW","( 1 ) ) over some illustrative examples in Fig .","Joint training on all tasks helps .","PE LS","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","1 hop","MemNN","10","LS RN","11","12","13","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","MemNN [ 22 ]","LSTM","15","17","18","Strongly","Baseline","Supervised","3 hops","WSH","We give examples of","MemN2N","4","PE","more computational hops give improved performance .","2 and in","the hops performed ( via the values of eq .","RN","2 hops","20"],"mergedAllColumns":["by clear improvements on tasks 4 , 5 , 15 and 18 , where word ordering is particularly important .","Appendix B .","On 10k training data"],"numberCells":[{"number":"0.1","isBolded":false,"associatedRows":["1 : 1 supporting fact"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"3.6","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"2.0","isBolded":false,"associatedRows":["10 : indefinite knowledge"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"64.0","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"22.8","isBolded":false,"associatedRows":["4 : 2 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["2 : 2 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"40.4","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"31.7","isBolded":false,"associatedRows":["3 : 3 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.3","isBolded":false,"associatedRows":["12 : conjunction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.5","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"6.7%for","isBolded":false,"associatedRows":["The best MemN2N models are reasonably close to the supervised models ( e . g .","1k :"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"68.7","isBolded":false,"associatedRows":["10 : indefinite knowledge"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.3","isBolded":false,"associatedRows":["13 : compound coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"52.0","isBolded":false,"associatedRows":["6 : yes / no questions"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"10.6","isBolded":false,"associatedRows":["10 : indefinite knowledge"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"51.0","isBolded":false,"associatedRows":["7 : counting"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"64.2","isBolded":false,"associatedRows":["3 : 3 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"11.1","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"12.4","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["12 : conjunction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"58.8","isBolded":false,"associatedRows":["3 : 3 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"1.7","isBolded":false,"associatedRows":["14 : time reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"11.6","isBolded":false,"associatedRows":["4 : 2 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"80.0","isBolded":false,"associatedRows":["2 : 2 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"15.9","isBolded":false,"associatedRows":["7 : counting"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"40.2","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"17.6","isBolded":false,"associatedRows":["2 : 2 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"9.3","isBolded":false,"associatedRows":["8 : lists / sets"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"50.0","isBolded":false,"associatedRows":["1 : 1 supporting fact"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.9","isBolded":false,"associatedRows":["11 : basic coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.2","isBolded":false,"associatedRows":["13 : compound coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"53.6%error,whileusingLSreducesitto1.6%.","isBolded":false,"associatedRows":["PE alone gets"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where"],"associatedMergedColumns":["by clear improvements on tasks 4 , 5 , 15 and 18 , where word ordering is particularly important ."]},{"number":"3.3","isBolded":false,"associatedRows":["11 : basic coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"5.1","isBolded":false,"associatedRows":["9 : simple negation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"5.7","isBolded":false,"associatedRows":["4 : 2 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"12.8","isBolded":false,"associatedRows":["2 : 2 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"3.1","isBolded":false,"associatedRows":["9 : simple negation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"8.3","isBolded":false,"associatedRows":["2 : 2 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"10.1","isBolded":false,"associatedRows":["8 : lists / sets"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"30.0","isBolded":false,"associatedRows":["5 : 3 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.4","isBolded":false,"associatedRows":["12 : conjunction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"76.9","isBolded":false,"associatedRows":["3 : 3 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"6.6","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN","11"],"associatedMergedColumns":["On 10k training data"]},{"number":"0.4","isBolded":false,"associatedRows":["13 : compound coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"9.6","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"77.0","isBolded":false,"associatedRows":["16 : basic induction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"45.4","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"2.7","isBolded":false,"associatedRows":["16 : basic induction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"51.0","isBolded":false,"associatedRows":["6 : yes / no questions"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"21.6","isBolded":false,"associatedRows":["2 : 2 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"23.3","isBolded":false,"associatedRows":["9 : simple negation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"13.6","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"12.6","isBolded":false,"associatedRows":["8 : lists / sets"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"6.5","isBolded":false,"associatedRows":["10 : indefinite knowledge"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["1 : 1 supporting fact"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.1","isBolded":false,"associatedRows":["12 : conjunction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"17.0","isBolded":false,"associatedRows":["9 : simple negation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"47.4","isBolded":false,"associatedRows":["16 : basic induction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"52.0","isBolded":false,"associatedRows":["16 : basic induction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"44.4","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"9.2","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"12.9","isBolded":false,"associatedRows":["5 : 3 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"13.2","isBolded":false,"associatedRows":["8 : lists / sets"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"18.6","isBolded":false,"associatedRows":["10 : indefinite knowledge"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"9.4","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE","13"],"associatedMergedColumns":["On 10k training data"]},{"number":"89.7","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"35.0","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.1","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"4.1givesa","isBolded":false,"associatedRows":["â€¢","Jittering the time index with random empty memories ( RN ) as described in Section"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where"],"associatedMergedColumns":["by clear improvements on tasks 4 , 5 , 15 and 18 , where word ordering is particularly important ."]},{"number":"11.4","isBolded":false,"associatedRows":["2 : 2 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"17.3","isBolded":false,"associatedRows":["7 : counting"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["9 : simple negation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"6.6","isBolded":false,"associatedRows":["10 : indefinite knowledge"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["12 : conjunction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"6.3","isBolded":false,"associatedRows":["13 : compound coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.3","isBolded":false,"associatedRows":["12 : conjunction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"11.0","isBolded":false,"associatedRows":["5 : 3 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"87.4","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"15.7","isBolded":false,"associatedRows":["5 : 3 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"12.6%forMemN2Nwithpositionencoding+linearstart+randomnoise,jointly","isBolded":false,"associatedRows":["MemNN vs"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"12.7","isBolded":false,"associatedRows":["8 : lists / sets"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"14.4","isBolded":false,"associatedRows":["5 : 3 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"25.4","isBolded":false,"associatedRows":["7 : counting"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"3.5","isBolded":false,"associatedRows":["16 : basic induction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.2","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"49.0","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"38.0","isBolded":false,"associatedRows":["11 : basic coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"10.1","isBolded":false,"associatedRows":["12 : conjunction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"85.6","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"41.2","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"2.0","isBolded":false,"associatedRows":["6 : yes / no questions"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"4.3","isBolded":false,"associatedRows":["11 : basic coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"19.7","isBolded":false,"associatedRows":["13 : compound coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"42.6","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"51.0","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"4.2%forMemN2Nwithpositionencoding+linearstart+","isBolded":false,"associatedRows":["trained and 10k :"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"80.0","isBolded":false,"associatedRows":["3 : 3 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"56.0","isBolded":false,"associatedRows":["10 : indefinite knowledge"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.8","isBolded":false,"associatedRows":["1 : 1 supporting fact"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"17.9","isBolded":false,"associatedRows":["7 : counting"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"36.0","isBolded":false,"associatedRows":["9 : simple negation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"37.8","isBolded":false,"associatedRows":["8 : lists / sets"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.9","isBolded":false,"associatedRows":["11 : basic coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"23.5","isBolded":false,"associatedRows":["7 : counting"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"13.4","isBolded":false,"associatedRows":["5 : 3 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"8.7","isBolded":false,"associatedRows":["6 : yes / no questions"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"18.3","isBolded":false,"associatedRows":["14 : time reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"82.8","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"16.3","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"3.3","isBolded":false,"associatedRows":["6 : yes / no questions"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"7.2","isBolded":false,"associatedRows":["6 : yes / no questions"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"2.0","isBolded":false,"associatedRows":["14 : time reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"100.0","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"13.1","isBolded":false,"associatedRows":["5 : 3 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"6.0","isBolded":false,"associatedRows":["13 : compound coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"17.4","isBolded":false,"associatedRows":["10 : indefinite knowledge"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"3.2%forMemNNvs","isBolded":false,"associatedRows":["trained and 10k :"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"13.4","isBolded":false,"associatedRows":["4 : 2 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["11 : basic coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"10.0","isBolded":false,"associatedRows":["8 : lists / sets"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"51.3","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"17.5","isBolded":false,"associatedRows":["4 : 2 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["16 : basic induction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"10.1","isBolded":false,"associatedRows":["7 : counting"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"39.2","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH","18"],"associatedMergedColumns":["On 10k training data"]},{"number":"11.0","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint","10"],"associatedMergedColumns":["On 10k training data"]},{"number":"15.0","isBolded":false,"associatedRows":["7 : counting"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"3.2","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]","4"],"associatedMergedColumns":["On 10k training data"]},{"number":"18.3","isBolded":false,"associatedRows":["5 : 3 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"36.9","isBolded":false,"associatedRows":["14 : time reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"14.8","isBolded":false,"associatedRows":["5 : 3 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"64.8","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"1.5","isBolded":false,"associatedRows":["9 : simple negation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"48.0","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["6 : yes / no questions"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"15.6","isBolded":false,"associatedRows":["2 : 2 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"15.4","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW","15"],"associatedMergedColumns":["On 10k training data"]},{"number":"15.1","isBolded":false,"associatedRows":["10 : indefinite knowledge"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"79.0","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"10.1","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"21.6","isBolded":false,"associatedRows":["7 : counting"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.2","isBolded":false,"associatedRows":["13 : compound coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"88.0","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.2","isBolded":false,"associatedRows":["12 : conjunction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"8.4","isBolded":false,"associatedRows":["11 : basic coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["1 : 1 supporting fact"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"50.1","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.1","isBolded":false,"associatedRows":["1 : 1 supporting fact"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"2.8","isBolded":false,"associatedRows":["6 : yes / no questions"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"9.9","isBolded":false,"associatedRows":["13 : compound coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"89.9","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"8.1","isBolded":false,"associatedRows":["14 : time reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"18.8","isBolded":false,"associatedRows":["2 : 2 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"20.3","isBolded":false,"associatedRows":["7 : counting"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"90.6","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"9.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.5","isBolded":false,"associatedRows":["13 : compound coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"14.1","isBolded":false,"associatedRows":["5 : 3 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"2.2","isBolded":false,"associatedRows":["4 : 2 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"2.6","isBolded":false,"associatedRows":["10 : indefinite knowledge"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"9.4","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"32.0","isBolded":false,"associatedRows":["4 : 2 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"2.0","isBolded":false,"associatedRows":["14 : time reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"21.1","isBolded":false,"associatedRows":["9 : simple negation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"24.3","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"22.8","isBolded":false,"associatedRows":["10 : indefinite knowledge"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"33.1","isBolded":false,"associatedRows":["3 : 3 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"1.9","isBolded":false,"associatedRows":["9 : simple negation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"7.9","isBolded":false,"associatedRows":["6 : yes / no questions"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"55.0","isBolded":false,"associatedRows":["8 : lists / sets"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"90.2","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.1","isBolded":false,"associatedRows":["12 : conjunction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.6","isBolded":false,"associatedRows":["1 : 1 supporting fact"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"1.8","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"10.9","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint","11"],"associatedMergedColumns":["On 10k training data"]},{"number":"50.9","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"9.2","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"16.3","isBolded":false,"associatedRows":["5 : 3 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["13 : compound coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"49.0","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"76.4","isBolded":false,"associatedRows":["3 : 3 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.3","isBolded":false,"associatedRows":["11 : basic coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"40.3","isBolded":false,"associatedRows":["3 : 3 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"14.0","isBolded":false,"associatedRows":["2 : 2 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"7.2","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS","12"],"associatedMergedColumns":["On 10k training data"]},{"number":"13.3","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"42.8","isBolded":false,"associatedRows":["2 : 2 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"1.4","isBolded":false,"associatedRows":["13 : compound coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"7.5","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint","11"],"associatedMergedColumns":["On 10k training data"]},{"number":"0.0","isBolded":false,"associatedRows":["1 : 1 supporting fact"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"21.9","isBolded":false,"associatedRows":["3 : 3 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"10.5","isBolded":false,"associatedRows":["13 : compound coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"9.0","isBolded":false,"associatedRows":["8 : lists / sets"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"40.3","isBolded":false,"associatedRows":["4 : 2 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"4.1","isBolded":false,"associatedRows":["11 : basic coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"73.0","isBolded":false,"associatedRows":["14 : time reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"11.7","isBolded":false,"associatedRows":["8 : lists / sets"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"1.3","isBolded":false,"associatedRows":["16 : basic induction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"5.0","isBolded":false,"associatedRows":["10 : indefinite knowledge"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"15.2","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"8.7","isBolded":false,"associatedRows":["6 : yes / no questions"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"2.0","isBolded":false,"associatedRows":["5 : 3 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.1","isBolded":false,"associatedRows":["1 : 1 supporting fact"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"1.0","isBolded":false,"associatedRows":["14 : time reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"92.0","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"24.5","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint","17"],"associatedMergedColumns":["On 10k training data"]},{"number":"50.5","isBolded":false,"associatedRows":["16 : basic induction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"13.9","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["1 : 1 supporting fact"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"1.3","isBolded":false,"associatedRows":["14 : time reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"6.7","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["11 : basic coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"44.5","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"10.3","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"39.0","isBolded":false,"associatedRows":["4 : 2 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"8.2","isBolded":false,"associatedRows":["14 : time reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"62.0","isBolded":false,"associatedRows":["2 : 2 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"30.0","isBolded":false,"associatedRows":["11 : basic coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"15.6","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"2.0","isBolded":false,"associatedRows":["9 : simple negation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"25.1","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"36.1","isBolded":false,"associatedRows":["7 : counting"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"20.3","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.3","isBolded":false,"associatedRows":["12 : conjunction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"48.1","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"6.9","isBolded":false,"associatedRows":["14 : time reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["4 : 2 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"36.4","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]","20"],"associatedMergedColumns":["On 10k training data"]},{"number":"2.3","isBolded":false,"associatedRows":["6 : yes / no questions"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"7.6","isBolded":false,"associatedRows":["6 : yes / no questions"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.1","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.2","isBolded":false,"associatedRows":["1 : 1 supporting fact"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"3.8","isBolded":false,"associatedRows":["4 : 2 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"1.6","isBolded":false,"associatedRows":["16 : basic induction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","LS"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["3 : 3 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.0","isBolded":false,"associatedRows":["12 : conjunction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"51.3","isBolded":false,"associatedRows":["16 : basic induction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"5.0","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","Supervised","MemNN [ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"7.9","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","3 hops","PE LS","joint","11"],"associatedMergedColumns":["On 10k training data"]},{"number":"0.0","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"6.1","isBolded":false,"associatedRows":["8 : lists / sets"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"90.7","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"25.8","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"46.4","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","1 hop","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"35.9","isBolded":false,"associatedRows":["9 : simple negation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"71.0","isBolded":false,"associatedRows":["3 : 3 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]},{"number":"2.8","isBolded":false,"associatedRows":["4 : 2 argument relations"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"1.8","isBolded":false,"associatedRows":["14 : time reasoning"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"26.0","isBolded":false,"associatedRows":["12 : conjunction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"51.0","isBolded":false,"associatedRows":["16 : basic induction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","2 and in","MemN2N","PE LS","LW","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"1.2","isBolded":false,"associatedRows":["11 : basic coreference"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"52.1","isBolded":false,"associatedRows":["16 : basic induction"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","PE","PE"],"associatedMergedColumns":["Appendix B ."]},{"number":"51.3","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","LSTM","[ 22 ]"],"associatedMergedColumns":["Appendix B ."]},{"number":"18.3","isBolded":false,"associatedRows":["7 : counting"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS RN","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"0.1","isBolded":false,"associatedRows":["1 : 1 supporting fact"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","the hops performed ( via the values of eq .","Baseline","Strongly","MemNN","WSH"],"associatedMergedColumns":["Appendix B ."]},{"number":"13.2","isBolded":false,"associatedRows":["9 : simple negation"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","MemN2N","PE","LS","RN"],"associatedMergedColumns":["Appendix B ."]},{"number":"31.6","isBolded":false,"associatedRows":["3 : 3 supporting facts"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","We give examples of","( 1 ) ) over some illustrative examples in Fig .","MemN2N","2 hops","PE LS","joint"],"associatedMergedColumns":["Appendix B ."]},{"number":"11.4","isBolded":false,"associatedRows":["8 : lists / sets"],"associatedColumns":["4 , although the supervised models are still superior .","All variants of our proposed model comfortably beat the weakly supervised baseline methods .","The position encoding ( PE ) representation improves over bag - of - words ( BoW ) , as demonstrated","The linear start ( LS ) to training seems to help avoid local minima . See task 16 in Table 1 , where","Joint training on all tasks helps .","more computational hops give improved performance .","( 1 ) ) over some illustrative examples in Fig .","Baseline","PE","MemNN","BoW"],"associatedMergedColumns":["Appendix B ."]}]},{"caption":"Table 3: Test error rates (%) on the 20 bAbI QA tasks for models using 10k training examples. \nKey: BoW \u003d bag-of-words representation; PE \u003d position encoding representation; LS \u003d linear start \ntraining; RN \u003d random injection of time index noise; LW \u003d RNN-style layer-wise weight tying (if \nnot stated, adjacent weight tying is used); joint \u003d joint training on all tasks (as opposed to per-task \ntraining);  *  \u003d this is a larger model with non-linearity (embedding dimension is d \u003d 100 and ReLU \napplied to the internal state after each hop. This was inspired by [17] and crucial for getting better \nperformance on tasks 17 and 19). \n","rows":["17 : positional reasoning","15 : basic deduction","19 : path finding","20 : agent \u0027 s motivation","18 , 6","9","Mean error ( % )","18 : size reasoning"],"columns":[],"mergedAllColumns":[],"numberCells":[{"number":"41.1","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"50.9","isBolded":false,"associatedRows":["17 : positional reasoning","9"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"40.0","isBolded":false,"associatedRows":["17 : positional reasoning","18 , 6"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"75.4","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"66.7","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"51.1","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"10.1","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"49.2","isBolded":false,"associatedRows":["17 : positional reasoning","9"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"15.4","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"24.5","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"8.0","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"68.8","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"91.0","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.4","isBolded":false,"associatedRows":["17 : positional reasoning","9"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"78.7","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["17 : positional reasoning","9"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"5.3","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"9.2","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"66.6","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"2.1","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"41.3","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"48.6","isBolded":false,"associatedRows":["17 : positional reasoning","9"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"80.8","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"3.2","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"66.5","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"47.3","isBolded":false,"associatedRows":["17 : positional reasoning","9"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"2.3","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"8.6","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"7.9","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"39.2","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"4.2","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"4.1","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"7.2","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"39.7","isBolded":false,"associatedRows":["17 : positional reasoning","18 , 6"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.4","isBolded":false,"associatedRows":["17 : positional reasoning","9"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"8.4","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"42.6","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"7.4","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"36.4","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"73.3","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"10.9","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"6.8","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"90.3","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"45.8","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.2","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"41.8","isBolded":false,"associatedRows":["17 : positional reasoning","18 , 6"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"2.1","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"50.9","isBolded":false,"associatedRows":["17 : positional reasoning","9"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"40.7","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"47.4","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"9.4","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"40.3","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"6.6","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"8.6","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"6.7","isBolded":false,"associatedRows":["18 : size reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"100.0","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"40.0","isBolded":false,"associatedRows":["17 : positional reasoning","18 , 6"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"51.8","isBolded":false,"associatedRows":["17 : positional reasoning","9"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"46.4","isBolded":false,"associatedRows":["17 : positional reasoning","9"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"31.9","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["20 : agent \u0027 s motivation"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"11.0","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"7.5","isBolded":false,"associatedRows":["Mean error ( % )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"12.5","isBolded":false,"associatedRows":["15 : basic deduction"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"89.5","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"41.7","isBolded":false,"associatedRows":["17 : positional reasoning","18 , 6"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"24.6","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.2","isBolded":false,"associatedRows":["17 : positional reasoning","9"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"50.1","isBolded":false,"associatedRows":["17 : positional reasoning"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"75.7","isBolded":false,"associatedRows":["19 : path finding"],"associatedColumns":[],"associatedMergedColumns":[]}]}]