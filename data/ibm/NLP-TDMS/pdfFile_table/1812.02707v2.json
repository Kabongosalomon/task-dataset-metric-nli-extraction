[{"caption":"Trunk Head \nQPr \nGT Boxes Params (M) GFlops Val mAP \n\nI3D \nI3D \n-\n16.2 \n6.5 \n21.3 \nI3D \nI3D \n-\n16.2 \n6.5 \n23.4 \n\nI3D \nTx LowRes \n13.9 \n33.2 \n17.8 \nI3D \nTx HighRes \n19.3 \n39.6 \n18.9 \nI3D \nTx LowRes \n13.9 \n33.2 \n29.1 \nI3D \nTx HighRes \n19.3 \n39.6 \n27.6 \n\nTable 1: Action classification with GT person boxes. To iso-\nlate classification from localization performance, we evaluate our \nmodels when assuming groundtruth box locations are known. It \ncan be seen that the Action Transformer head has far stronger per-\nformance than the I3D head when GT boxes are used. All perfor-\nmance reported with R \u003d 64 proposals. To put the complexity \nnumbers into perspective, a typical video recognition model, 16-\nframe R(2+1)D network on Kinetics, is 41 GFlops [43]. For a \nsense of random variation, we retrain the basic Tx model (line 5) \nthree times, and get a std deviation of 0.45 (on an mAP of 29.1). \n\n","rows":["three times , and get a std deviation of","Tx","HighRes","I3D","-","LowRes"],"columns":["GFlops","frame R ( 2+1 ) D network on Kinetics , is 41 GFlops [ 43 ] .","models when assuming groundtruth box locations are known .","To put the complexity","Params ( M )","Val mAP","Table 1 : Action classification with GT person boxes ."],"mergedAllColumns":["sense of random variation , we retrain the basic Tx model ( line 5 )"],"numberCells":[{"number":"6.5","isBolded":false,"associatedRows":["I3D","I3D","-"],"associatedColumns":["GFlops"],"associatedMergedColumns":[]},{"number":"29.1","isBolded":false,"associatedRows":["I3D","Tx","LowRes"],"associatedColumns":["Val mAP"],"associatedMergedColumns":[]},{"number":"6.5","isBolded":false,"associatedRows":["I3D","I3D","-"],"associatedColumns":["GFlops"],"associatedMergedColumns":[]},{"number":"13.9","isBolded":false,"associatedRows":["I3D","Tx","LowRes"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"33.2","isBolded":false,"associatedRows":["I3D","Tx","LowRes"],"associatedColumns":["GFlops"],"associatedMergedColumns":[]},{"number":"17.8","isBolded":false,"associatedRows":["I3D","Tx","LowRes"],"associatedColumns":["Val mAP"],"associatedMergedColumns":[]},{"number":"18.9","isBolded":false,"associatedRows":["I3D","Tx","HighRes"],"associatedColumns":["Val mAP"],"associatedMergedColumns":[]},{"number":"16.2","isBolded":false,"associatedRows":["I3D","I3D","-"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"39.6","isBolded":false,"associatedRows":["I3D","Tx","HighRes"],"associatedColumns":["GFlops"],"associatedMergedColumns":[]},{"number":"23.4","isBolded":false,"associatedRows":["I3D","I3D","-"],"associatedColumns":["Val mAP"],"associatedMergedColumns":[]},{"number":"0.45(onanmAPof29.1).","isBolded":false,"associatedRows":["three times , and get a std deviation of"],"associatedColumns":["Val mAP","Table 1 : Action classification with GT person boxes .","models when assuming groundtruth box locations are known .","To put the complexity","frame R ( 2+1 ) D network on Kinetics , is 41 GFlops [ 43 ] ."],"associatedMergedColumns":["sense of random variation , we retrain the basic Tx model ( line 5 )"]},{"number":"39.6","isBolded":false,"associatedRows":["I3D","Tx","HighRes"],"associatedColumns":["GFlops"],"associatedMergedColumns":[]},{"number":"19.3","isBolded":false,"associatedRows":["I3D","Tx","HighRes"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"27.6","isBolded":false,"associatedRows":["I3D","Tx","HighRes"],"associatedColumns":["Val mAP"],"associatedMergedColumns":[]},{"number":"21.3","isBolded":false,"associatedRows":["I3D","I3D","-"],"associatedColumns":["Val mAP"],"associatedMergedColumns":[]},{"number":"13.9","isBolded":false,"associatedRows":["I3D","Tx","LowRes"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"16.2","isBolded":false,"associatedRows":["I3D","I3D","-"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"33.2","isBolded":false,"associatedRows":["I3D","Tx","LowRes"],"associatedColumns":["GFlops"],"associatedMergedColumns":[]},{"number":"19.3","isBolded":false,"associatedRows":["I3D","Tx","HighRes"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]}]},{"caption":"Table 2: Localization performance (action agnostic). We \nperform classification-agnostic evaluation to evaluate the perfor-\nmance of the heads for person detection. We observe that the I3D \nhead is superior to Action Transformer-head model, though using \nthe HighRes query transformation (QPr) improves it significantly. \nAll performance reported with R \u003d 64 proposals. \n\n","rows":["RPN","300","Tx","HighRes","I3D","Tx+I3D HighRes","-","64","LowRes"],"columns":["IOU@0 . 75","QPr","IOU@0 . 5","#proposals Val mAP","Val mAP"],"mergedAllColumns":[],"numberCells":[{"number":"63.3","isBolded":false,"associatedRows":["I3D","-","RPN","64","HighRes","Tx"],"associatedColumns":["Val mAP","IOU@0 . 75"],"associatedMergedColumns":[]},{"number":"18.9","isBolded":false,"associatedRows":["Tx","HighRes","RPN","64","-"],"associatedColumns":["QPr","IOU@0 . 5","#proposals Val mAP"],"associatedMergedColumns":[]},{"number":"92.9","isBolded":false,"associatedRows":["I3D","-","RPN","64","-","I3D"],"associatedColumns":["Val mAP","IOU@0 . 5"],"associatedMergedColumns":[]},{"number":"24.9","isBolded":false,"associatedRows":["Tx+I3D HighRes","RPN","300","-"],"associatedColumns":["QPr","IOU@0 . 5","#proposals Val mAP"],"associatedMergedColumns":[]},{"number":"77.5","isBolded":false,"associatedRows":["I3D","-","RPN","64","-","I3D"],"associatedColumns":["Val mAP","IOU@0 . 75"],"associatedMergedColumns":[]},{"number":"77.5","isBolded":false,"associatedRows":["I3D","-","RPN","64","LowRes","Tx"],"associatedColumns":["Val mAP","IOU@0 . 5"],"associatedMergedColumns":[]},{"number":"43.5","isBolded":false,"associatedRows":["I3D","-","RPN","64","LowRes","Tx"],"associatedColumns":["Val mAP","IOU@0 . 75"],"associatedMergedColumns":[]},{"number":"87.7","isBolded":false,"associatedRows":["I3D","-","RPN","64","HighRes","Tx"],"associatedColumns":["Val mAP","IOU@0 . 5"],"associatedMergedColumns":[]},{"number":"24.4","isBolded":false,"associatedRows":["Tx","HighRes","RPN","300","-"],"associatedColumns":["QPr","IOU@0 . 5","#proposals Val mAP"],"associatedMergedColumns":[]},{"number":"21.3","isBolded":false,"associatedRows":["I3D","-","RPN","64","-"],"associatedColumns":["QPr","IOU@0 . 5","#proposals Val mAP"],"associatedMergedColumns":[]},{"number":"20.5","isBolded":false,"associatedRows":["I3D","-","RPN","300","-"],"associatedColumns":["QPr","IOU@0 . 5","#proposals Val mAP"],"associatedMergedColumns":[]}]},{"caption":"Table 4: Augmentation, pre-training and class-agnostic regres-\nsion. We evaluate the importance of certain design choices such \nas class agnostic box regression, data augmentation and Kinetics \npre-training, by reporting the performance when each of those is \nremoved from the model. We use the I3D head model as the base-\nline. Clearly, removing any leads to a significant drop in perfor-\nmance. All performance reported with R \u003d 64 proposals. \n\n","rows":["Val mAP"],"columns":["head","Cls - specific","No","Scratch","bbox - reg","Data Aug","I3D","From"],"mergedAllColumns":[],"numberCells":[{"number":"19.1","isBolded":false,"associatedRows":["Val mAP"],"associatedColumns":["From","Scratch"],"associatedMergedColumns":[]},{"number":"21.3","isBolded":false,"associatedRows":["Val mAP"],"associatedColumns":["I3D","head"],"associatedMergedColumns":[]},{"number":"19.2","isBolded":false,"associatedRows":["Val mAP"],"associatedColumns":["Cls - specific","bbox - reg"],"associatedMergedColumns":[]},{"number":"16.6","isBolded":false,"associatedRows":["Val mAP"],"associatedColumns":["No","Data Aug"],"associatedMergedColumns":[]}]},{"caption":"Table 5: Ablating the number of heads and layers. We find \nfewer heads and more layers tends to give slightly better vali-\ndation mAP. All performance reported with Action Transformer \nhead, when using GT boxes as proposals. \n\n","rows":["Tx","R3D + NL [ 47 ]","R3D [ 47 ]","I3D","-","LowRes"],"columns":["Params ( M )","Val mAP"],"mergedAllColumns":[],"numberCells":[{"number":"17.7","isBolded":false,"associatedRows":["R3D [ 47 ]","Tx","LowRes"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"23.4","isBolded":false,"associatedRows":["I3D","I3D","-"],"associatedColumns":["Val mAP"],"associatedMergedColumns":[]},{"number":"16.2","isBolded":false,"associatedRows":["I3D","I3D","-"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"26.6","isBolded":false,"associatedRows":["R3D [ 47 ]","Tx","LowRes"],"associatedColumns":["Val mAP"],"associatedMergedColumns":[]},{"number":"16.2","isBolded":false,"associatedRows":["I3D","I3D","-"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"25.1","isBolded":false,"associatedRows":["R3D + NL [ 47 ]","Tx","LowRes"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"21.3","isBolded":false,"associatedRows":["I3D","I3D","-"],"associatedColumns":["Val mAP"],"associatedMergedColumns":[]},{"number":"13.9","isBolded":false,"associatedRows":["I3D","Tx","LowRes"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"28.5","isBolded":false,"associatedRows":["I3D","Tx","LowRes"],"associatedColumns":["Val mAP"],"associatedMergedColumns":[]},{"number":"27.2","isBolded":false,"associatedRows":["R3D + NL [ 47 ]","Tx","LowRes"],"associatedColumns":["Val mAP"],"associatedMergedColumns":[]}]},{"caption":"Table 7: Comparison with previous state of the art and chal-\nlenge submissions. Our model outperforms the previous state of \nthe art by \u003e 7.5% on the validation set, and the CVPR\u002718 chal-\nlenge winner by \u003e 3.5% on the test set. We do so while only using \na single model (no ensembles), running on raw RGB frames as in-\nput. This is in contrast to the various previous methods listed here, \nwhich use various modalities and ensembles of multiple architec-\ntures. The model abbreviations used here refer to the following. R-\n50: ResNet-50 ","rows":["Ours ( Tx - only head )","Ours ( Tx+I3D head )","R - 50 , FRCNN","I3D , Tx","S3D - G , RN","ARCN [ 42 ]","RGB","-","AVA baseline [ 16 ]","Ours ( Tx+I3D+96f )","Tsinghua / Megvii [ 23 ]","Fudan University","I3D , FRCNN , R - 50","YH Technologies [ 52 ]","Single frame [ 16 ]","RGB , Flow","P3D , FRCNN"],"columns":["Test mAP","Val mAP"],"mergedAllColumns":["C2D , P3D , C3D , FPN","-"],"numberCells":[{"number":"21.08","isBolded":false,"associatedRows":["Tsinghua / Megvii [ 23 ]","RGB , Flow","P3D , FRCNN","-"],"associatedColumns":["Test mAP"],"associatedMergedColumns":["-"]},{"number":"24.30","isBolded":false,"associatedRows":["Ours ( Tx - only head )","RGB","I3D , Tx","-"],"associatedColumns":["Test mAP"],"associatedMergedColumns":["C2D , P3D , C3D , FPN"]},{"number":"24.4","isBolded":false,"associatedRows":["Ours ( Tx - only head )","RGB","I3D , Tx"],"associatedColumns":["Val mAP"],"associatedMergedColumns":["C2D , P3D , C3D , FPN"]},{"number":"14.7","isBolded":false,"associatedRows":["Single frame [ 16 ]","RGB , Flow","R - 50 , FRCNN"],"associatedColumns":["Val mAP"],"associatedMergedColumns":[]},{"number":"17.16","isBolded":false,"associatedRows":["Fudan University","-","-","-"],"associatedColumns":["Test mAP"],"associatedMergedColumns":["-"]},{"number":"25.0","isBolded":true,"associatedRows":["Ours ( Tx+I3D+96f )","RGB","I3D , Tx"],"associatedColumns":["Val mAP"],"associatedMergedColumns":["C2D , P3D , C3D , FPN"]},{"number":"24.9","isBolded":false,"associatedRows":["Ours ( Tx+I3D head )","RGB","I3D , Tx"],"associatedColumns":["Val mAP"],"associatedMergedColumns":["C2D , P3D , C3D , FPN"]},{"number":"17.4","isBolded":false,"associatedRows":["ARCN [ 42 ]","RGB , Flow","S3D - G , RN"],"associatedColumns":["Val mAP"],"associatedMergedColumns":["-"]},{"number":"19.60","isBolded":false,"associatedRows":["YH Technologies [ 52 ]","RGB , Flow","P3D , FRCNN","-"],"associatedColumns":["Test mAP"],"associatedMergedColumns":["-"]},{"number":"15.6","isBolded":false,"associatedRows":["AVA baseline [ 16 ]","RGB , Flow","I3D , FRCNN , R - 50"],"associatedColumns":["Val mAP"],"associatedMergedColumns":["-"]},{"number":"24.60","isBolded":false,"associatedRows":["Ours ( Tx+I3D head )","RGB","I3D , Tx","-"],"associatedColumns":["Test mAP"],"associatedMergedColumns":["C2D , P3D , C3D , FPN"]},{"number":"24.93","isBolded":true,"associatedRows":["Ours ( Tx+I3D+96f )","RGB","I3D , Tx","-"],"associatedColumns":["Test mAP"],"associatedMergedColumns":["C2D , P3D , C3D , FPN"]}]}]