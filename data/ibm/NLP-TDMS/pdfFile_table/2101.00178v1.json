[{"caption":"Model \n\nReader Type Reader Size NaturalQuestions TriviaQA \n\nREALM(Guu et al., 2020) \nExtractive \n110M \n40.4 \n-\nRAG(Lewis et al., 2020b) \nGenerative \n400M \n44.5 \n56.1 \n\nDPR(Karpukhin et al., 2020) \nExtractive \n110M \n41.5 \n57.9 \nT5-FID base (Izacard and Grave, 2020) \nGenerative \n220M \n48.2 \n65.0 \nT5-FID large (Izacard and Grave, 2020) Generative \n770M \n51.4 \n67.6 \n\nUnitedQA-E base (Ours) \nExtractive \n110M \n47.7 \n66.3 \nUnitedQA-E large (Ours) \nExtractive \n330M \n51.9 \n68.9 \nUnitedQA-G large (Ours) \nGenerative \n770M \n52.3 \n67.0 \n\nUnitedQA-E large ++ (Ours) \nEnsemble \n3x330M \n52.4 \n69.6 \nUnitedQA-G large ++ (Ours) \nEnsemble \n3x770M \n53.3 \n67.8 \nUnitedQA (Ours) \nHybrid \n-\n54.7 \n70.3 \n\nTable 1: Comparison to state-of-the-art models. Exact match score is used for evaluation. The overall best model \nis in Box , the best single model is in bold, and the best model with the smallest reader size is in underline. \n\n","rows":["400M","REALM ( Guu et al . , 2020 )","Generative","770M","large ( Ours )","UnitedQA - G","UnitedQA - E","Hybrid","large ( Izacard and Grave , 2020 )","Extractive","110M","220M","330M","RAG ( Lewis et al . , 2020b )","-","base ( Ours )","UnitedQA ( Ours )","Ensemble","3x330M","DPR ( Karpukhin et al . , 2020 )","large ++ ( Ours )","base ( Izacard and Grave , 2020 )","T5 - FID","3x770M"],"columns":["TriviaQA","NaturalQuestions"],"mergedAllColumns":["-"],"numberCells":[{"number":"53.3","isBolded":false,"associatedRows":["UnitedQA - G","large ++ ( Ours )","Ensemble","3x770M"],"associatedColumns":["NaturalQuestions"],"associatedMergedColumns":["-"]},{"number":"40.4","isBolded":false,"associatedRows":["REALM ( Guu et al . , 2020 )","base ( Izacard and Grave , 2020 )","Extractive","110M"],"associatedColumns":["NaturalQuestions"],"associatedMergedColumns":[]},{"number":"70.3","isBolded":true,"associatedRows":["UnitedQA ( Ours )","large ( Izacard and Grave , 2020 )","Hybrid","-"],"associatedColumns":["TriviaQA"],"associatedMergedColumns":["-"]},{"number":"51.4","isBolded":false,"associatedRows":["T5 - FID","large ( Izacard and Grave , 2020 )","Generative","770M"],"associatedColumns":["NaturalQuestions"],"associatedMergedColumns":["-"]},{"number":"41.5","isBolded":false,"associatedRows":["DPR ( Karpukhin et al . , 2020 )","large ( Izacard and Grave , 2020 )","Extractive","110M"],"associatedColumns":["NaturalQuestions"],"associatedMergedColumns":["-"]},{"number":"52.3","isBolded":true,"associatedRows":["UnitedQA - G","large ( Ours )","Generative","770M"],"associatedColumns":["NaturalQuestions"],"associatedMergedColumns":["-"]},{"number":"65.0","isBolded":false,"associatedRows":["T5 - FID","base ( Izacard and Grave , 2020 )","Generative","220M"],"associatedColumns":["TriviaQA"],"associatedMergedColumns":["-"]},{"number":"67.6","isBolded":false,"associatedRows":["T5 - FID","large ( Izacard and Grave , 2020 )","Generative","770M"],"associatedColumns":["TriviaQA"],"associatedMergedColumns":["-"]},{"number":"51.9","isBolded":false,"associatedRows":["UnitedQA - E","large ( Ours )","Extractive","330M"],"associatedColumns":["NaturalQuestions"],"associatedMergedColumns":["-"]},{"number":"67.0","isBolded":false,"associatedRows":["UnitedQA - G","large ( Ours )","Generative","770M"],"associatedColumns":["TriviaQA"],"associatedMergedColumns":["-"]},{"number":"68.9","isBolded":true,"associatedRows":["UnitedQA - E","large ( Ours )","Extractive","330M"],"associatedColumns":["TriviaQA"],"associatedMergedColumns":["-"]},{"number":"56.1","isBolded":false,"associatedRows":["RAG ( Lewis et al . , 2020b )","base ( Izacard and Grave , 2020 )","Generative","400M"],"associatedColumns":["TriviaQA"],"associatedMergedColumns":["-"]},{"number":"54.7","isBolded":true,"associatedRows":["UnitedQA ( Ours )","large ( Izacard and Grave , 2020 )","Hybrid","-"],"associatedColumns":["NaturalQuestions"],"associatedMergedColumns":["-"]},{"number":"48.2","isBolded":false,"associatedRows":["T5 - FID","base ( Izacard and Grave , 2020 )","Generative","220M"],"associatedColumns":["NaturalQuestions"],"associatedMergedColumns":["-"]},{"number":"47.7","isBolded":false,"associatedRows":["UnitedQA - E","base ( Ours )","Extractive","110M"],"associatedColumns":["NaturalQuestions"],"associatedMergedColumns":["-"]},{"number":"69.6","isBolded":false,"associatedRows":["UnitedQA - E","large ++ ( Ours )","Ensemble","3x330M"],"associatedColumns":["TriviaQA"],"associatedMergedColumns":["-"]},{"number":"67.8","isBolded":false,"associatedRows":["UnitedQA - G","large ++ ( Ours )","Ensemble","3x770M"],"associatedColumns":["TriviaQA"],"associatedMergedColumns":["-"]},{"number":"52.4","isBolded":false,"associatedRows":["UnitedQA - E","large ++ ( Ours )","Ensemble","3x330M"],"associatedColumns":["NaturalQuestions"],"associatedMergedColumns":["-"]},{"number":"44.5","isBolded":false,"associatedRows":["RAG ( Lewis et al . , 2020b )","base ( Izacard and Grave , 2020 )","Generative","400M"],"associatedColumns":["NaturalQuestions"],"associatedMergedColumns":["-"]},{"number":"66.3","isBolded":false,"associatedRows":["UnitedQA - E","base ( Ours )","Extractive","110M"],"associatedColumns":["TriviaQA"],"associatedMergedColumns":["-"]},{"number":"57.9","isBolded":false,"associatedRows":["DPR ( Karpukhin et al . , 2020 )","large ( Izacard and Grave , 2020 )","Extractive","110M"],"associatedColumns":["TriviaQA"],"associatedMergedColumns":["-"]}]},{"caption":"Table 2: Ablation experiments of the extractive model \non the development sets of NaturalQuestions (NQ) and \nTriviaQA. Exact match score is reported. \n\n","rows":["- PDR","ELECTRA","( Cheng et al . , 2020a ) +PDR","\u0026","- Multi - obj","BERT","base","PDR"],"columns":["NQ","TriviaQA"],"mergedAllColumns":[],"numberCells":[{"number":"60.1","isBolded":false,"associatedRows":["( Cheng et al . , 2020a ) +PDR"],"associatedColumns":["TriviaQA"],"associatedMergedColumns":[]},{"number":"58.5","isBolded":false,"associatedRows":["- Multi - obj","\u0026","PDR"],"associatedColumns":["TriviaQA"],"associatedMergedColumns":[]},{"number":"44.2","isBolded":false,"associatedRows":["BERT","base"],"associatedColumns":["NQ"],"associatedMergedColumns":[]},{"number":"43.3","isBolded":false,"associatedRows":["( Cheng et al . , 2020a ) +PDR"],"associatedColumns":["NQ"],"associatedMergedColumns":[]},{"number":"60.2","isBolded":false,"associatedRows":["- PDR"],"associatedColumns":["TriviaQA"],"associatedMergedColumns":[]},{"number":"40.6","isBolded":false,"associatedRows":["- Multi - obj","\u0026","PDR"],"associatedColumns":["NQ"],"associatedMergedColumns":[]},{"number":"62.2","isBolded":false,"associatedRows":["BERT","base"],"associatedColumns":["TriviaQA"],"associatedMergedColumns":[]},{"number":"41.8","isBolded":false,"associatedRows":["- PDR"],"associatedColumns":["NQ"],"associatedMergedColumns":[]},{"number":"46.0","isBolded":false,"associatedRows":["ELECTRA","base"],"associatedColumns":["NQ"],"associatedMergedColumns":[]},{"number":"65.4","isBolded":false,"associatedRows":["ELECTRA","base"],"associatedColumns":["TriviaQA"],"associatedMergedColumns":[]}]},{"caption":"Table 4: Retieval top-k accuracy and end-to-end QA \nextact match scores on the test sets of NaturalQuestions \n(NQ) and TriviaQA. United-E and United-G stand for \nour extractive and generative models respectively. \n\n","rows":["Retrieval","NQ","United - G","TriviaQA","United - E"],"columns":["Top - 20","Top - 100"],"mergedAllColumns":["+9%","+6%","+4%","+3%"],"numberCells":[{"number":"63.4","isBolded":false,"associatedRows":["TriviaQA","United - G"],"associatedColumns":["Top - 20"],"associatedMergedColumns":["+3%"]},{"number":"78.4","isBolded":false,"associatedRows":["NQ","Retrieval"],"associatedColumns":["Top - 20"],"associatedMergedColumns":[]},{"number":"51.9","isBolded":false,"associatedRows":["NQ","United - E"],"associatedColumns":["Top - 100"],"associatedMergedColumns":["+9%"]},{"number":"52.3","isBolded":false,"associatedRows":["TriviaQA","United - G"],"associatedColumns":["Top - 100"],"associatedMergedColumns":["+4%"]},{"number":"79.9","isBolded":false,"associatedRows":["TriviaQA","Retrieval"],"associatedColumns":["Top - 20"],"associatedMergedColumns":["+6%"]},{"number":"84.4","isBolded":false,"associatedRows":["TriviaQA","Retrieval"],"associatedColumns":["Top - 100"],"associatedMergedColumns":["+6%"]},{"number":"67.1","isBolded":false,"associatedRows":["TriviaQA","United - E"],"associatedColumns":["Top - 20"],"associatedMergedColumns":["+6%"]},{"number":"85.4","isBolded":false,"associatedRows":["TriviaQA","Retrieval"],"associatedColumns":["Top - 100"],"associatedMergedColumns":[]},{"number":"49.3","isBolded":false,"associatedRows":["TriviaQA","United - G"],"associatedColumns":["Top - 20"],"associatedMergedColumns":["+4%"]},{"number":"67.0","isBolded":false,"associatedRows":["TriviaQA","United - G"],"associatedColumns":["Top - 100"],"associatedMergedColumns":["+3%"]},{"number":"49.8","isBolded":false,"associatedRows":["NQ","United - E"],"associatedColumns":["Top - 20"],"associatedMergedColumns":["+9%"]},{"number":"68.9","isBolded":false,"associatedRows":["TriviaQA","United - E"],"associatedColumns":["Top - 100"],"associatedMergedColumns":["+6%"]}]},{"caption":"Table 5. In comparison \nto their corresponding overall performance, both \nthe extractive and generative models achieve much \nbetter performance on the \"Overlap\" categories \n(i.e. \"Question Overlap\" and \"Answer Overlap\") \nfor both NaturalQuestions and TrivaQA, which in-\ndicates that both models perform well for question \nand answer memorization. Different from question \nand answer memorization, there is a pronounced \nperformance drop for both models on the \"Answer \nOverlap Only\" category where certain amount of \nrelevance inference capability is required to suc-\nceed. Lastly, we see that both extractive and gener-\native models suffer some significant performance \ndegradation for the \"No Overlap\" column which \nhighlights model\u0027s generalization evaluation. Nev-\nertheless, the extractive model demonstrate a bet-\nter QA generalization by achieving a better per-\n\nformance on the \"No Overlap\" category on both \ndatasets. \nError Analysis. Here, we conduct analyses into \nprediction errors made by the extractive and gen-\nerative models based on automatic evaluation. For \nthis study, we use the dev set introduced by the Ef-\nficientQA competition 7 which is constructed in the \nsame way as the original NaturalQuestions dataset. \nSpecifically, we group prediction errors into three \ncategorizes: 1) common prediction errors made \nby both the extractive and generative models, 2) \nprediction errors made by the extractive model, 3) \nprediction errors produced by the generative model. \nIn the following, we first carry out a manual inspec-\ntion into the common errors. Then, we compare the \nprediction errors made by extractive and generative \nmodels, respectively. \nFirst of all, there is an error rate of 29% of those \nconsensus predictions made by both extractive and \ngenerative models according to the automatic eval-\nuation. Based on 30 randomly selected examples, \nwe find that around 30% of those predictions are \nactually valid answers as shown in the top part of \n","rows":["Retrieval","NQ","United - G","TriviaQA","United - E"],"columns":["Top - 20","Top - 100"],"mergedAllColumns":["+9%","+6%","+4%","+3%"],"numberCells":[{"number":"78.4","isBolded":false,"associatedRows":["NQ","Retrieval"],"associatedColumns":["Top - 20"],"associatedMergedColumns":[]},{"number":"51.9","isBolded":false,"associatedRows":["NQ","United - E"],"associatedColumns":["Top - 100"],"associatedMergedColumns":["+9%"]},{"number":"67.0","isBolded":false,"associatedRows":["TriviaQA","United - G"],"associatedColumns":["Top - 100"],"associatedMergedColumns":["+3%"]},{"number":"52.3","isBolded":false,"associatedRows":["TriviaQA","United - G"],"associatedColumns":["Top - 100"],"associatedMergedColumns":["+4%"]},{"number":"79.9","isBolded":false,"associatedRows":["TriviaQA","Retrieval"],"associatedColumns":["Top - 20"],"associatedMergedColumns":["+6%"]},{"number":"67.1","isBolded":false,"associatedRows":["TriviaQA","United - E"],"associatedColumns":["Top - 20"],"associatedMergedColumns":["+6%"]},{"number":"84.4","isBolded":false,"associatedRows":["TriviaQA","Retrieval"],"associatedColumns":["Top - 100"],"associatedMergedColumns":["+6%"]},{"number":"63.4","isBolded":false,"associatedRows":["TriviaQA","United - G"],"associatedColumns":["Top - 20"],"associatedMergedColumns":["+3%"]},{"number":"49.3","isBolded":false,"associatedRows":["TriviaQA","United - G"],"associatedColumns":["Top - 20"],"associatedMergedColumns":["+4%"]},{"number":"85.4","isBolded":false,"associatedRows":["TriviaQA","Retrieval"],"associatedColumns":["Top - 100"],"associatedMergedColumns":[]},{"number":"68.9","isBolded":false,"associatedRows":["TriviaQA","United - E"],"associatedColumns":["Top - 100"],"associatedMergedColumns":["+6%"]},{"number":"49.8","isBolded":false,"associatedRows":["NQ","United - E"],"associatedColumns":["Top - 20"],"associatedMergedColumns":["+9%"]}]},{"caption":"Table 5: Breakdown evaluation on NaturalQuestions and TriviaQA based on test splits defined in ","rows":["Accuracy","Relative","UnitedQA - G","UnitedQA - E"],"columns":["No","Answer","Generative","Dataset","Total","Only","Question","Overlap"],"mergedAllColumns":["TriviaQA","NaturalQuestions"],"numberCells":[{"number":"45.1","isBolded":false,"associatedRows":["Accuracy","UnitedQA - E"],"associatedColumns":["Answer","Answer","Overlap","Overlap","Only"],"associatedMergedColumns":[]},{"number":"68.9","isBolded":false,"associatedRows":["Accuracy","UnitedQA - E"],"associatedColumns":["No","Question","Total","Overlap","Overlap"],"associatedMergedColumns":["NaturalQuestions"]},{"number":"52.3","isBolded":false,"associatedRows":["Accuracy","UnitedQA - G"],"associatedColumns":["No","Question","Total","Overlap","Overlap"],"associatedMergedColumns":[]},{"number":"86.6","isBolded":false,"associatedRows":["Accuracy","UnitedQA - G"],"associatedColumns":["No","Question","Total","Overlap","Overlap"],"associatedMergedColumns":["NaturalQuestions"]},{"number":"0.10","isBolded":true,"associatedRows":["Relative"],"associatedColumns":["No","Question","Dataset","Overlap","Overlap","Generative"],"associatedMergedColumns":["TriviaQA"]},{"number":"70.6","isBolded":false,"associatedRows":["Accuracy","UnitedQA - E"],"associatedColumns":["Answer","Answer","Overlap","Overlap","Only"],"associatedMergedColumns":["NaturalQuestions"]},{"number":"0.05","isBolded":true,"associatedRows":["Relative"],"associatedColumns":["No","Question","Dataset","Overlap","Overlap","Generative"],"associatedMergedColumns":["TriviaQA"]},{"number":"78.6","isBolded":false,"associatedRows":["Accuracy","UnitedQA - E"],"associatedColumns":["Answer","Answer","Question","Overlap","Overlap"],"associatedMergedColumns":["NaturalQuestions"]},{"number":"62.7","isBolded":false,"associatedRows":["Accuracy","UnitedQA - E"],"associatedColumns":["No","Question","Question","Overlap","Overlap"],"associatedMergedColumns":["NaturalQuestions"]},{"number":"40.5","isBolded":false,"associatedRows":["Accuracy","UnitedQA - G"],"associatedColumns":["No","Question","Question","Overlap","Overlap"],"associatedMergedColumns":[]},{"number":"42.6","isBolded":false,"associatedRows":["Accuracy","UnitedQA - G"],"associatedColumns":["Answer","No","Overlap","Overlap","Only"],"associatedMergedColumns":["NaturalQuestions"]},{"number":"34.0","isBolded":false,"associatedRows":["Accuracy","UnitedQA - G"],"associatedColumns":["Answer","No","Overlap","Overlap","Only"],"associatedMergedColumns":[]},{"number":"44.3","isBolded":false,"associatedRows":["Accuracy","UnitedQA - E"],"associatedColumns":["Answer","No","Overlap","Overlap","Only"],"associatedMergedColumns":["NaturalQuestions"]},{"number":"69.4","isBolded":false,"associatedRows":["Accuracy","UnitedQA - E"],"associatedColumns":["No","Question","Total","Overlap","Overlap"],"associatedMergedColumns":[]},{"number":"37.6","isBolded":false,"associatedRows":["Accuracy","UnitedQA - E"],"associatedColumns":["Answer","No","Overlap","Overlap","Only"],"associatedMergedColumns":[]},{"number":"60.1","isBolded":false,"associatedRows":["Accuracy","UnitedQA - E"],"associatedColumns":["Answer","Answer","Question","Overlap","Overlap"],"associatedMergedColumns":[]},{"number":"62.7","isBolded":false,"associatedRows":["Accuracy","UnitedQA - G"],"associatedColumns":["Answer","Answer","Question","Overlap","Overlap"],"associatedMergedColumns":[]},{"number":"0.00","isBolded":true,"associatedRows":["Relative"],"associatedColumns":["No","Question","Dataset","Overlap","Overlap","Generative"],"associatedMergedColumns":["TriviaQA"]},{"number":"0.05","isBolded":true,"associatedRows":["Accuracy"],"associatedColumns":["No","Question","Dataset","Overlap","Overlap","Generative"],"associatedMergedColumns":["TriviaQA"]},{"number":"69.1","isBolded":false,"associatedRows":["Accuracy","UnitedQA - G"],"associatedColumns":["Answer","Answer","Overlap","Overlap","Only"],"associatedMergedColumns":["NaturalQuestions"]},{"number":"51.9","isBolded":false,"associatedRows":["Accuracy","UnitedQA - E"],"associatedColumns":["No","Question","Total","Overlap","Overlap"],"associatedMergedColumns":[]},{"number":"41.5","isBolded":false,"associatedRows":["Accuracy","UnitedQA - E"],"associatedColumns":["No","Question","Question","Overlap","Overlap"],"associatedMergedColumns":[]},{"number":"89.3","isBolded":false,"associatedRows":["Accuracy","UnitedQA - E"],"associatedColumns":["No","Question","Total","Overlap","Overlap"],"associatedMergedColumns":["NaturalQuestions"]},{"number":"72.2","isBolded":false,"associatedRows":["Accuracy","UnitedQA - G"],"associatedColumns":["No","Question","Total","Overlap","Overlap"],"associatedMergedColumns":[]},{"number":"67.0","isBolded":false,"associatedRows":["Accuracy","UnitedQA - G"],"associatedColumns":["No","Question","Total","Overlap","Overlap"],"associatedMergedColumns":["NaturalQuestions"]},{"number":"62.3","isBolded":false,"associatedRows":["Accuracy","UnitedQA - G"],"associatedColumns":["No","Question","Question","Overlap","Overlap"],"associatedMergedColumns":["NaturalQuestions"]},{"number":"76.5","isBolded":false,"associatedRows":["Accuracy","UnitedQA - G"],"associatedColumns":["Answer","Answer","Question","Overlap","Overlap"],"associatedMergedColumns":["NaturalQuestions"]},{"number":"45.4","isBolded":false,"associatedRows":["Accuracy","UnitedQA - G"],"associatedColumns":["Answer","Answer","Overlap","Overlap","Only"],"associatedMergedColumns":[]},{"number":"0.10","isBolded":true,"associatedRows":["Accuracy"],"associatedColumns":["No","Question","Dataset","Overlap","Overlap","Generative"],"associatedMergedColumns":["TriviaQA"]}]}]