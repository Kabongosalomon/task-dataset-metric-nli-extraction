[{"caption":"Table 1. Comparison to count-based exploration actor-critic agents \non hard exploration Atari games. A3C+ and Reactor+ correspond \nto A3C-CTS ","rows":["352","0","GRAVITAR","239","FROSTBITE","MONTEZUMA","PRIVATEEYE","VENTURE"],"columns":["99","33","34","N / A","273","28000","SIMHASH","A2C+SIL","100","200","33069","REACTOR+ †","75","32"],"mergedAllColumns":["482"],"numberCells":[{"number":"5214","isBolded":false,"associatedRows":["FROSTBITE","352"],"associatedColumns":["SIMHASH","75","33","N / A","N / A"],"associatedMergedColumns":["482"]},{"number":"8684","isBolded":true,"associatedRows":["PRIVATEEYE"],"associatedColumns":["A2C+SIL","273","34","33069"],"associatedMergedColumns":[]},{"number":"2722","isBolded":true,"associatedRows":["GRAVITAR"],"associatedColumns":["A2C+SIL","273","34","33069","99"],"associatedMergedColumns":[]},{"number":"1400","isBolded":true,"associatedRows":["VENTURE","0","0"],"associatedColumns":["REACTOR+ †","100","32","28000","200"],"associatedMergedColumns":["482"]},{"number":"6439","isBolded":true,"associatedRows":["FROSTBITE"],"associatedColumns":["A2C+SIL","273","34","33069","99"],"associatedMergedColumns":["482"]},{"number":"2500","isBolded":true,"associatedRows":["MONTEZUMA"],"associatedColumns":["A2C+SIL"],"associatedMergedColumns":[]},{"number":"1600","isBolded":false,"associatedRows":["GRAVITAR","239"],"associatedColumns":["REACTOR+ †","100","32","28000","200"],"associatedMergedColumns":[]},{"number":"4800","isBolded":false,"associatedRows":["FROSTBITE","352"],"associatedColumns":["REACTOR+ †","100","32","28000","200"],"associatedMergedColumns":["482"]}]},{"caption":"Table 2. Performance of agents on 49 Atari games after 50M steps \n(200M frames) of training. \u0027ACPER\u0027 represents A2C with prior-\nitized replay using ACER objective. \u0027Median\u0027 shows median of \nhuman-normalized scores. \u0027\u003eHuman\u0027 shows the number of games \nwhere the agent outperforms human experts. \n\n","rows":["A2C+SIL","A2C","ACPER"],"columns":["\u003eHUMAN","MEDIAN"],"mergedAllColumns":[],"numberCells":[{"number":"46.8%","isBolded":false,"associatedRows":["ACPER"],"associatedColumns":["MEDIAN"],"associatedMergedColumns":[]},{"number":"29","isBolded":true,"associatedRows":["A2C+SIL"],"associatedColumns":["\u003eHUMAN"],"associatedMergedColumns":[]},{"number":"18","isBolded":false,"associatedRows":["ACPER"],"associatedColumns":["\u003eHUMAN"],"associatedMergedColumns":[]},{"number":"96.1%","isBolded":false,"associatedRows":["A2C"],"associatedColumns":["MEDIAN"],"associatedMergedColumns":[]},{"number":"138.7%","isBolded":true,"associatedRows":["A2C+SIL"],"associatedColumns":["MEDIAN"],"associatedMergedColumns":[]},{"number":"23","isBolded":false,"associatedRows":["A2C"],"associatedColumns":["\u003eHUMAN"],"associatedMergedColumns":[]}]},{"caption":"Table 3. A2C+SIL hyperparameters on Atari games. \n\n","rows":["Entropy regularization ( α )","SIL update per iteration ( M )","Replay buffer size","Learning rate","Number of steps per iteration","SIL loss weight","SIL value loss weight ( β s il )","Exponent for prioritization","10","Bias correction for prioritized replay"],"columns":["16","512","Value","Conv ( 32 - 8x8 - 4 )"],"mergedAllColumns":["- FC ( 512 )"],"numberCells":[{"number":"0.0007","isBolded":false,"associatedRows":["Learning rate"],"associatedColumns":["Value","Conv ( 32 - 8x8 - 4 )"],"associatedMergedColumns":["- FC ( 512 )"]},{"number":"5","isBolded":false,"associatedRows":["Number of steps per iteration"],"associatedColumns":["Value","Conv ( 32 - 8x8 - 4 )","16"],"associatedMergedColumns":["- FC ( 512 )"]},{"number":"0.1forhardexplorationexperiment(Section","isBolded":false,"associatedRows":["Bias correction for prioritized replay"],"associatedColumns":["Value","Conv ( 32 - 8x8 - 4 )","16","512"],"associatedMergedColumns":["- FC ( 512 )"]},{"number":"5","isBolded":true,"associatedRows":["Replay buffer size","10"],"associatedColumns":["Value","Conv ( 32 - 8x8 - 4 )","16","512"],"associatedMergedColumns":["- FC ( 512 )"]},{"number":"5.4)","isBolded":false,"associatedRows":["Bias correction for prioritized replay"],"associatedColumns":["Value","Conv ( 32 - 8x8 - 4 )","16","512"],"associatedMergedColumns":["- FC ( 512 )"]},{"number":"1","isBolded":false,"associatedRows":["SIL loss weight"],"associatedColumns":["Value","Conv ( 32 - 8x8 - 4 )","16","512"],"associatedMergedColumns":["- FC ( 512 )"]},{"number":"0.01","isBolded":false,"associatedRows":["Entropy regularization ( α )"],"associatedColumns":["Value","Conv ( 32 - 8x8 - 4 )","16"],"associatedMergedColumns":["- FC ( 512 )"]},{"number":"0.01","isBolded":false,"associatedRows":["SIL value loss weight ( β s il )"],"associatedColumns":["Value","Conv ( 32 - 8x8 - 4 )","16","512"],"associatedMergedColumns":["- FC ( 512 )"]},{"number":"4","isBolded":false,"associatedRows":["SIL update per iteration ( M )"],"associatedColumns":["Value","Conv ( 32 - 8x8 - 4 )","16"],"associatedMergedColumns":["- FC ( 512 )"]},{"number":"0.6","isBolded":false,"associatedRows":["Exponent for prioritization"],"associatedColumns":["Value","Conv ( 32 - 8x8 - 4 )","16","512"],"associatedMergedColumns":["- FC ( 512 )"]},{"number":"5.3)","isBolded":false,"associatedRows":["Bias correction for prioritized replay"],"associatedColumns":["Value","Conv ( 32 - 8x8 - 4 )","16","512"],"associatedMergedColumns":["- FC ( 512 )"]},{"number":"0.4foroverallevaluation(Section","isBolded":false,"associatedRows":["Bias correction for prioritized replay"],"associatedColumns":["Value","Conv ( 32 - 8x8 - 4 )","16","512"],"associatedMergedColumns":["- FC ( 512 )"]}]},{"caption":"Hyperparameters \nValue \n\nArchitecture \nConv(32-8x8-4) \n-Conv(64-4x4-2) \n-Conv(64-3x3-1) \n-FC(512) \nLearning rate \n0.0007 \nNumber of environments \n16 \nNumber of steps per iteration \n5 \nEntropy regularization (α) \n0.01 \n\nSIL update per iteration (M ) \n4 \nSIL batch size \n512 \nSIL loss weight \n1 \nSIL value loss weight (β s il) \n0.01 \nReplay buffer size \n10 5 \nExponent for prioritization \n0.6 \nBias correction for prioritized replay 0.1 for hard exploration experiment (Section 5.3) \n0.4 for overall evaluation (Section 5.4) \n\nTable 4. PPO+SIL hyperparameters on MuJoCo. \n\n","rows":["GAE parameter ( λ )","Learning rate","SIL loss weight","Exponent for prioritization","Best chosen from {0 . 0003 ,","Best chosen from {0 . 01 ,","Discount factor ( γ )","SIL value loss weight ( β )","Best chosen from {0 . 6 ,","Bias correction for prioritized replay"],"columns":["0","50000","2048","512","Value","64","FC ( 64 ) - FC ( 64 )","10"],"mergedAllColumns":[],"numberCells":[{"number":"1.0}","isBolded":false,"associatedRows":["Exponent for prioritization","Best chosen from {0 . 6 ,"],"associatedColumns":["Value","FC ( 64 ) - FC ( 64 )","2048","10","64","0","10","512","50000"],"associatedMergedColumns":[]},{"number":"0.00003}","isBolded":false,"associatedRows":["Learning rate","Best chosen from {0 . 0003 ,"],"associatedColumns":["Value","FC ( 64 ) - FC ( 64 )"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["Bias correction for prioritized replay"],"associatedColumns":["Value","FC ( 64 ) - FC ( 64 )","2048","10","64","0","10","512","50000"],"associatedMergedColumns":[]},{"number":"0.95","isBolded":false,"associatedRows":["GAE parameter ( λ )"],"associatedColumns":["Value","FC ( 64 ) - FC ( 64 )","2048","10","64"],"associatedMergedColumns":[]},{"number":"0.05}","isBolded":false,"associatedRows":["SIL value loss weight ( β )","Best chosen from {0 . 01 ,"],"associatedColumns":["Value","FC ( 64 ) - FC ( 64 )","2048","10","64","0","10","512"],"associatedMergedColumns":[]},{"number":"0.0001,","isBolded":false,"associatedRows":["Learning rate","Best chosen from {0 . 0003 ,"],"associatedColumns":["Value","FC ( 64 ) - FC ( 64 )"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["SIL loss weight"],"associatedColumns":["Value","FC ( 64 ) - FC ( 64 )","2048","10","64","0","10","512"],"associatedMergedColumns":[]},{"number":"0.99","isBolded":false,"associatedRows":["Discount factor ( γ )"],"associatedColumns":["Value","FC ( 64 ) - FC ( 64 )","2048","10","64"],"associatedMergedColumns":[]},{"number":"0.00005,","isBolded":false,"associatedRows":["Learning rate","Best chosen from {0 . 0003 ,"],"associatedColumns":["Value","FC ( 64 ) - FC ( 64 )"],"associatedMergedColumns":[]}]}]