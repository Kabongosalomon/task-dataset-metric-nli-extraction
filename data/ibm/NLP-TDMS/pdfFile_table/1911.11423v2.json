[{"caption":"Model \nHeads Valid Test \nParams \n\nLarge RHN (Zilly et al., 2016) \n0 − \n1.27 \n46M \n3 layer AWD-LSTM (Merity et al., 2018b) \n0 − \n1.232 \n47M \nT12 (12 layer) (Al-Rfou et al., 2019) \n24 − \n1.11 \n44M \nLSTM (Melis et al., 2019) \n0 1.182 1.195 \n48M \nMogrifier LSTM (Melis et al., 2019) \n0 1.135 1.146 \n48M \n\n4 layer SHA-LSTM (h \u003d 1024, no attention head) \n0 1.312 1.330 \n51M \n4 layer SHA-LSTM (h \u003d 1024, single attention head) \n1 1.100 1.076 \n52M \n4 layer SHA-LSTM (h \u003d 1024, attention head per layer) \n4 1.096 1.068 \n54M \n\nT64 (64 layer) (Al-Rfou et al., 2019) \n128 − \n1.06 \n235M \nTransformer-XL (12 layer) (Dai et al., 2019)  160 − \n1.06 \n41M \nTransformer-XL (18 layer) (Dai et al., 2019)  160 − \n1.03 \n88M \nAdaptive Transformer (12 layer) (Sukhbaatar et al., 2019)  96 1.04 \n1.02 \n39M \nSparse Transformer (30 layer) (Child et al., 2019)  240 − \n0.99 \n95M \n\nTable 1. Bits Per Character (BPC) on enwik8. The single attention SHA-LSTM has an attention head on the second last layer and had \nbatch size 16 due to lower memory use. Directly comparing the head count for LSTM models and Transformer models obviously doesn\u0027t \nmake sense but neither does comparing zero-headed LSTMs against bajillion headed models and then declaring an entire species dead. \nThe hyper-parameters for the fully headed SHA-LSTM were used for the other SHA-LSTM experiments with zero tuning. \n\n","rows":["Transformer - XL ( 18 layer ) ( Dai et al . , 2019 )","24","LSTM ( Melis et al . , 2019 )","T64 ( 64 layer ) ( Al - Rfou et al . , 2019 )","4 layer SHA - LSTM ( h \u003d 1024 , single attention head )","3 layer AWD - LSTM ( Merity et al . , 2018b )","160","Adaptive Transformer ( 12 layer ) ( Sukhbaatar et al . , 2019 )","Mogrifier LSTM ( Melis et al . , 2019 )","240","Large RHN ( Zilly et al . , 2016 )","0","4 layer SHA - LSTM ( h \u003d 1024 , attention head per layer )","Sparse Transformer ( 30 layer ) ( Child et al . , 2019 )","1","−","4","128","4 layer SHA - LSTM ( h \u003d 1024 , no attention head )","Transformer - XL ( 12 layer ) ( Dai et al . , 2019 )","T12 ( 12 layer ) ( Al - Rfou et al . , 2019 )","96"],"columns":["Test","Valid"],"mergedAllColumns":["52M","41M","51M","39M","48M","47M","46M","44M","88M","54M","235M"],"numberCells":[{"number":"1.146","isBolded":false,"associatedRows":["Mogrifier LSTM ( Melis et al . , 2019 )","0","−"],"associatedColumns":["Test"],"associatedMergedColumns":["48M"]},{"number":"1.096","isBolded":false,"associatedRows":["4 layer SHA - LSTM ( h \u003d 1024 , attention head per layer )","4"],"associatedColumns":["Valid"],"associatedMergedColumns":["52M"]},{"number":"1.27","isBolded":false,"associatedRows":["Large RHN ( Zilly et al . , 2016 )","0","−"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"1.182","isBolded":false,"associatedRows":["LSTM ( Melis et al . , 2019 )","0"],"associatedColumns":["Valid"],"associatedMergedColumns":["44M"]},{"number":"1.04","isBolded":false,"associatedRows":["Adaptive Transformer ( 12 layer ) ( Sukhbaatar et al . , 2019 )","96"],"associatedColumns":["Valid"],"associatedMergedColumns":["88M"]},{"number":"1.11","isBolded":false,"associatedRows":["T12 ( 12 layer ) ( Al - Rfou et al . , 2019 )","24","−"],"associatedColumns":["Test"],"associatedMergedColumns":["47M"]},{"number":"1.03","isBolded":false,"associatedRows":["Transformer - XL ( 18 layer ) ( Dai et al . , 2019 )","160","−"],"associatedColumns":["Test"],"associatedMergedColumns":["41M"]},{"number":"1.068","isBolded":false,"associatedRows":["4 layer SHA - LSTM ( h \u003d 1024 , attention head per layer )","4","−"],"associatedColumns":["Test"],"associatedMergedColumns":["52M"]},{"number":"1.06","isBolded":false,"associatedRows":["Transformer - XL ( 12 layer ) ( Dai et al . , 2019 )","160","−"],"associatedColumns":["Test"],"associatedMergedColumns":["235M"]},{"number":"0.99","isBolded":false,"associatedRows":["Sparse Transformer ( 30 layer ) ( Child et al . , 2019 )","240","−"],"associatedColumns":["Test"],"associatedMergedColumns":["39M"]},{"number":"1.02","isBolded":false,"associatedRows":["Adaptive Transformer ( 12 layer ) ( Sukhbaatar et al . , 2019 )","96","−"],"associatedColumns":["Test"],"associatedMergedColumns":["88M"]},{"number":"1.195","isBolded":false,"associatedRows":["LSTM ( Melis et al . , 2019 )","0","−"],"associatedColumns":["Test"],"associatedMergedColumns":["44M"]},{"number":"1.330","isBolded":false,"associatedRows":["4 layer SHA - LSTM ( h \u003d 1024 , no attention head )","0","−"],"associatedColumns":["Test"],"associatedMergedColumns":["48M"]},{"number":"1.100","isBolded":false,"associatedRows":["4 layer SHA - LSTM ( h \u003d 1024 , single attention head )","1"],"associatedColumns":["Valid"],"associatedMergedColumns":["51M"]},{"number":"1.312","isBolded":false,"associatedRows":["4 layer SHA - LSTM ( h \u003d 1024 , no attention head )","0"],"associatedColumns":["Valid"],"associatedMergedColumns":["48M"]},{"number":"1.232","isBolded":false,"associatedRows":["3 layer AWD - LSTM ( Merity et al . , 2018b )","0","−"],"associatedColumns":["Test"],"associatedMergedColumns":["46M"]},{"number":"1.135","isBolded":false,"associatedRows":["Mogrifier LSTM ( Melis et al . , 2019 )","0"],"associatedColumns":["Valid"],"associatedMergedColumns":["48M"]},{"number":"1.076","isBolded":false,"associatedRows":["4 layer SHA - LSTM ( h \u003d 1024 , single attention head )","1","−"],"associatedColumns":["Test"],"associatedMergedColumns":["51M"]},{"number":"1.06","isBolded":false,"associatedRows":["T64 ( 64 layer ) ( Al - Rfou et al . , 2019 )","128","−"],"associatedColumns":["Test"],"associatedMergedColumns":["54M"]}]},{"caption":"10 Discussed in a tweet: \nhttps://twitter.com/ \nSmerity/status/1192252146126688256 \n\nRNN Cell \n\nLSTM \n\nLayers \n4 \nAll hidden sizes \n1024 \nInput embedding size \n1024 \nBoom hidden size \n4096 \nDropout (e/h/i/o) \n0/0.1/0.1/0.1 \n\nOptimizer \nMin Trust LAMB \nWeight decay \n0 \nBPTT length \n1024 \nMemory window size \n5000 \nBatch size \n8 \nLearning rate \n0.002 \n\nTable 2. Hyper-parameters for the language modeling experiment \nover the enwik8 dataset. Dropout refers to embedding, (RNN) \nhidden, input, and output. Notice the lack of specifics in the hyper-\nparameters due to not using bonfires of compute. \n\n","rows":["12","14","16","Learning rate","18","Weight decay","character","Batch size","Bits","Layers","validation","on","10"],"columns":["Min Trust LAMB","Boom hidden size","4096","LSTM","5000","Optimizer","( bpc )","Input embedding size","All hidden sizes","BPTT length","Dropout ( e / h / i / o )","RNN Cell","1024","per","0 / 0 . 1 / 0 . 1 / 0 . 1"],"mergedAllColumns":[],"numberCells":[{"number":"6","isBolded":false,"associatedRows":["Bits"],"associatedColumns":["RNN Cell","All hidden sizes","Input embedding size","Boom hidden size","( bpc )","Optimizer","BPTT length","per"],"associatedMergedColumns":[]},{"number":"4","isBolded":true,"associatedRows":["validation","Layers"],"associatedColumns":["LSTM"],"associatedMergedColumns":[]},{"number":"0.002","isBolded":true,"associatedRows":["Bits","10","12","14","16","18","Learning rate"],"associatedColumns":["LSTM","1024","1024","4096","0 / 0 . 1 / 0 . 1 / 0 . 1","Min Trust LAMB","1024","5000"],"associatedMergedColumns":[]},{"number":"1.15","isBolded":false,"associatedRows":["character"],"associatedColumns":["RNN Cell","All hidden sizes","Input embedding size","Boom hidden size","( bpc )","Optimizer"],"associatedMergedColumns":[]},{"number":"1.20","isBolded":false,"associatedRows":["character"],"associatedColumns":["RNN Cell","All hidden sizes","Input embedding size","Boom hidden size","Dropout ( e / h / i / o )"],"associatedMergedColumns":[]},{"number":"8","isBolded":true,"associatedRows":["Bits","Batch size"],"associatedColumns":["LSTM","1024","1024","4096","0 / 0 . 1 / 0 . 1 / 0 . 1","Min Trust LAMB","1024","5000"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Bits"],"associatedColumns":["RNN Cell","All hidden sizes","Input embedding size","Boom hidden size","( bpc )","Optimizer","BPTT length","per"],"associatedMergedColumns":[]},{"number":"1.10","isBolded":false,"associatedRows":["Bits"],"associatedColumns":["RNN Cell","All hidden sizes","Input embedding size","Boom hidden size","( bpc )","Optimizer","BPTT length","per"],"associatedMergedColumns":[]},{"number":"0","isBolded":true,"associatedRows":["character","Weight decay"],"associatedColumns":["LSTM","1024","1024","4096","0 / 0 . 1 / 0 . 1 / 0 . 1","Min Trust LAMB"],"associatedMergedColumns":[]},{"number":"0","isBolded":false,"associatedRows":["Bits"],"associatedColumns":["RNN Cell","All hidden sizes","Input embedding size","Boom hidden size","( bpc )","Optimizer","BPTT length","per"],"associatedMergedColumns":[]},{"number":"1.25","isBolded":false,"associatedRows":["on"],"associatedColumns":["RNN Cell","All hidden sizes","Input embedding size"],"associatedMergedColumns":[]},{"number":"1.30","isBolded":false,"associatedRows":["validation"],"associatedColumns":["RNN Cell"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["Bits"],"associatedColumns":["RNN Cell","All hidden sizes","Input embedding size","Boom hidden size","( bpc )","Optimizer","BPTT length","per"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["Bits"],"associatedColumns":["RNN Cell","All hidden sizes","Input embedding size","Boom hidden size","( bpc )","Optimizer","BPTT length","per"],"associatedMergedColumns":[]}]}]