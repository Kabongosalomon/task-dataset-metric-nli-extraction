[{"caption":"Table 1. Linear classification accuracy, and comparison to other \nself-supervised methods. In all cases the feature extractor is opti-\nmized in an unsupervised manner, using one of the methods listed \nbelow. A linear classifier is then trained on top using all labels in \nthe ImageNet dataset, and evaluated using a single crop. Prior art \nreported from [1] Wu et al. (2018), [2] Zhuang et al. (2019), [3] He \net al. (2019), [4] Misra \u0026 van der Maaten (2019), [5] Doersch \u0026 \nZisserman (2017), [6] Kolesnikov et al. (2019), [7] van den Oord \net al. (2018), [8] Donahue \u0026 Simonyan (2019), [9] Bachman et al. \n(2019), [10] Tian et al. (2019). \n\n","rows":["24","PIRL [ 4 ]","MOCO [ 3 ]","MOCO [ 2 ]","28","LOCAL AGGR . [ 2 ]","CPC V1 [ 7 ]","BIGBIGAN [ 8 ]","INSTANCE DISCR . [ 1 ]","-","ROTATION [ 6 ]","CPC V2 - RESNET - 50","CMC [ 10 ]","188","375","MULTI - TASK [ 5 ]","AMDIM [ 9 ]","305","626","86","CPC V2 - RESNET - 161"],"columns":["TOP - 1","TOP - 5"],"mergedAllColumns":["Methods using ResNet - 50 :","Methods using different architectures :","-"],"numberCells":[{"number":"85.3","isBolded":true,"associatedRows":["CPC V2 - RESNET - 50","24"],"associatedColumns":["TOP - 5"],"associatedMergedColumns":["-"]},{"number":"61.3","isBolded":false,"associatedRows":["BIGBIGAN [ 8 ]","86"],"associatedColumns":["TOP - 1"],"associatedMergedColumns":["-"]},{"number":"68.4","isBolded":false,"associatedRows":["CMC [ 10 ]","188"],"associatedColumns":["TOP - 1"],"associatedMergedColumns":["-"]},{"number":"55.4","isBolded":false,"associatedRows":["ROTATION [ 6 ]","86"],"associatedColumns":["TOP - 1"],"associatedMergedColumns":["Methods using different architectures :"]},{"number":"54.0","isBolded":false,"associatedRows":["INSTANCE DISCR . [ 1 ]","24"],"associatedColumns":["TOP - 1"],"associatedMergedColumns":["Methods using ResNet - 50 :"]},{"number":"68.1","isBolded":false,"associatedRows":["AMDIM [ 9 ]","626"],"associatedColumns":["TOP - 1"],"associatedMergedColumns":["-"]},{"number":"60.6","isBolded":false,"associatedRows":["MOCO [ 3 ]","24"],"associatedColumns":["TOP - 1"],"associatedMergedColumns":["-"]},{"number":"88.2","isBolded":false,"associatedRows":["CMC [ 10 ]","188"],"associatedColumns":["TOP - 5"],"associatedMergedColumns":["-"]},{"number":"90.1","isBolded":true,"associatedRows":["CPC V2 - RESNET - 161","305"],"associatedColumns":["TOP - 5"],"associatedMergedColumns":["-"]},{"number":"81.9","isBolded":false,"associatedRows":["BIGBIGAN [ 8 ]","86"],"associatedColumns":["TOP - 5"],"associatedMergedColumns":["-"]},{"number":"73.6","isBolded":false,"associatedRows":["CPC V1 [ 7 ]","28"],"associatedColumns":["TOP - 5"],"associatedMergedColumns":["-"]},{"number":"71.5","isBolded":true,"associatedRows":["CPC V2 - RESNET - 161","305"],"associatedColumns":["TOP - 1"],"associatedMergedColumns":["-"]},{"number":"69.3","isBolded":false,"associatedRows":["MULTI - TASK [ 5 ]","28","-"],"associatedColumns":["TOP - 5"],"associatedMergedColumns":["Methods using different architectures :"]},{"number":"58.8","isBolded":false,"associatedRows":["LOCAL AGGR . [ 2 ]","24"],"associatedColumns":["TOP - 1"],"associatedMergedColumns":["-"]},{"number":"68.6","isBolded":false,"associatedRows":["MOCO [ 2 ]","375"],"associatedColumns":["TOP - 1"],"associatedMergedColumns":["-"]},{"number":"63.8","isBolded":true,"associatedRows":["CPC V2 - RESNET - 50","24"],"associatedColumns":["TOP - 1"],"associatedMergedColumns":["-"]},{"number":"48.7","isBolded":false,"associatedRows":["CPC V1 [ 7 ]","28"],"associatedColumns":["TOP - 1"],"associatedMergedColumns":["-"]},{"number":"63.6","isBolded":false,"associatedRows":["PIRL [ 4 ]","24"],"associatedColumns":["TOP - 1"],"associatedMergedColumns":["-"]}]},{"caption":"Table 2. Data-efficient image classification. We compare the accuracy of two ResNet classifiers, one trained on the raw image pixels, \nthe other on the proposed CPC v2 features, for varying amounts of labeled data. Note that we also fine-tune the CPC features for the \nsupervised task, given the limited amount of labeled data. Regardless, the ResNet trained on CPC features systematically surpasses the \none trained on pixels, even when given 2-5× less labels to learn from. The red (respectively, blue) boxes highlight comparisons between \nthe two classifiers, trained with different amounts of data, which illustrate a 5× (resp. 2×) gain in data-efficiency in the low-data (resp. \nhigh-data) regime. \n\n","rows":["5×","2×","RESNET - 200 TRAINED ON PIXELS","RESNET - 33 TRAINED ON CPC FEATURES","GAIN IN DATA - EFFICIENCY"],"columns":["5%","2%","100%","1%","10%","20%","50%"],"mergedAllColumns":["TOP - 1 ACCURACY","TOP - 5 ACCURACY"],"numberCells":[{"number":"96.5","isBolded":false,"associatedRows":["RESNET - 33 TRAINED ON CPC FEATURES","5×","2×"],"associatedColumns":["100%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"75.2","isBolded":true,"associatedRows":["RESNET - 200 TRAINED ON PIXELS","5×"],"associatedColumns":["5%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"81.2","isBolded":true,"associatedRows":["RESNET - 33 TRAINED ON CPC FEATURES","5×","2×"],"associatedColumns":["50%"],"associatedMergedColumns":["TOP - 1 ACCURACY"]},{"number":"75.9","isBolded":false,"associatedRows":["RESNET - 200 TRAINED ON PIXELS","5×","2×"],"associatedColumns":["50%"],"associatedMergedColumns":["TOP - 1 ACCURACY"]},{"number":"80.2","isBolded":true,"associatedRows":["RESNET - 200 TRAINED ON PIXELS","5×","2×"],"associatedColumns":["100%"],"associatedMergedColumns":["TOP - 1 ACCURACY"]},{"number":"60.4","isBolded":false,"associatedRows":["RESNET - 33 TRAINED ON CPC FEATURES","5×"],"associatedColumns":["2%"],"associatedMergedColumns":["TOP - 1 ACCURACY"]},{"number":"2.5×","isBolded":false,"associatedRows":["GAIN IN DATA - EFFICIENCY","5×","2×","2×"],"associatedColumns":["20%"],"associatedMergedColumns":["TOP - 1 ACCURACY"]},{"number":"95.6","isBolded":true,"associatedRows":["RESNET - 33 TRAINED ON CPC FEATURES","5×","2×"],"associatedColumns":["50%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"34.8","isBolded":false,"associatedRows":["RESNET - 200 TRAINED ON PIXELS","5×"],"associatedColumns":["2%"],"associatedMergedColumns":["TOP - 1 ACCURACY"]},{"number":"2.5×","isBolded":false,"associatedRows":["GAIN IN DATA - EFFICIENCY","5×"],"associatedColumns":["2%"],"associatedMergedColumns":["TOP - 1 ACCURACY"]},{"number":"73.1","isBolded":false,"associatedRows":["RESNET - 33 TRAINED ON CPC FEATURES","5×","2×"],"associatedColumns":["10%"],"associatedMergedColumns":["TOP - 1 ACCURACY"]},{"number":"91.2","isBolded":false,"associatedRows":["RESNET - 33 TRAINED ON CPC FEATURES","5×","2×"],"associatedColumns":["10%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"78.3","isBolded":true,"associatedRows":["RESNET - 33 TRAINED ON CPC FEATURES"],"associatedColumns":["1%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"23.1","isBolded":false,"associatedRows":["RESNET - 200 TRAINED ON PIXELS"],"associatedColumns":["1%"],"associatedMergedColumns":["TOP - 1 ACCURACY"]},{"number":"70.3","isBolded":false,"associatedRows":["RESNET - 200 TRAINED ON PIXELS","5×","2×"],"associatedColumns":["20%"],"associatedMergedColumns":["TOP - 1 ACCURACY"]},{"number":"59.9","isBolded":false,"associatedRows":["RESNET - 200 TRAINED ON PIXELS","5×"],"associatedColumns":["2%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"52.7","isBolded":true,"associatedRows":["RESNET - 33 TRAINED ON CPC FEATURES"],"associatedColumns":["1%"],"associatedMergedColumns":["TOP - 1 ACCURACY"]},{"number":"68.1","isBolded":false,"associatedRows":["RESNET - 33 TRAINED ON CPC FEATURES","5×"],"associatedColumns":["5%"],"associatedMergedColumns":["TOP - 1 ACCURACY"]},{"number":"83.9","isBolded":false,"associatedRows":["RESNET - 200 TRAINED ON PIXELS","5×","2×"],"associatedColumns":["10%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"93.3","isBolded":false,"associatedRows":["RESNET - 33 TRAINED ON CPC FEATURES","5×","2×"],"associatedColumns":["20%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"95.2","isBolded":true,"associatedRows":["RESNET - 200 TRAINED ON PIXELS","5×","2×"],"associatedColumns":["100%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"93.1","isBolded":false,"associatedRows":["RESNET - 200 TRAINED ON PIXELS","5×","2×"],"associatedColumns":["50%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"83.9","isBolded":false,"associatedRows":["RESNET - 33 TRAINED ON CPC FEATURES","5×"],"associatedColumns":["2%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"62.5","isBolded":false,"associatedRows":["RESNET - 200 TRAINED ON PIXELS","5×","2×"],"associatedColumns":["10%"],"associatedMergedColumns":["TOP - 1 ACCURACY"]},{"number":"76.7","isBolded":false,"associatedRows":["RESNET - 33 TRAINED ON CPC FEATURES","5×","2×"],"associatedColumns":["20%"],"associatedMergedColumns":["TOP - 1 ACCURACY"]},{"number":"88.8","isBolded":false,"associatedRows":["RESNET - 33 TRAINED ON CPC FEATURES","5×"],"associatedColumns":["5%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"50.6","isBolded":true,"associatedRows":["RESNET - 200 TRAINED ON PIXELS","5×"],"associatedColumns":["5%"],"associatedMergedColumns":["TOP - 1 ACCURACY"]},{"number":"83.4","isBolded":false,"associatedRows":["RESNET - 33 TRAINED ON CPC FEATURES","5×","2×"],"associatedColumns":["100%"],"associatedMergedColumns":["TOP - 1 ACCURACY"]},{"number":"89.4","isBolded":false,"associatedRows":["RESNET - 200 TRAINED ON PIXELS","5×","2×"],"associatedColumns":["20%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"2.5×","isBolded":false,"associatedRows":["GAIN IN DATA - EFFICIENCY","5×","5×","2×"],"associatedColumns":["10%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"44.1","isBolded":false,"associatedRows":["RESNET - 200 TRAINED ON PIXELS"],"associatedColumns":["1%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]}]},{"caption":"Table 3. Comparison to other methods for semi-supervised learn-\ning. Representation learning methods use a classifier to discrimi-\nnate an unsupervised representation, and optimize it solely with \nrespect to labeled data. Label-propagation methods on the other \nhand further constrain the classifier with smoothness and entropy \ncriteria on unlabeled data, making the additional assumption that \nall training images fit into a single (unknown) testing category. \nWhen evaluating CPC v2, BigBiGAN, and AMDIM, we train a \nResNet-33 on top of the representation, while keeping the repre-\nsentation fixed or allowing it to be fine-tuned. All other results are \nreported from their respective papers: [1] Zhai et al. (2019), [2] \nXie et al. (2019), [3] Wu et al. (2018), [4] Misra \u0026 van der Maaten \n(2019). \n\n","rows":["PIRL [ 4 ]","VAT + ENTROPY MIN . [ 1 ]","ROTATION [ 1 ]","BIGBIGAN ( FIXED )","PSEUDOLABELING [ 1 ]","-","CPC V2 ( FIXED )","INSTANCE DISCR . [ 3 ]","AMDIM ( FIXED )","ROT . + VAT + ENT . MIN . [ 1 ]","top of BigBiGAN and AMDIM achieve","CPC V2 ( FINE - TUNED )","UNSUP . DATA AUG . [ 2 ]","SUPERVISED BASELINE","over fixed ones ( e . g ."],"columns":["100%","1%","LABELED DATA","10%"],"mergedAllColumns":["Methods using label - propagation :","since fine - tuned representations yield only marginal gains","Methods using representation learning only :","TOP - 5 ACCURACY","-","fixed . Given 1% of ImageNet labels , classifiers trained on"],"numberCells":[{"number":"77.4","isBolded":false,"associatedRows":["INSTANCE DISCR . [ 3 ]","-"],"associatedColumns":["10%"],"associatedMergedColumns":["Methods using representation learning only :"]},{"number":"55.2","isBolded":false,"associatedRows":["BIGBIGAN ( FIXED )"],"associatedColumns":["1%"],"associatedMergedColumns":["-"]},{"number":"96.2","isBolded":false,"associatedRows":["CPC V2 ( FIXED )","-"],"associatedColumns":["100%"],"associatedMergedColumns":["-"]},{"number":"78.8","isBolded":false,"associatedRows":["BIGBIGAN ( FIXED )","-"],"associatedColumns":["10%"],"associatedMergedColumns":["-"]},{"number":"92.2","isBolded":false,"associatedRows":["AMDIM ( FIXED )","-"],"associatedColumns":["100%"],"associatedMergedColumns":["-"]},{"number":"39.2","isBolded":false,"associatedRows":["INSTANCE DISCR . [ 3 ]"],"associatedColumns":["1%"],"associatedMergedColumns":["Methods using representation learning only :"]},{"number":"91.2","isBolded":true,"associatedRows":["CPC V2 ( FINE - TUNED )","-"],"associatedColumns":["10%"],"associatedMergedColumns":["-"]},{"number":"95.0","isBolded":false,"associatedRows":["ROT . + VAT + ENT . MIN . [ 1 ]","-"],"associatedColumns":["100%"],"associatedMergedColumns":["-"]},{"number":"77.1%vs","isBolded":false,"associatedRows":["over fixed ones ( e . g ."],"associatedColumns":["LABELED DATA"],"associatedMergedColumns":["since fine - tuned representations yield only marginal gains"]},{"number":"95.2","isBolded":false,"associatedRows":["SUPERVISED BASELINE","-"],"associatedColumns":["100%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"78.3","isBolded":true,"associatedRows":["CPC V2 ( FINE - TUNED )"],"associatedColumns":["1%"],"associatedMergedColumns":["-"]},{"number":"51.6","isBolded":false,"associatedRows":["PSEUDOLABELING [ 1 ]"],"associatedColumns":["1%"],"associatedMergedColumns":["Methods using label - propagation :"]},{"number":"57.5","isBolded":false,"associatedRows":["ROTATION [ 1 ]"],"associatedColumns":["1%"],"associatedMergedColumns":["-"]},{"number":"47.0","isBolded":false,"associatedRows":["VAT + ENTROPY MIN . [ 1 ]"],"associatedColumns":["1%"],"associatedMergedColumns":["-"]},{"number":"67.4","isBolded":false,"associatedRows":["AMDIM ( FIXED )"],"associatedColumns":["1%"],"associatedMergedColumns":["-"]},{"number":"90.5","isBolded":false,"associatedRows":["CPC V2 ( FIXED )","-"],"associatedColumns":["10%"],"associatedMergedColumns":["-"]},{"number":"55.2%and","isBolded":false,"associatedRows":["top of BigBiGAN and AMDIM achieve"],"associatedColumns":["10%"],"associatedMergedColumns":["fixed . Given 1% of ImageNet labels , classifiers trained on"]},{"number":"88.5","isBolded":false,"associatedRows":["UNSUP . DATA AUG . [ 2 ]","-"],"associatedColumns":["10%"],"associatedMergedColumns":["-"]},{"number":"78.3%Top-5accuracygiven","isBolded":false,"associatedRows":["over fixed ones ( e . g ."],"associatedColumns":["100%"],"associatedMergedColumns":["since fine - tuned representations yield only marginal gains"]},{"number":"87.0","isBolded":false,"associatedRows":["BIGBIGAN ( FIXED )","-"],"associatedColumns":["100%"],"associatedMergedColumns":["-"]},{"number":"83.8","isBolded":false,"associatedRows":["PIRL [ 4 ]","-"],"associatedColumns":["10%"],"associatedMergedColumns":["-"]},{"number":"44.1","isBolded":false,"associatedRows":["SUPERVISED BASELINE"],"associatedColumns":["1%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"96.5","isBolded":true,"associatedRows":["CPC V2 ( FINE - TUNED )","-"],"associatedColumns":["100%"],"associatedMergedColumns":["-"]},{"number":"57.2","isBolded":false,"associatedRows":["PIRL [ 4 ]"],"associatedColumns":["1%"],"associatedMergedColumns":["-"]},{"number":"91.2","isBolded":true,"associatedRows":["ROT . + VAT + ENT . MIN . [ 1 ]","-"],"associatedColumns":["10%"],"associatedMergedColumns":["-"]},{"number":"86.4","isBolded":false,"associatedRows":["ROTATION [ 1 ]","-"],"associatedColumns":["10%"],"associatedMergedColumns":["-"]},{"number":"85.8","isBolded":false,"associatedRows":["AMDIM ( FIXED )","-"],"associatedColumns":["10%"],"associatedMergedColumns":["-"]},{"number":"82.4","isBolded":false,"associatedRows":["PSEUDOLABELING [ 1 ]","-"],"associatedColumns":["10%"],"associatedMergedColumns":["Methods using label - propagation :"]},{"number":"67.4%","isBolded":false,"associatedRows":["top of BigBiGAN and AMDIM achieve"],"associatedColumns":["100%"],"associatedMergedColumns":["fixed . Given 1% of ImageNet labels , classifiers trained on"]},{"number":"83.9","isBolded":false,"associatedRows":["SUPERVISED BASELINE","-"],"associatedColumns":["10%"],"associatedMergedColumns":["TOP - 5 ACCURACY"]},{"number":"77.1","isBolded":false,"associatedRows":["CPC V2 ( FIXED )"],"associatedColumns":["1%"],"associatedMergedColumns":["-"]},{"number":"83.4","isBolded":false,"associatedRows":["VAT + ENTROPY MIN . [ 1 ]","-"],"associatedColumns":["10%"],"associatedMergedColumns":["-"]}]},{"caption":"Table 4. Comparison of PASCAL VOC 2007 object detection ac-\ncuracy to other transfer methods. The supervised baseline learns \nfrom the entire labeled ImageNet dataset and fine-tunes for PAS-\nCAL detection. The second class of methods learns from the same \nunlabeled images before transferring. The architecture column \nspecifies the object detector (Fast-RCNN or Faster-RCNN) and \nthe feature extractor (ResNet-50, -101, -152, or -161). All of these \nmethods pre-train on the ImageNet dataset, except for Deeper-\nCluster which learns from the larger, but uncurated, YFCC100M \ndataset (Thomee et al., 2015). All methods fine-tune on the PAS-\nCAL 2007 training set, and are evaluted in terms of mean average \nprecision (mAP). Prior art reported from [1] Dosovitskiy et al. \n(2014), [2] Doersch \u0026 Zisserman (2017), [3] Pathak et al. (2016), \n[4] Zhang et al. (2016), [5] Doersch et al. (2015), [6] Wu et al. \n(2018), [7] Caron et al. (2018), [8] Caron et al. (2019), [9] Zhuang \net al. (2019), [10] Misra \u0026 van der Maaten (2019) [11] He et al. \n(2019). \n\n","rows":["LOCAL AGGREGATION [ 9 ]","FASTER : R50","DEEP CLUSTER [ 7 ]","INSTANCE DISCR . [ 6 ]","FASTER : R152","EXEMPLAR [ 1 ] BY [ 2 ]","RELATIVE POS . [ 5 ] BY [ 2 ]","FASTER : R101","MOMENTUM CONTRAST [ 11 ]","MULTI - TASK [ 2 ]","PIRL [ 10 ]","MOTION SEGM . [ 3 ] BY [ 2 ]","FAST : VGG - 16","COLORIZATION [ 4 ] BY [ 2 ]","DEEPER CLUSTER [ 8 ]","SUPERVISED BASELINE","CPC V2","FASTER : R161"],"columns":["MAP"],"mergedAllColumns":["Transfer using labeled data :","Transfer using unlabeled data :"],"numberCells":[{"number":"65.4","isBolded":false,"associatedRows":["INSTANCE DISCR . [ 6 ]","FASTER : R50"],"associatedColumns":["MAP"],"associatedMergedColumns":["Transfer using unlabeled data :"]},{"number":"70.5","isBolded":false,"associatedRows":["MULTI - TASK [ 2 ]","FASTER : R101"],"associatedColumns":["MAP"],"associatedMergedColumns":["Transfer using unlabeled data :"]},{"number":"61.1","isBolded":false,"associatedRows":["MOTION SEGM . [ 3 ] BY [ 2 ]","FASTER : R101"],"associatedColumns":["MAP"],"associatedMergedColumns":["Transfer using unlabeled data :"]},{"number":"65.5","isBolded":false,"associatedRows":["COLORIZATION [ 4 ] BY [ 2 ]","FASTER : R101"],"associatedColumns":["MAP"],"associatedMergedColumns":["Transfer using unlabeled data :"]},{"number":"73.4","isBolded":false,"associatedRows":["PIRL [ 10 ]","FASTER : R50"],"associatedColumns":["MAP"],"associatedMergedColumns":["Transfer using unlabeled data :"]},{"number":"60.9","isBolded":false,"associatedRows":["EXEMPLAR [ 1 ] BY [ 2 ]","FASTER : R101"],"associatedColumns":["MAP"],"associatedMergedColumns":["Transfer using unlabeled data :"]},{"number":"76.6","isBolded":true,"associatedRows":["CPC V2","FASTER : R161"],"associatedColumns":["MAP"],"associatedMergedColumns":["Transfer using unlabeled data :"]},{"number":"65.9","isBolded":false,"associatedRows":["DEEP CLUSTER [ 7 ]","FAST : VGG - 16"],"associatedColumns":["MAP"],"associatedMergedColumns":["Transfer using unlabeled data :"]},{"number":"69.1","isBolded":false,"associatedRows":["LOCAL AGGREGATION [ 9 ]","FASTER : R50"],"associatedColumns":["MAP"],"associatedMergedColumns":["Transfer using unlabeled data :"]},{"number":"74.9","isBolded":false,"associatedRows":["MOMENTUM CONTRAST [ 11 ]","FASTER : R50"],"associatedColumns":["MAP"],"associatedMergedColumns":["Transfer using unlabeled data :"]},{"number":"74.7","isBolded":false,"associatedRows":["SUPERVISED BASELINE","FASTER : R152"],"associatedColumns":["MAP"],"associatedMergedColumns":["Transfer using labeled data :"]},{"number":"66.8","isBolded":false,"associatedRows":["RELATIVE POS . [ 5 ] BY [ 2 ]","FASTER : R101"],"associatedColumns":["MAP"],"associatedMergedColumns":["Transfer using unlabeled data :"]},{"number":"67.8","isBolded":false,"associatedRows":["DEEPER CLUSTER [ 8 ]","FAST : VGG - 16"],"associatedColumns":["MAP"],"associatedMergedColumns":["Transfer using unlabeled data :"]}]}]