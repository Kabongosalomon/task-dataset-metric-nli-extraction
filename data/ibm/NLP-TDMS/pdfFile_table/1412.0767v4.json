[{"caption":"Table 1. C3D compared to best published results. C3D outperforms all previous best reported methods on a range of benchmarks except \nfor Sports-1M and UCF101. On UCF101, we report accuracy for two groups of methods. The first set of methods use only RGB frame \ninputs while the second set of methods (in parentheses) use all possible features (e.g. optical flow, improved Dense Trajectory). \n\n","rows":["C3D","Result"],"columns":["[ 9 ]","object recognition","[ 39 ] ( [ 25 ] )","[ 32 ]","[ 31 ]","YUPENN","Sport1M","UCF101","[ 29 ]","UMD","action similarity labeling","action recognition","Object","scene classification","ASLAN"],"mergedAllColumns":[],"numberCells":[{"number":"85.2(90.4)","isBolded":true,"associatedRows":["C3D"],"associatedColumns":["UCF101","action recognition","[ 39 ] ( [ 25 ] )"],"associatedMergedColumns":[]},{"number":"12.0","isBolded":false,"associatedRows":["Result"],"associatedColumns":["Object","object recognition","[ 32 ]"],"associatedMergedColumns":[]},{"number":"96.2","isBolded":false,"associatedRows":["Result"],"associatedColumns":["YUPENN","scene classification","[ 9 ]"],"associatedMergedColumns":[]},{"number":"77.7","isBolded":false,"associatedRows":["Result"],"associatedColumns":["UMD","scene classification","[ 9 ]"],"associatedMergedColumns":[]},{"number":"98.1","isBolded":true,"associatedRows":["C3D"],"associatedColumns":["YUPENN","scene classification","[ 9 ]"],"associatedMergedColumns":[]},{"number":"85.2","isBolded":false,"associatedRows":["C3D"],"associatedColumns":["Sport1M","action recognition","[ 29 ]"],"associatedMergedColumns":[]},{"number":"75.8(89.1)","isBolded":false,"associatedRows":["Result"],"associatedColumns":["UCF101","action recognition","[ 39 ] ( [ 25 ] )"],"associatedMergedColumns":[]},{"number":"90.8","isBolded":true,"associatedRows":["Result"],"associatedColumns":["Sport1M","action recognition","[ 29 ]"],"associatedMergedColumns":[]},{"number":"22.3","isBolded":true,"associatedRows":["C3D"],"associatedColumns":["Object","object recognition","[ 32 ]"],"associatedMergedColumns":[]},{"number":"68.7","isBolded":false,"associatedRows":["Result"],"associatedColumns":["ASLAN","action similarity labeling","[ 31 ]"],"associatedMergedColumns":[]},{"number":"87.7","isBolded":true,"associatedRows":["C3D"],"associatedColumns":["UMD","scene classification","[ 9 ]"],"associatedMergedColumns":[]},{"number":"78.3","isBolded":true,"associatedRows":["C3D"],"associatedColumns":["ASLAN","action similarity labeling","[ 31 ]"],"associatedMergedColumns":[]}]},{"caption":"%) \nImagenet + linear SVM \n68.8 \niDT w/ BoW + linear SVM \n76.2 \nDeep networks [18]  65.4 \nSpatial stream network [36]  72.6 \nLRCN [6]  71.1 \nLSTM composite model [39]  75.8 \nC3D (1 net) + linear SVM \n82.3 \nC3D (3 nets) + linear SVM \n85.2 \niDT w/ Fisher vector [31]  87.9 \nTemporal stream network [36]  83.7 \nTwo-stream networks [36]  88.0 \nLRCN [6]  82.9 \nLSTM composite model [39]  84.3 \nConv. pooling on long clips [29]  88.2 \nLSTM on long clips [29]  88.6 \nMulti-skip feature stacking [25]  89.1 \nC3D (3 nets) + iDT + linear SVM \n90.4 \n\nTable 3. Action recognition results on UCF101. C3D compared \nwith baselines and current state-of-the-art methods. Top: sim-\nple features with linear SVM; Middle: methods taking only RGB \nframes as inputs; Bottom: methods using multiple feature combi-\nnations. \n\n","rows":["Spatial stream network [ 36 ]","Deep networks [ 18 ]","LRCN [ 6 ]","C3D ( 1 net ) + linear SVM","C3D ( 3 nets ) + iDT + linear SVM","C3D ( 3 nets ) + linear SVM","iDT w / Fisher vector [ 31 ]","Two - stream networks [ 36 ]","Imagenet + linear SVM","LSTM composite model [ 39 ]","Temporal stream network [ 36 ]","LSTM on long clips [ 29 ]","iDT w / BoW + linear SVM","Multi - skip feature stacking [ 25 ]","Conv . pooling on long clips [ 29 ]"],"columns":["% )"],"mergedAllColumns":[],"numberCells":[{"number":"87.9","isBolded":false,"associatedRows":["iDT w / Fisher vector [ 31 ]"],"associatedColumns":["% )"],"associatedMergedColumns":[]},{"number":"68.8","isBolded":false,"associatedRows":["Imagenet + linear SVM"],"associatedColumns":["% )"],"associatedMergedColumns":[]},{"number":"76.2","isBolded":false,"associatedRows":["iDT w / BoW + linear SVM"],"associatedColumns":["% )"],"associatedMergedColumns":[]},{"number":"84.3","isBolded":false,"associatedRows":["LSTM composite model [ 39 ]"],"associatedColumns":["% )"],"associatedMergedColumns":[]},{"number":"71.1","isBolded":false,"associatedRows":["LRCN [ 6 ]"],"associatedColumns":["% )"],"associatedMergedColumns":[]},{"number":"82.9","isBolded":false,"associatedRows":["LRCN [ 6 ]"],"associatedColumns":["% )"],"associatedMergedColumns":[]},{"number":"88.6","isBolded":false,"associatedRows":["LSTM on long clips [ 29 ]"],"associatedColumns":["% )"],"associatedMergedColumns":[]},{"number":"65.4","isBolded":false,"associatedRows":["Deep networks [ 18 ]"],"associatedColumns":["% )"],"associatedMergedColumns":[]},{"number":"72.6","isBolded":false,"associatedRows":["Spatial stream network [ 36 ]"],"associatedColumns":["% )"],"associatedMergedColumns":[]},{"number":"85.2","isBolded":true,"associatedRows":["C3D ( 3 nets ) + linear SVM"],"associatedColumns":["% )"],"associatedMergedColumns":[]},{"number":"75.8","isBolded":false,"associatedRows":["LSTM composite model [ 39 ]"],"associatedColumns":["% )"],"associatedMergedColumns":[]},{"number":"90.4","isBolded":true,"associatedRows":["C3D ( 3 nets ) + iDT + linear SVM"],"associatedColumns":["% )"],"associatedMergedColumns":[]},{"number":"82.3","isBolded":false,"associatedRows":["C3D ( 1 net ) + linear SVM"],"associatedColumns":["% )"],"associatedMergedColumns":[]},{"number":"89.1","isBolded":false,"associatedRows":["Multi - skip feature stacking [ 25 ]"],"associatedColumns":["% )"],"associatedMergedColumns":[]},{"number":"83.7","isBolded":false,"associatedRows":["Temporal stream network [ 36 ]"],"associatedColumns":["% )"],"associatedMergedColumns":[]},{"number":"88.0","isBolded":false,"associatedRows":["Two - stream networks [ 36 ]"],"associatedColumns":["% )"],"associatedMergedColumns":[]},{"number":"88.2","isBolded":false,"associatedRows":["Conv . pooling on long clips [ 29 ]"],"associatedColumns":["% )"],"associatedMergedColumns":[]}]},{"caption":"STIP \nlinear 60.9 65.3 \n[22]  STIP \nmetric 64.3 69.1 \n[20]  MIP \nmetric 65.5 71.9 \n[11]  MIP+STIP+MBH metric 66.1 73.2 \n[45]  iDT+FV \nmetric 68.7 75.4 \nBaseline \nImagenet \nlinear 67.5 73.8 \nOurs \nC3D \nlinear 78.3 86.5 \n\nTable 4. Action similarity labeling result on ASLAN. C3D sig-\nnificantly outperforms state-of-the-art method [45] by 9.6% in ac-\ncuracy and by 11.1% in area under ROC curve. \n\n","rows":["[ 11 ]","[ 22 ]","Ours","linear","[ 20 ]","[ 45 ]","Baseline","STIP","nificantly outperforms state - of - the - art method [ 45 ] by","iDT+FV","metric","MIP","MIP+STIP+MBH","C3D","Imagenet","curacy and by"],"columns":["Table 4 . Action similarity labeling result on ASLAN . C3D sig -"],"mergedAllColumns":[],"numberCells":[{"number":"9.6%inac-","isBolded":true,"associatedRows":["nificantly outperforms state - of - the - art method [ 45 ] by"],"associatedColumns":["Table 4 . Action similarity labeling result on ASLAN . C3D sig -"],"associatedMergedColumns":[]},{"number":"11.1%inareaunderROCcurve.","isBolded":true,"associatedRows":["curacy and by"],"associatedColumns":["Table 4 . Action similarity labeling result on ASLAN . C3D sig -"],"associatedMergedColumns":[]},{"number":"68.7","isBolded":false,"associatedRows":["[ 45 ]","iDT+FV","metric"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"86.5","isBolded":true,"associatedRows":["Ours","C3D","linear"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"73.2","isBolded":false,"associatedRows":["[ 11 ]","MIP+STIP+MBH","metric"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"67.5","isBolded":false,"associatedRows":["Baseline","Imagenet","linear"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"64.3","isBolded":false,"associatedRows":["[ 22 ]","STIP","metric"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"73.8","isBolded":false,"associatedRows":["Baseline","Imagenet","linear"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"60.9","isBolded":false,"associatedRows":["[ 22 ]","STIP","linear"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"69.1","isBolded":false,"associatedRows":["[ 22 ]","STIP","metric"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"78.3","isBolded":true,"associatedRows":["Ours","C3D","linear"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"65.3","isBolded":false,"associatedRows":["nificantly outperforms state - of - the - art method [ 45 ] by","STIP","linear"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"65.5","isBolded":false,"associatedRows":["[ 20 ]","MIP","metric"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"75.4","isBolded":false,"associatedRows":["[ 45 ]","iDT+FV","metric"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"71.9","isBolded":false,"associatedRows":["[ 20 ]","MIP","metric"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"66.1","isBolded":false,"associatedRows":["[ 11 ]","MIP+STIP+MBH","metric"],"associatedColumns":[],"associatedMergedColumns":[]}]},{"caption":"Dataset \n\n[4] \n[41] \n[8] \n[9] \nImagenet C3D \nMaryland \n43.1 74.6 67.7 77.7 \n87.7 \n87.7 \nYUPENN 80.7 85.0 86.0 96.2 \n96.7 \n98.1 \nTable 5. Scene recognition accuracy. C3D using a simple linear \nSVM outperforms current methods on Maryland and YUPENN. \n\n","rows":["YUPENN","Maryland"],"columns":["[ 9 ]","[ 8 ]","C3D","[ 41 ]","Imagenet","[ 4 ]"],"mergedAllColumns":[],"numberCells":[{"number":"87.7","isBolded":true,"associatedRows":["Maryland"],"associatedColumns":["C3D"],"associatedMergedColumns":[]},{"number":"86.0","isBolded":false,"associatedRows":["YUPENN"],"associatedColumns":["[ 8 ]"],"associatedMergedColumns":[]},{"number":"87.7","isBolded":true,"associatedRows":["Maryland"],"associatedColumns":["Imagenet"],"associatedMergedColumns":[]},{"number":"80.7","isBolded":false,"associatedRows":["YUPENN"],"associatedColumns":["[ 4 ]"],"associatedMergedColumns":[]},{"number":"98.1","isBolded":true,"associatedRows":["YUPENN"],"associatedColumns":["C3D"],"associatedMergedColumns":[]},{"number":"77.7","isBolded":false,"associatedRows":["Maryland"],"associatedColumns":["[ 9 ]"],"associatedMergedColumns":[]},{"number":"74.6","isBolded":false,"associatedRows":["Maryland"],"associatedColumns":["[ 41 ]"],"associatedMergedColumns":[]},{"number":"96.7","isBolded":false,"associatedRows":["YUPENN"],"associatedColumns":["Imagenet"],"associatedMergedColumns":[]},{"number":"67.7","isBolded":false,"associatedRows":["Maryland"],"associatedColumns":["[ 8 ]"],"associatedMergedColumns":[]},{"number":"96.2","isBolded":false,"associatedRows":["YUPENN"],"associatedColumns":["[ 9 ]"],"associatedMergedColumns":[]},{"number":"43.1","isBolded":false,"associatedRows":["Maryland"],"associatedColumns":["[ 4 ]"],"associatedMergedColumns":[]},{"number":"85.0","isBolded":false,"associatedRows":["YUPENN"],"associatedColumns":["[ 41 ]"],"associatedMergedColumns":[]}]},{"caption":"Table 6. Runtime analysis on UCF101. C3D is 91x faster than \nimproved dense trajectories ","rows":["FPS","x Slower","RT ( hours )"],"columns":["C3D","iDT","CPU","Brox \u0027 s","GPU"],"mergedAllColumns":[],"numberCells":[{"number":"274.6","isBolded":false,"associatedRows":["x Slower"],"associatedColumns":["Brox \u0027 s","GPU"],"associatedMergedColumns":[]},{"number":"3.5","isBolded":false,"associatedRows":["FPS"],"associatedColumns":["iDT","CPU"],"associatedMergedColumns":[]},{"number":"2.2","isBolded":false,"associatedRows":["RT ( hours )"],"associatedColumns":["C3D","GPU"],"associatedMergedColumns":[]},{"number":"202.2","isBolded":false,"associatedRows":["RT ( hours )"],"associatedColumns":["iDT","CPU"],"associatedMergedColumns":[]},{"number":"2513.9","isBolded":false,"associatedRows":["RT ( hours )"],"associatedColumns":["Brox \u0027 s","CPU"],"associatedMergedColumns":[]},{"number":"607.8","isBolded":false,"associatedRows":["RT ( hours )"],"associatedColumns":["Brox \u0027 s","GPU"],"associatedMergedColumns":[]},{"number":"1135.9","isBolded":false,"associatedRows":["x Slower"],"associatedColumns":["Brox \u0027 s","CPU"],"associatedMergedColumns":[]},{"number":"313.9","isBolded":false,"associatedRows":["FPS"],"associatedColumns":["C3D","GPU"],"associatedMergedColumns":[]},{"number":"0.3","isBolded":false,"associatedRows":["FPS"],"associatedColumns":["Brox \u0027 s","CPU"],"associatedMergedColumns":[]},{"number":"1.2","isBolded":false,"associatedRows":["FPS"],"associatedColumns":["Brox \u0027 s","GPU"],"associatedMergedColumns":[]},{"number":"91.4","isBolded":false,"associatedRows":["x Slower"],"associatedColumns":["iDT","CPU"],"associatedMergedColumns":[]}]},{"caption":"Net \nnet-64 net-128 net-256 \n# of params (M) \n11.1 \n17.5 \n34.8 \nTrain time (mins/epoch) \n92 \n270 \n1186 \n\nTable 7. Number of parameters and training time comparison of \n3D ConvNets with different input resolutions. Note that net-128 is \nequivalent to the depth-3 net in the paper. \n\n","rows":["# of params ( M )"],"columns":["net - 64","net - 128","net - 256"],"mergedAllColumns":[],"numberCells":[{"number":"17.5","isBolded":false,"associatedRows":["# of params ( M )"],"associatedColumns":["net - 128"],"associatedMergedColumns":[]},{"number":"11.1","isBolded":false,"associatedRows":["# of params ( M )"],"associatedColumns":["net - 64"],"associatedMergedColumns":[]},{"number":"34.8","isBolded":false,"associatedRows":["# of params ( M )"],"associatedColumns":["net - 256"],"associatedMergedColumns":[]}]}]