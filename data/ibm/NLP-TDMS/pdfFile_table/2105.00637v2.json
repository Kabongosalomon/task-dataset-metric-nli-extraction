[{"caption":"Table 1: Ablations. We train on the COCO train2017 split, and report mask as well as box APs on the val2017 split. \n\n","rows":["max","L1","L2","dice","w / o","l\u003d784","-","avg","L2+dice","cosine","l\u003d60","dynamic","l\u003d80","multi - head","mask","l\u003d40"],"columns":["mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m","b","the mask cost function brings expected gains .","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","embeddings also has a slight improvement . Using cosine similarity as","L","the mask cost . The L1 loss between the predicted and encoded mask","( c ) Loss Functions : Learning masks with both a pixel - level dice","m","( d ) Attention Type : Dynamic attention brings significant gains com -","AP","AP b","dings instead of masks brings better performance to the mask APs .","pared with multi - head attention in fusing the RoI and image features .","and saturates when the dimension l\u003d80 .","Directly expanding the","S","The performance improves when the embedding dimension l\u003d60 ,","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","50","loss and a embedding - level L2 loss yields gains in mask APs .","75"],"mergedAllColumns":["features"],"numberCells":[{"number":"35.6","isBolded":false,"associatedRows":["l\u003d60","max","L1"],"associatedColumns":["AP","75"],"associatedMergedColumns":[]},{"number":"32.5","isBolded":false,"associatedRows":["dice","max","multi - head"],"associatedColumns":["AP b","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","b"],"associatedMergedColumns":[]},{"number":"17.3","isBolded":false,"associatedRows":["L2","max"],"associatedColumns":["b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","S"],"associatedMergedColumns":[]},{"number":"55.2","isBolded":false,"associatedRows":["l\u003d784","-"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"36.1","isBolded":false,"associatedRows":["L2","max"],"associatedColumns":["AP b","50","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m"],"associatedMergedColumns":[]},{"number":"47.6","isBolded":false,"associatedRows":["dice","max"],"associatedColumns":["AP b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m","L"],"associatedMergedColumns":[]},{"number":"31.6","isBolded":false,"associatedRows":["mask"],"associatedColumns":["m","AP"],"associatedMergedColumns":[]},{"number":"36.4","isBolded":false,"associatedRows":["l\u003d60","max"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"33.8","isBolded":false,"associatedRows":["mask","max","w / o"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"41.0","isBolded":false,"associatedRows":["L2","max","dynamic"],"associatedColumns":["b","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP b"],"associatedMergedColumns":[]},{"number":"49.5","isBolded":false,"associatedRows":["L2+dice","avg","L1"],"associatedColumns":["AP m","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP m","L"],"associatedMergedColumns":[]},{"number":"50.6","isBolded":false,"associatedRows":["L2+dice","avg","L1"],"associatedColumns":["AP m","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP m","L"],"associatedMergedColumns":["features"]},{"number":"42.6","isBolded":false,"associatedRows":["L2+dice","max","dynamic"],"associatedColumns":["AP b","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP b","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","b"],"associatedMergedColumns":[]},{"number":"40.6","isBolded":false,"associatedRows":["l\u003d80","max"],"associatedColumns":["b","AP"],"associatedMergedColumns":[]},{"number":"57.6","isBolded":false,"associatedRows":["L2+dice","max","dynamic"],"associatedColumns":["AP m","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","m","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","b"],"associatedMergedColumns":[]},{"number":"53.3","isBolded":false,"associatedRows":["mask","-"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"58.6","isBolded":false,"associatedRows":["l\u003d80","max"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"34.0","isBolded":false,"associatedRows":["l\u003d60","max","L1"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"55.0","isBolded":false,"associatedRows":["L2+dice","-","-","dynamic"],"associatedColumns":["AP b","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","b","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","b"],"associatedMergedColumns":[]},{"number":"41.1","isBolded":false,"associatedRows":["l\u003d40","max"],"associatedColumns":["b","AP"],"associatedMergedColumns":[]},{"number":"33.3","isBolded":false,"associatedRows":["L2+dice","max"],"associatedColumns":["AP m","75","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m","( c ) Loss Functions : Learning masks with both a pixel - level dice","loss and a embedding - level L2 loss yields gains in mask APs .","AP m"],"associatedMergedColumns":[]},{"number":"58.9","isBolded":false,"associatedRows":["l\u003d60","max","L1"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"43.5","isBolded":false,"associatedRows":["L2+dice","max","dynamic"],"associatedColumns":["AP","75","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP b"],"associatedMergedColumns":[]},{"number":"44.1","isBolded":false,"associatedRows":["l\u003d80","max"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"35.2","isBolded":false,"associatedRows":["L2+dice","max"],"associatedColumns":["AP b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m","( c ) Loss Functions : Learning masks with both a pixel - level dice","loss and a embedding - level L2 loss yields gains in mask APs .","AP m"],"associatedMergedColumns":[]},{"number":"55.3","isBolded":false,"associatedRows":["L2+dice","avg"],"associatedColumns":["b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","S","( c ) Loss Functions : Learning masks with both a pixel - level dice","loss and a embedding - level L2 loss yields gains in mask APs .","AP m"],"associatedMergedColumns":[]},{"number":"36.5","isBolded":false,"associatedRows":["L2+dice","avg"],"associatedColumns":["AP b","75","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","L","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP m"],"associatedMergedColumns":["features"]},{"number":"58.4","isBolded":false,"associatedRows":["mask","max"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"55.6","isBolded":false,"associatedRows":["L2+dice","max"],"associatedColumns":["AP m","50","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m"],"associatedMergedColumns":[]},{"number":"43.2","isBolded":false,"associatedRows":["L2+dice","-","-","dynamic"],"associatedColumns":["AP b","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP b","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","b"],"associatedMergedColumns":[]},{"number":"43.3","isBolded":false,"associatedRows":["L2+dice","avg","dynamic"],"associatedColumns":["AP b","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP b","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","b"],"associatedMergedColumns":["features"]},{"number":"34.8","isBolded":false,"associatedRows":["dice","max"],"associatedColumns":["AP b","50","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m"],"associatedMergedColumns":[]},{"number":"34.2","isBolded":false,"associatedRows":["L2+dice"],"associatedColumns":["m","50","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m"],"associatedMergedColumns":[]},{"number":"17.3","isBolded":false,"associatedRows":["dice","max"],"associatedColumns":["b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","S"],"associatedMergedColumns":[]},{"number":"35.5","isBolded":false,"associatedRows":["l\u003d40","max"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"35.5","isBolded":false,"associatedRows":["L2","max"],"associatedColumns":["AP m","75","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m"],"associatedMergedColumns":[]},{"number":"50.6","isBolded":false,"associatedRows":["L2+dice","max"],"associatedColumns":["AP b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m","L"],"associatedMergedColumns":[]},{"number":"+11.7","isBolded":true,"associatedRows":["L2+dice","max","multi - head"],"associatedColumns":["AP m","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","m","AP","50"],"associatedMergedColumns":[]},{"number":"40.9","isBolded":false,"associatedRows":["L2+dice","-","-","dynamic"],"associatedColumns":["AP m","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP b"],"associatedMergedColumns":[]},{"number":"41.0","isBolded":false,"associatedRows":["L2+dice","avg","dynamic"],"associatedColumns":["AP m","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP b"],"associatedMergedColumns":["features"]},{"number":"44.4","isBolded":false,"associatedRows":["l\u003d60","max"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"41.0","isBolded":false,"associatedRows":["l\u003d60","max"],"associatedColumns":["b","AP"],"associatedMergedColumns":[]},{"number":"17.6","isBolded":false,"associatedRows":["L2+dice","max"],"associatedColumns":["b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","S"],"associatedMergedColumns":[]},{"number":"40.2","isBolded":false,"associatedRows":["mask","max"],"associatedColumns":["b","AP"],"associatedMergedColumns":[]},{"number":"17.4","isBolded":false,"associatedRows":["L2+dice","-","-"],"associatedColumns":["AP b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m","( c ) Loss Functions : Learning masks with both a pixel - level dice","loss and a embedding - level L2 loss yields gains in mask APs .","AP m","S"],"associatedMergedColumns":[]},{"number":"33.8","isBolded":false,"associatedRows":["l\u003d80"],"associatedColumns":["AP m","AP"],"associatedMergedColumns":[]},{"number":"22.0","isBolded":false,"associatedRows":["dice","max","multi - head"],"associatedColumns":["AP m","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m"],"associatedMergedColumns":[]},{"number":"41.1","isBolded":false,"associatedRows":["l\u003d784","max"],"associatedColumns":["b","AP"],"associatedMergedColumns":[]},{"number":"33.9","isBolded":false,"associatedRows":["l\u003d40","max","dice"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"55.6","isBolded":false,"associatedRows":["l\u003d40","-"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"32.6","isBolded":false,"associatedRows":["dice"],"associatedColumns":["m","50","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m"],"associatedMergedColumns":[]},{"number":"+11.9","isBolded":true,"associatedRows":["L2+dice","max","multi - head"],"associatedColumns":["AP b","75","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","b","AP","75"],"associatedMergedColumns":[]},{"number":"49.5","isBolded":false,"associatedRows":["L2","max"],"associatedColumns":["AP b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m","L"],"associatedMergedColumns":[]},{"number":"40.7","isBolded":false,"associatedRows":["mask","max","w / o"],"associatedColumns":["b","AP"],"associatedMergedColumns":[]},{"number":"58.8","isBolded":false,"associatedRows":["mask","max","w / o"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"40.6","isBolded":false,"associatedRows":["L2+dice","avg","dynamic"],"associatedColumns":["AP m","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP b"],"associatedMergedColumns":[]},{"number":"55.5","isBolded":false,"associatedRows":["mask","max","w / o"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"58.9","isBolded":false,"associatedRows":["L2+dice","avg","dynamic"],"associatedColumns":["AP m","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","m","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","b"],"associatedMergedColumns":["features"]},{"number":"55.4","isBolded":false,"associatedRows":["l\u003d40","max","dice"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"35.6","isBolded":false,"associatedRows":["L2+dice","max"],"associatedColumns":["AP b","75","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","L","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP m"],"associatedMergedColumns":[]},{"number":"40.8","isBolded":false,"associatedRows":["l\u003d40","max","dice"],"associatedColumns":["b","AP"],"associatedMergedColumns":[]},{"number":"44.3","isBolded":false,"associatedRows":["l\u003d60","max","L1"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"34.2","isBolded":false,"associatedRows":["l\u003d80","max","cosine"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"58.9","isBolded":false,"associatedRows":["l\u003d40","max"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"+8.4","isBolded":true,"associatedRows":["L2+dice","max","multi - head"],"associatedColumns":["AP b","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP b"],"associatedMergedColumns":[]},{"number":"44.8","isBolded":false,"associatedRows":["l\u003d40","max"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"34.0","isBolded":false,"associatedRows":["L2+dice","-","-"],"associatedColumns":["AP m","75","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m","( c ) Loss Functions : Learning masks with both a pixel - level dice","loss and a embedding - level L2 loss yields gains in mask APs .","AP m"],"associatedMergedColumns":[]},{"number":"19.6","isBolded":false,"associatedRows":["dice","max","multi - head"],"associatedColumns":["m","75","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m"],"associatedMergedColumns":[]},{"number":"55.3","isBolded":false,"associatedRows":["L2+dice","avg","dynamic"],"associatedColumns":["AP b","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","b","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","b"],"associatedMergedColumns":["features"]},{"number":"36.5","isBolded":false,"associatedRows":["L2+dice","max"],"associatedColumns":["AP b","50","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m"],"associatedMergedColumns":[]},{"number":"36.3","isBolded":false,"associatedRows":["L2+dice","-","-"],"associatedColumns":["AP b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m","( c ) Loss Functions : Learning masks with both a pixel - level dice","loss and a embedding - level L2 loss yields gains in mask APs .","AP m"],"associatedMergedColumns":[]},{"number":"44.3","isBolded":false,"associatedRows":["L2+dice","-","-","dynamic"],"associatedColumns":["AP","75","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP b"],"associatedMergedColumns":[]},{"number":"55.3","isBolded":false,"associatedRows":["L2+dice","-","-"],"associatedColumns":["b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","S","( c ) Loss Functions : Learning masks with both a pixel - level dice","loss and a embedding - level L2 loss yields gains in mask APs .","AP m"],"associatedMergedColumns":[]},{"number":"33.7","isBolded":false,"associatedRows":["l\u003d40"],"associatedColumns":["m","AP"],"associatedMergedColumns":[]},{"number":"36.4","isBolded":false,"associatedRows":["L2","max","dynamic"],"associatedColumns":["m","75","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m"],"associatedMergedColumns":[]},{"number":"31.2","isBolded":false,"associatedRows":["dice","max","multi - head"],"associatedColumns":["b","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP b"],"associatedMergedColumns":[]},{"number":"33.8","isBolded":false,"associatedRows":["L2"],"associatedColumns":["m","50","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m"],"associatedMergedColumns":[]},{"number":"50.5","isBolded":false,"associatedRows":["dice","max","multi - head"],"associatedColumns":["AP b","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP b"],"associatedMergedColumns":[]},{"number":"24.8","isBolded":false,"associatedRows":["L2+dice","avg","dynamic"],"associatedColumns":["b","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP b","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP b","S"],"associatedMergedColumns":["features"]},{"number":"55.6","isBolded":false,"associatedRows":["L2","max","dynamic"],"associatedColumns":["AP m","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","50"],"associatedMergedColumns":[]},{"number":"43.2","isBolded":false,"associatedRows":["L2+dice","avg","dynamic"],"associatedColumns":["AP b","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP b","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","b"],"associatedMergedColumns":[]},{"number":"58.8","isBolded":false,"associatedRows":["L2+dice","-","-","dynamic"],"associatedColumns":["AP m","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","m","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","b"],"associatedMergedColumns":[]},{"number":"34.2","isBolded":false,"associatedRows":["l\u003d60"],"associatedColumns":["AP m","AP"],"associatedMergedColumns":[]},{"number":"60.0","isBolded":false,"associatedRows":["l\u003d784","max"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"34.2","isBolded":false,"associatedRows":["L2+dice","avg"],"associatedColumns":["AP m","75","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m","( c ) Loss Functions : Learning masks with both a pixel - level dice","loss and a embedding - level L2 loss yields gains in mask APs .","AP m"],"associatedMergedColumns":["features"]},{"number":"54.6","isBolded":false,"associatedRows":["L2+dice","max"],"associatedColumns":["b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","S","( c ) Loss Functions : Learning masks with both a pixel - level dice","loss and a embedding - level L2 loss yields gains in mask APs .","AP m"],"associatedMergedColumns":[]},{"number":"36.4","isBolded":false,"associatedRows":["l\u003d80","max","cosine"],"associatedColumns":["AP","75"],"associatedMergedColumns":[]},{"number":"54.1","isBolded":false,"associatedRows":["L2+dice","avg","dynamic"],"associatedColumns":["AP b","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","b","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","b"],"associatedMergedColumns":[]},{"number":"17.3","isBolded":false,"associatedRows":["L2+dice","avg"],"associatedColumns":["AP b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m","( c ) Loss Functions : Learning masks with both a pixel - level dice","loss and a embedding - level L2 loss yields gains in mask APs .","AP m","S"],"associatedMergedColumns":[]},{"number":"55.6","isBolded":false,"associatedRows":["l\u003d60","-"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"+12.2","isBolded":true,"associatedRows":["L2+dice","max","dynamic"],"associatedColumns":["AP m","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m"],"associatedMergedColumns":[]},{"number":"35.9","isBolded":false,"associatedRows":["l\u003d80","max"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"44.4","isBolded":false,"associatedRows":["l\u003d784","max"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"58.9","isBolded":false,"associatedRows":["l\u003d80","max","cosine"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"55.6","isBolded":false,"associatedRows":["L2+dice","avg"],"associatedColumns":["b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","S","( c ) Loss Functions : Learning masks with both a pixel - level dice","loss and a embedding - level L2 loss yields gains in mask APs .","AP m"],"associatedMergedColumns":["features"]},{"number":"44.4","isBolded":false,"associatedRows":["l\u003d80","max","cosine"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"55.6","isBolded":false,"associatedRows":["l\u003d80","max","cosine"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"33.6","isBolded":false,"associatedRows":["L2+dice","avg"],"associatedColumns":["AP m","75","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m","( c ) Loss Functions : Learning masks with both a pixel - level dice","loss and a embedding - level L2 loss yields gains in mask APs .","AP m"],"associatedMergedColumns":[]},{"number":"58.9","isBolded":false,"associatedRows":["l\u003d60","max"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"35.7","isBolded":false,"associatedRows":["mask","max","w / o"],"associatedColumns":["AP","75"],"associatedMergedColumns":[]},{"number":"31.4","isBolded":false,"associatedRows":["l\u003d784"],"associatedColumns":["AP m","AP"],"associatedMergedColumns":[]},{"number":"41.0","isBolded":false,"associatedRows":["l\u003d80","max","cosine"],"associatedColumns":["b","AP"],"associatedMergedColumns":[]},{"number":"55.5","isBolded":false,"associatedRows":["L2","max"],"associatedColumns":["AP m","50","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m"],"associatedMergedColumns":[]},{"number":"+9.8","isBolded":true,"associatedRows":["L2+dice","max","multi - head"],"associatedColumns":["b","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP b"],"associatedMergedColumns":[]},{"number":"55.5","isBolded":false,"associatedRows":["l\u003d60","max","L1"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"36.4","isBolded":false,"associatedRows":["L2+dice","avg"],"associatedColumns":["AP b","75","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","L","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP m"],"associatedMergedColumns":[]},{"number":"23.9","isBolded":false,"associatedRows":["L2+dice","avg","dynamic"],"associatedColumns":["b","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP b","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP b","S"],"associatedMergedColumns":[]},{"number":"35.5","isBolded":false,"associatedRows":["L2+dice","avg"],"associatedColumns":["AP b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m","( c ) Loss Functions : Learning masks with both a pixel - level dice","loss and a embedding - level L2 loss yields gains in mask APs .","AP m"],"associatedMergedColumns":[]},{"number":"40.0","isBolded":false,"associatedRows":["L2+dice","max","dynamic"],"associatedColumns":["AP m","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP b"],"associatedMergedColumns":[]},{"number":"24.2","isBolded":false,"associatedRows":["L2+dice","max","dynamic"],"associatedColumns":["b","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP b","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP b","S"],"associatedMergedColumns":[]},{"number":"+16.8","isBolded":true,"associatedRows":["L2+dice","max","multi - head"],"associatedColumns":["m","75","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m","75"],"associatedMergedColumns":[]},{"number":"17.4","isBolded":false,"associatedRows":["L2+dice","max"],"associatedColumns":["AP b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m","( c ) Loss Functions : Learning masks with both a pixel - level dice","loss and a embedding - level L2 loss yields gains in mask APs .","AP m","S"],"associatedMergedColumns":[]},{"number":"44.1","isBolded":false,"associatedRows":["L2+dice","avg","dynamic"],"associatedColumns":["AP","75","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP b"],"associatedMergedColumns":[]},{"number":"35.8","isBolded":false,"associatedRows":["l\u003d40","max","dice"],"associatedColumns":["AP","75"],"associatedMergedColumns":[]},{"number":"33.5","isBolded":false,"associatedRows":["dice","max"],"associatedColumns":["AP m","75","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m"],"associatedMergedColumns":[]},{"number":"36.4","isBolded":false,"associatedRows":["L2+dice","-","-"],"associatedColumns":["AP b","75","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","L","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP m"],"associatedMergedColumns":[]},{"number":"58.8","isBolded":false,"associatedRows":["L2+dice","avg","dynamic"],"associatedColumns":["AP m","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","m","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","b"],"associatedMergedColumns":[]},{"number":"40.9","isBolded":false,"associatedRows":["l\u003d60","max","L1"],"associatedColumns":["b","AP"],"associatedMergedColumns":[]},{"number":"44.1","isBolded":false,"associatedRows":["mask","max","w / o"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"55.0","isBolded":false,"associatedRows":["dice","max"],"associatedColumns":["AP m","50","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m"],"associatedMergedColumns":[]},{"number":"34.2","isBolded":false,"associatedRows":["L2","max","dynamic"],"associatedColumns":["AP m","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m"],"associatedMergedColumns":[]},{"number":"58.9","isBolded":false,"associatedRows":["L2","max","dynamic"],"associatedColumns":["AP b","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP b"],"associatedMergedColumns":[]},{"number":"33.0","isBolded":false,"associatedRows":["mask","max"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"48.5","isBolded":false,"associatedRows":["L2+dice","max","L1"],"associatedColumns":["AP m","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP m","L"],"associatedMergedColumns":[]},{"number":"24.3","isBolded":false,"associatedRows":["L2+dice","-","-","dynamic"],"associatedColumns":["b","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP b","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP b","S"],"associatedMergedColumns":[]},{"number":"36.4","isBolded":false,"associatedRows":["L2+dice","avg"],"associatedColumns":["AP b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m","( c ) Loss Functions : Learning masks with both a pixel - level dice","loss and a embedding - level L2 loss yields gains in mask APs .","AP m"],"associatedMergedColumns":["features"]},{"number":"43.9","isBolded":false,"associatedRows":["dice","max","multi - head"],"associatedColumns":["AP m","50","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","50"],"associatedMergedColumns":[]},{"number":"36.4","isBolded":false,"associatedRows":["L2+dice","max"],"associatedColumns":["AP m","75","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","and saturates when the dimension l\u003d80 .","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m"],"associatedMergedColumns":[]},{"number":"17.6","isBolded":false,"associatedRows":["L2+dice","avg"],"associatedColumns":["AP b","( a ) Mask vs . Mask Embeddings : Regression with mask embed -","dings instead of masks brings better performance to the mask APs .","The performance improves when the embedding dimension l\u003d60 ,","Directly expanding the","mask as embeddings , i . e . , l\u003d784 , has worse performance .","AP m","( c ) Loss Functions : Learning masks with both a pixel - level dice","loss and a embedding - level L2 loss yields gains in mask APs .","AP m","S"],"associatedMergedColumns":["features"]},{"number":"43.6","isBolded":false,"associatedRows":["mask","max"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"52.6","isBolded":false,"associatedRows":["L2+dice","max","dynamic"],"associatedColumns":["AP b","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","b","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","b"],"associatedMergedColumns":[]},{"number":"58.8","isBolded":false,"associatedRows":["l\u003d40","max","dice"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"31.8","isBolded":false,"associatedRows":["l\u003d784","max"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"44.3","isBolded":false,"associatedRows":["l\u003d40","max","dice"],"associatedColumns":["AP b"],"associatedMergedColumns":[]},{"number":"44.4","isBolded":false,"associatedRows":["L2","max","dynamic"],"associatedColumns":["AP b","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","b"],"associatedMergedColumns":[]},{"number":"55.3","isBolded":false,"associatedRows":["l\u003d80","-"],"associatedColumns":["AP m"],"associatedMergedColumns":[]},{"number":"50.5","isBolded":false,"associatedRows":["L2+dice","-","-","L1"],"associatedColumns":["AP m","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP m","L"],"associatedMergedColumns":[]},{"number":"44.4","isBolded":false,"associatedRows":["L2+dice","avg","dynamic"],"associatedColumns":["AP","75","( b ) Mask Cost Functions : Matching with the dice loss between the","predicted and ground truth masks performs slightly better than w / o","the mask cost . The L1 loss between the predicted and encoded mask","embeddings also has a slight improvement . Using cosine similarity as","the mask cost function brings expected gains .","AP m","( d ) Attention Type : Dynamic attention brings significant gains com -","pared with multi - head attention in fusing the RoI and image features .","AP b"],"associatedMergedColumns":["features"]}]},{"caption":"Table 2: Quantitative Results of ISTR on the COCO test-dev split. All the models are learned by multi-scale training. \nThe results of FPS are measured with a single 1080Ti GPU. The performance of Mask R-CNN is the result of the modified \nversion with implementation details in TensorMask ","rows":["MEInst [ 53 ]","36","SOLOv2 [ 46 ]","16","DETR [ 3 ]","ResNet101 - FPN","ResNet50 - FPN","Sparse R - CNN [ 37 ]","-","ResNet50","500","CondInst [ 40 ]","Mask R - CNN [ 16 ]","ResNet101","BlendMask [ 4 ]","ISTR , ours"],"columns":["FPS","Time","-"],"mergedAllColumns":["1080Ti"],"numberCells":[{"number":"35.3","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet101 - FPN","36"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"43.5","isBolded":false,"associatedRows":["DETR [ 3 ]","ResNet101","500","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"61.1","isBolded":true,"associatedRows":["DETR [ 3 ]","ResNet50","500","-","-","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"22.8","isBolded":true,"associatedRows":["ISTR , ours","ResNet101 - FPN","36","-"],"associatedColumns":["FPS","-","-","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"18.8","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet50 - FPN","36","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"48.0","isBolded":false,"associatedRows":["DETR [ 3 ]","ResNet101","500","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"24.2","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":[]},{"number":"48.1","isBolded":true,"associatedRows":["ISTR , ours","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"21.5","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet101 - FPN","36","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"25.1","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"50.6","isBolded":false,"associatedRows":["ISTR , ours","ResNet50 - FPN","36","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"41.9","isBolded":false,"associatedRows":["ISTR , ours","ResNet101 - FPN","36","-","-"],"associatedColumns":["FPS","-","-","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"43.5","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"38.8","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet101 - FPN","36"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"38.6","isBolded":true,"associatedRows":["ISTR , ours","ResNet50 - FPN","36"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"25.6","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"47.5","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"55.4","isBolded":true,"associatedRows":["SOLOv2 [ 46 ]","ResNet50 - FPN","36","-","16","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"91.3","isBolded":false,"associatedRows":["ISTR , ours","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["Time","-","-","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"61.6","isBolded":false,"associatedRows":["Sparse R - CNN [ 37 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"9.0","isBolded":false,"associatedRows":["SOLOv2 [ 46 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"85.0","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["Time","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"54.3","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"86.6","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["Time","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"42.9","isBolded":true,"associatedRows":["SOLOv2 [ 46 ]","ResNet101 - FPN","36","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"61.5","isBolded":false,"associatedRows":["ISTR , ours","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"21.8","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet101 - FPN","36","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"52.1","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"57.6","isBolded":false,"associatedRows":["SOLOv2 [ 46 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"33.5","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet50 - FPN","36"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"37.8","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet50 - FPN","36"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"89.3","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["Time","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"54.0","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"25.8","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"61.8","isBolded":true,"associatedRows":["DETR [ 3 ]","ResNet101","500","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"95.5","isBolded":false,"associatedRows":["SOLOv2 [ 46 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["Time"],"associatedMergedColumns":["1080Ti"]},{"number":"27.8","isBolded":true,"associatedRows":["ISTR , ours","ResNet50 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"26.6","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"15.3","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":[]},{"number":"28.7","isBolded":true,"associatedRows":["ISTR , ours","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"42.5","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"22.1","isBolded":true,"associatedRows":["ISTR , ours","ResNet50 - FPN","36","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"38.2","isBolded":false,"associatedRows":["SOLOv2 [ 46 ]","ResNet50 - FPN","36"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"18.4","isBolded":false,"associatedRows":["SOLOv2 [ 46 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"66.8","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["Time"],"associatedMergedColumns":["1080Ti"]},{"number":"41.7","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet101 - FPN","36","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"42.2","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet101 - FPN","36","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"52.3","isBolded":false,"associatedRows":["ISTR , ours","ResNet101 - FPN","36","-","-","-"],"associatedColumns":["FPS","-","-","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"44.5","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"40.7","isBolded":false,"associatedRows":["SOLOv2 [ 46 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"20.4","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet101 - FPN","36","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"46.8","isBolded":true,"associatedRows":["ISTR , ours","ResNet50 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"44.5","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet101 - FPN","36","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"47.2","isBolded":false,"associatedRows":["Sparse R - CNN [ 37 ]","ResNet50 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-"],"associatedMergedColumns":["1080Ti"]},{"number":"47.3","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"42.6","isBolded":false,"associatedRows":["SOLOv2 [ 46 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"48.7","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet50 - FPN","36","-","16","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"44.5","isBolded":false,"associatedRows":["Sparse R - CNN [ 37 ]","ResNet50 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-"],"associatedMergedColumns":["1080Ti"]},{"number":"43.1","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"48.3","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet50 - FPN","36","-","16","-"],"associatedColumns":["FPS"],"associatedMergedColumns":[]},{"number":"43.5","isBolded":false,"associatedRows":["SOLOv2 [ 46 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"54.1","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"46.0","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"66.8","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["Time"],"associatedMergedColumns":["1080Ti"]},{"number":"17.3","isBolded":false,"associatedRows":["SOLOv2 [ 46 ]","ResNet101 - FPN","36","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"111.6","isBolded":false,"associatedRows":["SOLOv2 [ 46 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["Time","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"40.9","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet50 - FPN","36","-","16"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"83.2","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["Time","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"51.4","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet101 - FPN","36","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"51.7","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":[]},{"number":"11.0","isBolded":false,"associatedRows":["ISTR , ours","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"20.5","isBolded":false,"associatedRows":["DETR [ 3 ]","ResNet50","500","-","-","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"39.6","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet101 - FPN","36"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"50.4","isBolded":true,"associatedRows":["ISTR , ours","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"46.7","isBolded":false,"associatedRows":["SOLOv2 [ 46 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"21.0","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet50 - FPN","36","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"25.1","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"39.6","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet50 - FPN","36","-","16"],"associatedColumns":["FPS"],"associatedMergedColumns":[]},{"number":"40.4","isBolded":false,"associatedRows":["ISTR , ours","ResNet50 - FPN","36","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"39.7","isBolded":false,"associatedRows":["SOLOv2 [ 46 ]","ResNet101 - FPN","36"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"65.6","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["Time"],"associatedMergedColumns":[]},{"number":"21.1","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet50 - FPN","36","-"],"associatedColumns":["FPS"],"associatedMergedColumns":[]},{"number":"57.4","isBolded":true,"associatedRows":["SOLOv2 [ 46 ]","ResNet101 - FPN","36","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"39.9","isBolded":true,"associatedRows":["ISTR , ours","ResNet101 - FPN","36"],"associatedColumns":["FPS","-","-","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"37.8","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet101 - FPN","36","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"26.9","isBolded":false,"associatedRows":["Sparse R - CNN [ 37 ]","ResNet50 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-"],"associatedMergedColumns":["1080Ti"]},{"number":"37.5","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet50 - FPN","36"],"associatedColumns":["FPS"],"associatedMergedColumns":[]},{"number":"10.5","isBolded":false,"associatedRows":["SOLOv2 [ 46 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"12.0","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"40.3","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet50 - FPN","36","-","16"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"44.7","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"65.0","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["Time"],"associatedMergedColumns":["1080Ti"]},{"number":"53.6","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet50 - FPN","36","-","16","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"48.7","isBolded":true,"associatedRows":["ISTR , ours","ResNet50 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"39.1","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet101 - FPN","36"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"22.3","isBolded":false,"associatedRows":["SOLOv2 [ 46 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"45.8","isBolded":false,"associatedRows":["DETR [ 3 ]","ResNet50","500","-","-","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"43.0","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"50.5","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet101 - FPN","36","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"43.6","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":[]},{"number":"54.9","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"15.0","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"13.8","isBolded":false,"associatedRows":["ISTR , ours","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"56.3","isBolded":false,"associatedRows":["SOLOv2 [ 46 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"52.2","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"50.9","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet101 - FPN","36","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"59.5","isBolded":false,"associatedRows":["Sparse R - CNN [ 37 ]","ResNet50 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-"],"associatedMergedColumns":["1080Ti"]},{"number":"15.4","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"15.0","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"11.5","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"22.4","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet101 - FPN","36","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"42.1","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet50 - FPN","36","-","16","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"11.8","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"25.3","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"21.9","isBolded":false,"associatedRows":["DETR [ 3 ]","ResNet101","500","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"59.9","isBolded":false,"associatedRows":["ISTR , ours","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"46.0","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"19.3","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet50 - FPN","36","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"45.1","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"37.8","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet50 - FPN","36"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"41.2","isBolded":true,"associatedRows":["SOLOv2 [ 46 ]","ResNet50 - FPN","36","-","16"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"48.3","isBolded":false,"associatedRows":["Sparse R - CNN [ 37 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"42.1","isBolded":false,"associatedRows":["CondInst [ 40 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"55.6","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"45.6","isBolded":false,"associatedRows":["Sparse R - CNN [ 37 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"72.5","isBolded":false,"associatedRows":["ISTR , ours","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["Time","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"41.3","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":[]},{"number":"44.5","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"26.8","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"42.0","isBolded":false,"associatedRows":["DETR [ 3 ]","ResNet50","500","-","-","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"35.7","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet50 - FPN","36","-","16"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"45.4","isBolded":false,"associatedRows":["BlendMask [ 4 ]","ResNet50 - FPN","36","-","16","-","-"],"associatedColumns":["FPS"],"associatedMergedColumns":["1080Ti"]},{"number":"41.4","isBolded":false,"associatedRows":["Mask R - CNN [ 16 ]","ResNet101 - FPN","36","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"28.3","isBolded":true,"associatedRows":["Sparse R - CNN [ 37 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-","-"],"associatedMergedColumns":["1080Ti"]},{"number":"11.2","isBolded":false,"associatedRows":["MEInst [ 53 ]","ResNet101 - FPN","36","-","-","-","-"],"associatedColumns":["FPS","-","-"],"associatedMergedColumns":["1080Ti"]}]}]