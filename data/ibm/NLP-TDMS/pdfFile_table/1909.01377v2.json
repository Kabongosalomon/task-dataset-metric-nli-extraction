[{"caption":"Table 1: DEQ achieves strong performance on the long-range copy-memory task. \n\n","rows":["Copy Memory T \u003d400 Loss","2 . 7e - 5","3 . 5e - 6"],"columns":["GRU [ 14 ] ( 14K )","Models ( Size )","LSTM [ 26 ] ( 14K )"],"mergedAllColumns":[],"numberCells":[{"number":"0.0501","isBolded":false,"associatedRows":["Copy Memory T \u003d400 Loss","3 . 5e - 6","2 . 7e - 5"],"associatedColumns":["Models ( Size )","LSTM [ 26 ] ( 14K )"],"associatedMergedColumns":[]},{"number":"0.0491","isBolded":false,"associatedRows":["Copy Memory T \u003d400 Loss","3 . 5e - 6","2 . 7e - 5"],"associatedColumns":["Models ( Size )","GRU [ 14 ] ( 14K )"],"associatedMergedColumns":[]}]},{"caption":"Table 2: DEQ achieves competitive performance on word-level Penn Treebank language modeling \n(on par with SOTA results, without fine-tuning steps [34]).  † The memory footprints are benchmarked \n(for fairness) on input sequence length 150 and batch size 15, which does not reflect the actual \nhyperparameters used; the values also do not include the memory for word embeddings. \n\n","rows":["Variational LSTM [ 22 ]","24M","23M","66M","-","54M","20M","DARTS architecture search ( second order ) [ 29 ]","60 - layer TrellisNet ( w / auxiliary loss , w / o MoS ) [ 8 ]","DEQ - TrellisNet ( ours )","NAS Cell [ 55 ]","NAS ( w / black - box hyperparameter tuner ) [ 32 ]","AWD - LSTM [ 34 ]"],"columns":["Word - level Language Modeling w / Penn Treebank ( PTB )","Test perplexity"],"mergedAllColumns":["model size","8 . 5GB","-"],"numberCells":[{"number":"58.8","isBolded":false,"associatedRows":["AWD - LSTM [ 34 ]","24M","20M"],"associatedColumns":["Word - level Language Modeling w / Penn Treebank ( PTB )","Test perplexity"],"associatedMergedColumns":["-"]},{"number":"57.1","isBolded":false,"associatedRows":["DEQ - TrellisNet ( ours )","24M","20M"],"associatedColumns":["Word - level Language Modeling w / Penn Treebank ( PTB )","Test perplexity"],"associatedMergedColumns":["8 . 5GB"]},{"number":"55.7","isBolded":true,"associatedRows":["DARTS architecture search ( second order ) [ 29 ]","23M","20M"],"associatedColumns":["Word - level Language Modeling w / Penn Treebank ( PTB )","Test perplexity"],"associatedMergedColumns":["-"]},{"number":"59.7","isBolded":false,"associatedRows":["NAS ( w / black - box hyperparameter tuner ) [ 32 ]","24M","20M"],"associatedColumns":["Word - level Language Modeling w / Penn Treebank ( PTB )","Test perplexity"],"associatedMergedColumns":["-"]},{"number":"73.4","isBolded":false,"associatedRows":["Variational LSTM [ 22 ]","66M","-"],"associatedColumns":["Word - level Language Modeling w / Penn Treebank ( PTB )","Test perplexity"],"associatedMergedColumns":["model size"]},{"number":"57.0","isBolded":false,"associatedRows":["60 - layer TrellisNet ( w / auxiliary loss , w / o MoS ) [ 8 ]","24M","20M"],"associatedColumns":["Word - level Language Modeling w / Penn Treebank ( PTB )","Test perplexity"],"associatedMergedColumns":["-"]},{"number":"62.4","isBolded":false,"associatedRows":["NAS Cell [ 55 ]","54M","-"],"associatedColumns":["Word - level Language Modeling w / Penn Treebank ( PTB )","Test perplexity"],"associatedMergedColumns":["-"]}]},{"caption":"Table 3: DEQ-based models are competitive with SOTA deep networks of the same model size on the \nWikiText-103 corpus, with significantly less memory.  † See Table 2 for more details on the memory \nbenchmarking. Transformer-XL models are not weight-tied, unless specified otherwise. \n\n","rows":["Relational Memory Core [ 40 ]","139M","70 - layer TrellisNet with gradient checkpointing","DEQ - Transformer ( small , ours ) .","230M","165M","Time","Step","Broyden","34M","Transformer - XL ( X - large , adaptive embed . , on TPU ) [ 16 ]","44M","224M","70 - layer TrellisNet ( + auxiliary loss , etc . ) [ 8 ]","159M","72M","60M","180M","70M","150M","172M","DEQ - Transformer ( medium , adaptive embed . , ours )","Generic TCN [ 7 ]","per","138M","Transformer - XL ( small , 4 layers )","Transformer - XL ( medium , 18 layers , adaptive embed . )","Iter .","DEQ - Transformer ( medium , ours ) .","AWD - QRNN [ 33 ]","110M","45M","-","257M","43M","51M","Gated Linear ConvNet [ 17 ]","195M","DEQ - TrellisNet ( ours )","Transformer - XL ( small , weight - tied 16 layers )","Transformer - XL ( medium , 16 layers )"],"columns":["Forward ( eps\u003d1e - 6 )","# Params","||f ( x ) - x||","Test perplexity","10 1","10 2","Word - level Language Modeling w / WikiText - 103 ( WT103 )","Model","DEQ - Transformer on WT103 ( Seq . Length\u003d150 )","Norm","10 0"],"mergedAllColumns":["3 . 7GB","4 . 8GB","10 1","9 . 0GB","10 2","1 . 1GB","5 . 2GB","-","24 . 7GB","6 . 8GB","7 . 1GB","2 . 7GB","12 . 0GB","8 . 5GB","10 0","Model Size","3 . 3GB"],"numberCells":[{"number":"29.2","isBolded":false,"associatedRows":["Step","70 - layer TrellisNet ( + auxiliary loss , etc . ) [ 8 ]","180M","45M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["12 . 0GB"]},{"number":"4.5M","isBolded":false,"associatedRows":["Step","Transformer - XL ( small , weight - tied 16 layers )","138M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["4 . 8GB"]},{"number":"35.8","isBolded":false,"associatedRows":["Step","Transformer - XL ( small , 4 layers )","139M","70M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["3 . 7GB"]},{"number":"4.5M","isBolded":false,"associatedRows":["Step","DEQ - Transformer ( small , ours ) .","138M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["6 . 8GB"]},{"number":"18.7","isBolded":true,"associatedRows":["Transformer - XL ( X - large , adaptive embed . , on TPU ) [ 16 ]","Step","257M","224M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["-"]},{"number":"29.2","isBolded":false,"associatedRows":["Step","70 - layer TrellisNet with gradient checkpointing","180M","45M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["24 . 7GB"]},{"number":"29.0","isBolded":true,"associatedRows":["Step","DEQ - TrellisNet ( ours )","180M","45M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["5 . 2GB"]},{"number":"0.4","isBolded":false,"associatedRows":["Broyden"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Model","DEQ - Transformer on WT103 ( Seq . Length\u003d150 )","||f ( x ) - x||","Norm","10 1","10 2","Forward ( eps\u003d1e - 6 )"],"associatedMergedColumns":["10 0"]},{"number":"24.2","isBolded":false,"associatedRows":["Step","DEQ - Transformer ( medium , ours ) .","172M","43M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["8 . 5GB"]},{"number":"45.2","isBolded":false,"associatedRows":["Step","Generic TCN [ 7 ]","150M","34M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["Model Size"]},{"number":"23.6","isBolded":false,"associatedRows":["Transformer - XL ( medium , 18 layers , adaptive embed . )","Step","110M","72M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["2 . 7GB"]},{"number":"0.7","isBolded":false,"associatedRows":["per"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Model","DEQ - Transformer on WT103 ( Seq . Length\u003d150 )","||f ( x ) - x||"],"associatedMergedColumns":["10 1"]},{"number":"33.0","isBolded":false,"associatedRows":["Step","AWD - QRNN [ 33 ]","159M","51M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["-"]},{"number":"37.2","isBolded":false,"associatedRows":["Step","Gated Linear ConvNet [ 17 ]","230M","-"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["-"]},{"number":"31.6","isBolded":false,"associatedRows":["Step","Relational Memory Core [ 40 ]","195M","60M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["7 . 1GB"]},{"number":"23.2","isBolded":true,"associatedRows":["DEQ - Transformer ( medium , adaptive embed . , ours )","Step","110M","70M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["9 . 0GB"]},{"number":"4.9M","isBolded":false,"associatedRows":["Step","Transformer - XL ( small , 4 layers )","139M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","# Params"],"associatedMergedColumns":["3 . 7GB"]},{"number":"32.4","isBolded":true,"associatedRows":["Step","DEQ - Transformer ( small , ours ) .","138M","70M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["6 . 8GB"]},{"number":"0.8","isBolded":false,"associatedRows":["Time"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Model","DEQ - Transformer on WT103 ( Seq . Length\u003d150 )","10 2"],"associatedMergedColumns":["10 2"]},{"number":"0.9","isBolded":false,"associatedRows":["Step"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Model","DEQ - Transformer on WT103 ( Seq . Length\u003d150 )"],"associatedMergedColumns":["1 . 1GB"]},{"number":"0.5","isBolded":false,"associatedRows":["Broyden"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Model","DEQ - Transformer on WT103 ( Seq . Length\u003d150 )","||f ( x ) - x||","Norm","10 1"],"associatedMergedColumns":["10 0"]},{"number":"34.9","isBolded":false,"associatedRows":["Step","Transformer - XL ( small , weight - tied 16 layers )","138M","70M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["4 . 8GB"]},{"number":"0.6","isBolded":false,"associatedRows":["Iter ."],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Model","DEQ - Transformer on WT103 ( Seq . Length\u003d150 )","||f ( x ) - x||","10 0"],"associatedMergedColumns":["10 0"]},{"number":"24.3","isBolded":false,"associatedRows":["Step","Transformer - XL ( medium , 16 layers )","165M","44M"],"associatedColumns":["Word - level Language Modeling w / WikiText - 103 ( WT103 )","Test perplexity"],"associatedMergedColumns":["3 . 3GB"]}]},{"caption":"Table 4: Runtime ratios between DEQs and corresponding deep networks at training and inference \n(\u003e 1× implies DEQ is slower). The ratios are benchmarked on WikiText-103. \n\n","rows":["Validation","100","200","125","Perplexity","DEQ - Transformer"],"columns":["Training","DEQ / 18 - layer Transformer","DEQ - Transformer on WT103","DEQ / 70 - layer TrellisNet","Inference"],"mergedAllColumns":["DEQ - Transformer","150","175"],"numberCells":[{"number":"24","isBolded":false,"associatedRows":["Validation","100","Validation"],"associatedColumns":["DEQ / 70 - layer TrellisNet","Training","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]},{"number":"29","isBolded":false,"associatedRows":["Perplexity","200","DEQ - Transformer","Perplexity"],"associatedColumns":["DEQ / 70 - layer TrellisNet","Training","DEQ - Transformer on WT103"],"associatedMergedColumns":[]},{"number":"20","isBolded":false,"associatedRows":["Validation","100","Validation"],"associatedColumns":["DEQ / 70 - layer TrellisNet","Training","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]},{"number":"40","isBolded":false,"associatedRows":["Validation","100","Validation"],"associatedColumns":["DEQ / 70 - layer TrellisNet","Inference","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]},{"number":"101","isBolded":false,"associatedRows":["Validation","100"],"associatedColumns":["DEQ / 18 - layer Transformer","Inference","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]},{"number":"28","isBolded":false,"associatedRows":["Perplexity","200","Perplexity"],"associatedColumns":["DEQ / 70 - layer TrellisNet","Training","DEQ - Transformer on WT103"],"associatedMergedColumns":["DEQ - Transformer"]},{"number":"26","isBolded":false,"associatedRows":["Validation","100","Validation"],"associatedColumns":["DEQ / 70 - layer TrellisNet","Training","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]},{"number":"106","isBolded":false,"associatedRows":["Validation","100"],"associatedColumns":["DEQ / 18 - layer Transformer","Training","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]},{"number":"50","isBolded":false,"associatedRows":["Validation"],"associatedColumns":["DEQ / 18 - layer Transformer","Training","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]},{"number":"1.76×","isBolded":true,"associatedRows":["Perplexity","200"],"associatedColumns":["DEQ / 18 - layer Transformer","Inference"],"associatedMergedColumns":[]},{"number":"80","isBolded":false,"associatedRows":["Validation","100","Validation"],"associatedColumns":["DEQ / 70 - layer TrellisNet","Inference","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]},{"number":"27","isBolded":false,"associatedRows":["Perplexity","125","Perplexity"],"associatedColumns":["DEQ / 70 - layer TrellisNet","Training","DEQ - Transformer on WT103"],"associatedMergedColumns":["175"]},{"number":"2.40×","isBolded":true,"associatedRows":["Perplexity","200","Perplexity"],"associatedColumns":["DEQ / 70 - layer TrellisNet","Training"],"associatedMergedColumns":[]},{"number":"104","isBolded":false,"associatedRows":["Validation","100"],"associatedColumns":["DEQ / 18 - layer Transformer","Training","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]},{"number":"25","isBolded":false,"associatedRows":["Validation"],"associatedColumns":["DEQ / 18 - layer Transformer","Training","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]},{"number":"103","isBolded":false,"associatedRows":["Validation","100"],"associatedColumns":["DEQ / 18 - layer Transformer","Training","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]},{"number":"100","isBolded":false,"associatedRows":["Validation","100"],"associatedColumns":["DEQ / 18 - layer Transformer","Inference","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]},{"number":"60","isBolded":false,"associatedRows":["Validation","100","Validation"],"associatedColumns":["DEQ / 70 - layer TrellisNet","Inference","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]},{"number":"25","isBolded":false,"associatedRows":["Validation","100","Validation"],"associatedColumns":["DEQ / 70 - layer TrellisNet","Training","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]},{"number":"75","isBolded":false,"associatedRows":["Validation"],"associatedColumns":["DEQ / 18 - layer Transformer","Training","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]},{"number":"2.82×","isBolded":true,"associatedRows":["Perplexity","200"],"associatedColumns":["DEQ / 18 - layer Transformer","Training"],"associatedMergedColumns":[]},{"number":"1.64×","isBolded":true,"associatedRows":["Perplexity","200","Perplexity"],"associatedColumns":["DEQ / 70 - layer TrellisNet","Inference"],"associatedMergedColumns":[]},{"number":"105","isBolded":false,"associatedRows":["Validation","100"],"associatedColumns":["DEQ / 18 - layer Transformer","Training","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]},{"number":"102","isBolded":false,"associatedRows":["Validation","100"],"associatedColumns":["DEQ / 18 - layer Transformer","Inference","DEQ - Transformer on WT103"],"associatedMergedColumns":["150"]}]}]