[{"caption":"Methods \n\nInception Score ↑ \nFID ↓ \nCA ↑ \nCOCO \nVG \nCOCO VG \nCOCO VG \n\nReal Images \n\n64 × 64 \n16.3 ± 0.4 \n13.9 ± 0.5 \n0 \n0 \n54.48 \n49.57 \n128 × 128 \n22.3 ± 0.5 \n20.5 ± 1.5 \n0 \n0 \n60.71 \n56.25 \n256 × 256 \n28.10 ± 0.5 \n28.6 ± 1.2 \n0 \n0 \n63.04 \n60.40 \n\n64 × 64 \n\nSG2Im (Johnson, Gupta, and Fei-Fei 2018) † \n7.3 ± 0.1 \n6.3 ± 0.2 \n67.96 \n74.61 \n30.04 \n40.29 \nPix2PixHD (Wang et al. 2018) \n7.2 ± 0.2 \n6.6 ± 0.3 \n59.95 \n47.71 \n20.82 \n16.98 \nSPADE (Park et al. 2019) \n8.5 ± 0.3 \n7.3 ± 0.1 \n43.31 \n35.74 \n31.61 \n23.81 \nLayout2Im (Zhao et al. 2019) † \n9.1 ± 0.1 \n8.1 ± 0.1 \n38.14 \n31.25 \n50.84 \n48.09 \nSOARISG (Ashual and Wolf 2019) *   † \n10.3 ± 0.1 \nN/A \n48.7 \nN/A \n46.1 \nN/A \nOC-GAN (ours) \n10.5 ± 0.3 \n8.9 ± 0.3 \n33.1 \n22.61 56.88 57.73 \n64 × 64 \nLostGAN (Sun and Wu 2019) (flips)  † \n9.8 ± 0.2 \n8.7 ± 0.4 \n34.31 \n34.75 \n37.15 \n27.1 \nwith flips \nOC-GAN (ours) \n10.8 ± 0.5 \n9.3 ± 0.2 \n29.57 20.27 60.39 60.79 \n\n128 × 128 \n\nPix2PixHD (Wang et al. 2018) \n10.4 ± 0.3 \n9.8 ± 0.3 \n62 \n46.55 \n26.67 \n25.03 \nSPADE (Park et al. 2019) \n13.1 ± 0.5 \n11.3 ± 0.4 \n40.04 \n33.29 \n41.74 \n34.11 \nLayout2Im (Zhao et al. 2019) \n12.0 ± 0.4 \n10.1 ± 0.3 \n43.21 \n38.21 \n49.06 \n51.13 \nSOARISG (Ashual and Wolf 2019)  † *  \n12.5 ± 0.3 \nN/A \n59.5 \nN/A \n44.6 \nN/A \nOC-GAN (ours) \n14.0 ± 0.2 \n11.9 ± 0.5 \n36.04 28.91 60.32 58.03 \n128 × 128 \nLostGAN (Sun and Wu 2019)  † \n13.8 ± 0.4 \n11.1 ± 0.6 \n29.65 \n29.36 \n41.38 \n28.76 \nwith flips \nLostGAN-V2 (Sun and Wu 2020)  † \n14.2 ± 0.4 10.71 ± 0.27 24.76 \n29.00 \n43.27 \n35.17 \nOC-GAN (ours) \n14.6 ± 0.4 \n12.3 ± 0.4 \n36.31 \n28.26 59.44 59.40 \n\n256 × 256 \nSOARISG (Ashual and Wolf 2019)  † *  \n15.2 ± 0.1 \nN/A \n65.95 \nN/A \n45.3 \nN/A \nOC-GAN (ours) \n17.0 ± 0.1 \n14.4 ± 0.6 \n45.96 \n39.07 \n53.47 57.89 \n256 × 256 \nLostGAN-V2 (Sun and Wu 2020)  † \n18.0 ± 0.5 \n14.1 ± 0.4 \n42.55 \n47.62 \n54.40 \n53.02 \nwith flips \nOC-GAN (ours) \n17.8 ± 0.2 \n14.7 ± 0.2 \n41.65 40.85 57.16 53.28 \n\nTable 1: Performance on 64, 128 and 256 dimension images. All models use ground-truth layouts. We use  † to denote results \ntaken from the original paper.  *  denotes a model that uses pixel-level semantic segmentation during training. denotes models \nfor which the openly available source code was not adapted to generation at a specific image size. We altered the code to allow \nthis and ran a hyperparameter search on the new models. \n\n","rows":["†","SOARISG ( Ashual and Wolf 2019 ) † *","N / A","SG2Im ( Johnson , Gupta , and Fei - Fei 2018 ) †","LostGAN ( Sun and Wu 2019 ) ( flips ) †","Layout2Im ( Zhao et al . 2019 )","LostGAN ( Sun and Wu 2019 ) †","LostGAN ( Sun and Wu 2019 ) ( flips )","0","256 × 256","Pix2PixHD ( Wang et al . 2018 )","Real Images","128 × 128","LostGAN - V2 ( Sun and Wu 2020 ) †","OC - GAN ( ours w / flips )","64 × 64","62","with flips","OC - GAN ( ours )","SPADE ( Park et al . 2019 )","Layout2Im ( Zhao et al . 2019 ) †","SOARISG ( Ashual and Wolf 2019 ) *"],"columns":["CA ↑","VG","taken from the original paper . * denotes a model that uses pixel - level semantic segmentation during training .","Inception Score ↑","COCO","FID ↓","Table 1 : Performance on 64 , 128 and 256 dimension images . All models use ground - truth layouts . We use"],"mergedAllColumns":["256 × 256","N / A","64 × 64","SceneFID ↓","Methods"],"numberCells":[{"number":"29.57","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours )","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["N / A"]},{"number":"40.04","isBolded":false,"associatedRows":["128 × 128","SPADE ( Park et al . 2019 )","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["N / A"]},{"number":"47.71","isBolded":false,"associatedRows":["Real Images","Pix2PixHD ( Wang et al . 2018 )","N / A","0"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["Methods"]},{"number":"0.1","isBolded":false,"associatedRows":["Real Images","SOARISG ( Ashual and Wolf 2019 ) *","†"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["64 × 64"]},{"number":"28.91","isBolded":false,"associatedRows":["128 × 128","OC - GAN ( ours )","N / A","62"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["N / A"]},{"number":"34.31","isBolded":false,"associatedRows":["64 × 64","LostGAN ( Sun and Wu 2019 ) ( flips ) †","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["N / A"]},{"number":"0.4","isBolded":false,"associatedRows":["256 × 256","LostGAN - V2 ( Sun and Wu 2020 ) †","N / A"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["256 × 256"]},{"number":"0.3","isBolded":false,"associatedRows":["128 × 128","Pix2PixHD ( Wang et al . 2018 )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"45.96","isBolded":false,"associatedRows":["256 × 256","OC - GAN ( ours )","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["N / A"]},{"number":"6.3±","isBolded":false,"associatedRows":["Real Images","SG2Im ( Johnson , Gupta , and Fei - Fei 2018 ) †"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"53.02","isBolded":false,"associatedRows":["256 × 256","LostGAN - V2 ( Sun and Wu 2020 ) †","N / A","62","N / A"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["256 × 256"]},{"number":"0.2","isBolded":false,"associatedRows":["Real Images","Pix2PixHD ( Wang et al . 2018 )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"38.14","isBolded":false,"associatedRows":["Real Images","Layout2Im ( Zhao et al . 2019 ) †","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["Methods"]},{"number":"20.03","isBolded":false,"associatedRows":["with flips","LostGAN ( Sun and Wu 2019 ) ( flips )"],"associatedColumns":["Inception Score ↑","COCO","Table 1 : Performance on 64 , 128 and 256 dimension images . All models use ground - truth layouts . We use","taken from the original paper . * denotes a model that uses pixel - level semantic segmentation during training .","COCO"],"associatedMergedColumns":["N / A"]},{"number":"1.5","isBolded":false,"associatedRows":["Real Images","128 × 128","N / A"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"60.39","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours )","N / A","62","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"54.40","isBolded":false,"associatedRows":["256 × 256","LostGAN - V2 ( Sun and Wu 2020 ) †","N / A","62","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["256 × 256"]},{"number":"27.1","isBolded":false,"associatedRows":["64 × 64","LostGAN ( Sun and Wu 2019 ) ( flips ) †","N / A","62","N / A"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"28.26","isBolded":false,"associatedRows":["128 × 128","OC - GAN ( ours )","N / A","62"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["N / A"]},{"number":"0.2","isBolded":false,"associatedRows":["Real Images","SG2Im ( Johnson , Gupta , and Fei - Fei 2018 ) †"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"35.17","isBolded":false,"associatedRows":["with flips","LostGAN - V2 ( Sun and Wu 2020 ) †","N / A","62","N / A"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"0.6","isBolded":false,"associatedRows":["256 × 256","OC - GAN ( ours )","N / A"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"59.44","isBolded":false,"associatedRows":["128 × 128","OC - GAN ( ours )","N / A","62","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"0.1","isBolded":false,"associatedRows":["Real Images","SPADE ( Park et al . 2019 )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"0.1","isBolded":false,"associatedRows":["256 × 256","SOARISG ( Ashual and Wolf 2019 ) † *"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"49.57","isBolded":false,"associatedRows":["Real Images","64 × 64","N / A","0","0"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"41.65","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours )","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["256 × 256"]},{"number":"8.1±","isBolded":false,"associatedRows":["Real Images","Layout2Im ( Zhao et al . 2019 ) †"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"0.1","isBolded":false,"associatedRows":["Real Images","Layout2Im ( Zhao et al . 2019 ) †"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"59.40","isBolded":false,"associatedRows":["128 × 128","OC - GAN ( ours )","N / A","62","N / A"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"43.21","isBolded":false,"associatedRows":["128 × 128","Layout2Im ( Zhao et al . 2019 )","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["N / A"]},{"number":"10.4±","isBolded":false,"associatedRows":["128 × 128","Pix2PixHD ( Wang et al . 2018 )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"0.3","isBolded":false,"associatedRows":["Real Images","Pix2PixHD ( Wang et al . 2018 )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"28.6±","isBolded":false,"associatedRows":["Real Images","256 × 256"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"16.3±","isBolded":false,"associatedRows":["Real Images","64 × 64"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"60.79","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours )","N / A","62","N / A"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"46.55","isBolded":false,"associatedRows":["128 × 128","Pix2PixHD ( Wang et al . 2018 )","N / A","62"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["N / A"]},{"number":"20.27","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours )","N / A","62"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["N / A"]},{"number":"0.1","isBolded":false,"associatedRows":["Real Images","Layout2Im ( Zhao et al . 2019 ) †"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"59.5","isBolded":false,"associatedRows":["128 × 128","SOARISG ( Ashual and Wolf 2019 ) † *","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["N / A"]},{"number":"42.55","isBolded":false,"associatedRows":["256 × 256","LostGAN - V2 ( Sun and Wu 2020 ) †","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["256 × 256"]},{"number":"0.5","isBolded":false,"associatedRows":["Real Images","64 × 64","N / A"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"53.47","isBolded":false,"associatedRows":["256 × 256","OC - GAN ( ours )","N / A","62","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"9.8±","isBolded":false,"associatedRows":["128 × 128","Pix2PixHD ( Wang et al . 2018 )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"29.36","isBolded":false,"associatedRows":["128 × 128","LostGAN ( Sun and Wu 2019 ) †","N / A","62"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["N / A"]},{"number":"74.61","isBolded":false,"associatedRows":["Real Images","SG2Im ( Johnson , Gupta , and Fei - Fei 2018 ) †","N / A","0"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["Methods"]},{"number":"57.73","isBolded":false,"associatedRows":["Real Images","OC - GAN ( ours )","N / A","62","N / A"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"15.2±","isBolded":false,"associatedRows":["256 × 256","SOARISG ( Ashual and Wolf 2019 ) † *"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"9.3±","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"14.0±","isBolded":false,"associatedRows":["128 × 128","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"20.5±","isBolded":false,"associatedRows":["Real Images","128 × 128"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"9.1±","isBolded":false,"associatedRows":["Real Images","Layout2Im ( Zhao et al . 2019 ) †"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"13.17","isBolded":false,"associatedRows":["with flips","LostGAN ( Sun and Wu 2019 ) ( flips )"],"associatedColumns":["Inception Score ↑","COCO","Table 1 : Performance on 64 , 128 and 256 dimension images . All models use ground - truth layouts . We use","taken from the original paper . * denotes a model that uses pixel - level semantic segmentation during training .","VG"],"associatedMergedColumns":["N / A"]},{"number":"7.2±","isBolded":false,"associatedRows":["Real Images","Pix2PixHD ( Wang et al . 2018 )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"11.9±","isBolded":false,"associatedRows":["128 × 128","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"0.2","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"13.8±","isBolded":false,"associatedRows":["128 × 128","LostGAN ( Sun and Wu 2019 ) †"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"54.48","isBolded":false,"associatedRows":["Real Images","64 × 64","N / A","0","0"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"35.74","isBolded":false,"associatedRows":["Real Images","SPADE ( Park et al . 2019 )","N / A","0"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["Methods"]},{"number":"0.1","isBolded":false,"associatedRows":["256 × 256","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"37.15","isBolded":false,"associatedRows":["64 × 64","LostGAN ( Sun and Wu 2019 ) ( flips ) †","N / A","62","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"18.0±","isBolded":false,"associatedRows":["256 × 256","LostGAN - V2 ( Sun and Wu 2020 ) †"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["256 × 256"]},{"number":"31.61","isBolded":false,"associatedRows":["Real Images","SPADE ( Park et al . 2019 )","N / A","0","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"41.74","isBolded":false,"associatedRows":["128 × 128","SPADE ( Park et al . 2019 )","N / A","62","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"22.3±","isBolded":false,"associatedRows":["Real Images","128 × 128"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"0.3","isBolded":false,"associatedRows":["Real Images","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"0.5","isBolded":false,"associatedRows":["Real Images","256 × 256"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"8.5±","isBolded":false,"associatedRows":["Real Images","SPADE ( Park et al . 2019 )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"60.71","isBolded":false,"associatedRows":["Real Images","128 × 128","N / A","0","0"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"17.8±","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["256 × 256"]},{"number":"10.3±","isBolded":false,"associatedRows":["Real Images","SOARISG ( Ashual and Wolf 2019 ) *","†"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["64 × 64"]},{"number":"0.1","isBolded":false,"associatedRows":["Real Images","SG2Im ( Johnson , Gupta , and Fei - Fei 2018 ) †"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"0.5","isBolded":false,"associatedRows":["256 × 256","LostGAN - V2 ( Sun and Wu 2020 ) †"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["256 × 256"]},{"number":"23.81","isBolded":false,"associatedRows":["Real Images","SPADE ( Park et al . 2019 )","N / A","0","N / A"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"34.75","isBolded":false,"associatedRows":["64 × 64","LostGAN ( Sun and Wu 2019 ) ( flips ) †","N / A","62"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["N / A"]},{"number":"25.03","isBolded":false,"associatedRows":["128 × 128","Pix2PixHD ( Wang et al . 2018 )","N / A","62","N / A"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"11.1±","isBolded":false,"associatedRows":["128 × 128","LostGAN ( Sun and Wu 2019 ) †"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"49.06","isBolded":false,"associatedRows":["128 × 128","Layout2Im ( Zhao et al . 2019 )","N / A","62","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"0.5","isBolded":false,"associatedRows":["128 × 128","SPADE ( Park et al . 2019 )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"0.4","isBolded":false,"associatedRows":["128 × 128","SPADE ( Park et al . 2019 )","N / A"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"58.03","isBolded":false,"associatedRows":["128 × 128","OC - GAN ( ours )","N / A","62","N / A"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"60.40","isBolded":false,"associatedRows":["Real Images","256 × 256","N / A","0","0"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"29.00","isBolded":false,"associatedRows":["with flips","LostGAN - V2 ( Sun and Wu 2020 ) †","N / A","62"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["N / A"]},{"number":"39.07","isBolded":false,"associatedRows":["256 × 256","OC - GAN ( ours )","N / A","62"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["N / A"]},{"number":"1.2","isBolded":false,"associatedRows":["Real Images","256 × 256","N / A"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"44.6","isBolded":false,"associatedRows":["128 × 128","SOARISG ( Ashual and Wolf 2019 ) † *","N / A","62","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"0.2","isBolded":false,"associatedRows":["128 × 128","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"40.85","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours )","N / A","62"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["256 × 256"]},{"number":"50.84","isBolded":false,"associatedRows":["Real Images","Layout2Im ( Zhao et al . 2019 ) †","N / A","0","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"28.76","isBolded":false,"associatedRows":["128 × 128","LostGAN ( Sun and Wu 2019 ) †","N / A","62","N / A"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"0.4","isBolded":false,"associatedRows":["128 × 128","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"56.88","isBolded":false,"associatedRows":["Real Images","OC - GAN ( ours )","N / A","62","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"16.76","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours w / flips )"],"associatedColumns":["Inception Score ↑","COCO","Table 1 : Performance on 64 , 128 and 256 dimension images . All models use ground - truth layouts . We use","taken from the original paper . * denotes a model that uses pixel - level semantic segmentation during training .","COCO"],"associatedMergedColumns":["N / A"]},{"number":"59.95","isBolded":false,"associatedRows":["Real Images","Pix2PixHD ( Wang et al . 2018 )","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["Methods"]},{"number":"60.32","isBolded":false,"associatedRows":["128 × 128","OC - GAN ( ours )","N / A","62","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"24.76","isBolded":false,"associatedRows":["with flips","LostGAN - V2 ( Sun and Wu 2020 ) †","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["N / A"]},{"number":"40.29","isBolded":false,"associatedRows":["Real Images","SG2Im ( Johnson , Gupta , and Fei - Fei 2018 ) †","N / A","0","0"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"0.4","isBolded":false,"associatedRows":["with flips","LostGAN - V2 ( Sun and Wu 2020 ) †"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"0.3","isBolded":false,"associatedRows":["128 × 128","SOARISG ( Ashual and Wolf 2019 ) † *"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"29.65","isBolded":false,"associatedRows":["128 × 128","LostGAN ( Sun and Wu 2019 ) †","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["N / A"]},{"number":"16.72","isBolded":false,"associatedRows":["with flips","SPADE ( Park et al . 2019 )"],"associatedColumns":["Inception Score ↑","COCO","Table 1 : Performance on 64 , 128 and 256 dimension images . All models use ground - truth layouts . We use","taken from the original paper . * denotes a model that uses pixel - level semantic segmentation during training .","VG"],"associatedMergedColumns":["SceneFID ↓"]},{"number":"36.31","isBolded":false,"associatedRows":["128 × 128","OC - GAN ( ours )","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["N / A"]},{"number":"0.5","isBolded":false,"associatedRows":["Real Images","128 × 128"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"7.3±","isBolded":false,"associatedRows":["Real Images","SPADE ( Park et al . 2019 )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"14.7±","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["256 × 256"]},{"number":"31.25","isBolded":false,"associatedRows":["Real Images","Layout2Im ( Zhao et al . 2019 ) †","N / A","0"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["Methods"]},{"number":"45.3","isBolded":false,"associatedRows":["256 × 256","SOARISG ( Ashual and Wolf 2019 ) † *","N / A","62","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"11.3±","isBolded":false,"associatedRows":["128 × 128","SPADE ( Park et al . 2019 )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"10.1±","isBolded":false,"associatedRows":["128 × 128","Layout2Im ( Zhao et al . 2019 )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"43.27","isBolded":false,"associatedRows":["with flips","LostGAN - V2 ( Sun and Wu 2020 ) †","N / A","62","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"9.63","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours w / flips )"],"associatedColumns":["Inception Score ↑","COCO","Table 1 : Performance on 64 , 128 and 256 dimension images . All models use ground - truth layouts . We use","taken from the original paper . * denotes a model that uses pixel - level semantic segmentation during training .","VG"],"associatedMergedColumns":["N / A"]},{"number":"28.10±","isBolded":false,"associatedRows":["Real Images","256 × 256"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"0.3","isBolded":false,"associatedRows":["128 × 128","Layout2Im ( Zhao et al . 2019 )","N / A"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"48.7","isBolded":false,"associatedRows":["Real Images","SOARISG ( Ashual and Wolf 2019 ) *","†","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["64 × 64"]},{"number":"7.3±","isBolded":false,"associatedRows":["Real Images","SG2Im ( Johnson , Gupta , and Fei - Fei 2018 ) †"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"42.92","isBolded":false,"associatedRows":["with flips","Pix2PixHD ( Wang et al . 2018 )"],"associatedColumns":["Inception Score ↑","COCO","Table 1 : Performance on 64 , 128 and 256 dimension images . All models use ground - truth layouts . We use","taken from the original paper . * denotes a model that uses pixel - level semantic segmentation during training .","COCO"],"associatedMergedColumns":["SceneFID ↓"]},{"number":"12.5±","isBolded":false,"associatedRows":["128 × 128","SOARISG ( Ashual and Wolf 2019 ) † *"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"57.89","isBolded":false,"associatedRows":["256 × 256","OC - GAN ( ours )","N / A","62","N / A"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"0.3","isBolded":false,"associatedRows":["128 × 128","Pix2PixHD ( Wang et al . 2018 )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"0.4","isBolded":false,"associatedRows":["128 × 128","LostGAN ( Sun and Wu 2019 ) †"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"56.25","isBolded":false,"associatedRows":["Real Images","128 × 128","N / A","0","0"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"10.71±","isBolded":false,"associatedRows":["with flips","LostGAN - V2 ( Sun and Wu 2020 ) †"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"42.98","isBolded":false,"associatedRows":["with flips","Pix2PixHD ( Wang et al . 2018 )"],"associatedColumns":["Inception Score ↑","COCO","Table 1 : Performance on 64 , 128 and 256 dimension images . All models use ground - truth layouts . We use","taken from the original paper . * denotes a model that uses pixel - level semantic segmentation during training .","VG"],"associatedMergedColumns":["SceneFID ↓"]},{"number":"34.11","isBolded":false,"associatedRows":["128 × 128","SPADE ( Park et al . 2019 )","N / A","62","N / A"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"0.4","isBolded":false,"associatedRows":["128 × 128","OC - GAN ( ours )","N / A"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"20.82","isBolded":false,"associatedRows":["Real Images","Pix2PixHD ( Wang et al . 2018 )","N / A","0","0"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"36.04","isBolded":false,"associatedRows":["128 × 128","OC - GAN ( ours )","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["N / A"]},{"number":"22.61","isBolded":false,"associatedRows":["Real Images","OC - GAN ( ours )","N / A","62"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["N / A"]},{"number":"13.9±","isBolded":false,"associatedRows":["Real Images","64 × 64"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"63.04","isBolded":false,"associatedRows":["Real Images","256 × 256","N / A","0","0"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"0.3","isBolded":false,"associatedRows":["Real Images","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"14.4±","isBolded":false,"associatedRows":["256 × 256","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"51.13","isBolded":false,"associatedRows":["128 × 128","Layout2Im ( Zhao et al . 2019 )","N / A","62","N / A"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"23.44","isBolded":false,"associatedRows":["with flips","SPADE ( Park et al . 2019 )"],"associatedColumns":["Inception Score ↑","COCO","Table 1 : Performance on 64 , 128 and 256 dimension images . All models use ground - truth layouts . We use","taken from the original paper . * denotes a model that uses pixel - level semantic segmentation during training .","COCO"],"associatedMergedColumns":["SceneFID ↓"]},{"number":"14.2±","isBolded":false,"associatedRows":["with flips","LostGAN - V2 ( Sun and Wu 2020 ) †"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"8.7±","isBolded":false,"associatedRows":["64 × 64","LostGAN ( Sun and Wu 2019 ) ( flips ) †"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"57.16","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours )","N / A","62","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["256 × 256"]},{"number":"33.29","isBolded":false,"associatedRows":["128 × 128","SPADE ( Park et al . 2019 )","N / A","62"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["N / A"]},{"number":"0.2","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours )","N / A"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["256 × 256"]},{"number":"0.5","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"0.3","isBolded":false,"associatedRows":["Real Images","SPADE ( Park et al . 2019 )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"48.09","isBolded":false,"associatedRows":["Real Images","Layout2Im ( Zhao et al . 2019 ) †","N / A","0","N / A"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"10.5±","isBolded":false,"associatedRows":["Real Images","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"0.4","isBolded":false,"associatedRows":["128 × 128","Layout2Im ( Zhao et al . 2019 )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"30.04","isBolded":false,"associatedRows":["Real Images","SG2Im ( Johnson , Gupta , and Fei - Fei 2018 ) †","N / A","0","0"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"33.1","isBolded":false,"associatedRows":["Real Images","OC - GAN ( ours )","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["N / A"]},{"number":"13.1±","isBolded":false,"associatedRows":["128 × 128","SPADE ( Park et al . 2019 )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"65.95","isBolded":false,"associatedRows":["256 × 256","SOARISG ( Ashual and Wolf 2019 ) † *","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["N / A"]},{"number":"12.56","isBolded":false,"associatedRows":["with flips","Layout2Im ( Zhao et al . 2019 )"],"associatedColumns":["Inception Score ↑","COCO","Table 1 : Performance on 64 , 128 and 256 dimension images . All models use ground - truth layouts . We use","taken from the original paper . * denotes a model that uses pixel - level semantic segmentation during training .","VG"],"associatedMergedColumns":["SceneFID ↓"]},{"number":"46.1","isBolded":false,"associatedRows":["Real Images","SOARISG ( Ashual and Wolf 2019 ) *","†","N / A","62","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["64 × 64"]},{"number":"0.4","isBolded":false,"associatedRows":["Real Images","64 × 64"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["Methods"]},{"number":"22.76","isBolded":false,"associatedRows":["with flips","Layout2Im ( Zhao et al . 2019 )"],"associatedColumns":["Inception Score ↑","COCO","Table 1 : Performance on 64 , 128 and 256 dimension images . All models use ground - truth layouts . We use","taken from the original paper . * denotes a model that uses pixel - level semantic segmentation during training .","COCO"],"associatedMergedColumns":["SceneFID ↓"]},{"number":"6.6±","isBolded":false,"associatedRows":["Real Images","Pix2PixHD ( Wang et al . 2018 )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"12.0±","isBolded":false,"associatedRows":["128 × 128","Layout2Im ( Zhao et al . 2019 )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"0.2","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["256 × 256"]},{"number":"67.96","isBolded":false,"associatedRows":["Real Images","SG2Im ( Johnson , Gupta , and Fei - Fei 2018 ) †","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["Methods"]},{"number":"53.28","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours )","N / A","62","N / A"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["256 × 256"]},{"number":"9.8±","isBolded":false,"associatedRows":["64 × 64","LostGAN ( Sun and Wu 2019 ) ( flips ) †"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"26.67","isBolded":false,"associatedRows":["128 × 128","Pix2PixHD ( Wang et al . 2018 )","N / A","62","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"0.5","isBolded":false,"associatedRows":["128 × 128","OC - GAN ( ours )","N / A"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"0.6","isBolded":false,"associatedRows":["128 × 128","LostGAN ( Sun and Wu 2019 ) †","N / A"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"17.0±","isBolded":false,"associatedRows":["256 × 256","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"12.3±","isBolded":false,"associatedRows":["128 × 128","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"8.9±","isBolded":false,"associatedRows":["Real Images","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["N / A"]},{"number":"10.8±","isBolded":false,"associatedRows":["with flips","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"41.38","isBolded":false,"associatedRows":["128 × 128","LostGAN ( Sun and Wu 2019 ) †","N / A","62","N / A"],"associatedColumns":["CA ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"16.98","isBolded":false,"associatedRows":["Real Images","Pix2PixHD ( Wang et al . 2018 )","N / A","0","N / A"],"associatedColumns":["CA ↑","VG"],"associatedMergedColumns":["Methods"]},{"number":"14.1±","isBolded":false,"associatedRows":["256 × 256","LostGAN - V2 ( Sun and Wu 2020 ) †"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["256 × 256"]},{"number":"43.31","isBolded":false,"associatedRows":["Real Images","SPADE ( Park et al . 2019 )","N / A"],"associatedColumns":["FID ↓","COCO"],"associatedMergedColumns":["Methods"]},{"number":"38.21","isBolded":false,"associatedRows":["128 × 128","Layout2Im ( Zhao et al . 2019 )","N / A","62"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["N / A"]},{"number":"47.62","isBolded":false,"associatedRows":["256 × 256","LostGAN - V2 ( Sun and Wu 2020 ) †","N / A","62"],"associatedColumns":["FID ↓","VG"],"associatedMergedColumns":["256 × 256"]},{"number":"0.27","isBolded":false,"associatedRows":["with flips","LostGAN - V2 ( Sun and Wu 2020 ) †","N / A"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"33.46","isBolded":false,"associatedRows":["with flips","SOARISG ( Ashual and Wolf 2019 ) *"],"associatedColumns":["Inception Score ↑","COCO","Table 1 : Performance on 64 , 128 and 256 dimension images . All models use ground - truth layouts . We use","taken from the original paper . * denotes a model that uses pixel - level semantic segmentation during training .","COCO"],"associatedMergedColumns":["SceneFID ↓"]},{"number":"0.2","isBolded":false,"associatedRows":["64 × 64","LostGAN ( Sun and Wu 2019 ) ( flips ) †"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"14.6±","isBolded":false,"associatedRows":["128 × 128","OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":["N / A"]},{"number":"0.4","isBolded":false,"associatedRows":["64 × 64","LostGAN ( Sun and Wu 2019 ) ( flips ) †"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":["N / A"]}]},{"caption":"Table 2: SceneFID scores on object crops resized to size 224 \n× 224, extracted from the 128 × 128 outputs of the differ-\nent models, for both datasets. All models use ground-truth \nlayouts.  *  denotes a model that uses pixel-level semantic \nsegmentation during training. SOARISG cannot be trained \non VG due to the absence of pixel-level semantic segmenta-\ntions. \n\n","rows":["Pix2PixHD ( Wang et al . 2018 )","OC - GAN ( ours w / flips )","Layout2Im ( Zhao et al . 2019 )","SPADE ( Park et al . 2019 )","SOARISG ( Ashual and Wolf 2019 ) *","LostGAN ( Sun and Wu 2019 ) ( flips )"],"columns":["VG","COCO","SceneFID ↓"],"mergedAllColumns":["N / A"],"numberCells":[{"number":"16.72","isBolded":false,"associatedRows":["SPADE ( Park et al . 2019 )"],"associatedColumns":["SceneFID ↓","VG"],"associatedMergedColumns":[]},{"number":"16.76","isBolded":true,"associatedRows":["OC - GAN ( ours w / flips )"],"associatedColumns":["SceneFID ↓","COCO"],"associatedMergedColumns":["N / A"]},{"number":"33.46","isBolded":false,"associatedRows":["SOARISG ( Ashual and Wolf 2019 ) *"],"associatedColumns":["SceneFID ↓","COCO"],"associatedMergedColumns":[]},{"number":"42.98","isBolded":false,"associatedRows":["Pix2PixHD ( Wang et al . 2018 )"],"associatedColumns":["SceneFID ↓","VG"],"associatedMergedColumns":[]},{"number":"23.44","isBolded":false,"associatedRows":["SPADE ( Park et al . 2019 )"],"associatedColumns":["SceneFID ↓","COCO"],"associatedMergedColumns":[]},{"number":"13.17","isBolded":false,"associatedRows":["LostGAN ( Sun and Wu 2019 ) ( flips )"],"associatedColumns":["SceneFID ↓","VG"],"associatedMergedColumns":["N / A"]},{"number":"42.92","isBolded":false,"associatedRows":["Pix2PixHD ( Wang et al . 2018 )"],"associatedColumns":["SceneFID ↓","COCO"],"associatedMergedColumns":[]},{"number":"12.56","isBolded":false,"associatedRows":["Layout2Im ( Zhao et al . 2019 )"],"associatedColumns":["SceneFID ↓","VG"],"associatedMergedColumns":[]},{"number":"9.63","isBolded":true,"associatedRows":["OC - GAN ( ours w / flips )"],"associatedColumns":["SceneFID ↓","VG"],"associatedMergedColumns":["N / A"]},{"number":"20.03","isBolded":false,"associatedRows":["LostGAN ( Sun and Wu 2019 ) ( flips )"],"associatedColumns":["SceneFID ↓","COCO"],"associatedMergedColumns":["N / A"]},{"number":"22.76","isBolded":false,"associatedRows":["Layout2Im ( Zhao et al . 2019 )"],"associatedColumns":["SceneFID ↓","COCO"],"associatedMergedColumns":[]}]},{"caption":"Table 3: User study results. 10 computer-science profession-\nals were shown 100 COCO-Stuff and 100 VG test set lay-\nouts and corresponding images generated by various mod-\nels, shuffled randomly. Users were asked to select the highest \nlayout-fidelity image for each layout at 128×128 resolution. \nSOARISG is marked marked non-rated (N/R), as it cannot \nbe trained on VG. \n\n","rows":["COCO - Stuff","VG","N / R"],"columns":["Ours","LostGAN","SOARISG"],"mergedAllColumns":[],"numberCells":[{"number":"68.6%","isBolded":true,"associatedRows":["VG","N / R"],"associatedColumns":["Ours"],"associatedMergedColumns":[]},{"number":"16.8%","isBolded":false,"associatedRows":["COCO - Stuff"],"associatedColumns":["SOARISG"],"associatedMergedColumns":[]},{"number":"36.8%","isBolded":false,"associatedRows":["COCO - Stuff"],"associatedColumns":["LostGAN"],"associatedMergedColumns":[]},{"number":"46.4%","isBolded":true,"associatedRows":["COCO - Stuff"],"associatedColumns":["Ours"],"associatedMergedColumns":[]},{"number":"31.4%","isBolded":false,"associatedRows":["VG","N / R"],"associatedColumns":["LostGAN"],"associatedMergedColumns":[]}]},{"caption":"Table 4: Quantitative comparison of different ablated ver-\nsions of our model on the COCO-Stuff dataset (64 × 64 im-\nages). These results highlight the importance of the SGSM \n(and its positive interaction with the perceptual loss) in the \nbottom row block, as well as the impact of removing some \nof the discriminators (middle row block). \n\n","rows":["No perceptual loss","No objectD","No patchD","No SGSM","Single patchD","No objectD , no SGSM","No perceptual loss , no SGSM","No bounding - box instance boundaries","Full"],"columns":["CA ↑","FID ↓"],"mergedAllColumns":[],"numberCells":[{"number":"52.57","isBolded":false,"associatedRows":["No SGSM"],"associatedColumns":["CA ↑"],"associatedMergedColumns":[]},{"number":"33.15","isBolded":false,"associatedRows":["No objectD , no SGSM"],"associatedColumns":["FID ↓"],"associatedMergedColumns":[]},{"number":"41.50","isBolded":false,"associatedRows":["No objectD , no SGSM"],"associatedColumns":["CA ↑"],"associatedMergedColumns":[]},{"number":"31.62","isBolded":false,"associatedRows":["No objectD"],"associatedColumns":["FID ↓"],"associatedMergedColumns":[]},{"number":"48.03","isBolded":false,"associatedRows":["No objectD"],"associatedColumns":["CA ↑"],"associatedMergedColumns":[]},{"number":"57.22","isBolded":false,"associatedRows":["No perceptual loss"],"associatedColumns":["CA ↑"],"associatedMergedColumns":[]},{"number":"29.57","isBolded":true,"associatedRows":["Full"],"associatedColumns":["FID ↓"],"associatedMergedColumns":[]},{"number":"30.12","isBolded":false,"associatedRows":["No bounding - box instance boundaries"],"associatedColumns":["FID ↓"],"associatedMergedColumns":[]},{"number":"59.86","isBolded":false,"associatedRows":["Single patchD"],"associatedColumns":["CA ↑"],"associatedMergedColumns":[]},{"number":"47.94","isBolded":false,"associatedRows":["No perceptual loss , no SGSM"],"associatedColumns":["CA ↑"],"associatedMergedColumns":[]},{"number":"33.85","isBolded":false,"associatedRows":["No patchD"],"associatedColumns":["FID ↓"],"associatedMergedColumns":[]},{"number":"34.32","isBolded":false,"associatedRows":["No SGSM"],"associatedColumns":["FID ↓"],"associatedMergedColumns":[]},{"number":"31.14","isBolded":false,"associatedRows":["No perceptual loss"],"associatedColumns":["FID ↓"],"associatedMergedColumns":[]},{"number":"62.48","isBolded":true,"associatedRows":["No patchD"],"associatedColumns":["CA ↑"],"associatedMergedColumns":[]},{"number":"59.54","isBolded":false,"associatedRows":["No bounding - box instance boundaries"],"associatedColumns":["CA ↑"],"associatedMergedColumns":[]},{"number":"36.54","isBolded":false,"associatedRows":["No perceptual loss , no SGSM"],"associatedColumns":["FID ↓"],"associatedMergedColumns":[]},{"number":"60.27","isBolded":false,"associatedRows":["Full"],"associatedColumns":["CA ↑"],"associatedMergedColumns":[]},{"number":"30.54","isBolded":false,"associatedRows":["Single patchD"],"associatedColumns":["FID ↓"],"associatedMergedColumns":[]}]},{"caption":"Table 5. \nThis method outperforms most of the other baselines, but \nstill performs worse than our method. \n\n","rows":["PasteGAN ( Li et al . 2019c ) †","OC - GAN ( ours )"],"columns":["VG","Inception Score ↑","COCO"],"mergedAllColumns":["FID ↓"],"numberCells":[{"number":"0.2","isBolded":true,"associatedRows":["PasteGAN ( Li et al . 2019c ) †"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":[]},{"number":"8.2±","isBolded":true,"associatedRows":["PasteGAN ( Li et al . 2019c ) †"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":[]},{"number":"38.29","isBolded":true,"associatedRows":["PasteGAN ( Li et al . 2019c ) †"],"associatedColumns":["Inception Score ↑","COCO","COCO"],"associatedMergedColumns":["FID ↓"]},{"number":"0.3","isBolded":false,"associatedRows":["OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":[]},{"number":"35.25","isBolded":true,"associatedRows":["PasteGAN ( Li et al . 2019c ) †"],"associatedColumns":["Inception Score ↑","VG","VG"],"associatedMergedColumns":["FID ↓"]},{"number":"10.5±","isBolded":false,"associatedRows":["OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":[]},{"number":"0.3","isBolded":false,"associatedRows":["OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":[]},{"number":"33.10","isBolded":false,"associatedRows":["OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO","COCO"],"associatedMergedColumns":["FID ↓"]},{"number":"10.2±","isBolded":true,"associatedRows":["PasteGAN ( Li et al . 2019c ) †"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":[]},{"number":"0.2","isBolded":true,"associatedRows":["PasteGAN ( Li et al . 2019c ) †"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":[]},{"number":"8.9±","isBolded":false,"associatedRows":["OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":[]},{"number":"22.61","isBolded":false,"associatedRows":["OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","VG","VG"],"associatedMergedColumns":["FID ↓"]}]},{"caption":"Table 5: Comparison of our method with the semi-\nparametric method PasteGAN ","rows":["PasteGAN ( Li et al . 2019c ) †","OC - GAN ( ours )"],"columns":["VG","Inception Score ↑","COCO"],"mergedAllColumns":["FID ↓"],"numberCells":[{"number":"10.2±","isBolded":true,"associatedRows":["PasteGAN ( Li et al . 2019c ) †"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":[]},{"number":"0.2","isBolded":true,"associatedRows":["PasteGAN ( Li et al . 2019c ) †"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":[]},{"number":"10.5±","isBolded":false,"associatedRows":["OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":[]},{"number":"38.29","isBolded":true,"associatedRows":["PasteGAN ( Li et al . 2019c ) †"],"associatedColumns":["Inception Score ↑","COCO","COCO"],"associatedMergedColumns":["FID ↓"]},{"number":"8.9±","isBolded":false,"associatedRows":["OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":[]},{"number":"33.10","isBolded":false,"associatedRows":["OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO","COCO"],"associatedMergedColumns":["FID ↓"]},{"number":"22.61","isBolded":false,"associatedRows":["OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","VG","VG"],"associatedMergedColumns":["FID ↓"]},{"number":"0.2","isBolded":true,"associatedRows":["PasteGAN ( Li et al . 2019c ) †"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":[]},{"number":"0.3","isBolded":false,"associatedRows":["OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","COCO"],"associatedMergedColumns":[]},{"number":"0.3","isBolded":false,"associatedRows":["OC - GAN ( ours )"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":[]},{"number":"35.25","isBolded":true,"associatedRows":["PasteGAN ( Li et al . 2019c ) †"],"associatedColumns":["Inception Score ↑","VG","VG"],"associatedMergedColumns":["FID ↓"]},{"number":"8.2±","isBolded":true,"associatedRows":["PasteGAN ( Li et al . 2019c ) †"],"associatedColumns":["Inception Score ↑","VG"],"associatedMergedColumns":[]}]},{"caption":"Table 6. \n\n","rows":["1","24","2","5","# Test Images","# Objects","# Valid Images","62","# Train Images"],"columns":["COCO - Stuff","VG"],"mergedAllColumns":[],"numberCells":[{"number":"088","isBolded":false,"associatedRows":["# Test Images","2","5"],"associatedColumns":["VG"],"associatedMergedColumns":[]},{"number":"048","isBolded":false,"associatedRows":["# Test Images","2"],"associatedColumns":["COCO - Stuff"],"associatedMergedColumns":[]},{"number":"171","isBolded":false,"associatedRows":["# Objects","2"],"associatedColumns":["COCO - Stuff"],"associatedMergedColumns":[]},{"number":"565","isBolded":false,"associatedRows":["# Train Images","24","62"],"associatedColumns":["VG"],"associatedMergedColumns":[]},{"number":"506","isBolded":false,"associatedRows":["# Valid Images","1","5"],"associatedColumns":["VG"],"associatedMergedColumns":[]},{"number":"972","isBolded":false,"associatedRows":["# Train Images","24"],"associatedColumns":["COCO - Stuff"],"associatedMergedColumns":[]},{"number":"024","isBolded":false,"associatedRows":["# Valid Images","1"],"associatedColumns":["COCO - Stuff"],"associatedMergedColumns":[]},{"number":"178","isBolded":false,"associatedRows":["# Objects","2","5"],"associatedColumns":["VG"],"associatedMergedColumns":[]}]},{"caption":"Table 6: Statistics of the COCO-Stuff and Visual Genome \ndatasets. \n\n","rows":["1","24","2","5","# Test Images","# Objects","# Valid Images","62","# Train Images"],"columns":["COCO - Stuff","VG"],"mergedAllColumns":[],"numberCells":[{"number":"565","isBolded":false,"associatedRows":["# Train Images","24","62"],"associatedColumns":["VG"],"associatedMergedColumns":[]},{"number":"024","isBolded":false,"associatedRows":["# Valid Images","1"],"associatedColumns":["COCO - Stuff"],"associatedMergedColumns":[]},{"number":"178","isBolded":false,"associatedRows":["# Objects","2","5"],"associatedColumns":["VG"],"associatedMergedColumns":[]},{"number":"171","isBolded":false,"associatedRows":["# Objects","2"],"associatedColumns":["COCO - Stuff"],"associatedMergedColumns":[]},{"number":"088","isBolded":false,"associatedRows":["# Test Images","2","5"],"associatedColumns":["VG"],"associatedMergedColumns":[]},{"number":"048","isBolded":false,"associatedRows":["# Test Images","2"],"associatedColumns":["COCO - Stuff"],"associatedMergedColumns":[]},{"number":"972","isBolded":false,"associatedRows":["# Train Images","24"],"associatedColumns":["COCO - Stuff"],"associatedMergedColumns":[]},{"number":"506","isBolded":false,"associatedRows":["# Valid Images","1","5"],"associatedColumns":["VG"],"associatedMergedColumns":[]}]}]