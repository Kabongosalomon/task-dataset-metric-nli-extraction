[{"caption":"Table 1: Classification accuracies for gradual self-train (ST) and baselines on 3 datasets, with 90% \nconfidence intervals for the mean over 5 runs. Gradual ST does better than self-training directly on \nthe target or self-training on all the unlabeled data pooled together. \n\n","rows":[],"columns":["Target ST","Gradual ST","Gaussian","Source","All ST"],"mergedAllColumns":[],"numberCells":[{"number":"5.1","isBolded":false,"associatedRows":[],"associatedColumns":["Gaussian","Source","Target ST","All ST","Gradual ST"],"associatedMergedColumns":[]}]},{"caption":"Table 2: Classification accuracies for gradual self-train with explicit regularization and hard labels \n(Gradual ST), without regularization but with hard labels (No Reg), and with regularization but \nwith soft labels (Soft Labels). Gradual self-train does best with explicit regularization and hard \nlabels, as our theory suggests, even for neural networks with implicit regularization. \n\n","rows":[],"columns":[],"mergedAllColumns":[],"numberCells":[]},{"caption":"Table 3: Classification accuracies for gradual self-train on rotating MNIST as we vary the number \nof samples. Unlike in previous experiments, here the same N samples are rotated, so the models \ndo not have to generalize to unseen images, but seen images at different angles. The gap between \nregularized and unregularized gradual self-training does not shrink much with more data. \n\n","rows":[],"columns":[],"mergedAllColumns":[],"numberCells":[]},{"caption":"Table 4: Classification accuracies for gradual self-train (ST) and baselines without confidence \nthresholding/filtering, with 90% confidence intervals for the mean over 5 runs. All methods do \nworse without confidence thresholding but gradual self-training does significantly better than the \nother methods. \n\n","rows":[],"columns":[],"mergedAllColumns":[],"numberCells":[]}]