[{"caption":"Table 1. Comparison with CaiT [33]. Our model exploits less \ntraining techniques, model size, and computations but achieve \nidentical result to CaiT. \nSettings \nLV-ViT (Ours) \nCaiT [33] \n\nTransformer Blocks \n20 \n36 \n#Head in Self-attention \n8 \n12 \nMLP Expansion Ratio \n3 \n4 \nEmbedding Dimension \n512 \n384 \nStochastic Depth ","rows":["ImageNet Top - 1 Acc .","Training Epoch","Test Resolution","Embedding Dimension"],"columns":["12","36","42B","69M","56M","3","4","Our model exploits less","LV - ViT ( Ours )","CaiT [ 33 ]","8","20","Table 1 . Comparison with CaiT [ 33 ] .","48B"],"mergedAllColumns":["identical result to CaiT .","Token Labeling ( Ours )"],"numberCells":[{"number":"384×","isBolded":true,"associatedRows":["Test Resolution"],"associatedColumns":["Our model exploits less","CaiT [ 33 ]","36","12","4"],"associatedMergedColumns":["Token Labeling ( Ours )"]},{"number":"85.4","isBolded":false,"associatedRows":["ImageNet Top - 1 Acc ."],"associatedColumns":["Our model exploits less","CaiT [ 33 ]","36","12","4","69M","48B"],"associatedMergedColumns":["Token Labeling ( Ours )"]},{"number":"384","isBolded":true,"associatedRows":["Test Resolution"],"associatedColumns":["Our model exploits less","CaiT [ 33 ]","36","12","4"],"associatedMergedColumns":["Token Labeling ( Ours )"]},{"number":"384","isBolded":false,"associatedRows":["Embedding Dimension"],"associatedColumns":["Our model exploits less","CaiT [ 33 ]","36","12","4"],"associatedMergedColumns":["identical result to CaiT ."]},{"number":"400","isBolded":false,"associatedRows":["Training Epoch"],"associatedColumns":["Our model exploits less","CaiT [ 33 ]","36","12","4","69M","48B"],"associatedMergedColumns":["Token Labeling ( Ours )"]},{"number":"384×","isBolded":true,"associatedRows":["Test Resolution"],"associatedColumns":["Table 1 . Comparison with CaiT [ 33 ] .","LV - ViT ( Ours )","20","8","3"],"associatedMergedColumns":["Token Labeling ( Ours )"]},{"number":"512","isBolded":false,"associatedRows":["Embedding Dimension"],"associatedColumns":["Our model exploits less","LV - ViT ( Ours )","20","8","3"],"associatedMergedColumns":["identical result to CaiT ."]},{"number":"300","isBolded":false,"associatedRows":["Training Epoch"],"associatedColumns":["Our model exploits less","LV - ViT ( Ours )","20","8","3","56M","42B"],"associatedMergedColumns":["Token Labeling ( Ours )"]},{"number":"85.4","isBolded":false,"associatedRows":["ImageNet Top - 1 Acc ."],"associatedColumns":["Table 1 . Comparison with CaiT [ 33 ] .","LV - ViT ( Ours )","20","8","3","56M","42B"],"associatedMergedColumns":["Token Labeling ( Ours )"]},{"number":"384","isBolded":true,"associatedRows":["Test Resolution"],"associatedColumns":["Our model exploits less","LV - ViT ( Ours )","20","8","3"],"associatedMergedColumns":["Token Labeling ( Ours )"]}]},{"caption":"Table 2. Default hyper-parameters for our experiments. Note that \nwe do not use the MixUp augmentation method when re-labeling \nand token labeling are used. \n\n","rows":["Stoch . Depth","Weight decay","MixUp alpha","Erasing prob ."],"columns":["Token labeling","0","300","1e - 3 · batch size","5","1024","1e - 3· batch size","cosine","Standard","Re - labeling","-"],"mergedAllColumns":[],"numberCells":[{"number":"0.25","isBolded":false,"associatedRows":["Erasing prob ."],"associatedColumns":["Standard","300","1024","1e - 3 · batch size","cosine","5","0","-"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["Stoch . Depth"],"associatedColumns":["Standard","300","1024","1e - 3 · batch size","cosine","5","0"],"associatedMergedColumns":[]},{"number":"0.25","isBolded":false,"associatedRows":["Erasing prob ."],"associatedColumns":["Token labeling","300","1024","1e - 3· batch size","cosine","5","0","-"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["Stoch . Depth"],"associatedColumns":["Token labeling","300","1024","1e - 3· batch size","cosine","5","0"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["Stoch . Depth"],"associatedColumns":["Re - labeling","300","1024","1e - 3· batch size","cosine","5","0"],"associatedMergedColumns":[]},{"number":"0.25","isBolded":false,"associatedRows":["Erasing prob ."],"associatedColumns":["Re - labeling","300","1024","1e - 3· batch size","cosine","5","0","-"],"associatedMergedColumns":[]},{"number":"0.05","isBolded":false,"associatedRows":["Weight decay"],"associatedColumns":["Token labeling","300","1024","1e - 3· batch size","cosine"],"associatedMergedColumns":[]},{"number":"0.05","isBolded":false,"associatedRows":["Weight decay"],"associatedColumns":["Standard","300","1024","1e - 3 · batch size","cosine"],"associatedMergedColumns":[]},{"number":"0.8","isBolded":false,"associatedRows":["MixUp alpha"],"associatedColumns":["Standard","300","1024","1e - 3 · batch size","cosine","5","0"],"associatedMergedColumns":[]},{"number":"0.05","isBolded":false,"associatedRows":["Weight decay"],"associatedColumns":["Re - labeling","300","1024","1e - 3· batch size","cosine"],"associatedMergedColumns":[]}]},{"caption":"Table 4. Ablation on patch embedding. Baseline is set as 16 layer \nViT with embedding size 384 and MLP expansion ratio of 3. All \nconvolution layers except the last block have 64 filters. #Convs in-\ndicatie the total number of convolution for patch embedding, while \nthe kernel size and stride correspond to each layer are shown as a \nlist in the ","rows":["[ 16 ]","[ 2 , 1 , 1 , 1 , 1 , 8 ]","[ 7 , 3 , 8 ]","[ 2 , 8 ]","26M","25M","[ 7 , 3 , 3 , 3 , 3 , 8 ]","1","[ 2 , 2 , 1 , 4 ]","2","3","4","[ 7 , 8 ]","6","[ 2 , 1 , 1 , 8 ]","[ 2 , 1 , 8 ]","[ 7 , 3 , 3 , 8 ]","[ 2 , 2 , 4 ]"],"columns":["Top - 1 Acc . ( % )","table ."],"mergedAllColumns":[],"numberCells":[{"number":"82.2","isBolded":true,"associatedRows":["6","[ 7 , 3 , 3 , 3 , 3 , 8 ]","[ 2 , 1 , 1 , 1 , 1 , 8 ]","26M"],"associatedColumns":["table .","Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"81.4","isBolded":false,"associatedRows":["2","[ 7 , 8 ]","[ 2 , 8 ]","25M"],"associatedColumns":["table .","Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"81.9","isBolded":false,"associatedRows":["3","[ 7 , 3 , 8 ]","[ 2 , 1 , 8 ]","26M"],"associatedColumns":["table .","Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"82.2","isBolded":true,"associatedRows":["4","[ 7 , 3 , 3 , 8 ]","[ 2 , 1 , 1 , 8 ]","26M"],"associatedColumns":["table .","Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"81.1","isBolded":false,"associatedRows":["1","[ 16 ]","[ 16 ]","25M"],"associatedColumns":["table .","Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"81.5","isBolded":false,"associatedRows":["4","[ 7 , 3 , 3 , 8 ]","[ 2 , 2 , 1 , 4 ]","26M"],"associatedColumns":["table .","Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"81.4","isBolded":false,"associatedRows":["3","[ 7 , 3 , 8 ]","[ 2 , 2 , 4 ]","25M"],"associatedColumns":["table .","Top - 1 Acc . ( % )"],"associatedMergedColumns":[]}]},{"caption":"Table 5. Ablation on enhancing residual connection by applying \na scaling factor. Baseline is a 16-layer vision transformer with 4-\nlayer convolutional patch embedding. Here, function F represents \neither self-attention (SA) or feed forward (FF). \n\n","rows":["[ 16 ]","[ 2 , 1 , 1 , 1 , 1 , 8 ]","[ 7 , 3 , 8 ]","[ 2 , 8 ]","26M","25M","[ 7 , 3 , 3 , 3 , 3 , 8 ]","1","[ 2 , 2 , 1 , 4 ]","2","3","4","[ 7 , 8 ]","6","[ 2 , 1 , 1 , 8 ]","[ 2 , 1 , 8 ]","[ 7 , 3 , 3 , 8 ]","[ 2 , 2 , 4 ]"],"columns":["Top - 1 Acc . ( % )","table ."],"mergedAllColumns":[],"numberCells":[{"number":"82.2","isBolded":true,"associatedRows":["6","[ 7 , 3 , 3 , 3 , 3 , 8 ]","[ 2 , 1 , 1 , 1 , 1 , 8 ]","26M"],"associatedColumns":["table .","Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"81.1","isBolded":false,"associatedRows":["1","[ 16 ]","[ 16 ]","25M"],"associatedColumns":["table .","Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"81.9","isBolded":false,"associatedRows":["3","[ 7 , 3 , 8 ]","[ 2 , 1 , 8 ]","26M"],"associatedColumns":["table .","Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"82.2","isBolded":true,"associatedRows":["4","[ 7 , 3 , 3 , 8 ]","[ 2 , 1 , 1 , 8 ]","26M"],"associatedColumns":["table .","Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"81.4","isBolded":false,"associatedRows":["2","[ 7 , 8 ]","[ 2 , 8 ]","25M"],"associatedColumns":["table .","Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"81.4","isBolded":false,"associatedRows":["3","[ 7 , 3 , 8 ]","[ 2 , 2 , 4 ]","25M"],"associatedColumns":["table .","Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"81.5","isBolded":false,"associatedRows":["4","[ 7 , 3 , 3 , 8 ]","[ 2 , 2 , 1 , 4 ]","26M"],"associatedColumns":["table .","Top - 1 Acc . ( % )"],"associatedMergedColumns":[]}]},{"caption":"Table 7. Ablation on different widely-used data augmentations. \nWe empirically found that our proposed MixToken performs even \nbetter than the combination of MixUp and CutMix in vision trans-\nformers. \nMixToken MixUp CutOut RandAug Top-1 Acc. (%) \n\n\n83.3 \n\n\n81.3 \n83.1 \n\n\n83.0 \n\n\n82.8 \n\n","rows":[],"columns":["Top - 1 Acc . ( % )","Table 7 . Ablation on different widely - used data augmentations ."],"mergedAllColumns":["formers ."],"numberCells":[{"number":"82.8","isBolded":false,"associatedRows":[],"associatedColumns":["Table 7 . Ablation on different widely - used data augmentations .","Top - 1 Acc . ( % )"],"associatedMergedColumns":["formers ."]},{"number":"83.3","isBolded":true,"associatedRows":[],"associatedColumns":["Table 7 . Ablation on different widely - used data augmentations .","Top - 1 Acc . ( % )"],"associatedMergedColumns":["formers ."]},{"number":"81.3","isBolded":false,"associatedRows":[],"associatedColumns":["Table 7 . Ablation on different widely - used data augmentations .","Top - 1 Acc . ( % )"],"associatedMergedColumns":["formers ."]},{"number":"83.1","isBolded":false,"associatedRows":[],"associatedColumns":["Table 7 . Ablation on different widely - used data augmentations .","Top - 1 Acc . ( % )"],"associatedMergedColumns":["formers ."]},{"number":"83.0","isBolded":false,"associatedRows":[],"associatedColumns":["Table 7 . Ablation on different widely - used data augmentations .","Top - 1 Acc . ( % )"],"associatedMergedColumns":["formers ."]}]},{"caption":"Table 8. Performance of the proposed LV-ViT with different model \nsizes. Here, \u0027depth\u0027 denotes the number of transformer blocks \nused in different models. By default, the test resolution is set to \n224 × 224 except the last one which is 288 × 288. \n\n","rows":["12","LV - ViT - L","16 layer vision transformer with","LV - ViT - M","24","16","150M","LV - ViT - S","LV - ViT - T","26M","56M","20"],"columns":["Embed dim .","Top - 1 Acc . ( % )","#Parameters","The baseline is set as"],"mergedAllColumns":[],"numberCells":[{"number":"83.3","isBolded":false,"associatedRows":["LV - ViT - S","16","26M"],"associatedColumns":["Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"79.1","isBolded":false,"associatedRows":["LV - ViT - T","12","26M"],"associatedColumns":["Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"512","isBolded":false,"associatedRows":["LV - ViT - M","20"],"associatedColumns":["Embed dim ."],"associatedMergedColumns":[]},{"number":"8.5M","isBolded":false,"associatedRows":["LV - ViT - T","12"],"associatedColumns":["#Parameters"],"associatedMergedColumns":[]},{"number":"85.3","isBolded":true,"associatedRows":["LV - ViT - L","24","150M"],"associatedColumns":["Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"240","isBolded":false,"associatedRows":["LV - ViT - T","12"],"associatedColumns":["Embed dim ."],"associatedMergedColumns":[]},{"number":"768","isBolded":false,"associatedRows":["LV - ViT - L","24"],"associatedColumns":["Embed dim ."],"associatedMergedColumns":[]},{"number":"384embeddingdimensionand4","isBolded":false,"associatedRows":["16 layer vision transformer with"],"associatedColumns":["Top - 1 Acc . ( % )","The baseline is set as"],"associatedMergedColumns":[]},{"number":"384","isBolded":false,"associatedRows":["LV - ViT - S","16"],"associatedColumns":["Embed dim ."],"associatedMergedColumns":[]},{"number":"84.0","isBolded":false,"associatedRows":["LV - ViT - M","20","56M"],"associatedColumns":["Top - 1 Acc . ( % )"],"associatedMergedColumns":[]}]},{"caption":"Name \nDepth Embed dim. #Parameters Top-1 Acc. (%) \n\nLV-ViT-T \n12 \n240 \n8.5M \n79.1 \nLV-ViT-S \n16 \n384 \n26M \n83.3 \nLV-ViT-M 20 \n512 \n56M \n84.0 \nLV-ViT-L \n24 \n768 \n150M \n85.3 \n\nTable 9. Ablation on normalization layer. The baseline is set as \n16 layer vision transformer with 384 embedding dimension and 4 \nlayer convolutional patch embedding. \n\n","rows":["GroupNorm","1","2","3","mension to 192 , we can achieve a top - 1 accuracy of","6","LayerNorm","26M","-","to other methods can be found in Sec"],"columns":["Top - 1 Acc . ( % )","#Parameters"],"mergedAllColumns":["using 16 transformer blocks and setting the embedding di -","further boost the performance . More experiments compared"],"numberCells":[{"number":"82.2","isBolded":false,"associatedRows":["GroupNorm","2","26M"],"associatedColumns":["Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"81.9","isBolded":false,"associatedRows":["GroupNorm","3","26M"],"associatedColumns":["Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"82.2","isBolded":false,"associatedRows":["GroupNorm","1","26M"],"associatedColumns":["Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"81.8","isBolded":false,"associatedRows":["GroupNorm","6","26M"],"associatedColumns":["Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"79.1.","isBolded":false,"associatedRows":["mension to 192 , we can achieve a top - 1 accuracy of"],"associatedColumns":["Top - 1 Acc . ( % )"],"associatedMergedColumns":["using 16 transformer blocks and setting the embedding di -"]},{"number":"82.2","isBolded":false,"associatedRows":["LayerNorm","-","26M"],"associatedColumns":["Top - 1 Acc . ( % )"],"associatedMergedColumns":[]},{"number":"4.4.","isBolded":false,"associatedRows":["to other methods can be found in Sec"],"associatedColumns":["#Parameters"],"associatedMergedColumns":["further boost the performance . More experiments compared"]}]}]