[{"caption":"Method \nACC \n\nPhone classification \nRandom initialization \n27.6 \nMFCC features \n39.7 \nCPC \n64.6 \nSupervised \n74.6 \n\nSpeaker classification \nRandom initialization \n1.87 \nMFCC features \n17.6 \nCPC \n97.4 \nSupervised \n98.5 \n\nTable 1: LibriSpeech phone and speaker \nclassification results. For phone classifi-\ncation there are 41 possible classes and \nfor speaker classification 251. All mod-\nels used the same architecture and the \nsame audio input sizes. \n\n","rows":["Random initialization","CPC","Supervised","MFCC features"],"columns":["ACC"],"mergedAllColumns":["Phone classification","Speaker classification"],"numberCells":[{"number":"1.87","isBolded":false,"associatedRows":["Random initialization"],"associatedColumns":["ACC"],"associatedMergedColumns":["Speaker classification"]},{"number":"98.5","isBolded":false,"associatedRows":["Supervised"],"associatedColumns":["ACC"],"associatedMergedColumns":["Speaker classification"]},{"number":"97.4","isBolded":false,"associatedRows":["CPC"],"associatedColumns":["ACC"],"associatedMergedColumns":["Speaker classification"]},{"number":"64.6","isBolded":false,"associatedRows":["CPC"],"associatedColumns":["ACC"],"associatedMergedColumns":["Phone classification"]},{"number":"74.6","isBolded":false,"associatedRows":["Supervised"],"associatedColumns":["ACC"],"associatedMergedColumns":["Phone classification"]},{"number":"17.6","isBolded":false,"associatedRows":["MFCC features"],"associatedColumns":["ACC"],"associatedMergedColumns":["Speaker classification"]},{"number":"39.7","isBolded":false,"associatedRows":["MFCC features"],"associatedColumns":["ACC"],"associatedMergedColumns":["Phone classification"]},{"number":"27.6","isBolded":false,"associatedRows":["Random initialization"],"associatedColumns":["ACC"],"associatedMergedColumns":["Phone classification"]}]},{"caption":"Table 2: LibriSpeech phone classifica-\ntion ablation experiments. More details \ncan be found in Section 3.1. \n\n","rows":["4 steps","16 steps","2 steps","Same speaker ( excl . )","Mixed speaker","Same speaker","Mixed speaker ( excl . )","Current sequence only","8 steps","12 steps"],"columns":["ACC"],"mergedAllColumns":["#steps predicted","Negative samples from"],"numberCells":[{"number":"28.5","isBolded":false,"associatedRows":["2 steps"],"associatedColumns":["ACC"],"associatedMergedColumns":["#steps predicted"]},{"number":"57.3","isBolded":false,"associatedRows":["Mixed speaker ( excl . )"],"associatedColumns":["ACC"],"associatedMergedColumns":["Negative samples from"]},{"number":"65.2","isBolded":false,"associatedRows":["Current sequence only"],"associatedColumns":["ACC"],"associatedMergedColumns":["Negative samples from"]},{"number":"64.6","isBolded":false,"associatedRows":["Mixed speaker"],"associatedColumns":["ACC"],"associatedMergedColumns":["Negative samples from"]},{"number":"65.5","isBolded":false,"associatedRows":["Same speaker"],"associatedColumns":["ACC"],"associatedMergedColumns":["Negative samples from"]},{"number":"57.6","isBolded":false,"associatedRows":["4 steps"],"associatedColumns":["ACC"],"associatedMergedColumns":["#steps predicted"]},{"number":"63.8","isBolded":false,"associatedRows":["16 steps"],"associatedColumns":["ACC"],"associatedMergedColumns":["#steps predicted"]},{"number":"64.6","isBolded":false,"associatedRows":["Same speaker ( excl . )"],"associatedColumns":["ACC"],"associatedMergedColumns":["Negative samples from"]},{"number":"63.6","isBolded":false,"associatedRows":["8 steps"],"associatedColumns":["ACC"],"associatedMergedColumns":["#steps predicted"]},{"number":"64.6","isBolded":false,"associatedRows":["12 steps"],"associatedColumns":["ACC"],"associatedMergedColumns":["#steps predicted"]}]},{"caption":"Table 3: ImageNet top-1 unsupervised classifi-\ncation results. *Jigsaw is not directly compa-\nrable to the other AlexNet results because of \narchitectural differences. \n\n","rows":["Video [ 28 ]","BiGan [ 35 ]","Exemplar [ 36 ]","Relative Position [ 11 ]","Colorization [ 10 ]","CPC","Jigsaw [ 29 ] *","Motion Segmentation [ 36 ]","Relative Position [ 36 ]","Colorization [ 36 ]"],"columns":["Top - 1 ACC"],"mergedAllColumns":["Using AlexNet conv5","Using ResNet - V2"],"numberCells":[{"number":"29.8","isBolded":false,"associatedRows":["Video [ 28 ]"],"associatedColumns":["Top - 1 ACC"],"associatedMergedColumns":["Using AlexNet conv5"]},{"number":"38.1","isBolded":false,"associatedRows":["Jigsaw [ 29 ] *"],"associatedColumns":["Top - 1 ACC"],"associatedMergedColumns":["Using AlexNet conv5"]},{"number":"48.7","isBolded":true,"associatedRows":["CPC"],"associatedColumns":["Top - 1 ACC"],"associatedMergedColumns":["Using ResNet - V2"]},{"number":"34.8","isBolded":false,"associatedRows":["BiGan [ 35 ]"],"associatedColumns":["Top - 1 ACC"],"associatedMergedColumns":["Using AlexNet conv5"]},{"number":"31.5","isBolded":false,"associatedRows":["Exemplar [ 36 ]"],"associatedColumns":["Top - 1 ACC"],"associatedMergedColumns":["Using ResNet - V2"]},{"number":"35.2","isBolded":false,"associatedRows":["Colorization [ 10 ]"],"associatedColumns":["Top - 1 ACC"],"associatedMergedColumns":["Using AlexNet conv5"]},{"number":"30.4","isBolded":false,"associatedRows":["Relative Position [ 11 ]"],"associatedColumns":["Top - 1 ACC"],"associatedMergedColumns":["Using AlexNet conv5"]},{"number":"39.6","isBolded":false,"associatedRows":["Colorization [ 36 ]"],"associatedColumns":["Top - 1 ACC"],"associatedMergedColumns":["Using ResNet - V2"]},{"number":"27.6","isBolded":false,"associatedRows":["Motion Segmentation [ 36 ]"],"associatedColumns":["Top - 1 ACC"],"associatedMergedColumns":["Using ResNet - V2"]},{"number":"36.2","isBolded":false,"associatedRows":["Relative Position [ 36 ]"],"associatedColumns":["Top - 1 ACC"],"associatedMergedColumns":["Using ResNet - V2"]}]},{"caption":"Table 4: ImageNet top-5 unsupervised classi-\nfication results. Previous results with MS, Ex, \nRP and Col were taken from [36] and are the \nbest reported results on this task. \n\n","rows":["Colorization ( Col )","MS + Ex + RP + Col","Exemplar ( Ex )","CPC","Relative Position ( RP )","Motion Segmentation ( MS )"],"columns":["Top - 5 ACC"],"mergedAllColumns":["Combination of"],"numberCells":[{"number":"62.5","isBolded":false,"associatedRows":["Colorization ( Col )"],"associatedColumns":["Top - 5 ACC"],"associatedMergedColumns":[]},{"number":"69.3","isBolded":false,"associatedRows":["MS + Ex + RP + Col"],"associatedColumns":["Top - 5 ACC"],"associatedMergedColumns":["Combination of"]},{"number":"53.1","isBolded":false,"associatedRows":["Exemplar ( Ex )"],"associatedColumns":["Top - 5 ACC"],"associatedMergedColumns":[]},{"number":"48.3","isBolded":false,"associatedRows":["Motion Segmentation ( MS )"],"associatedColumns":["Top - 5 ACC"],"associatedMergedColumns":[]},{"number":"73.6","isBolded":true,"associatedRows":["CPC"],"associatedColumns":["Top - 5 ACC"],"associatedMergedColumns":["Combination of"]},{"number":"59.2","isBolded":false,"associatedRows":["Relative Position ( RP )"],"associatedColumns":["Top - 5 ACC"],"associatedMergedColumns":[]}]},{"caption":"Table 5: Classification accuracy on five common NLP benchmarks. We follow the same transfer \nlearning setup from Skip-thought vectors ","rows":["Skip - thought vector [ 26 ]","CPC","Skip - thought + LN [ 41 ]","Paragraph - vector [ 40 ]"],"columns":["MR","MPQA","TREC","Subj","CR"],"mergedAllColumns":["-"],"numberCells":[{"number":"74.8","isBolded":false,"associatedRows":["Paragraph - vector [ 40 ]"],"associatedColumns":["MR"],"associatedMergedColumns":[]},{"number":"75.5","isBolded":false,"associatedRows":["Skip - thought vector [ 26 ]"],"associatedColumns":["MR"],"associatedMergedColumns":[]},{"number":"93.4","isBolded":false,"associatedRows":["Skip - thought + LN [ 41 ]"],"associatedColumns":["Subj"],"associatedMergedColumns":[]},{"number":"74.2","isBolded":false,"associatedRows":["Paragraph - vector [ 40 ]"],"associatedColumns":["MPQA"],"associatedMergedColumns":[]},{"number":"91.8","isBolded":false,"associatedRows":["Paragraph - vector [ 40 ]"],"associatedColumns":["TREC"],"associatedMergedColumns":[]},{"number":"79.3","isBolded":false,"associatedRows":["Skip - thought vector [ 26 ]"],"associatedColumns":["CR"],"associatedMergedColumns":[]},{"number":"90.5","isBolded":false,"associatedRows":["Paragraph - vector [ 40 ]"],"associatedColumns":["Subj"],"associatedMergedColumns":[]},{"number":"82.6","isBolded":false,"associatedRows":["Skip - thought + LN [ 41 ]"],"associatedColumns":["CR"],"associatedMergedColumns":[]},{"number":"96.8","isBolded":false,"associatedRows":["CPC"],"associatedColumns":["TREC"],"associatedMergedColumns":["-"]},{"number":"76.9","isBolded":false,"associatedRows":["CPC"],"associatedColumns":["MR"],"associatedMergedColumns":["-"]},{"number":"87.7","isBolded":false,"associatedRows":["CPC"],"associatedColumns":["MPQA"],"associatedMergedColumns":["-"]},{"number":"86.9","isBolded":false,"associatedRows":["Skip - thought vector [ 26 ]"],"associatedColumns":["MPQA"],"associatedMergedColumns":[]},{"number":"79.5","isBolded":false,"associatedRows":["Skip - thought + LN [ 41 ]"],"associatedColumns":["MR"],"associatedMergedColumns":[]},{"number":"89.0","isBolded":false,"associatedRows":["Skip - thought + LN [ 41 ]"],"associatedColumns":["MPQA"],"associatedMergedColumns":[]},{"number":"91.2","isBolded":false,"associatedRows":["CPC"],"associatedColumns":["Subj"],"associatedMergedColumns":["-"]},{"number":"80.1","isBolded":false,"associatedRows":["CPC"],"associatedColumns":["CR"],"associatedMergedColumns":["-"]},{"number":"78.1","isBolded":false,"associatedRows":["Paragraph - vector [ 40 ]"],"associatedColumns":["CR"],"associatedMergedColumns":[]},{"number":"92.1","isBolded":false,"associatedRows":["Skip - thought vector [ 26 ]"],"associatedColumns":["Subj"],"associatedMergedColumns":[]},{"number":"91.4","isBolded":false,"associatedRows":["Skip - thought vector [ 26 ]"],"associatedColumns":["TREC"],"associatedMergedColumns":[]}]}]