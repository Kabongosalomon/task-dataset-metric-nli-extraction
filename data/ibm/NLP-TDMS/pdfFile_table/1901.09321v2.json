[{"caption":"10 \n100 \n1000 \n10000 \n\nDepth \n\n25 \n\n30 \n\n35 \n\n40 \n\n45 \n\n50 \n\n55 \n\nFirst Epoch Test Accuracy (%) \n\n1 / 2 -scaling \n\nLSUV \nBatchNorm \nFixup \n\nFigure 3: Depth of residual networks versus test accuracy at the first epoch for various methods on \nCIFAR-10 with the default BatchNorm learning rate. We observe that Fixup is able to train very \ndeep networks with the same learning rate as batch normalization. (Higher is better.) \n\nFigure 3 shows the test accuracy at the first epoch as depth increases. Observe that Fixup matches \nthe performance of BatchNorm at the first epoch, even with 10,000 layers. LSUV and 1 / 2 -scaling \nare not able to train with the same learning rate as BatchNorm past 100 layers. \n\n4.2 IMAGE CLASSIFICATION \n\nIn this section, we evaluate the ability of Fixup to replace batch normalization in image classification \napplications. On the CIFAR-10 dataset, we first test on ResNet-110 (He et al., 2016) with default \nhyper-parameters; results are shown in Table 1. Fixup obtains 7% relative improvement in test error \ncompared with standard initialization; however, we note a substantial difference in the difficulty of \ntraining. While network with Fixup is trained with the same learning rate and converge as fast as \nnetwork with batch normalization, we fail to train a Xavier initialized ResNet-110 with 0.1x maximal \nlearning rate. 5 The test error gap in Table 1 is likely due to the regularization effect of BatchNorm \n","rows":["Accuracy","( % )","Test","Epoch","First","network with batch normalization , we fail to train a Xavier initialized ResNet - 110 with"],"columns":["1 /","1 / 2 - scaling","100","the performance of BatchNorm at the first epoch , even with 10 , 000 layers . LSUV and","10000","We observe that Fixup is able to train very","2 - scaling","CIFAR - 10 with the default BatchNorm learning rate ."],"mergedAllColumns":["BatchNorm","training . While network with Fixup is trained with the same learning rate and converge as fast as","Fixup","LSUV","are not able to train with the same learning rate as BatchNorm past 100 layers ."],"numberCells":[{"number":"50","isBolded":false,"associatedRows":["Accuracy"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"30","isBolded":false,"associatedRows":["Epoch"],"associatedColumns":["1 / 2 - scaling"],"associatedMergedColumns":["Fixup"]},{"number":"45","isBolded":false,"associatedRows":["Accuracy"],"associatedColumns":["1 / 2 - scaling"],"associatedMergedColumns":["LSUV"]},{"number":"55","isBolded":false,"associatedRows":["( % )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"25","isBolded":false,"associatedRows":["First"],"associatedColumns":["1 / 2 - scaling"],"associatedMergedColumns":["Fixup"]},{"number":"10","isBolded":false,"associatedRows":["First"],"associatedColumns":["1 / 2 - scaling"],"associatedMergedColumns":["Fixup"]},{"number":"40","isBolded":false,"associatedRows":["Test"],"associatedColumns":["1 / 2 - scaling"],"associatedMergedColumns":["BatchNorm"]},{"number":"35","isBolded":false,"associatedRows":["Epoch"],"associatedColumns":["1 / 2 - scaling"],"associatedMergedColumns":["Fixup"]},{"number":"4.2","isBolded":true,"associatedRows":[],"associatedColumns":["1 / 2 - scaling","100","CIFAR - 10 with the default BatchNorm learning rate .","the performance of BatchNorm at the first epoch , even with 10 , 000 layers . LSUV and"],"associatedMergedColumns":["are not able to train with the same learning rate as BatchNorm past 100 layers ."]},{"number":"0.1xmaximal","isBolded":true,"associatedRows":["network with batch normalization , we fail to train a Xavier initialized ResNet - 110 with"],"associatedColumns":["1 / 2 - scaling","10000","We observe that Fixup is able to train very","1 /","2 - scaling"],"associatedMergedColumns":["training . While network with Fixup is trained with the same learning rate and converge as fast as"]}]},{"caption":"rather than difficulty in optimization; when we train Fixup networks with better regularization, the \ntest error gap disappears and we obtain state-of-the-art results on CIFAR-10 and SVHN without \nnormalization layers (see Appendix C.2). \n\nDataset \nResNet-110 \nNormalization Large Î· Test Error (%) \n\nCIFAR-10 \n\nw/ BatchNorm (He et al., 2016) \n\n\n6.61 \n\nw/ Xavier Init (Shang et al., 2017) \n\n\n7.78 \nw/ Fixup-init \n\n\n7.24 \n\nTable 1: Results on CIFAR-10 with ResNet-110 (mean/median of 5 runs; lower is better). \n\n","rows":["Xavier Init ( Shang et al . , 2017 )","Fixup - init + Mixup","BatchNorm + Mixup ( Zhang et al . , 2017 )","GroupNorm + Mixup","Mixup coefficients are found through cross - validation : they are","BatchNorm ( Goyal et al . , 2017 )","Fixup - init","BatchNorm ( Zhang et al . , 2017 )"],"columns":["Similar to our finding on","Further inspection reveals that","Test Error ( % )","On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","( He et al . , 2016 ) , trained for 100 epochs and 200 epochs respectively .","( 3 ) there is a large test error gap between Fixup and BatchNorm .","rate of the scalar multiplier and bias by 10x when additional large regularization is used ."],"mergedAllColumns":["ResNet - 50","with better regularization , the performance of Fixup is on par with GroupNorm .","to the Fixup models using Mixup ( Zhang et al . , 2017 ) . We find it is beneficial to reduce the learning","ResNet - 101"],"numberCells":[{"number":"23.3","isBolded":true,"associatedRows":["BatchNorm + Mixup ( Zhang et al . , 2017 )"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["with better regularization , the performance of Fixup is on par with GroupNorm ."]},{"number":"0.1and","isBolded":true,"associatedRows":["Mixup coefficients are found through cross - validation : they are"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","( He et al . , 2016 ) , trained for 100 epochs and 200 epochs respectively .","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used ."],"associatedMergedColumns":["to the Fixup models using Mixup ( Zhang et al . , 2017 ) . We find it is beneficial to reduce the learning"]},{"number":"0.2,","isBolded":true,"associatedRows":["Mixup coefficients are found through cross - validation : they are"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","( He et al . , 2016 ) , trained for 100 epochs and 200 epochs respectively .","( 3 ) there is a large test error gap between Fixup and BatchNorm .","rate of the scalar multiplier and bias by 10x when additional large regularization is used ."],"associatedMergedColumns":["to the Fixup models using Mixup ( Zhang et al . , 2017 ) . We find it is beneficial to reduce the learning"]},{"number":"27.6","isBolded":false,"associatedRows":["Fixup - init"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["ResNet - 50"]},{"number":"21.4","isBolded":false,"associatedRows":["Fixup - init + Mixup"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["ResNet - 101"]},{"number":"31.5","isBolded":false,"associatedRows":["Xavier Init ( Shang et al . , 2017 )"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["ResNet - 50"]},{"number":"0.7forBatchNorm,","isBolded":true,"associatedRows":["Mixup coefficients are found through cross - validation : they are"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used ."],"associatedMergedColumns":["to the Fixup models using Mixup ( Zhang et al . , 2017 ) . We find it is beneficial to reduce the learning"]},{"number":"22.0","isBolded":false,"associatedRows":["BatchNorm ( Zhang et al . , 2017 )"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["ResNet - 50"]},{"number":"20.8","isBolded":true,"associatedRows":["BatchNorm + Mixup ( Zhang et al . , 2017 )"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["ResNet - 50"]},{"number":"21.4","isBolded":false,"associatedRows":["GroupNorm + Mixup"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["ResNet - 50"]},{"number":"23.6","isBolded":false,"associatedRows":["BatchNorm ( Goyal et al . , 2017 )"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["with better regularization , the performance of Fixup is on par with GroupNorm ."]},{"number":"23.9","isBolded":false,"associatedRows":["GroupNorm + Mixup"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["with better regularization , the performance of Fixup is on par with GroupNorm ."]},{"number":"24.0","isBolded":true,"associatedRows":["Fixup - init + Mixup"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["ResNet - 50"]}]},{"caption":"Table 2: ImageNet test results using the ResNet architecture. (Lower is better.) \n\n","rows":["Xavier Init ( Shang et al . , 2017 )","Fixup - init + Mixup","BatchNorm + Mixup ( Zhang et al . , 2017 )","GroupNorm + Mixup","Mixup coefficients are found through cross - validation : they are","BatchNorm ( Goyal et al . , 2017 )","Fixup - init","BatchNorm ( Zhang et al . , 2017 )"],"columns":["Similar to our finding on","Further inspection reveals that","Test Error ( % )","On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","( He et al . , 2016 ) , trained for 100 epochs and 200 epochs respectively .","( 3 ) there is a large test error gap between Fixup and BatchNorm .","rate of the scalar multiplier and bias by 10x when additional large regularization is used ."],"mergedAllColumns":["ResNet - 50","with better regularization , the performance of Fixup is on par with GroupNorm .","to the Fixup models using Mixup ( Zhang et al . , 2017 ) . We find it is beneficial to reduce the learning","ResNet - 101"],"numberCells":[{"number":"0.7forBatchNorm,","isBolded":true,"associatedRows":["Mixup coefficients are found through cross - validation : they are"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used ."],"associatedMergedColumns":["to the Fixup models using Mixup ( Zhang et al . , 2017 ) . We find it is beneficial to reduce the learning"]},{"number":"22.0","isBolded":false,"associatedRows":["BatchNorm ( Zhang et al . , 2017 )"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["ResNet - 50"]},{"number":"27.6","isBolded":false,"associatedRows":["Fixup - init"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["ResNet - 50"]},{"number":"0.2,","isBolded":true,"associatedRows":["Mixup coefficients are found through cross - validation : they are"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","( He et al . , 2016 ) , trained for 100 epochs and 200 epochs respectively .","( 3 ) there is a large test error gap between Fixup and BatchNorm .","rate of the scalar multiplier and bias by 10x when additional large regularization is used ."],"associatedMergedColumns":["to the Fixup models using Mixup ( Zhang et al . , 2017 ) . We find it is beneficial to reduce the learning"]},{"number":"20.8","isBolded":true,"associatedRows":["BatchNorm + Mixup ( Zhang et al . , 2017 )"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["ResNet - 50"]},{"number":"0.1and","isBolded":true,"associatedRows":["Mixup coefficients are found through cross - validation : they are"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","( He et al . , 2016 ) , trained for 100 epochs and 200 epochs respectively .","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used ."],"associatedMergedColumns":["to the Fixup models using Mixup ( Zhang et al . , 2017 ) . We find it is beneficial to reduce the learning"]},{"number":"23.6","isBolded":false,"associatedRows":["BatchNorm ( Goyal et al . , 2017 )"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["with better regularization , the performance of Fixup is on par with GroupNorm ."]},{"number":"24.0","isBolded":true,"associatedRows":["Fixup - init + Mixup"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["ResNet - 50"]},{"number":"21.4","isBolded":false,"associatedRows":["Fixup - init + Mixup"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["ResNet - 101"]},{"number":"23.9","isBolded":false,"associatedRows":["GroupNorm + Mixup"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["with better regularization , the performance of Fixup is on par with GroupNorm ."]},{"number":"21.4","isBolded":false,"associatedRows":["GroupNorm + Mixup"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["ResNet - 50"]},{"number":"31.5","isBolded":false,"associatedRows":["Xavier Init ( Shang et al . , 2017 )"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["ResNet - 50"]},{"number":"23.3","isBolded":true,"associatedRows":["BatchNorm + Mixup ( Zhang et al . , 2017 )"],"associatedColumns":["On the ImageNet dataset , we benchmark Fixup with the ResNet - 50 and ResNet - 101 architectures","Similar to our finding on","Further inspection reveals that","rate of the scalar multiplier and bias by 10x when additional large regularization is used .","Test Error ( % )"],"associatedMergedColumns":["with better regularization , the performance of Fixup is on par with GroupNorm ."]}]},{"caption":"Table 4: Additional results on CIFAR-10, SVHN datasets. \n","rows":["( Graham , 2014 )","BatchNorm + Mixup + Cutout","( Yamada et al . , 2018 )","Yes","Fixup - init + Mixup + Cutout","( Lee et al . , 2016 )","CIFAR - 10","( Zagoruyko \u0026 Komodakis , 2016 )","SVHN","( DeVries \u0026 Taylor , 2017 )"],"columns":["Test Error ( % )"],"mergedAllColumns":["No"],"numberCells":[{"number":"3.5","isBolded":false,"associatedRows":["CIFAR - 10","( Graham , 2014 )","Yes"],"associatedColumns":["Test Error ( % )"],"associatedMergedColumns":[]},{"number":"2.3","isBolded":true,"associatedRows":["CIFAR - 10","Fixup - init + Mixup + Cutout","Yes"],"associatedColumns":["Test Error ( % )"],"associatedMergedColumns":[]},{"number":"1.4","isBolded":false,"associatedRows":["SVHN","BatchNorm + Mixup + Cutout","Yes"],"associatedColumns":["Test Error ( % )"],"associatedMergedColumns":["No"]},{"number":"1.5","isBolded":false,"associatedRows":["CIFAR - 10","( Zagoruyko \u0026 Komodakis , 2016 )","Yes"],"associatedColumns":["Test Error ( % )"],"associatedMergedColumns":["No"]},{"number":"1.4","isBolded":true,"associatedRows":["CIFAR - 10","Fixup - init + Mixup + Cutout","Yes"],"associatedColumns":["Test Error ( % )"],"associatedMergedColumns":["No"]},{"number":"2.5","isBolded":false,"associatedRows":["CIFAR - 10","BatchNorm + Mixup + Cutout","Yes"],"associatedColumns":["Test Error ( % )"],"associatedMergedColumns":[]},{"number":"3.8","isBolded":true,"associatedRows":["CIFAR - 10","( Zagoruyko \u0026 Komodakis , 2016 )","Yes"],"associatedColumns":["Test Error ( % )"],"associatedMergedColumns":[]},{"number":"2.3","isBolded":true,"associatedRows":["CIFAR - 10","( Yamada et al . , 2018 )","Yes"],"associatedColumns":["Test Error ( % )"],"associatedMergedColumns":[]},{"number":"1.3","isBolded":true,"associatedRows":["CIFAR - 10","( DeVries \u0026 Taylor , 2017 )","Yes"],"associatedColumns":["Test Error ( % )"],"associatedMergedColumns":["No"]},{"number":"1.7","isBolded":false,"associatedRows":["CIFAR - 10","( Lee et al . , 2016 )","Yes"],"associatedColumns":["Test Error ( % )"],"associatedMergedColumns":["No"]}]}]