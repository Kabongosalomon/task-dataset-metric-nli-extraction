[{"caption":"Table 1. Comparison between performance with and without attention connections \non Charades (mAP). The models were trained for 50K iterations. \n\n","rows":["Peer","Self","Static","None"],"columns":["without object","with object"],"mergedAllColumns":[],"numberCells":[{"number":"52.39","isBolded":false,"associatedRows":["Peer"],"associatedColumns":["without object"],"associatedMergedColumns":[]},{"number":"48.82","isBolded":false,"associatedRows":["Static"],"associatedColumns":["without object"],"associatedMergedColumns":[]},{"number":"51.91","isBolded":false,"associatedRows":["Self"],"associatedColumns":["without object"],"associatedMergedColumns":[]},{"number":"47.18","isBolded":false,"associatedRows":["None"],"associatedColumns":["without object"],"associatedMergedColumns":[]},{"number":"55.40","isBolded":false,"associatedRows":["Self"],"associatedColumns":["with object"],"associatedMergedColumns":[]},{"number":"50.43","isBolded":false,"associatedRows":["None"],"associatedColumns":["with object"],"associatedMergedColumns":[]},{"number":"51.15","isBolded":false,"associatedRows":["Static"],"associatedColumns":["with object"],"associatedMergedColumns":[]},{"number":"56.38","isBolded":true,"associatedRows":["Peer"],"associatedColumns":["with object"],"associatedMergedColumns":[]}]},{"caption":"Table 2. Classification performance on the Charades dataset (mAP). \n\n","rows":["CoViAR [ 52 ] ( Compressed )","MultiScale TRN [ 56 ] ( RGB )","STRG [ 49 ] ( RGB - only )","Asyn - TF [ 35 ]","UCF101","SGFB - 101 [ 16 ] ( RGB - only )","I3D from [ 48 ] ( RGB - only )","AssembleNet++ 50 ( ours ) without object","AssembleNet++ 50 ( ours )","SlowFast - 101 [ 9 ] ( RGB+RGB )","I3D + Non - local [ 48 ] ( RGB - only )","AssembleNet - 50 [ 34 ]","Two - stream [ 35 ]","LFB - 101 [ 51 ] ( RGB - only )","I3D [ 4 ] ( RGB - only )","MiT","Kinetics","Two - stream ( 2+1 ) D ResNet - 101","AssembleNet - 101 [ 34 ]","EvaNet [ 28 ] ( RGB - only )","None","ImageNet"],"columns":["Pre - training mAP"],"mergedAllColumns":[],"numberCells":[{"number":"42.5","isBolded":false,"associatedRows":["LFB - 101 [ 51 ] ( RGB - only )","Kinetics"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"44.3","isBolded":false,"associatedRows":["SGFB - 101 [ 16 ] ( RGB - only )","Kinetics"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"59.8","isBolded":true,"associatedRows":["AssembleNet++ 50 ( ours )","None"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"56.6","isBolded":false,"associatedRows":["AssembleNet - 50 [ 34 ]","Kinetics"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"35.5","isBolded":false,"associatedRows":["I3D from [ 48 ] ( RGB - only )","Kinetics"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"53.0","isBolded":false,"associatedRows":["AssembleNet - 50 [ 34 ]","MiT"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"58.6","isBolded":false,"associatedRows":["AssembleNet - 101 [ 34 ]","Kinetics"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"39.7","isBolded":false,"associatedRows":["STRG [ 49 ] ( RGB - only )","Kinetics"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"50.6","isBolded":false,"associatedRows":["Two - stream ( 2+1 ) D ResNet - 101","Kinetics"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"21.9","isBolded":false,"associatedRows":["CoViAR [ 52 ] ( Compressed )","ImageNet"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"47.2","isBolded":false,"associatedRows":["AssembleNet - 50 [ 34 ]","None"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"25.2","isBolded":false,"associatedRows":["MultiScale TRN [ 56 ] ( RGB )","ImageNet"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"37.5","isBolded":false,"associatedRows":["I3D + Non - local [ 48 ] ( RGB - only )","Kinetics"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"32.9","isBolded":false,"associatedRows":["I3D [ 4 ] ( RGB - only )","Kinetics"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"38.1","isBolded":false,"associatedRows":["EvaNet [ 28 ] ( RGB - only )","Kinetics"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"45.2","isBolded":false,"associatedRows":["SlowFast - 101 [ 9 ] ( RGB+RGB )","Kinetics"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"54.98","isBolded":false,"associatedRows":["AssembleNet++ 50 ( ours ) without object","None"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"22.4","isBolded":false,"associatedRows":["Asyn - TF [ 35 ]","UCF101"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]},{"number":"18.6","isBolded":false,"associatedRows":["Two - stream [ 35 ]","UCF101"],"associatedColumns":["Pre - training mAP"],"associatedMergedColumns":[]}]},{"caption":"Table 3. Performance on the Toyota Smarthome dataset. Classification % and mean \nper-class accuracy % are reported. Note that our models are being trained from scratch \nwithout any pre-training, while the previous work (e.g., [5]) relies on Kinetics pre-\ntraining. \n\n","rows":["Ours ( object + peer - attention )","I3D ( pre - trained ) + separable STA [ 5 ]","I3D ( pre - trained ) + NL [ 48 ]","I3D ( with Kinetics pre - training )","LSTM [ 25 ]","Baseline + self - attention","Ours ( object + self - attention )","-","Baseline AssembleNet - 50"],"columns":["mean per - class","Classification %"],"mergedAllColumns":[],"numberCells":[{"number":"79.08","isBolded":false,"associatedRows":["Ours ( object + self - attention )"],"associatedColumns":["Classification %"],"associatedMergedColumns":[]},{"number":"57.42","isBolded":false,"associatedRows":["Baseline AssembleNet - 50","-"],"associatedColumns":["mean per - class"],"associatedMergedColumns":[]},{"number":"53.4","isBolded":false,"associatedRows":["I3D ( with Kinetics pre - training )","-"],"associatedColumns":["mean per - class"],"associatedMergedColumns":[]},{"number":"63.64","isBolded":true,"associatedRows":["Ours ( object + peer - attention )","-"],"associatedColumns":["mean per - class"],"associatedMergedColumns":[]},{"number":"53.6","isBolded":false,"associatedRows":["I3D ( pre - trained ) + NL [ 48 ]","-"],"associatedColumns":["mean per - class"],"associatedMergedColumns":[]},{"number":"57.84","isBolded":false,"associatedRows":["Baseline + self - attention","-"],"associatedColumns":["mean per - class"],"associatedMergedColumns":[]},{"number":"54.2","isBolded":false,"associatedRows":["I3D ( pre - trained ) + separable STA [ 5 ]","-"],"associatedColumns":["mean per - class"],"associatedMergedColumns":[]},{"number":"42.5","isBolded":false,"associatedRows":["LSTM [ 25 ]","-"],"associatedColumns":["mean per - class"],"associatedMergedColumns":[]},{"number":"80.64","isBolded":true,"associatedRows":["Ours ( object + peer - attention )"],"associatedColumns":["Classification %"],"associatedMergedColumns":[]},{"number":"72.0","isBolded":false,"associatedRows":["I3D ( with Kinetics pre - training )"],"associatedColumns":["Classification %"],"associatedMergedColumns":[]},{"number":"75.3","isBolded":false,"associatedRows":["I3D ( pre - trained ) + separable STA [ 5 ]"],"associatedColumns":["Classification %"],"associatedMergedColumns":[]},{"number":"62.30","isBolded":false,"associatedRows":["Ours ( object + self - attention )","-"],"associatedColumns":["mean per - class"],"associatedMergedColumns":[]},{"number":"77.59","isBolded":false,"associatedRows":["Baseline + self - attention"],"associatedColumns":["Classification %"],"associatedMergedColumns":[]},{"number":"77.77","isBolded":false,"associatedRows":["Baseline AssembleNet - 50"],"associatedColumns":["Classification %"],"associatedMergedColumns":[]}]},{"caption":"Method \nClassification % mean per-class \nLSTM [25] \n-\n42.5 \nI3D (with Kinetics pre-training) \n72.0 \n53.4 \nI3D (pre-trained) + NL [48] \n-\n53.6 \nI3D (pre-trained) + separable STA [5] \n75.3 \n54.2 \nBaseline AssembleNet-50 \n77.77 \n57.42 \nBaseline + self-attention \n77.59 \n57.84 \nOurs (object + self-attention) \n79.08 \n62.30 \nOurs (object + peer-attention) \n80.64 \n63.64 \n\nTable 4. Comparing AssembleNet++ using peer-attention vs. a modification using 1x1 \nconvolutional layer instead of attention. They use an identical number of parameters. \nCharades classification accuracy (mAP) and Toyota mean per-class accuracy (%) are \nreported. \n\n","rows":["Random peer - attention","Our peer - attention","Base","Base + 1x1 conv ."],"columns":["Toyota","Charades"],"mergedAllColumns":[],"numberCells":[{"number":"53.40","isBolded":false,"associatedRows":["Random peer - attention"],"associatedColumns":["Charades"],"associatedMergedColumns":[]},{"number":"50.43","isBolded":false,"associatedRows":["Base"],"associatedColumns":["Charades"],"associatedMergedColumns":[]},{"number":"59.44","isBolded":false,"associatedRows":["Base + 1x1 conv ."],"associatedColumns":["Toyota"],"associatedMergedColumns":[]},{"number":"60.23","isBolded":false,"associatedRows":["Random peer - attention"],"associatedColumns":["Toyota"],"associatedMergedColumns":[]},{"number":"50.24","isBolded":false,"associatedRows":["Base + 1x1 conv ."],"associatedColumns":["Charades"],"associatedMergedColumns":[]},{"number":"63.64","isBolded":true,"associatedRows":["Our peer - attention"],"associatedColumns":["Toyota"],"associatedMergedColumns":[]},{"number":"56.38","isBolded":true,"associatedRows":["Our peer - attention"],"associatedColumns":["Charades"],"associatedMergedColumns":[]},{"number":"59.16","isBolded":false,"associatedRows":["Base"],"associatedColumns":["Toyota"],"associatedMergedColumns":[]}]},{"caption":"Table 5. Comparison between original CNN models (without object modality and \nwithout attention) and their modifications based on our attention connectivity and \nobject modality. The value corresponding to \u0027AssembleNet++\u0027 for the column \u0027base\u0027 is \nobtained by manually removing connections from the object input block and removing \nattention from our final one-shot attention search model. Measured with Charades \nclassification (mAP, higher is better), trained from scratch for 50k iterations. \n\n","rows":["AssembleNet++","RGB R ( 2+1 ) D","AssembleNet","Two - stream R ( 2+1 ) D"],"columns":["+ object + attention","base"],"mergedAllColumns":[],"numberCells":[{"number":"45.30","isBolded":true,"associatedRows":["RGB R ( 2+1 ) D"],"associatedColumns":["+ object + attention"],"associatedMergedColumns":[]},{"number":"47.74","isBolded":true,"associatedRows":["Two - stream R ( 2+1 ) D"],"associatedColumns":["+ object + attention"],"associatedMergedColumns":[]},{"number":"53.48","isBolded":true,"associatedRows":["AssembleNet"],"associatedColumns":["+ object + attention"],"associatedMergedColumns":[]},{"number":"56.38","isBolded":true,"associatedRows":["AssembleNet++"],"associatedColumns":["+ object + attention"],"associatedMergedColumns":[]},{"number":"36.51","isBolded":false,"associatedRows":["RGB R ( 2+1 ) D"],"associatedColumns":["base"],"associatedMergedColumns":[]},{"number":"39.93","isBolded":false,"associatedRows":["Two - stream R ( 2+1 ) D"],"associatedColumns":["base"],"associatedMergedColumns":[]},{"number":"47.18","isBolded":false,"associatedRows":["AssembleNet"],"associatedColumns":["base"],"associatedMergedColumns":[]},{"number":"47.62","isBolded":false,"associatedRows":["AssembleNet++"],"associatedColumns":["base"],"associatedMergedColumns":[]}]}]